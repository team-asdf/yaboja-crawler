title,content,url,cnt,source,keyword,image,createdAt
Cross shard transactions at 10 million requests per second,"Dropbox stores petabytes of metadata to support user-facing features and to power our production infrastructure. The primary system we use to store this metadata is named Edgestore and is described in a previous blog post, (Re)Introducing Edgestore. In simple terms, Edgestore is a service and abstraction over thousands of MySQL nodes that provides users with strongly consistent, transactional reads and writes at low latency.Edgestore hides details of physical sharding from the application layer to allow developers to scale out their metadata storage needs without thinking about complexities of data placement and distribution. Central to building a distributed database on top of individual MySQL shards in Edgestore is the ability to collocate related data items together on the same shard. Developers express logical collocation of data via the concept of a colo, indicating that two pieces of data are typically accessed together. In turn, Edgestore provides low-latency, transactional guarantees for reads and writes within a given colo (by placing them on the same physical MySQL shard), but only best-effort support across colos.While the product use-cases at Dropbox are usually a good fit for collocation, over time we found that certain ones just aren’t easily partitionable. As a simple example, an association between a user and the content they share with another user is unlikely to be collocated, since the users likely live on different shards. Even if we were to attempt to reorganize physical storage such that related colos land on the same physical shards, we would never get a perfect cut of data.For data that was not easily collocatable, developers were forced to implement application-level primitives to mask over a lack of cross-shard transactionality, slowing down application development and incurring an unnecessary technical burden. This blog post focuses on our recent deployment of cross shard transactions, which addressed this deficiency in Edgestore’s API, allowing atomic transactions across colos. What follows is a description of our design, potential pitfalls one may encounter along the way, and how we safely validated and deployed this new feature to a live application serving more than ten million requests per second.The standard protocol for executing a transaction across database shards is two-phase commit, which has existed since at least the 1970s. We applied a modified version of this protocol to support cross-shard transactions in Edgestore.Two-phase commit requires a leader to execute a transaction across multiple participants, in our case Edgestore shards. The protocol works as follows:Phase 0: Transaction RecordThe addition of an external transaction record to serve as a source of truth about the state of a transaction is the main way in which our protocol differs from standard two-phase commit. This phase isn’t required in the standard protocol but provides both performance and correctness benefits in our use-case, which we’ll describe below.After writing the transaction record, the protocol follows its traditional path:Phase 1: Commit StagingAt this point the participants have indicated that they are willing to commit the transaction, but they don’t yet know if the leader has decided to commit it or not. To ensure correctness they must ensure that no other concurrent transactions can observe state that conflicts with the ultimate commit or abort decision made by the leader; we’ll refer to this below as filtering.Phase 2a: Transaction DecisionLike Phase 0, Phase 2a is not strictly required by the two-phase commit protocol but makes the commit point explicit. This means that even if all participants agree to the transaction and stage the mutations, the transaction is not considered committed until the transaction record is updated to reflect that. In consequence, any parallel operation can abort a transaction at any time up until the transaction record leaves its pending state, which makes recovery in the case of leader failure straightforward (just abort and move on, instead of contacting all participants and entering a recovery procedure) and improves the liveness of the system under error conditions. It also makes filtering straightforward—a concurrent read or write need only contact the node containing the transaction record to determine transaction state, which improves performance for latency-sensitive clients.The protocol completes with one final step:Phase 2b: Commit ApplicationTwo-phase commit is a relatively simple protocol in theory, but unfortunately there are a lot of practical barriers to implementing it. One key problem is read and write amplification: an increase in the number of reads and writes in the protocol path. Write amplification is inherent in the fact that not only do you need to write a transaction record, but also you need to durably stage a commit, which incurs at least one additional write per participant. The extra writes increase the critical section of the transaction, which can cause lock contention and application instability. Moreover, on every read, the database also needs to perform filtering to ensure that the read doesn’t observe any state that is dependent on a pending cross-shard transaction, which affects all reads in the system, even non-transactional ones.Therefore, in order to translate two-phase commit to Edgestore, we needed a design that answered three questions:As mentioned in the previous section introducing two-phase commit, we found that modifying the two-phase commit protocol to utilize an external transaction record minimized the performance cost of determining transaction state, since a concurrent request need only check the transaction record to determine the state of the transaction as opposed to contacting each of the participants. By extension, this limits the worst-case filtering penalty for reads, since they will need to contact at most two nodes—the local participant and the (possibly local) node storing the transaction record. We implemented the transaction record as an Edgestore object, which gave us all the original strong consistency guarantees of single-colo operations and allowed us to collocate the transaction record with one of the cross-shard transaction participants.A typical approach to implementing commit staging would be to store a fully materialized copy of the staged data alongside the existing data as in a multiversioned storage system. However, Edgestore’s underlying MySQL schema doesn’t support storing duplicates of a given object. Therefore, an approach where we write a full copy of an object to a secondary MySQL table to stage a commit and copy it into the primary data table to apply it would have the effect of doubling the cost (and latency) of every transaction.Instead, we chose to implement commit staging and application using copy-on-write. In order to stage a commit, we commit the raw mutations in a separate MySQL table, and then only on commit application do we materialize the transaction and apply the state to the primary data tables. By taking this approach, we reduced our write amplification by up to 95% over a naive multiversioning approach.Additionally, our copy-on-write implementation provided significant performance benefits for readers. In steady state, any given record is not likely to have an in-flight mutation, and the size of the secondary MySQL table should be roughly proportional to the rate of cross-shard transactions. This means that the typical filtering cost to a reader is simply the cost of a local existence check against a table that is small enough to fit in memory, after which point it can serve the read from the same node. If there is a staged commit, then it must either wait for or assist the application of the in-flight transaction, which involves contacting at most one other node.Of course, it’s one thing to have a design for a system you believe is correct but entirely another to prove it. Since Edgestore has been running for years and backs almost every request to Dropbox, the design itself had to accommodate validating our assumptions about consistency, correctness, and performance while providing a path for safe rollout.This was the fun part 😛The whole purpose of the project was to offer an Edgestore API with enhanced consistency. Therefore, we needed to prove to ourselves beyond a doubt that our implementation upheld the newly stated guarantees. To do this, we built an offline test harness that we ran continuously in our build environment (amounting to multiple years of CPU time) to corroborate our assertions: cross shard transactions in Edgestore are atomic and strictly serializable.The basic theory behind the test harness is that given a full history of all mutations, you can construct a precedence graph describing ordering relationships among the transactions responsible for those mutations. You construct a precedence graph by writing a node for each transaction, and then connecting it to another transaction with a directed edge if it performs an operation on shared data before that other transaction. The topological ordering defined by this graph describes the valid serial orderings of transactions, and therefore the graph should be acyclic. If it has a cycle, you have violated serializability.Taking the concept one step further, if you replace the transaction nodes with nodes representing each participant of the transaction, you can use the resulting graph to prove atomicity. Roughly, if the mutations applied to each participant also embed information about other participants (directed edges radiating outward from a node representing the participant), the subgraph formed by connecting each participant’s view of the mutation set should form a complete, directed graph (each complete subgraph can be identified with a transaction node in the precedence graph). If there are missing or dangling edges, you have violated atomicity.To build our harness, we created special Edgestore objects with two fields: one for the transaction identifier (which we filled by introspecting our RPC headers) to provide the directed edges for our precedence graph, and the other for the expected participant list. We then wrote a client that issues transactions on random subsets of the objects, updating the two fields as appropriate. We enabled application-level multiversioning so we could later recover the full mutation history, and we added random fault injection to increase coverage of error paths. (A careful reader will observe that we are missing the strict portion of strict serializability guarantee—we added some read-only threads with extra logic to handle this).In the end, the harness surfaced multiple bugs, one as basic as a dropped error, and another as subtle as a flaw in the read filtering protocol. These bugs would have been nearly impossible to detect, let alone root-cause, had they occurred outside of a simulated environment. That may seem like a small payoff for multiple CPU-years of testing, but for us and our users, it was worth every minute.In parallel with consistency verification, we also set out to validate the two other pieces of our design: the correctness of our copy-on-write implementation, and the performance cost of our two-phase commit design when inlined into the existing system. The first was a potential risk because if the materialization logic was wrong or the stored mutation did not fully encode user-facing write APIs, we could end up in a state where commit application would not know how to properly apply the transaction. The second was critical for the continued reliability of Edgestore—any drastic change in latency, traffic, or locking could cause system instability and break upstream applications.During the design phase for cross-shard transactions, we realized we could achieve both of these goals by introducing a modified two-phase commit protocol. In the “validation” version of the protocol, the commit staging phase would perform the shadow write, and then immediately delete the “staged” commit as part of the transaction generated by the existing Edgestore API, all without communicating back to the leader. In turn, a truncated commit application phase would no-op without attempting to recreate the transaction, since the data had already been written and the commit unstaged. Filtering, similarly, would always check for shadow transactions but ignore the result, since the transaction would have already been committed.The modified protocol preserved the existing API and was resilient to a bug in the shadow representation, since the original API writes remained the source of truth instead of a transaction reconstructed from a potentially incorrect shadow. Moreover, while the transaction semantics were unchanged from the perspective of Edgestore clients, Edgestore itself operated under the expected extra coordination and load from a two-phase commit. We thus validated our performance assumptions with very little risk, since we could increase the amount of traffic doing this pseudo-two-phase commit in a controlled manner.The “validation” two-phase commit also provided a straightforward way of corroborating the correctness of the shadow representation. Since the modified commit staging wrote a MySQL transaction with both the original API result and the shadow representation, we could extract the two atomically from the MySQL binary log and try converting between the two. If our conversion logic produced a different result from the shadow than the actually committed data, we would know there was a bug. This enabled us to identify a few features of our existing API that would have been incompatible with cross shard transactions and allowed us to proactively address the gaps before going live with the new protocol.Although two-phase commit was a fairly natural fit for Edgestore’s existing workload, it is not a silver bullet for those looking to improve their consistency guarantees. Edgestore data was already well-collocated, which meant that cross-shard transactions ended up being fairly rare in practice—only 5-10% of Edgestore transactions involve multiple shards. Had this not been the case, upstream applications might not have been able to handle the increased latency and lock contention that comes with two-phase commit. Moreover, in many cases cross shard transactions replaced more expensive, application-level protocols, which meant the change was a net win for performance in addition to simplifying developer logic.Two-phase commit in Edgestore also benefits from a strongly-consistent caching layer that absorbs upwards of 95% of client reads. This significantly cuts down on the read filtering penalty, which might have been otherwise untenable. Systems without an auxiliary cache or those optimized for simpler write patterns might similarly find two-phase commit unwieldy and prefer to opt for storage-level multiversioning or consistency abstractions provided as intermediary service layers between clients and the storage engine. This is a direction we are exploring for our next-generation metadata storage—combining a simple, key-value storage primitive and then building a suite of metadata services on top with varying levels of consistency and developer control. Stay tuned for updates.For a strongly consistent, distributed metadata store such as Edgestore—serving 10 million requests per second and storing multiple petabytes of metadata—writes spanning multiple physical storage nodes are an inevitability. Although our initial “best-effort” approach to multi-shard writes worked well for most use cases, over time the balance of complexity shifted too heavily on developers. Therefore, we decided to tackle the problem of building a scalable primitive for multi-shard writes and implemented cross-shard transactions.Although the basic protocol underlying our implementation has been known for a long time, actually retrofitting it into an existing system presented many challenges and required a creative approach to both implementation and validation. In the end, the up-front diligence paid dividends and enabled us to make a fundamental change to an existing system while maintaining our standards of trust and the safety of our users’ data. Think you’re up to the task? We’re hiring!Thanks to: Tanay Lathia, Robert Escriva, Mihnea Giurgea, Bashar Al-Rawi, Bogdan Munteanu, Mehant Baid, Zviad Metreveli, Aaron Staley, and James Cowling.",https://blogs.dropbox.com/tech/2018/11/cross-shard-transactions-at-10-million-requests-per-second/,0,dropbox,"json,docker,python,javascript,php,java,spring,backend",NULL,2018-11-10
Crash reporting in desktop Python applications,"One of the greatest challenges associated with maintaining a complex desktop application like Dropbox is that with hundreds of millions of installs, even the smallest bugs can end up affecting a very large number of users. Bugs inevitably will strike, and while most of them allow the application to recover, some cause the application to terminate. These terminations, or “crashes,” are highly disruptive events: when Dropbox stops, synchronization stops. To ensure uninterrupted sync for our users we automatically detect and report all crashes and take steps to restart our application when they occur.In 2016, faced with our impending transition to Python 3, we set out to revamp how we detect and report crashes. Today, our crash reporting pipeline is a reliable cornerstone for our desktop teams, both in volume and quality of reports. In this post, we’ll dive into how we designed this new system.Dropbox is partly written in Python, and while it certainly is a safe, high-level language, it is not immune to crashes. Most crashes (i.e. unhandled exceptions) are simple to deal with because they occur in Python, but many originate “below”: in non-Python code, within the interpreter code itself, or within Python extensions. These “native” crashes, as we refer to them, are nothing new: improper memory manipulation, for example, has plagued developers for decades.As our application grew more complex, we began to rely on other programming languages to build some of our features. This was particularly true when integrating with the operating system, where the easiest path tends to lead to platform-specific tooling and languages (e.g. COM on Windows and Objective-C on macOS). This has led to an increased share of non-Python code in our codebase, which has brought along an increased risk for dangling pointers, memory errors, data races, and unchecked array accesses: all of which can cause Dropbox to be unceremoniously terminated. As a result, a single crash report can now contain Python, C++, Objective-C, and C code in its stack trace!For several years, we relied on a simple in-process crash detection mechanism: a signal handler. This scheme allowed us to “trap” various UNIX signals (and on Windows, their analogues). Upon hitting a fatal signal (i.e. SIGFPE), our signal handler would attempt to:We would then attempt to securely upload this data to Dropbox’s servers.While this was adequate, a few fundamental issues affected reliability or limited its usefulness in debugging:One of the root causes of this is the nature of signal handling itself: while, thankfully, Python’s signal module takes care of most these, it also adds its own restrictions. For example, signals can only be called from the main thread and may not be run synchronously. This asynchronicity meant that some of the most common SIGSEGVs could often fail to be trapped from Python!1A more reliable crash reporting mechanism can be built by extracting the reporter outside of the main process. This is readily feasible, as both Windows and MacOS provide system facilities to trap out-of-process crashes. The Chromium project has developed a comprehensive crash capture/report solution that leverages this functionality and that can be used as a standalone library: Crashpad.Crashpad is deployed as a small helper process that monitors your application, waits for a signal that it has crashed, and then captures useful information, including:All of this is captured in a minidump payload, a Microsoft-authored format originally used on Windows and somewhat similar to a Unix-style core dump. The format is openly documented, and there exists excellent server-side tooling (again, mainly from Google and Mozilla) to process such data.The following diagram outlines Crashpad’s basic architecture: An application uses Crashpad by instantiating an in-process object—called the “client”—that reports to an out-of-process helper—called a “handler”—when it detects a crash.We decided to use this library to mitigate many of the reliability issues associated with an in-process signal handler. This was an easy choice due to its use by Chromium, one of the most popular desktop applications ever released. We were also enthused by more sophisticated support for Windows, a rather different platform from UNIX. faulthandler was (at the time) limited in its support for Windows-specific crashes, since it was very much based on signals, a UNIX/POSIX concept. Crashpad leverages Structured Exception Handling (or SEH), allowing it to catch a much broader range of fatal Windows-specific exceptions.A note on Linux: Though Linux support has been very recently introduced, Crashpad was only available for Windows and MacOS when we first deployed it, so we limited our use of the library to these platforms. On Linux, we continue to use the in-process signal handler, though we will re-visit this in the future.Dropbox, like most compiled applications, ships to users in a “Release” configuration, where several compiler optimizations are enabled and symbols are stripped to reduce binary size. This means the information gathered is mostly useless unless it can be “mapped” back to source code. This is referred to as “symbolication”.To achieve this, we preserve symbols for each Dropbox build on internal servers. This is a core part of our build process: symbol generation failure is considered a build failure, making it impossible for us to release a build that cannot later be symbolicated.When a minidump is received as part of a crash report, we use the symbols for the build to decipher each stack trace and link it back to source code. When system libraries are used, we defer to platform-specific symbols. This process allows our developers to quickly find where crashes originate in either first or third-party code.Microsoft maintains public symbol servers for all Windows builds in order for stack frames involving their functions to be mapped. Unfortunately, Apple does not have a similar system: instead, the platform’s frameworks include their matching symbols. To support this, we currently cache the symbols of various macOS frameworks (for a wide range of OS versions) using our testing VMs (though we can still occasionally end up with gaps in our coverage).Changing our crash reporting infrastructure from underneath millions of installations was a risky endeavor: we required validation that our new mechanism was working. It’s also important to note that not all terminations are necessarily crashes (e.g. the user closing the app or an automatic update). That being said, some terminations may still indicate problems. We therefore wanted a way to record and classify exits along with crashes. This also would provide us with a baseline to validate that our new crash reporter was capturing a high fraction of total crashes.To address this, we built yet another “sidecar” process we named “watchdog.” This is another small “companion” process (similar to Crashpad) that has a single responsibility: when the desktop app exits, it captures its exit status to determine whether it was “successful” (that is, a user or app-initiated shutdown instead of being forcibly terminated). This process is extremely simple by intention as we want it to be highly reliable.To provide a basis for comparison, a start event is generated by making our application send an event on launch. With both start and exit events, we are then able to measure the accuracy of exit monitoring itself: we can ensure it was successful for a very high percentage of our users (note that firewalls, corporate policies, and other programs prevent this from working 100% of the time). In addition, we can now match this exit event against crashes coming from Crashpad to make sure that exit codes in which we expect crashes indeed include crash reports from most users. The graphs below show the monitoring we have in place:We wrote the watchdog process in Rust, which we chose for a variety of reasons:Crashpad was primarily designed for native code, as Chromium is mostly written in C++. However, the Dropbox client is mostly written in Python. As Python is an interpreted language, most native crash reports we receive thus tend to look like this:This stack trace is not very helpful to a developer trying to discover the cause of a crash. Whereas faulthandler also included the Python stack frames of all threads, Crashpad does not have this ability by default. To make this report useful, we would need to include the relevant Python state. However, as Crashpad is not written in Python and is out-of-process, we don’t have access to faulthandler itself: how might we go about doing this?When the crashing program is suspended, all of its memory is available to Crashpad, which can read it to capture the program state. As the program is potentially in a bad state, we can’t execute any code within it. Instead we need to:We chose Crashpad in part for its customizability: it is fairly easy to extend. We therefore added code to the ProcessSnapshot class to capture Python stacks, and introduced our own custom minidump “stream” (supported by both the file format and Crashpad itself) to persist and report this information.First, we needed to know where to look. In CPython, interpreter threads are always backed by native threads. Therefore, in the Dropbox application, each native thread created by Python has an associated PyThreadState structure. The interpreter uses native thread-specific storage to create the connection between this object and the native thread. As Crashpad has access to the monitored process’ memory, it can read this state and include it as part of a report.As Dropbox ships a customized fork of CPython, we have effective control over its behavior. This means that not only can we use this knowledge to our advantage but we can rely on it knowing it won’t easily change from under us.In Python, thread-specific storage is implemented in platform-specific ways:Common to all platforms, however, is that the Python-specific state is stored at a specific offset of the native thread state. Sadly, this offset is not static: it can change depending on various factors. This offset is determined early in the Python runtime’s setup (see PyInitialize): this is referred to as the thread-specific storage “key”. This step creates a single “slot” of thread-specific storage for all threads in the process, which is then used by Python to store its thread-specific state.So if crashpad can retrieve the TSS “key” for the instance of the process, it will have the ability to read the PyThreadState for any given thread.We considered multiple ways of doing this, but settled on a method inspired by Crashpad itself. In the end, we modified our fork of Python to expose the runtime state (including the TSS key) in a named section of the binary (i.e. __DATA). Thus, all instances of Dropbox would now expose the Python runtime state in a way that makes it easy to retrieve it from Crashpad.Now that Crashpad can determine the TSS key, it has access to each thread’s PyThreadState. The next step is to interpret this state, extract the relevant information, and send it as part of a crash report.In CPython, “frames” are the unit of function execution, and the Python analogue to native stack frames. The PyThreadState maintains them as a stack of PyFrameObjects. The topmost frame at any given time is pointed to by the thread state using a single pointer. Given this setup and the TSS key, we can start from a native thread, find the PyThreadState, then “walk the stack” of PyFrameObjects.However, this is trickier than it sounds. We can’t just #include <Python.h> and call the same functions faulthandler does: as Crashpad’s handler runs in a separate process, it doesn’t have direct access to this state. Instead, we had to use Crashpad’s utilities to reach into the crashing process’s memory and maintain our own “copies” of the relevant Python structs to interpret the raw data. This is a necessarily brittle solution, but we’ve mitigated the cost of ongoing maintenance by introducing automated tests that ensure that any updates to Python’s core structs to also require an update our Crashpad fork.For every frame, our objective is to resolve it to a code location. Each PyFrameObject has a pointer to a PyCodeObject including information about the function name, file name, and line number (faulthandler leverages the same information).The filename and function name are maintained as Python strings. Decoding Python strings can be fairly involved, as they are built on a hierarchy of types (we’ll spare you the details, but see unicodeobject.h). For simplicity, we assume all function and file names are ASCII-encoded (mapping to the simple PyASCIIObject).Getting the line number is slightly more complicated. To save space, while being able to map every byte code instruction to Python source, Python compresses line numbers into a table (PyCodeObject‘s co_lnotab). The algorithm to decode this table is well-defined, so we re-implemented it in our Crashpad fork.A note on the Python 3 transition: As Python 2 and 3 have slightly different implementations, we maintained support for both versions of the Python structs in our Crashpad fork during the transition.Now that Crashpad’s reports include all the Python stack frames, we can improve symbolication. To do so, we modified our server infrastructure to parse our extensions to minidumps and extract these stacks. Specifically, we augmented our crash management system, Crashdash, to display Python stack frame information (if it is available) for native crash reports.This is achieved by “walking the stack” again, but this time, for each native frame calling PyEval_EvalFrameEx, we “pop” the matching PyFrameObject capture from the report. Since we now have the function name, file name, and line number for each of those frames we can now show the matching function calls. We can thus extract the underlying Python stack trace from the one above:With this system in place, our developers are capable of directly investigating all crashes, whether they occur in Python, C, C++, or Objective-C. In addition, the new monitoring we introduced to measure the system’s reliability has given us added confidence our application is performing as it should. The result is a more stable application for our desktop users. Case in point: using this new system, we were able to perform the Python 2 to 3 transition without fear that our users would be negatively affected.Nikhil gave a talk at PyGotham 2018 that dives into Python stack frame implementation details and explains our strategy for using Crashpad. The slides are available now (videos will be up soon).Interested? If the type of problem solving we described sounds fun and you want to take on the challenges of desktop Python development at scale, consider joining us!Footnotes: 1 A SIGSEGV cannot be handled asynchronously due to how signals are implemented. When a CPU instruction attempts to access an invalid location, it triggers a page fault. This is handled by the OS. The OS will first rewind the instruction pointer to go back to the beginning of this instruction, since it needs to resume execution of the program once the signal handler returns. The first time, the signal handler is triggered. Once it returns, execution resumes. The OS tracks that a handler was already invoked. When the fault is triggered again, this specific signal is masked so that no handler runs. At this point, the behavior for a SIGSEGV is to core dump the process and abort it. Our asynchronous handler effectively never runs. The faulthandler module specifically supports synchronous capture of Python stacks, but it can only save these to a file. No network activity or Python execution is allowed due to signal safety requirements. These various complications, and the clear benefits of Crashpad in other areas, made it compelling to switch.",https://blogs.dropbox.com/tech/2018/11/crash-reporting-in-desktop-python-applications/,0,dropbox,"frontend,python3,python,java,css",NULL,2018-11-05
What we learned at our first JS Guild Summit," At Dropbox, we work to keep teams flowing—so earlier this month, we convened a group of our frontend engineers to do just that. At the beginning of October, we held the first JS (JavaScript) Guild Summit at our San Francisco headquarters to bring together frontend engineers from our four engineering offices for two days of teaching, learning, and collaboration.The JS Guild is a grassroots initiative at Dropbox to improve our frontend engineering by fostering community, culture, and code quality. The group strives to teach frontend best practices to generalists and to help strong frontend engineers leverage and grow their domain knowledge.Over the past year, JS Guild members have championed several foundational improvements, including a large-scale migration of frontend code from Underscore to Lodash and early work for a migration from Flux to Redux. The JS Guild also circulates a biweekly newsletter, which is written by a rotating group of engineers and showcases frontend best practices, recently shipped projects, and news from the frontend world outside of Dropbox.At the beginning of the summer, the JS Guild identified a unique challenge and opportunity: how to leverage the technical contributions from our engineers in San Francisco, Seattle, New York, and Tel Aviv so that all Dropbox engineers could benefit from the breadth of frontend code shipped in each of our offices.The JS Guild Summit represented a dedicated effort to break down silos between teams and to facilitate the free flow of ideas between engineers. In the planning of the Summit, Matthew Gerstman, one of the lead JS Guild Summit organizers, thought about “breaking down silos” as “establishing many-to-many connections across engineers so that people can collaborate and communicate as powerfully and efficiently as possible.”At the beginning of October, 115 Dropbox frontend engineers participated in two days of talks, workshops, open-ended discussions, and social events—all intended to promote knowledge transfer, to celebrate the diversity of frontend work taking place in our four engineering offices, and to identify opportunities for increased collaboration.At the JS Guild Summit, frontend engineers shared knowledge and broke down silos by giving talks on topics like:We aimed to make the Summit inclusive, so we accepted talk proposals from engineers with a wide range of technical and public speaking experience. While we held a well-attended talk by VP Design Nicholas Jitkoff, several of our presenters had the opportunity at the Summit to deliver their first-ever technical talks and to gather feedback from a large group of engineers tackling similar technical challenges.At the Summit, we also had an opportunity to identify areas for technical and foundational improvement by holding an “unconference,” a series of participant-driven discussions on topics seeded by conference attendees. Over 50 engineers contributed to lively and productive discussions on topics like:In the spirit of sharing insights from our four engineering offices, David Goldstein, a software engineer on the Web Platform team in San Francisco, set out to demystify hot reloading (a utility that’s essential to many frontend workflows) and how his team tailored our implementation to Dropbox code by making it compatible with RequireJS.Imagine you’re rapidly iterating on a web component and making independent changes in rapid succession. How do you maintain your focus and flow while rapidly modifying the contents of local TypeScript and CSS files?That’s where hot reloading comes in. Hot reloading is a mechanism that allows the browser to incorporate code changes into a page without a full page reload. It supercharges frontend workflows because:Given our ongoing migration from Flux to Redux, David emphasized that the feasibility of hot reloading is tightly coupled with frontend architecture itself and gave an overview of which pieces of code are hot reloadable in a React/Redux app. With his talk, David aimed to share what happens under the hood of a hot reloading tool and thus to help other engineers better understand the critical pieces of their everyday workflows.Assaf Kamil, a software engineer from the Tel Aviv office who works on tools and interfaces for Dropbox Business admins, hosted a workshop on Rondo, a framework centered around tightly integrating reusable business behaviors and business flow with Redux. At its core, Rondo provides an interface for encapsulating the actions, state management, and side effects required to support a business flow. In his workshop, Assaf demonstrated how Rondo has accelerated his team’s efforts to build interfaces that help our business customers keep their flow and provided practical training to help other engineers bring Rondo to their teams and offices.Many timezones away from Tel Aviv in the Seattle office, Staff Software Engineer Josh Zana was an early contributor to our ongoing Redux migration. The migration from Flux to Redux originated last year in a Web Enhancement Proposal (WEP), an internal process that we use to propose foundational changes and to align on a universal approach to implementing such changes. The authors behind the Redux WEP envisioned that migrating from Flux to Redux would provide an opportunity to build a standard “repository” for UI data shared across multiple components, to reduce technical debt, and to improve test coverage.At a high level, migrating a store from Flux to Redux involves removing state that doesn’t belong in a given store and adding the Redux constructs of actions, selectors, and reducers. This might include porting derived state to Redux selectors (optionally memoized), migrating canonical state to reducers, or converting asynchronous Flux actions to Redux Thunks or Sagas.In his talk at the Summit, Josh aimed to educate other engineers about how they can safely migrate their teams’s legacy Flux stores to Redux. Over the last few months, Josh and his team have migrated about 80% of their legacy Flux logic to Redux and, along the way, distilled a set of important learnings about undertaking such a migration:At the Summit, we focused both on frontend technologies (hot reloading, Redux, Rondo, and more) and on product development methodologies. Representing the New York office, software engineer and tech lead Mike Lyons gave a talk on how rapid prototyping and iteration fueled his team’s efforts to ship Dropbox Showcase, a visually compelling sharing tool for professionals and enterprise customers. While his team had to make tradeoffs to get a product to market faster, the rapid prototyping approach reduced the cost and frequency of missteps and anchored their iteration to real-world use and feedback—putting the customer at the center of their work.Since the JS Guild Summit, numerous frontend engineers have remarked that the opportunity to meet in person with engineers from our other offices allowed them to “understand the human behind the [Slack] avatar” and collaborate more effectively. One attendee recently noted that “my biggest takeaways during the Summit came through hearing other [engineers] present problems that are similar to the ones my team faces. It… gave a lot of insight into how my team can implement our own solutions.”At the JS Guild Summit, we shared knowledge, broke down silos between engineers, teams, and offices, and invested in our frontend engineering community.Want to collaborate with Dropbox’s growing community of frontend engineers? We’re hiring for roles in our four engineering offices. Join us!",https://blogs.dropbox.com/tech/2018/10/what-we-learned-at-our-first-js-guild-summit/,0,dropbox,"database,docker,python,frontend,java,angular,react,backend,mongodb",NULL,2018-10-31
Dropbox traffic infrastructure: Edge network,"In this post we will describe the Edge network part of Dropbox traffic infrastructure. This is an extended transcript of our NginxConf 2018 presentation. Around the same time last year we described low-level aspects of our infra in the Optimizing web servers for high throughput and low latency post. This time we’ll cover higher-level things like our points of presence around the world, GSLB, RUM DNS, L4 loadbalancers, nginx setup and its dynamic configuration, and a bit of gRPC proxying.Dropbox has more than half a billion registered users who trust us with over an exabyte of data and petabytes of corresponding metadata. For the Traffic team this means millions of HTTP requests and terabits of traffic. To support all of that we’ve built an extensive network of points of presence (PoPs) around the world that we call Edge.Many of the readers know the basic idea of CDNs: terminating TCP and TLS connections closer to the user leads to improved user experience because of greatly reduced latencies. Here is a quick reminder: a comparison of an HTTP request latency made directly to a data center and the same request made through a PoP:Numbers are given based on: 20ms PoP↔user latency, 150ms PoP↔DC latency, 100ms server execution timeAs you can see, by simply putting the PoP close to the user one can improve latency by more than factor of two.But that is not all. Our users benefit greatly from faster file uploads and downloads. Let’s see how latency affects TCP congestion window (CWND) growth during file uploads:Here we can see a low and high latency connection comparison that represents a connection with and without a PoP respectively:Aside from all latency-related performance gains, building our own Edge network gives us (traffic engineers) a lot of freedom: for example we can easily experiment with new low- and high-level protocols, external and internal loadbalancing algorithms, and more closely integrate with the rest of the infrastructure. We can do things like research BBR congestion control effects on file download speed, latency, and packet loss.As of today, Dropbox has 20 PoPs around the world:We’ve just announced our PoP in Toronto, and will get two more in Scandinavia by the end of the year.In 2019, we are planning to look at increasing our Edge footprint by researching the viability of PoPs in LATAM, Middle East, and APAC.The process of PoP selection, which was easy at first, now becomes more and more complicated: we need to consider backbone capacity, peering connectivity, submarine cables, but most importantly the location with respect to all the other PoPs we have.The current PoP selection procedure is human guided but algorithm-assisted. Even with a small number of PoPs without assistive software it may be challenging to choose between, for example, a PoP in Brazil and a PoP in Australia. The problem persists as the number of PoPs grows: e.g. what location will benefit Dropbox users better, Vienna or Warsaw?We try to alternate new PoP placement between selecting the most advantageous PoP for the existing and potential Dropbox users.A tiny script helps us brute-force the problem by:By “population” one can use pretty much any metric we want to optimize, for example total number of people in the area, or number of existing/potential users. As for the loss function to determine the score of each placement one can use something standard like L1 or L2 loss. In our case we try to overcompensate for the effects of latency on the TCP throughput.Some of you may see that the problem here that can be solved by more sophisticated methods like Gradient Descent or Bayesian Optimization. This is indeed true, but because our problem space is so small (there are less than 100K 7th level s2 cells) we can just brute-force through it and get a definitively optimal result instead of the one that can get stuck on a local optimum.Let’s start with the most important part of the Edge—GSLB. GSLB is responsible for loadbalancing users across PoPs. That usually means sending each user to the closest PoP, unless it is over capacity or under maintenance.GSLB is called the “most important part” here because if it misroutes users to the suboptimal PoPs frequently, then it makes the Edge network useless, and potentially even harms performance.The following is a discussion of commonly used GSLB techniques, their pros and cons, and how we use them at Dropbox.Anycast is the easiest loadbalancing method that relies on the core internet routing protocol, BGP. To start using anycast it is sufficient to just start advertising the same subnet from all the PoPs and internet will deliver packet to the “optimal” one automagically.Even though we get automatic failover and simplicity of the setup, anycast has many drawbacks, so let’s go over them one by one.Anycast performanceAbove, we mentioned that BGP selects the “optimal” route and for the most part that is true. The problem is that BGP does not know anything about link latency, throughput, packet loss, and so on. Generally in the presence of multiple routes to the destination, it just selects one with the least number of hops.Anycast-based loadbalancing is mostly optimal but it behaves poorly on high percentiles.This is true for a small and medium number of PoPs. But there is a conjecture that “critical” misrouting probability (e.g. probability of routing user to a different continent) in an anycasted network drops sharply with number of PoPs. Therefore it is possible that with increasing number of PoPs, anycast may eventually start outperforming GeoDNS. We’ll continue looking at how our anycast performance scales with the number of PoPs.Traffic steering With anycast, we have very limited control over traffic. It is hard to explicitly move traffic from one PoP to another. We can do some traffic steering using MED attributes, prepending AS_PATHs to our announces, and by explicitly communicating with traffic providers, but this is not scalable.Also note that in the N WLLA OMNI mnemonic AS_PATH is somewhere in the middle. This effectively means that it can be easily overridden by an administrator and in practice this makes BGP anycast pick the “cheapest” route, not the “nearest” or the “fastest.”Another property of anycast is that graceful drain of the PoP is impossible—since BGP balances packets and not connections. After the routing table change, all inflight TCP sessions will immediately be routed to the next best PoP and users will get an RST from there.Troubleshooting Generally reasoning about traffic routing with anycast becomes very non-trivial, since it involves the state of internet routing at a given time. Troubleshooting performance issues with anycast is hard and usually involves a lot of traceroutes, looking glasses, and back and forth communication with providers along the way.Note that, as in the case of a PoP drain, any connectivity change in the internet has a possibility of breaking users’ connections to anycasted IP addresses. Troubleshooting intermittent connection issues due to internet routing changes or faulty/misconfigured hardware can be challenging.Here is an example of an epic anycast troubleshooting by Fastly NOC from NANOG mailing list: Service provider story about tracking down TCP RSTs. TL;DR SYNs passing through the router had different TTL and at the same time this IP field was used for the ECMP flow hashing.Tools Here are couple of tricks you can use to make troubleshooting a bit easier (especially in case of anycast).Of course having a random request ID associated with every request that goes through the system and can be traced in the logs is a must. In case of the Edge, it is also helpful to echo back a header with the name of the PoP you’re connected to (or embed this into the unique request ID).Another useful thing that is commonly used is to create “debug” sites that can pre-collect all the troubleshooting data for the user so that they can attach it to the support ticket e.g.: github-debug.com, fastly-debug.com, and of course dropbox-debug.com, which was heavily inspired by them.Traffic team projects like dropbox-debug, Brotli static precompression, BBR evaluation and rollout, RUM DNS, and many others came out of Hack Week: a company-wide event that inspires us to try something new and exciting!Anycast at Dropbox With all that said, we still use anycast for our APEX domains like dropbox.com (without www) and as a fallback in case of major DDoS attacks.Let’s talk about another common solution for implementing GSLB: GeoDNS. In this approach each PoP has its own unique unicast IP address space and DNS is responsible for handing off different IP addresses to different users based on their geographical location.This gives us control over traffic steering and allows graceful drain. It is worth mentioning that any kind of reasoning about unicast-based setup is much easier, therefore troubleshooting becomes simpler.As you can see, there are a lot of variables involved: we rely on a DNS provider guessing user IP by their DNS resolver (or trust EDNS CS data), then guessing user location by their IP address, then approximate physical proximity to latency.Note that different DNS providers will likely end up with different decisions, based on their algorithms and quality of their GeoIP database, therefore monitoring performance of multi-provider DNS setup is much harder.Aside from that, DNS also has a major problem with stale data. Long story short: DNS TTL is a lie. Even though we have TTL of one minute for www.dropbox.com, it still takes 15 minutes to drain 90% of traffic, and it may take a full hour to drain 95% of traffic:Here we also need to mention the myriad embedded devices using Dropbox API that range from video cameras to smart fridges which have a tendency of resolving DNS addresses only during power-on.GeoDNS at DropboxOur DNS setup evolved quite a bit over last few years: we started with a simple continent→PoP mappings, then switched to country→PoP with a per-state mapping data for serving network traffic to large countries like the US, Canada, etc. At the moment, we are juggling relatively complex LatLong-based routing with AS-based overrides to work around quirks in internet connectivity and peering.Let’s very briefly cover one of the composite approaches to GSLB: hybrid unicast/anycast setup. By combining unicast and anycast announces along with GeoDNS mapping, one can get all the benefits of unicast along with an ability to quickly drain PoPs in case of an outage.One can enable this hybrid GSLB by announcing both PoP’s unicast subnet (e.g. /24) and one of its supernets (e.g. /19) from all of the PoPs (including itself).This implies that every PoP should be set up to handle traffic destined to any PoP: i.e. have all the VIPs from all the PoPs in the BGP daemons/L4 balancers/L7 proxies configs.Such an approach gives us the ability to quickly switch between unicast and anycast addresses and therefore immediate fallback without waiting for DNS TTL to expire. This also allows graceful PoP draining and all the other benefits of DNS traffic steering. All of that comes at a relatively small operational cost of a more complicated setup and may cause scalability problems once you reach the high thousands of VIPs. On the bright side, all PoP configs now become more uniform.All the GSLB methods discussed up until now have one critical problem: none of them uses actual user-perceived performance as a signal, but instead rely on some approximations: BGP uses number of hops as a signal, while GeoIP uses physical proximity. We want to fix that by using Real User Metrics (RUM) collection pipeline based on performance data from our desktop clients.Companies that do not have an app usually do latency measurements with the JS-based prober on their website.Years ago we invested in an availability measurement framework in our Desktop Clients to help us estimate the user-perceived reliability of our Edge network. The system is pretty simple: once in a while a sample of clients run availability measurements against all of our PoPs and report back the results. We extended this system to also log latency information, which gave us sufficient data to start building our own map of the internet.We also built a separate resolver_ip→client_ip submap by joining DNS and HTTP server logs for http requests to random subdomain of a wildcard DNS record. On top of which we apply a tiny bit of post-processing for EDNS ClientSubnet-capable resolvers.We combine the aggregated latencies, resolver_ip→client_ip map, BGP fullview, peering information, and capacity data from our monitoring system to produce the final map of client_subnet→PoP.We are also considering adding a signal from the web server logs, since we already have TCP_INFO data, including number of retransmits, cwnd/rwnd, and rtt.After which we pack this map into a radix tree and upload it to a DNS server, after which it is compared to both anycast and GeoIP solutions.Specifics of map generation are up in the air right now: we’ve tried (and continue trying out) different approaches: from simple HiveQL query that does per-/24 aggregation to ML-based solutions like Random Forests, stacks of XGBoosts, and DNNs. Sophisticated solutions are giving slightly better, but ultimately comparable results, at the cost of way longer training and reverse engineering complexity. At least for now, we are sticking with the solution that is easier to reason about and easier to troubleshoot.Data anonymization and aggregation We anonymize and aggregate all latency and availability data by /24 subnet in case of IPv4 and /56 in case of IPv6. We don’t operate directly on real user IPs and enforce strict ACL and retention policies for all RUM data.Data cleanup Data cleanup is a very important step in the map data pipeline. Here are couple of common patterns that we’ve found during our map construction:Data extrapolation Currently we use the following techniques for speculatively expanding the resulting map:This technique allows us to double our map coverage, make it more robust to changes, and generate a map using a smaller dataset.Troubleshooting DNS map Once a RUM-based map is constructed, it is crucial to be able to estimate how good it is by using a single value, something like an F1 score used for binary classification or BLEU score used for evaluating machine translation. That way, one can not only automatically prevent bad maps from going live, but also numerically compare the quality of different map iterations and construction algorithms.Another common approach for the map evaluation is to test it against the subset of data that training process did not see.For interactive slicing and dicing of data and ad-hoc troubleshooting, we map subnets back into the lat/long coordinates, aggregate their stats by h3 regions and then draw them with kepler.gl. This is very helpful to quickly eyeball maps that have low scores.We went with h3 here instead of s2 because Kepler has built-in support for it, and generally h3 has simpler Python interface, therefore making it easier for us to experiment with visualizations. Whisper: also hexagons look cooler =)The same approach can be used for visualizing current performance, week-over-week difference, difference between GeoIP database versions, and much more.You can clearly see here where our PoPs are located. All of the big patches of blue and violet colors are getting their PoPs later this year or next year.Another way of visualizing IP maps is to skip the whole GeoDNS mapping step and plot IP addresses on the 2D plane by mapping them on a space filling curve, e.g. Hilbert curve. One can also place additional data in the height and color dimensions. This approach will require some heavy regularization for it to be consumable by humans and even more ColorBrewer2 magic to be aesthetically pleasing.RUM DNS at Dropbox RUM-based DNS is an actively evolving project, and we have not shipped it to our main VIPs yet, but the data we’ve collected from our GSLB experiments shows that it is the only way we can properly utilize more than 25-30 PoPs. This project will be one of our top priorities in 2019, because even metrics collected from an early map prototypes show that it can improve effectiveness of our Edge network by up to 30% using RUM DNS.It will also provide all the byproducts needed for the Explicit Loadbalancer… Speaking of which…A quick note about another more explicit way of routing users to PoPs. All these dances with guessing users’ IP address based on their resolver, GeoIP effectiveness, optimality of decisions made by BGP, etc. are all no longer necessary after a request arrives at the PoP. Because at that point in time, we know the users’ IP and even have an RTT measurement to them. At that point, we can route users on a higher level, like for example embedding a link to a specific PoP in the html, or handing off a different domain to a desktop client trying to download files.The same IP→PoP map that was constructed for RUM DNS can be reused here, now exposed as an RPC service.This loadbalancing method allows for a very granular traffic steering, even based on per-resource information, like user ID, file size, and physical location in our distributed storage. Another benefit is almost immediate draining of new connections, though references to resources that were once given out may live for extended periods of time.Very complex schemes can be invented here, for example we can hand off whole URLs that in the domain name embed information for external DNS-based routing and at the same time embed information for internal routing inside path/queryargs, that will allow PoP to make more optimal routing decision. Another approach is to put that additional data as an opaque blob into the encrypted/signed cookie. All of these possibilities sound exciting, so care must be taken to not overcomplicate the system.Explicit loadbalancing at Dropbox We are not currently using this as an external loadbalancing method but instead rely on it for internal re-routing. The traffic team is actively preparing foundation for using it though.Now let’s discuss what happens when traffic actually arrives at the PoP.PoPs consist of network equipment and sets of Linux servers. An average PoP has good connectivity: backbone, multiple transits, public and private peering. By increasing our network connectivity, we decrease the time packets spend in the public internet and therefore heavily decrease packet loss and improve TCP throughput. Currently about half of our traffic comes from peering.Dropbox has an open peering policy, so feel free to peer with us all around the world.You can read more about network setup in the Evolution of Dropbox’s Edge Network post.Our PoPs consist of multiple nginx boxes that are acting as L7 proxy and L4 loadbalancers (L4LBs) spreading load between them.We use standard techniques to scale L4LBs and make them more resilient to failure: BGP ECMP, DSR, and consistent hashing. IPVS acts as our dataplane—a kernel-level loadbalancer with netlink API. IPVS kernel module provides us with state tracking, pluggable scheduling algorithms, and IP-in-IP encapsulation for DSR.There are two main approaches for building high performance packet processors right now.Kernel Do packet processing early in network stack. This allows in-kernel data structures and TCP/IP parsing routines to be reused. For quite a while now, Linux has IPVS and netfilter modules that can be used for connection-level loadbalancing. Recent kernels have eBPF/XDP combo which allows for a safer and faster way to process packets in kernel space. Tight coupling with kernel though has some downsides: upgrade of such LB may require reboot, very strict requirements on kernel version, and difficult integration testing. This approach is used by companies like Facebook and Dropbox.Userspace Create a virtual NIC PCIe device with SRIO-V, bypass the kernel through DPDK/netmap/etc, and get RX/TX queues in an application address space. This gives programmers full control over the network, but tcp/ip parsing, data structures, and even memory management must be done manually (or provided by a 3rd party library). Testing this kind of setup is also much easier. This approach is used by companies like Google and Github.We currently use our homebrew version of consistent hashing module, but starting from linux-4.18 there is a Maglev Hash implementation: [ip_vs_mh](https://github.com/torvalds/linux/blob/master/net/netfilter/ipvs/ip_vs_mh.c). Compared to Ketama, Maglev Hash trades off some of the hash resiliency for more equal load distribution across backends and lookup speed. You can read more about Maglev Hash in the Maglev paper or the morning paper, or go over a quick summary of consistent hash techniques from Damian Gryski.We hash incoming packets based on 5-tuple (proto, sip, dip, sport, dport) which improves load distribution even further. This sadly means that any server-side caching becomes ineffective since different connections from the same client will likely end up on different backends. If our Edge did rely on local caching, we could use 3-tuple hashing mode where we would only hash on (protocol, sip, dip).Another interesting fact is that L4LB will need to do some special handling of ICMP’s Packet Too Big replies, since they will originate from a different host and therefore can’t use plain outer header hashing, but instead must be hashed based on the tcp/ip headers in the ICMP packet payload. Cloudflare uses another approach for solving this problem with its [pmtud](https://github.com/cloudflare/pmtud): broadcast incoming ICMP packets to all the boxes in the PoP. This can be useful if you do not have a separate routing layer and are ECMP’ing packets straight to your L7 proxies.Control plane for L4LBs is currently written in Go and closely integrated with our infrastructure and responsible for online reconfiguration, BGP connectivity, and health-checking of backends.Health checks on any encapsulating DSR-based L4LB is very tricky. Special care must be taken to run health checks through the same packet encapsulation process as data itself is going, otherwise it is easy to start sending traffic to the box that does not have a properly set up tunnel yet.Key properties of the L4LBs:Not to mention that now we can convert basically any server in production into a high performance loadbalancer just by running a binary on it.As for the future work, we have a number of things we want to try. First, replace the routing dataplane with either a DPDK or XDP/eBPF solution, or possibly just integrating an open-source project like Katran. Second, we currently use IP-in-IP for packet encapsulation and it’s about time we switch it to something more modern like GUE which is way more NIC-friendly in terms of steering and offload support.Having PoPs close to our users decreases the time needed for both TCP and TLS handshakes, which essentially leads to a faster TTFB. But owning this infrastructure instead of renting it from a CDN provider allows us to easily experiment and iterate on emerging technologies that optimize latency and throughput sensitive workloads even further. Let’s discuss some of them.TCP Starting from the lowest layers of the stack here is a Fair Queueing packet scheduler example: not only because it introduces fairness between flows, but also adds “pacing” to the upper level protocol. Let’s look at some specific examples.Without fair queueing, packets will be dumped to the network as they arrive from the TCP stack, which will lead to the huge Head-of-Line blocking further down network stack.With FQ packets of different flows are interleaved and one flow no longer blocks another.Without pacing, if you want to send multiple megabytes of data to the user, and current TCP congestion window allows that, then you’ll just dump thousands of packets onto the underlying network stack.With pacing, TCP will hint packet scheduler a desired sending rate (based on the congestion window and rtt) and then scheduler is responsible for submitting packets to the network stack every once in a while to maintain that steady sending rate:FQ comes at a relatively low CPU cost of around 5%, but it essentially makes routers and shapers along the path way happier, which leads to lower packet loss and less bufferbloat.Fun fact: when we first deployed FQ we’ve noticed that all the buffer drops on our Top-of-the-Rack (ToR) switches had gone away. Even though they were very beefy boxes capable of handling terabits of traffic it seems like they had shallow buffers and were susceptible to packet drop during microbursts.This is only one of the features that new Linux kernels provide, including but not limited to: Tail Loss Probe, TCP Small Queues, [TCP_NOTSENT_LOWAT](https://lwn.net/Articles/560082/), RACK, etc.We work on network- and transport-level optimizations from time to time—and when we do, it’s super fun and usually involves some amount of Wiresharking and packetdrilling. For example, one upcoming project for the Traffic team is to evaluate BBR v2 (once it is ready for public testing).TLS All connections to Dropbox are protected by TLS that encrypts and authenticates data in transit over the public internet. We also re-encrypt data and send it over an encrypted and mutually authenticated channel over our backbone network.Since we use the same TLS stack internally for gRPC, we are very invested in its performance, especially around the TLS handshake part, where we make sure our libraries are using the most efficient hardware instructions possible, and for large file transfers, where we try to minimize the number of memory copies that they perform.Our TLS setup is relatively simple: BoringSSL, TLS tickets with frequently rotated ephemeral keys, preferring AEAD ciphersuites with ChaCha20/Poly1305 for older hardware (we are very close the Cloudflare’s TLS config.) We are also in the process of rolling out the RFC version of the TLS 1.3 across our Edge network.As for the future plans: as our boxes get closer to 100Gbit we are starting to look towards [TCP_ULP](https://github.com/torvalds/linux/blob/master/Documentation/networking/tls.txt) and how we can add support for it to our software stack.HTTP The main job of the nginx proxies on the Edge is to maintain keep alive connections to the backends in data center over our fat-long-pipe backbone. This essentially means that we have a set of hot connections that are never constrained by CWND on an almost lossless link.Very quick note about how we build and deploy nginx: like everything else in Dropbox, we use Bazel to reproducibly and hermetically build a static nginx binary, copy over configs, package all of this into a squshfs, use torrent to distribute resulting package to all the servers, mount it (read-only), switch symlink, and finally run nginx upgrade. We probably should write a blog post on it too, since it is very simple and very efficient.Our nginx configuration is static and bundled with the binary therefore we need a way to dynamically configure some aspects of the configuration without full redeploy. Here is where Upstream Management Service kicks in. UMS is basically a look-aside external loadbalancer for nginx which allows us to reconfigure upstreams on the fly. One can create such system by:Because we are already relying on the Lua there, we’ve built a data plane for UMS with it by combining a balancer_by_lua_block directive and ngx.timer.every hook that periodically fetches configuration from control plane via https.A nice side effect of writing the balancer module in Lua: we can now quickly experiment with different loadbalancing algorithms before writing them in C. The downside of Lua is that it is tricky to test, especially in a company where Lua is not one of the primary languages.Control plane for UMS is a Golang service that gets information from our service discovery, monitoring system, and manual overrides, then aggregates and exposes it as a REST endpoint that nginx then accesses through a simple httpc:request_uri.gRPC Nginx is terminating HTTP, HTTP/2, and gRPC connections on the Edge. Ability to proxy gRPC through our stack allows us to experiment with our apps talking gRPC directly to the application servers. Being able to do that streamlines development process and unifies the way services communicate externally and internally.We are looking how we can use gRPC for all APIs. For the APIs that we can’t switch to gRPC, like web, we consider converting all HTTP requests into the gRPC method calls right at the Edge.All of this pretty much covers the external part of Traffic Infrastructure, but there is another half that is not directly visible to our users: gRPC-based service mesh, scalable and robust service discovery, and a distributed filesystem for config distribution with notification support. All of that is coming soon in the next series of blog posts.Do you like traffic-related stuff? Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of which is managed by a small team in Mountain View, CA.The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and loadbalancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? Dropbox is also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.This article describes the work done by many people over a course of multiple years. Thanks to all current and past traffic team members: all of you helped make Dropbox faster and more reliable: Ashwin Amit, Brian Pane, Dmitry Kopytkov, Dzmitry Markovich, Eduard Snesarev, Haowei Yuan, John Serrano, Jon Lee, Kannan Goundan, Konstantin Belyalov, Mario Brito, Oleg Guba, Patrick Lee, Preslav Le, Ross Delinger, Ruslan Nigmatullin, Vladimir Sheyda, Yi-Shu Tai.",https://blogs.dropbox.com/tech/2018/10/dropbox-traffic-infrastructure-edge-network/,0,dropbox,"database,docker,python,mysql,php,backend,django",NULL,2018-10-10
Using machine learning to index text from billions of images,"In our previous blog posts, we talked about how we updated the Dropbox search engine to add intelligence into our users’ workflow, and how we built our optical character recognition (OCR) pipeline. One of the most impactful benefits that users will see from these changes is that users on Dropbox Professional and Dropbox Business Advanced and Enterprise plans can search for English text within images and PDFs using a system we’re describing as automatic image text recognition.The potential benefit of automatically recognizing text in images (including PDFs containing images) is tremendous. People have stored more than 20 billion image and PDF files in Dropbox. Of those files, 10-20% are photos of documents—like receipts and whiteboard images—as opposed to documents themselves. These are now candidates for automatic image text recognition. Similarly, 25% of these PDFs are scans of documents that are also candidates for automatic text recognition.From a computer vision perspective, although a document and an image of a document might appear very similar to a person, there’s a big difference in the way computers see these files: a document can be indexed for search, allowing users to find it by entering some words from the file; an image is opaque to search indexing systems, since it appears as only a collection of pixels. Image formats (like JPEG, PNG, or GIF) are generally not indexable because they have no text content, while text-based document formats (like TXT, DOCX, or HTML) are generally indexable. PDF files fall in-between because they can contain a mixture of text and image content. Automatic image text recognition is able to intelligently distinguish between all of these documents to categorize data contained within.So now, when a user searches for English text that appears in one of these files, it will show up in the search results. This blog post describes how we built this feature.First, we set out to gauge the size of the task, specifically trying to understand the amount of data we would have to process. This would not only inform the cost estimate, but also confirm its usefulness. More specifically, we wanted to answer the following questions:The types of files we want to process are those that currently don’t have indexable text content. This includes image formats and PDF files without text data. However, not all images or PDFs contain text; in fact, most are just photos or illustrations without any text. So a key building block was a machine learning model that could determine if a given piece of content was OCR-able, in other words, whether it has text that has a good chance of being recognizable by our OCR system. This includes, for example, scans or photos of documents, but excludes things like images with a random street sign. The model we trained was a convolutional neural network which takes an input image before converting its output into a binary decision about whether it is likely to have text content.For images, the most common image type is JPEG, and we found that roughly 9% of JPEGs are likely to contain text. For PDFs, the situation is a bit more complicated, as a PDF can contain multiple pages, and each page can exist in one of three categories:We would like to skip pages in categories 1 and 3 and focus only on category 2, since this is where we can provide a benefit. It turns out that the distribution of pages in each of the 3 buckets is 69%, 28%, and 3%, respectively. Overall, our target users have roughly twice as many JPEGs as PDFs, but each PDF has 8.8 pages on average, and PDFs have a much higher likelihood to contain text images, so in terms of overall load on our system, PDFs would contribute over 10x as much as JPEGs! However, it turns out that we could reduce this number significantly through a simple analysis, described next.Once we decided on the file types and developed an estimate of how much OCR-able content lived on each page, we wanted to be strategic about the way we approached each file. Some PDF documents have a lot of pages, and processing those files is thus more costly. Fortunately, for long documents, we can take advantage of the fact that even indexing a few pages is likely to make the document much more accessible from searches. So we looked at the distribution of page counts across a sampling of PDFs to figure out how many pages we would index at most per file. It turns out that half of the PDFs only have 1 page, and roughly 90% have 10 pages or less. So we went with a cap of 10 pages—the first 10 in every document. This means that we index almost 90% of documents completely, and we index enough pages of the remaining documents to make them searchable.Once we started the process of extracting text with OCR on all the OCR-able files, we realized that we had two options for rendering the image data embedded in PDF files: We could extract all raster (i.e. pixel) image objects embedded in the file stream separately, or we could render entire pages of the PDF to raster image data. After experimenting with both, we opted for the latter, because we already had a robust large-scale PDF rendering infrastructure for our file previews feature. Some benefits of using this system include:The server-side rendering used in our preview infrastructure is based on PDFium, the PDF renderer in the Chromium project, an open-source project started by Google that’s the basis of the Chrome browser. The same software is also used for body text detection and to decide whether the document is “image-only,” which helps decide whether we want to apply OCR processing.Once we start rendering, the pages of each document are processed in parallel for lower latency, capped at the first 10 pages based on our analysis above. We render each page with a resolution that fills a 2048-by-2048-pixel rectangle, preserving the aspect ratio.Our OCR-able machine learning model was originally built for the Dropbox document scanner feature, in order to figure out if users took (normal) photos recently that we could suggest they “turn into a scan.” This classifier was built using a linear classifier on top of image features from a pre-trained ImageNet model (GoogLeNet/Inception). It was trained on several thousand images gathered from several different sources, including public images, user-donated images, and some Dropbox-employee donated images. The original development version was built using Caffe, and the model was later converted to TensorFlow to align with our other deployments.When fine-tuning this component’s performance, we learned an important lesson: In the beginning, the classifier would occasionally produce false positives (images it thought contained text, but actually didn’t) such as pictures of blank walls, skylines, or open water. Though they appear quite different to human eyes, the classifier saw something quite similar in all of these images: they all had smooth backgrounds and horizontal lines. By iteratively labeling and adding such so-called “hard negatives” to the training set, we significantly improved the precision of the classification, effectively teaching the classifier that even though these images had many of the characteristics of text documents, they did not contain actual text.Locating the corners of the document in the image and defining its (approximately) quadrangular shape is another key step before character recognition. Given the coordinates of the corners, the document in an image can be rectified (made into a right-angled rectangle) with a simple geometric transformation. The document corner detector component was built using another ImageNet deep convolutional network (Densenet-121), with its top layer replaced by a regressor that produces quad corner coordinates.The test data for training this model used only several hundred images. The labels, in the form of four or more 2-D points that define a closed document boundary polygon, were also drawn by Mechanical Turk workers using a custom-made UI, augmented by annotations from members of the Machine Learning team. Often, one or more of the corners of the document contained in the training images lie outside of the image bounds, necessitating some human intuition to fill in the missing data.Since the deep convolutional network is fed scaled-down images, the raw predicted location of the quadrangle is at a lower resolution than the original image. To improve precision, we apply a two-step process:From the coordinates of the quad, it is then easy to rectify the image into an aligned version.The actual optical character recognition system, which extracts text tokens (roughly corresponding to words), is described in our previous blog post. It takes rectified images from the corner detection step as input and generates token detections, which include bounding boxes for the tokens and the text of each token. These are arranged into a roughly sequential list of tokens and added to the search index. If there are multiple pages, the lists of tokens on each page are concatenated together to make one big list.To run automatic image text recognition on all potentially indexable files for all eligible users, we need a system that can ingest incoming file events (e.g., adds or edits) and kick off the relevant processing. This turns out to be a natural use case for Cape, the flexible, large-scale, low-latency framework for asynchronous event-stream processing that powers many Dropbox features. We added a new Cape micro-service worker (called a “lambda”) for OCR processing, as part of the general search indexing framework.The first several steps of processing take advantage of Dropbox’s general previews infrastructure. This is a system that can efficiently take a binary file as input and return a transformation of this file. For example, it might take a PowerPoint file and produce a thumbnail image of that PowerPoint file. The system is extensible via plugins that operate on specific types of files and return particular transformations; thus, adding a new file type or transformation is easy to do. Finally, the system also efficiently caches transformations, so that if we tried to generate a thumbnail image of the same PowerPoint file twice, the expensive thumbnail operation would only run once.We wrote several preview plugins for this feature, including (numbers correspond to the system diagram above):To increase robustness of the system in the case of transient/temporary errors during remote calls, we retry the remote calls using exponential backoff with jitter, a best-practice technique in distributed systems. For example, we achieved an 88% reduction in the failure rate for the PDF metadata extraction by retrying a second and third time.When we deployed an initial version of the pipeline to a fraction of traffic for testing, we found that the computational overhead of our machine learning models (corner detection, orientation detection, OCRing, etc.) would require an enormous cluster that would make this feature much too expensive to deploy. In addition, we found that the amount of traffic we were seeing flow through was about 2x what we estimated it should be based on historical growth rates.To address this, we began by improving the throughput of our OCR machine learning models, with the assumption that increasing the throughput offered the greatest leverage on reducing the size of the OCR cluster we would need.For accurate, controlled benchmarking, we built a dedicated sandboxed environment and command line tools that enabled us to send input data to the several sub-services to measure throughput and latency of each one individually. The stopwatch logs we used for benchmarking were sampled from actual live traffic with no residual data collection.We chose to approach performance optimization from the outside in, starting with configuration parameters. When dealing with CPU-bound machine learning bottlenecks, large performance increases can sometimes be achieved with simple configuration and library changes; we discuss a few examples below.A first boost came from picking the right degree of concurrency for code running in jails: For security, we run most code that directly touches user content in a software jail that restricts what operations can be run, isolates content from different users to prevent software bugs from corrupting data, and protects our infrastructure from malicious threat vectors. We typically deploy one jail per core on a machine to allow for maximum concurrency, while allowing each jail to only run single-threaded code (i.e., data parallelism).However, it turned out that the TensorFlow deep learning framework that we use for predicting characters from pixels is configured with multicore support by default. This meant that each jail was now running multi-threaded code, which resulted in a tremendous amount of context-switching overhead. So by turning off the multicore support in TensorFlow, we were able to improve throughput by about 3x.After this fix, we found that performance was still too slow—requests were getting bottlenecked even before hitting our machine learning models! Once we tuned the number of pre-allocated jails and RPC server instances for the number of CPU cores we were using, we finally started getting the expected throughput. We got an additional significant boost by enabling vectorized AVX2 instructions in TensorFlow and by pre-compiling the model and the runtime into a C++ library via TensorFlow XLA. Finally, we benchmarked the model to find that 2D convolutions on narrow intermediate layers were hotspots, and sped them up by manually unrolling them in the graph.Two important components of the document image pipeline are corner detection and orientation prediction, both implemented using deep convolutional neural networks. Compared to the Inception-Resnet-v2 model we had been using before, we found that Densenet-121 was almost twice as fast and only slightly less accurate in predicting the location of the document corners. To make sure we didn’t regress too much in accuracy, we ran an A/B test to assess the practical impact on usability, comparing how frequently users would manually correct the automatically predicted document corners. We concluded that the difference was negligible, and the increase in performance was worth it.Making document images searchable is the first step towards a deeper understanding of the structure and content of documents. With that information, Dropbox can help users organize their files better—a step on the road to a more enlightened way of working.Automatic image text recognition is a prime example of the type of large scale projects involving computer vision and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team.Thanks to: Alan Shieh, Brad Neuberg, David Kriegman, Jongmin Baek, Leonard Fink, Peter Belhumeur.",https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text-from-billions-of-images/,0,dropbox,"backend,angular,frontend,tensorflow,xml,typescript,python,machinelearning,java",NULL,2018-10-09
Validating performance and reliability of the new Dropbox search engine,"Each of the hundreds of our search leaves runs our retrieval engine, whose responsibilities include handling both updating the index when files get created, edited, and deleted (those are “writes”) as well as servicing search queries (these are “reads”). Dropbox traffic has the interesting characteristic that it is dominated by writes—that is, files are updated way more frequently than they are searched for. We typically observe a volume of writes which is 10x higher than reads. As such, we carefully considered those workloads when optimizing the data structures used in the retrieval engine.A “posting list” is a data structure that maps a token (i.e., a potential search term) to the list of documents containing that token. At its core, a retrieval engine’s primary job is to maintain a set of posting lists which constitute the inverted index. Posting lists are queried during search requests, and updated when updates are applied to the index. A common posting list format stores the token and an associated list of document ids, along with some metadata that can be used during scoring phase (ex: term frequency).This format is ideal for workloads which are primarily read-only: each search query only requires finding the appropriate set of posting lists (which can be O(1) using a hash table) and then adding each of the associated doc ids to the result set.However, in order to be able to handle the high update rates we observed in our users’ behavior, we made the decision to use an “exploded” posting list format for the reverse index. Our reverse index is backed by a key/value store (RocksDB) and each (namespace ID, token, document ID) tuple is stored as a separate row. More specifically, the format of the row key is ""<namespace ID>|<token>|<doc ID>"". Given a search query against documents in a specific namespace, we can efficiently run a prefix search for <namespace ID><token>| in the index to get a list of all matching documents. Having the namespace concept built into the format of the key has several benefits. First and very importantly, in terms of security since it prevents any possibility a query would return documents outside of the specified namespace the user has access to. Second, in terms of performance since it narrows the search to only the documents included in the namespace as opposed to the much larger set of documents indexed in the partition. With this scheme, the value of each row stores the metadata associated with the token.From a storage and retrieval perspective, this is less efficient compared to the more conventional format where all document ids are grouped and can be stored in a compact representation by using techniques such as delta encoding. But the “exploded” representation has the main benefit of handling index mutations particularly efficiently. For example, a document is added to the reverse index by inserting a row for each token it contains—this is a simple operation that performs very efficiently on most key/value stores, including RocksDB. The extra storage penalty is alleviated by the fact that RocksDB applies prefix compression of keys—overall we found the index size using an exploded representation was only about 15% larger compared to one using a conventional posting list representation.Responsive performance is critical to a smooth and interactive user experience. The primary metric we use for evaluating serving performance is query latency at 95th and 99th percentile, i.e., the slowest 5% and 1% of queries should be no slower than 500ms and 1sec, respectively (currently). The median query will, of course, be significantly faster. As the development of the Nautilus system progressed, we continuously measured and analyzed performance. Each component of the system is instrumented in order to be able to determine how much it contributes to the overall latency. We learned a few lessons along the way, including:Millions of users rely on Dropbox search in order to perform their work, so we paid special attention when designing the system to ensure that we could guarantee the uptime our users expect.In a large distributed system, network or hardware failures and software crashes happen regularly—they are inevitable. We kept this fact in mind when we designed Nautilus, focusing particularly on fault-tolerance and automatic recovery for components in the serving path.Some of the components were designed to be stateless, meaning that they rely on no external services or data to operate. This includes Octopus, our results merging and ranking system; and the root of the retrieval engine, which fans out search requests to all the leaves. These services can thus be easily deployed with multiple instances, each of which can be automatically reprovisioned as failures occur. The problem is more challenging when it comes to the leaves instances, since they maintain the index data.In Nautilus, each leaf instance is responsible for handling a subset of the search index, called a “partition.” We maintain a registry of all leaves and assign partitions to the leaves using a separate coordinator. The coordinator is responsible for ensuring continuous coverage for 100% of the partitions. When a new leaf instance is brought up, it stays idle until instructed by the coordinator to serve a partition, at which point it loads the index and then starts accepting search requests.What happens if a leaf is in the process of coming online, but there is a search request for data in its partition? We deal with this by maintaining replicas of each leaf, which together are called “leaf replica groups.” A replica group is an independent cluster of leaf instances providing full coverage of the index. For example, to ensure 2X replication for the system, we run two replica groups, with one coordinator per group. In addition to making it simple to reason about replication, this setup provides operational benefits for doing maintenance, such as:In both of these cases, Nautilus is fully able to answer all requests throughout the entire process.Each leaf group is over-provisioned with about 15% extra hardware capacity to have a pool of idle instances standing by. When the coordinator detects a drop in partition coverage (i.e., say a currently active leaf suddenly stops serving requests), it reacts by picking an idle leaf and instructing it to serve the lost partition. That leaf then performs the following steps:Nautilus is a prime example of the type of large scale projects involving data retrieval and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team.Thanks to: Adam Faulkner, Adhiraj Somani, Alan Shieh, Annie Zhou, Braeden Kepner, Elliott Jin, Franck Chastagnol, Han Lee, Harald Schiöberg, Ivan Traus, Kelly Liu, Michael Mi, Peng Wang, Rajesh Venkataraman, Ross Semenov, and Sammy Steele.",https://blogs.dropbox.com/tech/2018/10/validating-performance-and-reliability-of-the-new-dropbox-search-engine/,0,dropbox,,NULL,2018-10-02
"Architecture of Nautilus, the new Dropbox search engine","Over the last few months, the Search Infrastructure engineering team at Dropbox has been busy releasing a new full-text search engine called Nautilus, as a replacement for our previous search engine.Search presents a unique challenge when it comes to Dropbox due to our massive scale—with hundreds of billions of pieces of content—and also due to the need for providing a personalized search experience to each of our 500M+ registered users. It’s personalized in multiple ways: not only does each user have access to a different set of documents, but users also have different preferences and behaviors in how they search. This is in contrast to web search engines, where the focus on personalization is almost entirely on the latter aspect, but over a corpus of documents that are largely the same for each user (localities aside).In addition, some of the content that we’re indexing for search changes quite often. For example, think about a user (or several users) working on a report or a presentation. They will save multiple versions over time, each of which might change the search terms that the document should be retrievable by.More generally, we want to help users find the most relevant documents for a given query—at this particular moment in time—in the most efficient way possible. This requires being able to leverage machine intelligence at several stages in the search pipeline, from content-specific machine learning (such as image understanding systems) to learning systems that can better rank search results to suit each user’s preferences.The Nautilus team worked with our machine intelligence platform to scale our search ranking and content understanding models. These kind of systems require a lot of iteration to get right, and so it is crucial to be able to experiment with different algorithms and subsystems, and gradually improve the system over time, piece-by-piece. Thus, the primary objectives we set for ourselves when starting the Nautilus project were to:In this blog post we describe the architecture of the Nautilus system and its key characteristics, provide details about the choices we made in terms of technologies and approaches we chose for the design, and explain how we make use of machine learning (ML) at various stages of the system.Nautilus consists of two mostly-independent sub-systems: indexing and serving.The role of the indexing pipeline is to process file and user activity, extract content and metadata out of it, and create a search index. The serving system then uses this search index to return a set of results in response to user queries. Together, these systems span several geographically-distributed Dropbox data centers, running tens of thousands of processes on more than a thousand physical hosts.The simplest way to build an index would be to periodically iterate through all files in Dropbox, add them to the index, and then allow the serving system to answer requests. However, such a system wouldn’t be able to keep up with changes to documents in anything close to real-time, as we need to be able to do. So we follow a hybrid approach which is fairly common for search systems at large scale:Some other key pieces of the system that we’ll talk about are how to index different kinds of content, including using ML for document understanding and how to rank retrieved search results (including from other search indexes) using an ML-based ranking service.Before we talk about specific subsystems in Nautilus, let’s briefly discuss how we can achieve the level of scale we need. With hundreds of billions of pieces of content, we have an enormous amount of data that we need to index. We split, or “shard,” this data across multiple machines. To do this, we need to decide how to shard files such that search requests for each user complete quickly, while also balancing load relatively evenly across our machines.At Dropbox, we already have such a schema for grouping files, called a “namespace,” which can be thought of as a folder that one or more users have access to. One of the benefits of this approach is it allows us to only see search results from files that they have access to, and it is how we allow for shared folders. For example: the folder becomes a new namespace that both sharer and share recipient have access to. The set of files a Dropbox user can access is fully defined by the set of underlying namespaces she was granted access to. Given the above properties of namespaces, when a user searches for a term, we need to search all of the namespaces that they have access to and combine the results from all matches. This also means that by passing the namespaces to the search system we only search content that the querying user can access at the time the search is executed.We group a number of namespaces into a “partition,” which is the logical unit over which we store, index, and serve the data. We use a partitioning scheme that allows us to easily repartition namespaces in the future, as our needs change. What are the kinds of things users would like to search by? Of course there is the content of each document, i.e., the text in the file. But there are also numerous other types of data and metadata that are relevant.We designed Nautilus to flexibly handle all of these and more, through the ability to define a set of “extractors” each of which extracts some sort of output from the input file and writes to a column in our “document store.” The underlying technology has extra custom built layers that provide access control and data encryption. It contains one row per file, with each column containing the output from a particular extractor. One significant advantage of this schema is that we can easily update multiple columns on a row in parallel without worrying about changes from one extractor interfering with those from others.For most documents, we rely on Apache Tika to transform the original document into a canonical HTML representation, which then gets parsed in order to extract a list of “tokens” (i.e. words) and their “attributes” (i.e. formatting, position, etc…).After we extract the tokens, we can augment the data in various ways using a “Doc Understanding” pipeline, which is well suited for experimenting with extraction of optional metadata and signals. As input it takes the data extracted from the document itself and outputs a set of additional data which we call “annotations.” Pluggable modules called “annotators” are in charge of generating the annotations. An example of a simple annotator is the stemming module which generates stemmed tokens based on raw tokens. Another example is converting tokens to embeddings for more flexible search.The document store contains the entire search corpus, but it is not well-suited for running searches. This is because it stores extracted content mapped by document id. For search, we need an inverted index: a mapping from search term to list of documents. The offline build system is in charge of periodically re-building this search index from the document store. It runs the equivalent of a MapReduce job on our document store in order to build up a search index that can be queried extremely fast. Each partition ends up with a set of index files that are stored in an “index store.”By separating the document extraction process from the indexing process, we gain a lot of flexibility for experiments: The serving system is comprised of a front-end, which accepts and forwards user search queries; a retrieval engine which retrieves a large list of matching documents for each query; and a ranking system named Octopus that ranks results from multiple back-ends using machine learning. We’ll focus here on the latter two, as the front-end is a fairly straightforward set of APIs that all our clients use (web, desktop, and mobile).The retrieval engine is a distributed system which fetches documents that match a search query. The engine is optimized for performance and high recall—it aims to return the largest set of candidates possible in the given allocated time budget. These results will then be ranked by Octopus, our search orchestration layer, to achieve high precision, i.e., ensure that the most relevant results are highest in the list. The retrieval engine is divided into a set of “leaves” and a “root”:Our Search Orchestration layer is called Octopus. Upon receiving a query from a user, the first task performed by Octopus is to call Dropbox’s access-control service to determine the exact set of namespaces the user has read access to. This set defines the “scope” of the query that will be performed by the downstream retrieval engine, ensuring that only content accessible to the user will be searched.Besides fetching results from the Nautilus retrieval engine, we have to do a couple things before we can return a final set of results to the user:Note that all of these steps have to happen very fast—we target a budget of 500ms for the 95th percentile search (i.e., only 5% of searches should ever take longer than 500ms). In a future blog post, we will describe how we make that happen.As mentioned earlier, we tune our retrieval engine to return a large set of matching documents, without worrying too much about how relevant each document is to the user. The ranking step is where we focus on the opposite end of the spectrum: picking the documents that the user is most likely to want right now. (In technical terms, the retrieval engine is tuned for recall, while the ranker is tuned for precision.)The ranking engine is powered by a ML model that outputs a score for each document based on a variety of signals. Some signals measure the relevance of the document to the query (e.g., BM25), while others measure the relevance of the document to the user at the current moment in time (e.g., who the user has been interacting with, or what types of files the user has been working on).The model is trained using anonymized “click” data from our front-end, which excludes any personally identifiable data. Given searches in the past and which results were clicked on, we can learn general patterns of relevance. In addition, the model is retrained or updated frequently, adapting and learning from general users’ behaviors over time.The main advantage of using an ML-based solution for ranking is that we can use a large number of signals, as well as deal with new signals automatically. For example, you could imagine manually defining an “importance” for each type of signal we have available to us, such as which documents the user interacted with recently, or how many times the document contains the search terms. This might be doable if you only have a handful of signals, but as you add tens or hundreds or even thousands of signals, this becomes impossible to do in an optimal way. This is exactly where ML shines: it can automatically learn the right set of “importance weights” to use for ranking documents, such that the most relevant ones are shown to the user. For example, by experimentation, we determined that freshness-related signals contribute significantly to more relevant results.After a period of qualification where Nautilus was running in shadow mode, it is currently the primary search engine at Dropbox. We’ve already seen significant improvements to the time-to-index new and updated content, and there’s much more to come.Now that we have solid foundations in place, our team is busy building on top of the Nautilus platform to add new features and improve search quality. We’re exploring new capabilities, such as augmenting the existing posting-list-retrieval-algorithm with distance-based retrieval in an embedding space; unlocking search for image, video, and audio files; improving personalization using additional user activity signals; and much more. Find out about how we validated the performance and reliability of Nautilus in the second part of this post.Nautilus is a prime example of the type of large scale projects involving data retrieval and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team.Thanks to: Adam Faulkner, Adhiraj Somani, Alan Shieh, Annie Zhou, Braeden Kepner, Elliott Jin, Franck Chastagnol, Han Lee, Harald Schiöberg, Ivan Traus, Kelly Liu, Michael Mi, Peng Wang, Rajesh Venkataraman, Ross Semenov, and Sammy Steele.",https://blogs.dropbox.com/tech/2018/09/architecture-of-nautilus-the-new-dropbox-search-engine/,0,dropbox,"backend,cloud,java,docker,python",NULL,2018-09-27
How we rolled out one of the largest Python 3 migrations ever,"Dropbox is one of the most popular desktop applications in the world: You can install it today on Windows, macOS, and some flavors of Linux. What you may not know is that much of the application is written using Python. In fact, Drew’s very first lines of code for Dropbox were written in Python for Windows using venerable libraries such as pywin32.Though we’ve relied on Python 2 for many years (most recently, we used Python 2.7), we began moving to Python 3 back in 2015. This transition is now complete: If you’re using Dropbox today, the application is powered by a Dropbox-customized variant of Python 3.5. This post is the first in a series that explores how we planned, executed, and rolled out one of the largest Python 3 migrations ever.Python 3 adoption has long been a subject of debate in the Python community. This is still somewhat true, though it’s now reached widespread support, with some very popular projects such as Django dropping Python 2 support entirely. As for us, a few key factors influenced our decision to make the jump:Exciting new features Python 3 has seen rapid innovation. Apart from the (very) long list of general improvements (e.g. the str vs bytes rationalization), a few specific features caught our eye:Aging toolchains As Python 2 has aged, the set of toolchains initially compatible for deploying it has largely become obsolete. Due to these factors, continued use of Python 2 was associated with a growing maintenance burden:Initially, we relied on “freezer” scripts to create the native applications for each of our supported platforms. However, rather than use the native toolchains directly, such as Xcode for macOS, we delegated the creation of platform-compliant binaries to py2exe for Windows, py2app for macOS, and bbfreeze for Linux. This Python-focused build system was inspired by distutils: Our application was initially little more than a Python package, so we had a single setup.py-like script to build it.Over time, our codebase became more and more heterogenous. Today, Python is no longer the only language used for development. In fact, our code now consists of a mix of TypeScript/HTML, Rust, and Python, as well as Objective-C and C++ for some specific platform integrations. To support all these components, this setup.py script—internally named build-all.py—grew to be so large and messy that it became difficult to maintain.The tipping point came from changes to how we integrate with each operating system: First, we began introducing increasingly advanced OS extensions—like Smart Sync’s kernel components—that can’t and often shouldn’t be written in Python. Second, vendors like Microsoft and Apple began introducing new requirements for deploying applications that imposed the use of new, more sophisticated and often proprietary tools (e.g. code signing).On macOS, for example, version 10.10 introduced a new app extension for integrating with the Finder: [FinderSync]. Not merely an API, a FinderSync extension is a full-blown application package (.appex) with custom life cycle rules (i.e. it is launched by the OS) and more stringent requirements for inter-process communication. Put another way: Xcode makes leveraging these extensions easy, while py2app does not support them altogether.We were therefore faced with two problems:While we knew that we wanted to migrate to Python 3, this left us with a choice: invest in the freezer dependencies to add support for Python 3 (and thus the modern compilers) and platform-specific features (like app extensions), or move away from a Python-centric build system, doing away with “freezers” altogether. We chose the latter.A note on pyinstaller: We seriously considered using it in the early stages of the project, but it did not support Python 3 at the time, and more importantly, it suffers from similar limitations as other freezers. Regardless, it is an impressive project that we simply felt didn’t suit our needs.To solve this build and deploy problem, we decided on a new architecture to embed the Python runtime in our native application. Rather than delegate this process to the freezers, we would use tooling specific to each platform (e.g. Visual Studio on Windows) to build the various entry points ourselves. Further, we would abstract Python code behind a library, aiming to more directly support the “mixing and matching” of various languages.This would allow us to make use of each platform’s IDEs/toolchain directly (e.g. to add native targets like FinderSync on macOS) while retaining the ability to conveniently write much of our application logic in Python.We landed on the following rough structure:On the surface, the application would more closely resemble what the platform expects, while behind various libraries, teams would have more flexibility to use their choice of programming language or tooling.This architecture’s increased modularity would also provide a key side effect: It would now be possible to deploy both a Python 2 library and a Python 3 library side by side. Tying this back to the Python 3 migration, the process would thus require two steps: first, to implement the new architecture around Python 2, and second, to use it to “swap out” Python 2 in favor of Python 3.Our first step was to stop using the freezer scripts. Both bbfreeze and pywin32 lacked Python 3 support at this stage, leaving us little choice. Starting in 2016, we began to gradually make this change.First, we abstracted away the work of configuring the Python runtime and starting Python threads to a new library named libdropbox_bootstrap. This library would replicate some of what the freezer scripts provided. Though we no longer needed to rely on these scripts wholesale, it was still necessary to provide a minimum basis to run Python code:Packaging our code for on-device execution This ensures we ship compiled Python “bytecode” rather than raw Python source. Where each freezer script previously had its own on-disk format, we used this opportunity to introduce a single format for bundling our code across all platforms:Isolating our Python interpreter This prevents our application from running other on-device Python source. Interestingly, Python 3 makes this type of embedding much simpler. The new [Py_SetPath] function, for example, allowed us to isolate our code without having to do some of the more complicated work of isolation the freezer scripts had to do on Python 2. To support this in Python 2, we back-ported this function to our custom fork.Second, we introduced platform-specific entry points Dropbox.exe, Dropbox.app, and dropboxd to make use of this library. These entry points were built using each platform’s “standard” tooling: Visual Studio, Xcode, and make were used rather than distutils, allowing us to remove much of the custom patchwork imposed on the freezer scripts. For example, on Windows, this greatly simplified configuring DEP/NX for Dropbox.exe, embedding an application manifest as well as including resources.A note on Windows: At this point, continued use of Visual Studio 2008 was becoming highly costly. To transition properly, we needed a version capable of supporting both Python 2 and 3 simultaneously, so we settled on Visual Studio 2013. To support it, we extensively altered our custom fork of Python 2 to make it properly compile using that version. The cost of these changes further reinforced our belief that moving to Python 3 was the right decision.Successfully making a transition of this size (our application contains over 1 million Python LOCs) and at our scale (hundreds of millions of installs) would require a gradual process: We couldn’t simply “flip a switch” in a single release—this was especially true due to our release process, which deploys new versions to all our users every two weeks. There would have to be a way to expose a small/growing number of users to Python 3 in order to detect and fix bugs early.To achieve this, we decided to make it possible to build Dropbox using both Python 2 and 3. This entailed:We used the embedded design introduced through the previous step to our advantage: By abstracting away Python into a library and package, we could easily introduce another variant for another version. Choosing what Python version to use could then be controlled in the entry point itself (e.g. Dropbox.app) during early initialization.This was achieved by making the entry point manually link against libdropbox_bootstrap. On macOS and Linux, for example, we used dlopen/dlsym once a version of Python was chosen. On Windows, we used LoadLibrary and GetProcAddress.The choice of a runtime Python interpreter needed to be made before Python was loaded, so we made it possible for it to be influenced using both a command-line argument /py3 for development purposes and a persistent on-disk setting so it could be controlled by Stormcrow, our feature-gating system.With this in place, we were able to dynamically choose the Python version when launching the Dropbox client. This allowed us to set up additional jobs in our CI infrastructure to run unit and integration tests targeting Python 3. We also integrated automated checks to our commit queue to prevent changes from being pushed that would regress Python 3 support.Once we had gained enough confidence through automated testing, we began rolling out Python 3 to real users. This was achieved by incrementally opting in clients through a remote feature gate. We first rolled out the change to Dropboxers, which allowed us to identify and correct a majority of the underlying issues. We later expanded this to a fraction of our Beta population—which is a lot more heterogeneous when it comes to OS versions—eventually expanding to our Stable channel: Within 7 months, all Dropbox installs were running Python 3. In order to maximize quality, we adopted a policy requiring that all bugs identified as migration-related be fully investigated and corrected before expanding the number of exposed users.As of version 52, this process is complete: Python 2 has been removed altogether from Dropbox’s desktop client.There’s much more to tell about this process. In future posts, we’ll look at:Special thanks to the Dropboxers who contributed to this project:Aditya Jayaraman, Aisha Ferrazares, Allison Kaptur, Amandine Lee, Anaid Chacon, Angela Gong, Ben Newhouse, Benjamin Peterson, Billy Wood, Brandon Jue, Bryon Ross, Cary Yang, Case Larsen, Clarence Lee, Darsey Litzenberger, David Euresti, Denbeigh Stevens, Drew Haven, Eddy Escardo-Raffo, Elmer Le, Eric Swanson, Gautam Gupta, Geoff Song, Guido van Rossum, Isaac Goldberg, John Lai, Jonathan Chien, Joshua Warner, Michael Wu, Naphat Sanguansin, Nikhil Marathe, Nipunn Koorapati, Patrick Chenglo, Peter Vilim, Rafael Tello-Cabrales, Reginald Lips, Ritu Vincent, Ryan Kwon, Samer Masterson, Sean Stephens, Stefan Vainberg, Thomas Ballinger, Tony Grue, Will AndersonVery special thanks to a few members of the Python community:Steve Dower (@zooba): for his work behind Python 3’s excellent support for modern versions of Windows and Visual Studio. Ronald Oussoren (@RonaldOussoren): for his work maintaining PyObjC and his many years of contributions to Python on macOS. Zachary Ware: for his early work in supporting VS2013 in Python 2.",https://blogs.dropbox.com/tech/2018/09/how-we-rolled-out-one-of-the-largest-python-3-migrations-ever/,0,dropbox,"css,frontend,python,python3",NULL,2018-09-25
Machine intelligence at Dropbox: An update from our DBXi team,"Our workdays are getting noisier. Never-ending emails, text messages, constant notifications from more apps and more platforms—it’s disruptive and distracting. And then there’s content. All kinds of documents, spreadsheets, presentations, videos, and photos. Industry research shows that employees at larger organizations use an average of 36 cloud services at work, including tools for productivity, project management, communication, and storage. This information overload is a key source of pain for people at work—and a prime opportunity to leverage the help of machine intelligence.When we talk about machine intelligence at Dropbox, we mean the whole range of applied machine learning, from simple linear classifiers to advanced deep learning networks. For many years we’ve been building a world-class machine learning team, improving our platform behind the scenes. We started with a lot of foundational work on image recognition to improve our users’ experience of organizing the massive amount of photos they keep on our platform. The fact that many of those photos have text in them led us to invest in our mobile document scanning capabilities and custom optical character recognition (OCR) pipeline to help our users quickly scan and find their content. We combined classical machine vision techniques with advanced deep learning methods to create a mobile scanner experience that was faster and more accurate than any off-the-shelf solutions we could find.A lot of the content in Dropbox is already in text-based documents, so search is another area of significant investment for us in terms of machine learning. We’ve now completely rebuilt our search infrastructure to improve the quality and speed of results from the hundreds of billions of pieces of content our users entrust to our platform. Because of our granular sharing permissions, each user has a unique corpus of documents to search within. This creates whole dimensions of personalization that web search engines largely ignore. Add to that activity signals related to relevance—such as files with recent comments or those recently viewed by team members—and you have a really productive use case for machine learning.Each product innovation makes our users’ lives a little better, but how can we make larger leaps toward freeing people’s attention so they can find focus and flow at work? Instead of people working harder to keep up, can we make their tools smarter? To achieve this, we’re weaving our unique approach to machine intelligence into all of our products and surfaces. We call this effort the Dropbox intelligence initiative (DBXi), and we’re excited to share some updates.We see tremendous potential to use machine intelligence to improve the work experience itself. Quieting the noise for individuals is the first step toward helping people in organizations work better together. There are lots of engineering challenges behind creating an intelligent workspace, but the motivation comes from design and product insights. Our researchers have invested years studying how the rise of SaaS applications and mobile have changed the way contemporary knowledge workers do their jobs and run their teams.This research shows three kinds of activity that people spend way too much time on:These activities are an increasing part of modern work, and they all get in the way of maintaining focus. And these problems are compounded for every team member as well, so there’s a lot to gain from taming this complexity. We think our intelligence solutions can help address these pain points at work.One example of a user experience we’re exploring as part of the DBXi initiative is demonstrated in the animation above. In this prototype, we’ve evolved the existing desktop surface where our users check sync activity and see notifications into a more dynamic view that intelligently highlights their most important work connected to Dropbox.We suggest the most relevant content by traversing a user-specific graph that connects people, content, and activity signals in privacy-preserving ways. This includes not only files but also content like Google Docs, as well as collaborative activity in emails, messaging apps, and calendars—whatever a user chooses to connect to Dropbox. And we make all this content searchable in this surface as well. To cut through the noise, we prioritize notifications and show content related to calendar events in personalized activity feeds. And to declutter these feeds, we cluster content and collaborative activity across silos so users can immediately see which projects need their attention and be only a click away from the content they need.This prototype is only one of many surfaces we’re exploring for intelligence. The data graphs and models we build for products and search are generally reusable across surfaces and features. By scaling our internal machine intelligence platform, DBXi is multiplying the efforts of our dedicated intelligence product team so all our engineers can modify and validate models for intelligent features, improved search, and other business optimizations. These scalable methods and common infrastructure give our product managers and designers the flexibility they need to experiment and take chances on novel directions.From a technical perspective, these are important problems to solve, and success means not only intuitive user interfaces, but also lightning-fast response times, industry-leading prediction, and the highest standards for maintaining data privacy. Want to help us build the next-generation intelligent workspace? We’re hiring!",https://blogs.dropbox.com/tech/2018/09/machine-intelligence-at-dropbox-an-update-from-our-dbxi-team/,0,dropbox,"frontend,ruby,python,react",NULL,2018-09-13
Live-hacking Dropbox @ H1-3120,"In 2018, Dropbox has focused on improving our world-class bug bounty program. From increasing bounties to protecting our researchers, we’re always looking for more creative and meaningful ways to stay ahead of the game when it comes to running this program.As an example, we recently partnered with HackerOne to host their H1-3120 live-hacking event in Amsterdam. Live-hacking events let participants hack on a target—often in person—submit vulnerabilities, and receive bounties quickly, all during the course of the event. Live-hacking comes with a number of benefits over traditional bug bounty programs, such as real-time communication and relationship building, which makes finding vulnerabilities and receiving bounties much easier. The event was a huge success! We received plenty of stellar reports and doubled the highest amount we’ve ever paid in a single day for bug bounties.To prepare for the event, we sat down to determine how to get the most value possible out of the short time we had with the hackers. From that discussion, we came up with three objectives:Dropbox aims to have one of the most permissive scopes in the bug bounty world. Scope is the predefined set of targets that bug hunters are allowed to test. In addition to Dropbox assets, we’ve begun to migrate some of our external partners into scope as well. For H1-3120, we required more of our vendors to take on the challenge of participating in bug bounty research. Five SaaS vendors we use were placed in scope for the event, with Dropbox handling all of the triage and paying bounties for any reports.Both HackerOne staff and participants found this exciting. In fact, including vendors as part of the scope for a HackerOne live-hacking event had never been done before. For us, the decision just made sense: when Dropbox engages a vendor who will have access to sensitive Dropbox data, we hold them to very high security standards, including a commitment to welcome scrutiny by Dropbox and other security researchers.We wanted to ensure that our H1-3120 participants had the best possible opportunity to find vulnerabilities in Dropbox and our vendors. We made sure that someone was available to field questions over Slack the week prior to the event while the participants were doing reconnaissance. Additionally, we participated in a conference call with the hackers to answer questions and give advice.To help participants find more valuable bugs, we decided to show them a handful of vulnerabilities that Dropbox has had in the past as well as highlight places that we think have the highest risk for potential vulnerabilities. By getting more eyes on the least frequently tested parts of Dropbox, we help researchers—and ourselves—make Dropbox more secure.H1-3120 started off with a flurry of submissions. In the weeks prior, the hackers were already trying to find vulnerabilities in both Dropbox and our vendors. The plethora of submissions at H1-3120 showcased that effort. Within the first 30 minutes of the event, over 50 reports came in ranging from simple information disclosure to cross-site scripting. The hackers even found a remote code execution in the perimeter of one of our vendors!The Dropbox Security team had a blast meeting researchers, hunting complex vulnerabilities, and improving the security of our product. After the last reward was paid out, Dropbox had distributed more than $80,000 in bounties to researchers at the event, with over $5,000 of that donated to charities.The real value we’ve experienced from the event is the overall uptick in interest in our bug bounty program. Since H1-3120, we’ve had a 23% increase in submissions per day to our program, including a report from bug hunter detroitsmash with a $9,000 bounty.In addition our new relationships with the researchers are proving to be invaluable. We’ve had a number of conversations with our more frequent bug bounty participants leading to HackerOne reports that we’ve subsequently awarded on. We’re planning to connect more with our recurring researchers to build on these important relationships.Dropbox is committed to ensuring our bug bounty program draws the best bug hunting talent from around the world. When friendly hackers find the vulnerabilities before the bad actors do, that’s a huge win for the entire security community. Thanks to all the participants of H1-3120 as well as all the security researchers that send us reports every day!",https://blogs.dropbox.com/tech/2018/09/live-hacking-dropbox-h1-3120/,0,dropbox,"html,frontend,bitcoin,blockchain,php,python,css",NULL,2018-09-10
Migrating from Underscore to Lodash,"The core Dropbox web application is 10 years old and used by millions of users per day. Hundreds of front-end engineers across multiple cities actively work on it. Unsurprisingly, our codebase is very large and somewhat irregular. Recently written parts have thorough test coverage, other parts haven’t been updated in years.Over the past two years we’ve worked to modernize our front-end stack. We’ve successfully moved from CoffeeScript to TypeScript, from jQuery to React, and from a custom Flux implementation to Redux. Having completed these migrations we identified our utility library, Underscore, as one more candidate for migration.When we began our research, Underscore hadn’t seen an update in 3 years. Newer developers were hesitant to use a deprecated library. We wanted to fill that need.Lodash is a utility library composed of many individual modules. It can be used as a complete library, as individual modules, or as a custom build consisting of just the necessary functions. It’s the single most used utility library on the web, and as a result is extremely battle tested. It heavily optimizes for front-end CPU performance in a way that Underscore doesn’t. For example, Lodash is implemented to take advantage of JIT in JavaScript engines.It also offers new features that promote functional programming. For example, it’s well suited for building a functional selector layer between React and Redux, two technologies we use in our front-end codebase. Finally, Lodash is actively maintained, which is critical to long-term support of the library.We wanted to use a strategic migration approach. By gathering consensus from our internal community, doing research first, and constructing a bespoke build for our environment before migrating our entire codebase, we hoped to avoid serious problems.At Dropbox, we use a lightweight, but formal proposal process to align on technology changes. Web Enhancement Proposals (WEPs) are based on Python’s PEPs. They let developers debate the pros and cons of universal changes and reach consensus before making codebase changes that will affect many developers and users.Given the size of our codebase and the number of users that could be affected, creating a WEP for the migration was a natural first step. We identified the goal of creating a minimized, custom Lodash bundle that we can heavily cache and use throughout our primary web application, deprecating support for Underscore.js and migrating all currently used instances to Lodash. Over 100 engineers interacted with the document. We addressed concerns from front-end teams without too much heavy bureaucracy.Because we wanted a custom Lodash build with just the necessary functions, we needed to create a list of those functions.We looked at how Underscore was being used in our codebase. We also looked at cases where Javascript has evolved enough to permit using native solutions. Using this data we put together a list of Lodash functions we should use. It wasn’t exhaustive but it got us 90% of the way there.We also needed to get Lodash playing nicely with our toolchain. We use Bazel—an open-source, extensible, scalable build tool—to coordinate our entire build process. Static typing is enforced with TypeScript. Bazel optimizes for a deterministic build process, but doesn’t have any built-in support for tree shaking JavaScript. For the unfamiliar, tree shaking is a process by which unused code is eliminated from a bundle.We needed to pick the right tool to produce a custom Lodash build with our chosen set of functionality and an accurate subset of TypeScript types. Initially we expected the lodash-cli to provide the support we need. Unfortunately, lodash-cli doesn’t have any notion of types, does a poor job of tree shaking, and is being deprecated with Lodash 5.0.0 in favor of bundling with Webpack.Next we tried Rollup and Webpack, two popular JavaScript build/bundling tools. We were specifically interested in plugins that would allow us to minimize our bundle size as much as possible. The lodash-webpack-plugin and lodash-ts-imports-loader are both important for reducing bundle size. Webpack was the clear winner; it has much better plugin support for what we were building.Our first attempt was a single build that produced a pre-minified Lodash library and a TypeScript typings file. We had configured Webpack to produce an index.ts file that imported the Lodash functions we wanted and then reexported them for consumption by our web app.With that config we were able to produce a bundle size we were happy with(12k). However we quickly noticed that the generated typings file wasn’t going to play nicely with our actual codebase.We had assumed the build would produce a single lodash.d.ts file containing all of the types we needed within the file. This assumption was false. Instead, the build created a file that imported functions from individual modules and reexported them. It didn’t encapsulate everything under a single Lodash package so much as output a series of unlinked functions. The lodash-ts-imports-loader plugin converts from the input to the output formats below:InputOutputThis is great for treeshaking, but not so great for typing. While it produced a small javascript bundle, the types file attempted to import from individual Lodash modules. This didn’t work for us because developers needed to import from a single Lodash module. We wanted this both for developer experience and so we could serve it separately and trust browsers to cache it.The solution was splitting our build into a two-stage process. The first build created a typings file as though we weren’t doing any module splitting/tree shaking. The second build produced a properly treeshaken bundle. From this build we got a minified Lodash bundle and a typings file that looked like this:This build had a minor tradeoff in that we needed to check in a full version of the Lodash typings called lodash-full.d.ts.This allowed our developers to import * as lodash from'lodash'; and not worry about how it was built. It also provided a firewall of sorts, and caused a TypeError if a user attempted to use a Lodash function we didn’t include in our bundle.Integrating With Bazel: As soon as Webpack produced a working Lodash bundle and typings file, we turned to integrating Webpack with Bazel. We created a Bazel build file which provides rules for building both the treeshaken bundle as well as the typings:Everything worked in development mode and we were very happy.Problems with Minification and source maps: The rest of our build process didn’t like the Webpack-produced minified file instead of original source. Plus, the Webpack source maps didn’t integrate well with the rest of our Bazel-generated source maps. For a simple fix we reconfigured Uglify to produce an unminified but treeshaken file. We then let the rest of our toolchain handle minification and sourcemaps.We should note that the unminified Webpack bundle came with a fair amount of Webpack cruft: comments, dependency resolution, and other snippets Webpack needs to do its job. However we decided that letting Bazel minify and remove most of this was a reasonable tradeoff to make.The last issue we ran into involved LodashModuleReplacementPlugin. As we began the actual migration process, we realized we were stripping important functionality that some of our Underscore dependent code was using. For example we were missing Lodash shorthands which allow us to call a function like keyBy with just a string predicate instead of a function. We tweaked our config with this plugin and we were able to move on.Here are the final Webpack config files:Typings Webpack config:Treeshaking Webpack config:It was finally time for the big scary part: migrating the codebase. Everything until now had been implemented in a vacuum; we hadn’t yet gone out and touched anyone else’s code.Previously, our web platform team had automated a migration of our codebase while moving from CoffeeScript to TypeScript. We leveraged that expertise and used Codemod to help with this transition.First we needed a thorough list of Underscore functions in use. We discovered six different patterns for using Underscore and had to grep for each of them.We then constructed a table that looked like this:We took our list of functions and created a list of mappings for our codemods. We separated native and lodash replacements and added notes for nuances. The list now looked like this:Native replacements contains_.contains(list, value, [fromIndex]) => list.includes(value, [fromIndex])Lodash Replacements countBy_.countBy(list, iteratee,[context]) => lodash.countBy(list, iteratee)You can see the entirety of this research here: Underscore Replacements.We tested all of these by putting together a simple script that asserted equality.https://www.dropbox.com/s/8xptvxghqz8210r/replacements.jsWe made a point to migrate all of our application code first and do the test code separately. That allowed us to run the migrated application code against the unmigrated tests, giving us confidence that we hadn’t introduced bugs into both at the same time—hiding a problem.The actual codemods were mostly simple bash scripts that converted from one usage to another. We ran these on the entire codebase and then separated the changes into roughly ten different diffs (pull requests) organized by codebase ownership. This made it easier for us to track down owners for review.We also manually migrated code that was too complicated to convert automatically. Because we had already split the diffs by ownership, it was easy for us to show stakeholders any code we had manually migrated or that needed extra attention. For example, anything written with $u.chain syntax needed to be converted by hand. Functions like object, has, create, matches, template and object didn’t lend themselves to being automatically migrated to their Lodash or native equivalents. We also noted that forEach behaved differently for objects and arrays and we needed to be diligent here. Finally, we weren’t able to codemod imports of individual functions from Underscore.We spent a diligent week with these ten diffs open, constantly testing them, and rereading them with a thorough eye. We didn’t want to introduce any regressions with this migration.After testing, review, and gathering sign offs, we started landing the diffs. To minimize pages importing both Underscore and Lodash for any period of time we landed features related to each other. This took a couple of days and then we were technically done.After all this we had exactly one bug. There was a manual conversion on an internal tool that used splice where we should have used slice. We got a quick fix out and no external users were impacted.This was an involved process for what initially seemed like a straightforward migration. Using a lightweight proposal format before beginning work meant we were able to address the concerns of our users beforehand. Yet, some of our assumptions were wrong, and some tools didn’t do what we thought they would. Our requirements for a custom build resulted in extra steps. In the end, our approach of experimenting and testing first led to a nearly zero-problem migration. However, we knew we would have to actually teach the tool to make sure it gets used. We organized internal tech talks on functional programming with Lodash and showcased a Lodash “function of the week” in our internal frontend newsletter.We hope our research will make this process easier for the next person who attempts to migrate a large 10-year-old codebase from one library to another.",https://blogs.dropbox.com/tech/2018/09/migrating-from-underscore-to-lodash/,0,dropbox,,NULL,2018-09-05
Building better compression together with DivANS,"Compressing your files is a good way to save space on your hard drive. At Dropbox’s scale, it’s not just a good idea; it is essential. Even a 1% improvement in compression efficiency can make a huge difference. That’s why we conduct research into lossless compression algorithms that are highly tuned for certain classes of files and storage, like Lepton for jpeg images, and Pied-Piper-esque lossless video encoding. For other file types, Dropbox currently uses the zlib compression format, which saves almost 8% of disk storage.We introduce DivANS, our latest open-source contribution to compression, in this blog post. DivANS is a new way of structuring compression programs to make them more open to innovation in the wider community, by separating compression into multiple stages that can each be improved independently:The DivANS Compressor operates in 3 stages (top part of diagram):Conversely the DivANS Decompressor operates in just 2 stages (bottom part of diagram):At first glance, splitting a “single” operation into multiple steps might seem like adding unnecessary complexity. However, it’s instructive to consider the example of language compilers. Primitive compilers compiled a single programming language down to a single machine architecture, conceptually in a “single” operation. At some point in the evolution of compilers, intermediate representations were introduced which, while adding steps to the compilation process, had several advantages that helped propel the field forward:Crucially, by standardizing on an IR format, it was possible for researchers to work on all of these problems in parallel (independently). We hope to spur the same kinds of benefits for the field of compression with the introduction of the DivANS IR.In preliminary studies, we have already found concrete gains of 2% in compression ratios for many types of files by using DivANS. We describe these experiments in more detail later on in the post. In addition, we designed DivANS to also meet the following requirements:We chose to develop an IR that uses a Lempel-Ziv (LZ)-based representation of a compressed file. In an LZ representation, there are only 3 kinds of instructions for generating the stream of bytes of the original file:Given a sequence of such commands, we can exactly (losslessly) reconstruct any input stream, but not necessarily very compactly. To achieve compression, we need to be able to predict future bytes based on what we’ve seen so far. This is achieved via the use of several probability tables, that contain statistics of various types. For example, consider a file that consists of alternating chunks of bits all set to 1, and then bits all set to 0. In this case, we’d like to give the compressor a “hint” that says: “if you’re in a ’1’ chunk, then you should assume that the next bits are going to be 1 as well, but if you’re in a ’0’ chunk, then you should assume the next bits are going to be 0.”Thus, our IR defines a few kinds of such hints, which can help the compressor generate smaller files. These includeNote that these hints are completely optional—compressors and interpreters are free to ignore these lines and can still get back the original input—but they add enough flexibility to the IR to allow for many kinds of optimizations.Since there are an infinite number of IRs that describe the same stream of bytes, the key is to find the most efficient versions. In general, this is a balancing act between the extra space it takes to describe probabilities vs the space saved by using them; if this is done badly, it might be more efficient to just insert the bytes directly!IR generation exampleBelow is an example of the IR generated from a compression of the test string, “It snowed, rained, and hailed the same morning.” Blue letters indicate fresh data insertions, green letters indicates copies from previously in the file, and red letters indicate data pulled from the Brotli dictionary.Here’s what the IR looks like for this example:Notice that only the initial “It”, the first “ed”, and the “l” in hailed are inserted from “scratch”; everything else is copied, either from previously in the sentence or from the dictionary (with small alterations).Interactive IR DemoYou can see an expanded version of this example, as well as try DivANS on your own files (up to 2MB) interactively at https://dropbox.github.io/divans. Files larger than a megabyte tend to do better in DivANS. Once we have an IR file, possibly optimized in various ways, we need to serialize it to a stream of bytes that can be efficiently compressed. We rely on Asymmetrical Numeral System (ANS), a method introduced by Dr. Jarek Duda that matches the compression ratio of the more commonly used arithmetic coding, while being considerably faster. These methods offer compression performances close to the theoretical bound—for instance, we showed in our previous blog post on Lepton that transcoding JPEGs previously encoded with Huffman coding with arithmetic coding can reduce the file size by 22%.These are all entropy coding methods, which attempt to encode each symbol using a number of bits that is inverse of the probability of observing the symbol, i.e., common things are encoded using fewer bits than rarer things. There are theoretical guarantees that this is the most optimal compression scheme, but in practice, the actual compression efficiency is determined by how good your estimates of the probabilities are.Adaptively Computing ProbabilitiesDivANS provides the ANS coder with dynamically updated probabilities to achieve maximum compression. It does so by creating a table of Cumulative Distribution Functions (CDFs), known as the prior table, which describes the probabilities of different output symbols in different contexts. DivANS then updates them continuously as the file is processed. In the following discussion, note that DivANS decodes files in chunks of 4 bits (1 nibble) at a time, and each nibble is a symbol for the ANS coder.The full code is here, but the key takeaway is that several bits from recently observed bytes in the file are used to index into the prior table, where each CDF in the table can be thought of as a “situation.”Example situations could be:The situations aren’t limited to literal characters only, and can apply to the IR itself, such asDivANS uses a selected few of the bits of previous bytes to index into a large table of CDFs, represented as a cube in the diagram below. You can think of this as a lookup function that takes as input recent nibbles and outputs a probability value for the next nibble. Note that it is possible to implement a different set of rules, or even infer rules or the underlying CDF more generally with a deep-learning approach (e.g. with an RNN).After the next nibble is read, the prior probabilities are adjusted based on the actual value of the nibble. Initially, DivANS assumes equal probabilities for all outputs, and updates them as it goes through the file. One could also encode knowledge about numbers and capitalization rules to begin with, but we instead learn the CDFs without any such assumptions. As the file is encoded and similar situations are encountered over and over, the CDFs start to converge to accurate guesses for future nibbles.When decoding the compressed byte stream back into the IR, DivANS follows the same procedure to update probability tables so that they can be used to reconstruct what the original symbols would have been, given the actual bytes written to the stream. Note that decoding and encoding have to operate in opposite order of each other; for speed, DivANS encodes in reverse and can therefore decode in forward order.Test datasetsWe tested DivANS on two datasets. One was the well-known Silesia corpus, which contains medical images, giant html files, and books. However, this dataset has very different compression characteristics than the data stored in Dropbox, so we sampled 130,000 random chunks being uploaded to Dropbox. We measured the sizes and compression (encode) and decompression (decode) rates of DivANS, and several alternative algorithms (zlib, Brotli, Zstd, 7zip, bz2). For DivANS and Brotli, we compared 2 different compression levels (q9 and q11). For zlib, 7zip, bz2 and Zstd, we used maximum settings. All tests were performed on 2.6 GHz Intel Xeon E5 2650v2 servers in the Dropbox datacenters and no data chunks from the test were persisted.Since Dropbox uses Lepton to compress image files, we only benchmark files where the Lepton compressor was not effective and where zlib obtains at least 1% compression to avoid measuring compression of files that already have a compression algorithm applied. This focuses the test data set down to one third of the uploaded files.MeasurementsMeasuring the Dropbox dataset, DivANS q11 saves 12% over zlib, and more than 2.5% over other algorithms at maximum settings.If we choose to include files in the benchmark where neither zlib nor Lepton are effective, the benchmark covers all non-JPEG files, comprising two thirds of the uploaded files.DivANS usually cannot get much extra savings out of the already-compressed files, but on some of the files it can be significantly more efficient than zlib. However, 88% of the overall savings from DivANS comes from processing the third of files that zlib is most effective upon.At these settings, DivANS and Brotli have similar encoding speeds. However, DivANS decode times are still five times slower than Brotli. This is because Brotli decompresses bytes using just a pair of table lookups and some bookkeeping, whereas DivANS has to maintain probability models for each nibble decoded and uses several vector instructions to maintain the CDFs and the ANS state. We expect that a few straightforward optimizations to improve the bookkeeping will bring the speed up a bit, to more than 200Mbps.For the Silesia corpus, DivANS q11 gets 2.7% space savings over Brotli and Zstd but lags behind 7zip. However, as mentioned earlier, Silesia is not very representative of Dropbox data.Hybrid LZMA/Brotli Compression IR Mashup Notice that the 7zip LZMA algorithm does better on Silesia. This is because LZMA’s produced IR better fits one of the included binary files (mozilla), which occupies 20% of the archive. As a proof of concept, we demonstrated the flexibility of the IR to capture real-world compression gains. We added 3 print statements to the LZMA codec to produce the standard DivANS IR in its compression routine. Then we wrote a script to interleave the hints from Brotli with the IR from LZMA. The result from mashing up the Brotli and LZMA IR’s improves the overall compression of DivANS beyond the the other algorithms in Silesia. Automating and productionizing this technique would be likely to improve both compression speed and ratio in aggregate.Research directions The DivANS system today is already able to get a better compression ratio than alternatives on Dropbox data (albeit with a performance cost). However, we are even more excited about opening up the code and the DivANS Intermediate Representation (IR) to the world. We hope that by lowering the bar for creating new compression algorithms, the community can focus compression research on three separate directions:Using Rust: fast, reliable, productive: pick 3 DivANS is written in the Rust programming language, since Rust has guarantees about safety, security, and determinism in the safe subset of the language. Rust can also be as fast as hand-tuned C code and doesn’t need a garbage collector. Rust programs embed well in any programming language that support a C foreign function interface (FFI) and even allow runtime selection of the memory allocator through that C FFI. These properties make it easy to embed the DivANS codec in a webpage with WASM, as shown above.Rust also recently added support for SIMD intrinsics and has intuitive, safe support for multithreading, colloquially called “fearless concurrency”. We use vector instructions to manipulate the CDFs. We decode the IR commands and the literals themselves each on separate threads. The entire multithreading initiative was started after the decompressor was completely written, and it leaned heavily on rustc‘s guidance to refactor the code into independent halves.Call to action Ideally, several disparate algorithms would generate DivANS IR (not just Brotli, and not just in Rust), and ideally other compression algorithms could also take the DivANS IR as input and produce an efficient bitstream from that input. Pull requests welcome!",https://blogs.dropbox.com/tech/2018/06/building-better-compression-together-with-divans/,0,dropbox,"frontend,css",NULL,2018-06-19
Extending Magic Pocket Innovation with the first petabyte scale SMR drive deployment,"Magic Pocket, the exabyte scale custom infrastructure we built to drive efficiency and performance for all Dropbox products, is an ongoing platform for innovation. We continually look for opportunities to increase storage density, reduce latency, improve reliability, and lower costs. The next step in this evolution is our new deployment of specially configured servers filled to capacity with high-density SMR (Shingled Magnetic Recording) drives.Dropbox is the first major tech company to adopt SMR technology, and we’re currently adding hundreds of petabytes of new capacity with these high-density servers at a significant cost savings over conventional PMR (Perpendicular Magnetic Recording) drives. Off the shelf, SMR drives have the reputation of being slower to write to than conventional drives. So the challenge has been to benefit from the cost savings of the denser drives without sacrificing performance. After all, our new products support active collaboration between small teams all the way up to the largest enterprise customers. That’s a lot of data to write, and the experience has to be fast.As with our initial Magic Pocket launch, we attacked the problem through inventive software and server architecture to ensure that this solution matched our standards for annual data durability of over 99.9999999999%, and availability of over 99.99%. Our ambition here is to use our expertise in software for large distributed systems to enable us to take advantage of the ongoing developments in drive technology before our competitors. We believe that future storage innovations, including solid-state-drives (SSDs), will benefit from the same architectural approaches we’re developing for SMRs, so that our investment now will pay off in multiples.In this post, we’ll describe our adoption of SMR HDD technology in the Dropbox Storage platform, Magic Pocket. We discuss why we’ve chosen to use SMR, hardware tradeoffs and considerations, and some of the challenges we encountered along the way.Conventional Perpendicular Magnetic Recording (PMR) HDDs allow random writes across the entire disk. Shingled Magnetic Recording (SMR) HDDs offer increased density by sacrificing random writes for forced sequential writes. Squeezing the tracks on SMR disks together causes the head to erase the next track. A small, conventional area at the outside diameter allows for caching random writes as well as using SSD.SMR HDDs offer greater bit density and better cost structure ($/GB), decreasing the total cost of ownership on denser hardware. Our goal is to build the highest density Storage servers, and SMR currently provides the highest capacity, ahead of the traditional storage alternative, PMR.There are three types of SMR HDDs to consider: Drive/Device Managed, Host Aware, and Host Managed SMR disks. While we originally evaluated both Host Aware and Host Managed SMR disks, we finally settled on Host Managed disks for our fleet.Drive or Device Managed SMR disks allow the host to treat them like conventional drives. Non-sequential writes are buffered in the small conventional area on each disk, and then later on transcribed to the sequential zones. This involves reading the data from the sequential zone and writing the original data merged with the new data back to the sequential zone.Host Aware drives allow a host that understands SMR disks to control the writing of the sequential zones. Hosts can open and close zones, monitor write pointers, partially write sequential zones and avoid conventional area caching and performance bottlenecks caused by rewriting zones. Host Aware drives offer more control than Drive Managed SMR, which is our defining priority.Host Managed SMR drives require the host to manage the sequential zones on its own. The drive does no copying of new data to sequential zones and no caching of data in the conventional area. Hosts must explicitly open, fill and close sequential zones. Host Managed SMR offer the most control over the way data is stored on the drive and is consistent with how we have built things before.Magic Pocket (MP) stores user data in blocks, with ~4MB being the max size. Blocks are organized in 1GB extents and the MP platform operates accordingly. Since these extents are written in an append-only fashion, and are immutable, the sequential writes of SMR are ideal for MP’s workload. Check out our blog post for an in-depth overview of Dropbox’s Magic Pocket architecture.While implementing SMR disks, there were a number of hardware trade-offs we had to consider. We worked to optimize performance and data transfer speeds, but also needed to consider hardware reliability and total cost of ownership. This required us to look at just about every element of our hardware stack.Chassis density Our latest design fits approximately 100 LFF (Large Form Factor) disks in a single chassis, which makes them the densest storage system in production. The 100 LFF disk per chassis had design constraints of a physical limit of 4U in rack space in a 19” standard datacenter rack and a requirement to stay at a 42” depth limit. This keeps us from needing to design custom datacenter racks. We keep each rack limited to housing 8 fully configured chassis to avoid deviating from standard datacenter flooring specifications.Memory and CPU One thing that came out of our testing was the decision to increase the memory to 96GBs per host. We did this because we keep an in-memory index of blocks and their offsets/length on the disk. With 14TB SMR drives we significantly increased the capacity of an individual chassis; each machine will store 2.29 times more blocks than a chassis in our previous architecture. This means that our block index will need a proportional increase in memory resulting in 96GB per machine.We also had to slightly upgrade our CPUs by moving from 16 to 20 cores, 40 threads, per chassis. The additional processing power was necessary to keep the total chassis I/O performance above 40Gbps and 45Gbps for writes and reads.SAS Controller In order to further improve reliability and reduce complexity, we moved from a RAID controller to a Host Bus Adapter (HBA). The initial benefit to using RAID was to leverage the cache to reduce write latency. This proved a costly endeavor with a lot of overhead: we were creating individual RAID 0 and managing associated firmware and bugs in the raid controller only to expose a single block device. We also worked to enable Direct I/O to reduce CPU usage from double-buffering.An added benefit of removing the overhead of creating so many RAID 0 devices was cutting the overall provisioning time for this storage system from up to 2 hours total down to a quick 30 minutes. This allows us to focus more on realizing the technology and less time setting it up.Adding the HBA simplified our architecture at the expense of our initial cache device. We understood that any emerging technology is an exploration into the unknown. In order for us to reduce the amount of exposure we focused on removing complexity as a success criteria.Cache With the removal of the RAID controller, we discovered that we needed to compensate for the loss of write cache. Our solution to this was to add our own caching layer to maintain the performance requirements. An SSD drive could compensate for the decision to remove the RAID controller.In previous generations of storage systems, data from network had been directly written to the drives. Writing to large SMR drives is time-consuming so we needed to ensure the network doesn’t get stalled when the drives are busy. To make this process asynchronous and non-blocking, we added an SSD to cache the data which is then lazily flushed to the SMR disks in the background.While this design works for us now, we are seeing that as the density increases we are saturating the SATA bus and have a need to use another transfer protocol. We’ve discovered that we are pushing the limits of the SATA bus and it has become a bottleneck we can see from our SSDs. Future generations will likely have an NVMe design for caching.Network Magic Pocket started with lower density chassis (around 240TB). We moved up in density over time as network speeds increased and we didn’t have to compromise recovery time for density, which is a lever to lower TCO.Using 14TB SMR disks put the new chassis at 1.4PB per host. This level of storage density required another increase in network bandwidth to assist with system recovery. We are comfortable with an acceptable increase of the failure domain as long as the recovery time meets our SLA. We decided we needed to design the SMR based chassis with a 50Gbps NIC card per chassis and a non-blocking clos fabric network with 100Gbps uplinks. The benefit we gained is the ability to quickly add data to the chassis upon deployment and the ability to quickly drain the chassis in times of repair, ensuring Magic Pocket meets its SLA.Per the Magic Pocket design, Object Storage Devices (OSD) is a daemon the behaves very similar to key value store optimized for large-sized values. We run one daemon per disk per machine and only that daemon has access to the disk. OSD treats disks as a block device and directly manages data layout on the drive. By not using a filesystem on SMR, we are able to optimize head movements and prioritize disk IO operation based on the type fully within the software stack. To communicate with the SMR disk, we use Libzbc as the basis for disk IO.SMR stores metadata indexes on sequential zones. We got lucky with two factors. First, the capacity sizes evenly divide across 4 zones of logical space (256MB x 4 = 1GB). Had this not been divisible by 4, any excess space would have been lost or required more invasive changes to reclaim that space. Second, the ratio of metadata to block data is 0.03% which fits well with ratio of conventional area and sequential area.We discovered large writes are much better for SMR (averaging 4-5 MB). To optimize here, writes are buffered in certain stages. Originally, we tried flushing from SSD to SMR as soon as possible with multiple small writes, but this was not efficient, so we moved to model of buffering and less writes of larger size. Our highest priority was managing live traffic. The live traffic is made up of incoming new blocks and writes to support that user data. One of the biggest challenges here is latency!All the writes to the disk must be sequential and aligned to the 4k boundary; however, when the live data comes in, it doesn’t always fit into neat 4k chunks. This is where using a staging area comes to our rescue. The background process of flushing these blocks from SSD to disk takes care of alignment and making sure we do large writes.Managing background repairs, which require a huge amount of reads and writes, is less time-critical, so they can occur more slowly.The chief challenge of making a workload SMR-compatible is taking a random read/write activity and making it sequential. To accomplish this, we rewrote OSD so that metadata, which is frequently updated, is kept in the conventional area of the SMR disk where read/write writes are supported, while the immutable block data is kept on sequential zones.We needed to overcome dealing with sequential writes of SMR disks, which we accomplished with a few key workarounds. For example, we use an SSD as a staging area for live writes, while flushing them to the disk in the background. Since the SSD has a limited write endurance, we leverage the memory as a staging area for background operations. Our implementation of the software prioritizes live, user-facing read/writes over background job read/writes. We improved performance by batching writes into larger chunks, which avoids flushing the writes too often. Moving from Go to Rust also allowed us to handle more disks, and larger disks, without increased CPU and Memory costs by being able to directly control memory allocation and garbage collection. Check our presentation on how we used Rust to optimize storage at Dropbox to learn more.Through ongoing collaboration with our hardware partners, we leveraged cutting edge technology to ensure our entire chain of components were compatible. In our configuration, we used an expander to distribute the HBA Controller, allowing the HBA to evenly spread connection to all the drives. However, the expander was initially incompatible with SMR. In this case, and others like it, we collaborated with vendors and co-developed the firmware needed to create functional harmony within the hardware chain.One of the mechanical challenges of having an average of 100 drives in a chassis is that we are limited in how many hardware variations we can make when bottlenecks are discovered. Space is our limiter now, so looking at components that fit in the system design will present new challenges in the future.This new storage design now gives us the ability to work with future iterations of disk technologies. In the very immediate future we plan to focus on density designs and more efficient ways to handle large traffic volumes. With the total number of drives pushing the physical limit of this form factor our designs have to take into consideration potential failures from having that much data on a system while improving the efficacy of compute on the system.We’re committed to iterating and improving on Magic Pocket and the Dropbox infrastructure, and this deployment is just one step along the way. This has been an exciting and challenging journey introducing new storage technology in a reliable way. This journey involved not only thinking about the mechanical structure of a system, but also the major software updates that would be required. Without the collaboration between the engineering teams this would not have been possible. Our infrastructure will benefit twofold, thanks to greater density and a better cost structure unleashing our Dropbox user’s creative energy.Project Contributors: Chris Dudte, Victor Li, Preslav Le, Jennifer Basalone, Alexander Sosa, Rajat Goel, Ashley Clark, James Turner, Vlad Seliverstov, Sujay Jayakar, Rami Aljamal ",https://blogs.dropbox.com/tech/2018/06/extending-magic-pocket-innovation-with-the-first-petabyte-scale-smr-drive-deployment/,0,dropbox,"backend,frontend,python,cloud,docker,animation",NULL,2018-06-12
"Security culture, the Dropbox way","The Dropbox Security Team is responsible for securing around 1 exabyte of data, belonging to over half a billion registered users across the world. The responsibility for securing data at this scale extends far beyond the Dropbox Security Team—it takes a commitment from everyone at Dropbox to safeguard our users’ data every day. In other words, it takes a strong security culture.The first core company value at Dropbox is “Be Worthy of Trust.” From a security perspective, this means keeping our users’ stuff safe. Our culture of security is built on this foundation of trust and is a fundamental part of our identity. We have a dedicated Security Culture Team whose mission is to cultivate an environment where our employees make consistently secure and informed decisions that protect Dropbox, our users, our employees, and our physical spaces.To build their security cultures, companies have adopted approaches ranging from gamification to mandatory security trainings. At Dropbox, we do those things and more. Each year, the Security Team throws one of the largest companywide events at Dropbox, called Trustober, held during National Cyber Security Awareness Month in October. And yes, you read that right—the Security Team throws one of our biggest bashes!While nurturing a security culture is a year-round job, Trustober brings Dropbox employees together to learn more about ways we protect Dropbox, our users, and one another. It’s a month that helps us celebrate a culture of security both in the workplace and outside the office.During Trustober 2017, the Dropbox Security Team designed and held over 30 security programs globally. These ranged from short talks and Q&As to daylong workshops on topics related to security, safety, and trust, including:The majority of our talks and workshops are created in-house with a strong team of volunteers, and are shared across our offices globally. We’ve also been lucky to have friends from the security community, including Adam Shostack, David Molnar, Charlie Reis, Brad Hill, and Frans Rosén, share their stories and research with our employees.One way we do things differently at Dropbox is by creating immersive experiences that make security and safety top of mind for our employees. Our goal is to spark curiosity and a security-first mindset in our employees through creative, fun, and engaging programs.For instance, while social engineering is a familiar concept, Verizon’s 2018 Data Breach Investigations Report found that over 90% of data breaches have a phishing or social engineering aspect to them. During Trustober, a number of Dropbox employees volunteered for a daylong social engineering workshop designed and led by internal experts that immersed them in a hypothetical scenario involving a malicious insider. Participants conducted an investigation together in a fast-paced and collaborative exercise that took them out of their daily roles and routines. When asked for feedback, one employee shared:“Social engineering is everywhere and anyone can be a target or a social engineer… it was very eye opening (and a ton of fun!) to see how it’s done in real life instead of a Hollywood movie.”At Dropbox, we’ve worked to build a positive culture around reporting potential phishing emails by encouraging employees to report anything suspicious, running regular test campaigns, and holding fun workshops. During Trustober, we ran a hands-on workshop where Dropbox employees researched, crafted, and presented their own phishing schemes. By teaching them how to build targeting phishing schemes, our goal was for employees to understand what makes a phishing email look legitimate.We’ve also partnered with speakers such as Dr. Mark Baldwin, an international expert on the Enigma Machine, to bring immersive workshops on security-related topics to Dropbox. These experiences provide a deep dive into security from multiple angles. Dr. Baldwin, nicknamed “Dr. Enigma,” illustrated how human error, procedural flaws, and leaks of key information enabled the Bletchley Park team and others to crack the Enigma machine’s ciphers, despite its technical sophistication. Not only did Dr. Enigma’s talk illustrate how an organization can only be as secure as the people who are operating or taking care of it, but it also provided our employees with a historical and hands-on opportunity to understand the importance of their personal responsibility in keeping Dropbox and our users secure.A highlight of Trustober is our annual Capture the Flag (CTF), a competition which provides employees with a fun, hands-on opportunity to solve security-related puzzles. By teaching employees how to recognize potential security flaws, we get our employees excited about security and help them practice their offensive thinking. At Dropbox we design and run our CTF internally, a herculean effort which you’ll hear more about in this series of blog posts. In 2017 over 200 employees participated in the CTF, which focused on topics ranging from disk forensics to writing XSS payloads that bypass CSP. After the CTF, this was our favorite survey response:An event like Trustober also gives Dropbox an opportunity to celebrate our culture of safety by providing our employees with opportunities to learn about their physical safety, both within the workplace and at home. In 2017 we ran a number of First Aid and CPR certification workshops in partnership with the American Red Cross and the Irish Heart Foundation. Over 200 employees signed up for the voluntary workshops, and received certifications for first-aid and CPR from these organizations for successfully completing the courses. Our newly-certified Dropbox employees now help support their workplace with a high level of emergency preparedness.How do we know our efforts with security culture are making a difference? We look at the overall impact we’re driving, including conversations on internal company channels, attendance at events, and questions posed to our team, and solicit feedback from our employees.One way to observe engagement is by seeing internal discussions on company channels around badging, tailgating, and security challenges within a CTF. These may be difficult to measure but indicate we’re sparking security-oriented thinking within our company. Our events range from small, curious crowds to audiences of over 100, and it’s important to note the challenge of encouraging attendance amidst all other commitments employees have.Another way to analyze engagement is to survey your employees directly. Over 90% of Dropbox employees who responded to our survey found the content of Trustober helpful for security and safety in their role and workplace, including the following responses:“I found all the sessions I attended very interesting and educational. It’s amazing how much we take security for granted at Dropbox and how important it is continue to be vigilant, because the bad guys are always learning too!”“Security and trust is something that affects every [employee] and it’s for each of us to own it.”The scale of running something like Trustober is important to highlight, particularly if you’re interested in creating a similar event. Launching and running Trustober 2017 took 130 employees who volunteered across nearly a dozen Dropbox offices, and required close coordination and communication throughout. However, we’ve now created dozens of hours of security knowledge and resources in the talks, workshops, and programs that will help Dropbox employees continue to learn as we grow.Looking to start a security culture program at your company? Get started by diving into your current state of security behaviors, identifying where you’d like to move the needle, and defining your goals. It’s important to build your approach with your company’s culture in mind, and partner closely with those who can help you. When it comes to getting support, we recommend sharing the program and its goals broadly with your employees. Overall, it takes a significant amount of resources, skills, and support to build a transformative program that focuses on culture beyond awareness.The good news is that we’re in this together. As one of our employees put it best in their feedback on the state of security, “Holy crap, it’s crazy out there!” Whether you’re a security professional or simply curious, we encourage you to share your ideas, feedback, and questions to find ways to help us all stay secure.",https://blogs.dropbox.com/tech/2018/06/security-culture-the-dropbox-way/,0,dropbox,"frontend,animation,docker,python",NULL,2018-06-01
How we’re winning the battle against flaky tests,"Testing is a crucial part of maintaining a code base, but not all tests validate what they’re testing for. Flaky tests—tests that fail sometimes but not always—are a universal problem, particularly in UI testing. In this blog post, we will discuss a new and simple approach we have taken to solve this problem. In particular, we found that a large fraction of most test code is setting up the conditions to test the actual business-logic we are interested in, and consequently a lot of the flakiness is due to errors in this setup phase. However, these errors don’t tell us anything about whether the primary test condition succeeded or failed, and so rather than marking the test as failing, we should mark it as “unable to reach test condition.”We’ve operationalized this insight using 2 simple techniques:This has led to a significant reduction in flakiness, in turn reducing maintenance time and increasing code test coverage and developers’ trust in the testing process.End-to-end tests are a powerful tool for verifying product correctness, by testing end-user functionality directly. Unlike unit tests (their better-known counterparts), which test individual components in isolation, E2E tests provide the closest approximation to the production environment, and their success is the best guarantee for sane functionality in the real world. E2E tests are relatively easy to write, as they require only knowledge of the product’s expected behavior and don’t require (almost) any knowledge of the implementation. Well-written E2E tests reflect and describe the application behavior better than any existing spec, and they allow us to undertake significant code refactoring by providing a safety net that the product continues to behave as expected.At Dropbox, we use Selenium WebDriver for web E2E testing. Selenium sends commands to a web browser which simulate user interactions on a website. This allows the developer to verify a specific E2E functionality on the website. In Dropbox’s case, this includes actions such as adding, removing, or sharing a file; authentication flows; and user management. A typical test will describe a sequence of actions taken by a user in order to perform a certain operation, followed by a verification to ensure success. These actions can include navigating to pages, mouse clicks, or sending key strokes. Verifications are usually done by assertions of specific attributes of a web page that prove the success of the operation, such as success notifications or updates to the UI.For example, let’s say we want to test whether a user can successfully share a file. An E2E test for that might specify a sequence of actions such as:The test would then check to see if the notification of a successful share was displayed.(While the solution for flaky tests we will describe in this post can be applied to any testing framework, we will focus on Selenium tests, as that is where we have found the most use for it.)In order to better understand the problem of flaky tests and our solution for them, let’s look at a real E2E test: verifying that a Dropbox Team Admin can delete a Dropbox “group” (Dropbox teams can assign their users to groups). Here is an animated gif of screenshots depicting the process of creating a group, adding a users to it, and then deleting the group:The code to test this flow might look something like this:There’s a lot happening here, but most of it is setup—we’re creating a test team, creating a group within that team, adding members to it, and only then do we actually do the main purpose of the test: deleting the group and checking to make sure that it got deleted. Errors anywhere prior to that last step don’t tell us anything about whether the delete_group() functionality is correct or not.Furthermore, for this particular function (and many others that we would want to test), the amount of code executed during setup is much larger than the pieces being tested, and so if bugs were distributed evenly, we would expect failures of this test case to more likely be caused by irrelevant things rather than the delete functionality itself.How do we deal with this issue, and in a general way?Let’s take a step back and think about tests in general. A test is an experiment that aims to validate proper functionality of the system by demonstrating that an expected outcome occurs when a particular factor is manipulated. You can think of it like this:In our case of test_delete_group:Conventionally, the outputs of a test are either success or failure, based on whether any part of the test causes an error. But what happens if there’s an error even before we get to the test condition of if A exists ?Let’s look at an analogous situation: imagine an experiment to test if lightning transmits electricity. To do this, we’ll measure the current through a metal rod placed on top of a tall building during a storm. However, if lightning never strikes the rod, we cannot conclude either that lightning does or does not transmit electricity, since the conditions for the experiment weren’t satisfied.We call this result fail to verify, leaving us with the following possibilities for the outcome of a test:To implement this logic into our tests, we have to do two things:For the first step, we introduce a simple semantic addition to designate parts of a test as “under test.” In python (our primary language), this can be implemented as a context manager which we call under_test. This is used for the critical sections of code and wraps raised exceptions as UnderTestFailure. Here’s how test_delete_group looks with this new code construct (new code is bolded):For the second step, let’s look at before and after versions of how our tests are evaluated.Before:After:Note that the changes in the test code are extremely minimal: the critical section has just been placed inside a with under_test() block, and the rest of the code remains the same. However, this has a big impact on failures. The original code had 7 significant lines of code in the test, of which the new version moved 2 into the critical section. If we assume failures are evenly distributed across lines of code, then 5/7 of the errors in the original code would have actually been irrelevant to the functionality we are testing. And in practice, some of the setup code (such as setup_team()) is way more complex, thus often resulting in an order of magnitude reduction in the number of failures that fall inside the critical section!How does this simple change affect various scenarios? Let’s take a look at some common patterns:Our methodology is very effective with flakiness but we’ve introduced the possibility of missing some real bugs. In particular, consider this example:The bug in the non under_test() section will not be discovered by this test since the test gets marked as fail to verify. But this is true only locally—when we consider the entire test suite, we would hope that another test would include the bug from this test, inside an under_test()  section, so that the bug is actually caught and eventually fixed. Thus, we must follow a new rule: every piece of code that is no longer inside the under_test() block must be covered with its own dedicated test where it is under_test().In our test_delete_group example from above, the non-critical setup pieces such as team and group creation are, in fact, tested in other tests dedicated to those operations, such as test_create_group.In order to best utilize tests, we run them as part of an automated deployment environment. Understanding the deployment process is crucial for understanding the environmental effects of flaky tests, and how the changes described above help remediate these effects.Dropbox uses Continuous Integration (CI) across the organization. For each codebase, engineers commit code to the same mainline branch. Each commit (also called “build” at Dropbox) kicks off a suite of tests. Generally speaking, as a software organization grows, CI and well-written test suites are the first line of defense for automatically maintaining product quality. They document and enforce what the expected behavior of code is, which prevents one engineer (who may not know or regularly interact with everyone who commits code) from unknowingly mucking up another’s work—or from regressing their own features, for that matter.We went into some detail on our CI system in previous blog posts: Accelerating Iteration Velocity on Dropbox’s Desktop Client, Part 1 and Part 2. Here we will briefly review a few pieces that are relevant to us right now.Our test and build coordinator is a Dropbox open source project called Changes, which has an interface for each test suite that looks this:Each bar represents a commit, in reverse chronological order. The result could be totally-passing (green) or have at least one test failure (red), with occasional system errors (black). The time it took to run the job is represented by the height of the bar. The test suite being run is quite extensive, and includes both unit tests as well as E2E tests. Thus, it runs on a separate cluster of machines, and currently takes tens of minutes to run.At first, the workflow at Dropbox to add a new commit to the mainline branch was as follows:However, we started to get cascading failures increasingly often with this system: notice the sequence of red builds on the left and right sides of the above screenshot. This happened because if one build had an error and thus failed the test suite, the next several would most likely fail as well, since in the time it took to run the full test suite, several other commits would have been added to the mainline branch, all of which include the failing code from the first build.So we added an intermediate stage in the commit process: a “Commit Queue” (CQ). After passing unit tests, new commits now first have to go through the CQ, which runs the same suite of tests as on the main branch. Only builds that pass the CQ are submitted to the main branch, where they are again tested. This prevents cascading failures on the main branch, since each build has already been tested before being added. In the example above, the first bad build would have never been added to the mainline branch, since it would have been caught by the CQ. All subsequent builds would have gone through just fine (assuming they didn’t contain bugs of their own).Flakiness is the most common problem with E2E tests. What is so bad about flaky tests? Flaky tests are useless because they don’t provide a reliable signal on the particular code being tested. However, things get worse in the context of our Commit Queue (CQ), since a red build blocks engineers from landing code to the mainline, even if their code was fine, but a flaky test falsely marked the build as bad. Excessive flakiness can cause engineers to start losing faith in the entire CI process and starting pushing red builds anyways, in the hope that the build was red just due to flaky tests.In part, flakiness is a logical price for trying to simulate a production environment that has a lot of indeterministic variants, in contrast to unit tests that run in a sterile mocked environment. For example, in the production environment, a delay of a few seconds with an occurrence rate of once per million per operation might be tolerable. However, in our CI system, if we have 10,000 tests, each composed of 10 operations, this might result in a red build 10% of the time. This is not just specific to us; Google reports that 1.5% of all test runs report a “flaky” result.In our CQ, we try to discover flaky tests by rerunning failing tests and seeing if they succeed on rerun after failure. In the old system, if a test failed on retry, we would mark the test (and build) as truly failed; whereas if it succeeded on retry, we mark the test as flaky and wouldn’t include its results in evaluating the success of the build as a whole. We then moved the test into a quarantine, meaning it would not be evaluated on any future CQ builds, until the test was fixed by its author.In practice, fixing flaky tests would often take quite a while, since by definition they contain issues that only surface occasionally. And for the entire duration of repair, our test coverage was reduced. Furthermore, we found a fairly high rate of these tests remaining flaky after a “fix”—25% by some internal estimates. Over time, the quarantine would grow quite large, as engineers struggled to fix flaky tests fast enough to keep up with the discovery of new flaky tests.With the new under_test framework, only failures that are raised  under_test()  result in Failure  and block the commit queue. Failures outside the critical sections now return fail_to_verify and are skipped, meaning that they do not block the commit queue. There is no longer a quarantine; all future builds run all tests, including those previously returned fail_to_verify. Of course, tests which frequently return this error are marked and investigated to try to fix permanently, but now there is no urgency to do so right away.What happens when we run an entire test suite? Let’s say we have 10,000 tests, 0.1% of which fail due to real bugs in the code. In addition, let’s assume a 1.5% rate of flakiness in the other tests. Due to the time it takes to fix flaky tests and their cascading failures in the old approach, we might have as many as 10% of tests in quarantine at one time. Finally, let’s assume that only 10% of test code is inside an under_test() critical section.Let’s see what happens when we run this test suite through both old and new approaches:Notice that with the new approach, we not only reduced the number of failures due to flakiness (from 135 to 15), we also increased both our test coverage in successful cases (from 8,856 to 9,840) and the number of real bugs caught!By introducing a framework of less than 20 lines of code, we expanded our testing outcomes to include a new fail_to_verify result. We could then remove the “quarantine” for flaky tests from our continuous integration system, resulting in an improvement in all test metrics. In particular, we reduced flakiness by more than 90% from our test suite, transforming it from a lethal disease into a chronic—but treatable—condition. We hope this approach will prove useful to others.",https://blogs.dropbox.com/tech/2018/05/how-were-winning-the-battle-against-flaky-tests/,0,dropbox,java,NULL,2018-05-16
Introducing WebAuthn support for secure Dropbox sign in,"The easiest way to keep a secret is to not tell it to anyone. Unfortunately passwords don’t work that way. Every time you sign in you have to tell the website your password, making it more challenging to keep the secret safe. That’s why we recommend turning on two-step verification for your account, which adds an extra layer of difficulty for anyone who has guessed, eavesdropped on, or tricked you into giving them your password. And it’s why we’re excited today to announce support for WebAuthn (“Web Authentication”) in two-step verification, a new standard for strong authentication on the web.In most forms of two-step verification, a user enters a one time code after providing their username and password, and before being signed in. While easy to adopt, using one time codes for two-step verification has weaknesses. For example, a fake Dropbox sign in page could ask for your username, password, and the two-step code. That’s why Dropbox was one of the first services to adopt Universal 2nd Factor (U2F) for security keys in 2015. Security keys prevent phishing by giving Dropbox cryptographic proof that you both have your key and are using it on https://www.dropbox.com (instead of a phishing page).Introducing WebAuthn This cryptographic proof makes U2F security keys a very strong form of two-step verification, but adoption of U2F has been limited by browser and hardware support. We hope WebAuthn will change that. It’s a new way to interact with security keys and other “authenticators” that standardizes and builds on key parts of U2F, the result of a collaboration between the W3C and FIDO Alliance. While for years only Chrome supported U2F, browser vendors have committed to bringing WebAuthn to Chrome, Firefox, and Edge. More and more devices will have WebAuthn support built in, bringing stronger security to the many users who don’t own special security keys. These could include your laptop or phone, which might prompt you for your fingerprint or a PIN code as part of the authentication process. But this only matters if services actually let you use WebAuthn to securely sign in. Today, Dropbox is proud to help lead the way.What does this mean for me? You’ll now be able to use more types of security keys on more browsers for two-step verification. That starts with support for security keys in Firefox 60, releasing on May 9th. You can use security keys previously registered with U2F and register new ones with WebAuthn. Chrome and Edge support for WebAuthn will be coming soon, and you can still use your security keys in Chrome today with U2F.This means that as a user, you’ll enjoy much stronger sign in security on more browsers. Unlike passwords, the secrets used in WebAuthn never leave your security key, so they are significantly harder to steal. And before using a secret to authenticate to Dropbox, the security key checks that you are signing in to the right place. You can feel confident when signing in that it’s really us, and we can be confident it’s really you.Will this replace passwords? Right now, we’re using WebAuthn to make it easier for you to add an extra level of security to your account. A natural question is if we still need passwords too. Your credentials could be stored on a device like your phone, laptop, or security key, and services could use WebAuthn to sign in to your account after you scan your fingerprint or input a PIN on the device. There are still many security and usability factors to consider in these scenarios before replacing passwords entirely, and we believe that enabling WebAuthn for two-step verification strikes the right balance for most users right now.How do I use WebAuthn? To start using security keys that support WebAuthn on Dropbox, take a look at our Help Center article.Curious to know more? We collected a few helpful references with more technical details for you below:",https://blogs.dropbox.com/tech/2018/05/introducing-webauthn-support-for-secure-dropbox-sign-in/,0,dropbox,,NULL,2018-05-08
MacOS monitoring the open source way,"Let’s say a machine in your corporate fleet gets infected with malware. How would you detect it? How could you find out what happened on the machine? What did the malware do? Did it steal your browser’s passwords? What network connections did the malware make? Was it looking for crypto currency? By having good telemetry and a good host monitoring solution for your machines you can collect the context necessary to answer these important questions.Proper host monitoring on macOS can be very difficult for some organizations. It can be hard to find mature tools that proactively detect security incidents. Even when you do find a tool that fits all your needs, you may run into unexpected performance issues that make the machine nearly unusable by your employees. You might also experience issues like having hosts unexpectedly shut down due to a kernel panic. Even if you are able to pinpoint the cause of these issues you may still be unable to configure the tool to prevent the issue from recurring. Due to difficulties like these at Dropbox, we set out to find an alternative solution.One of the first things we did was create a list of requirements and success criteria:During the investigation we reviewed a number of tools that could solve some of our problems, but none of the tools could solve all of our problems. After careful review we decided that we didn’t want to reinvent the wheel and that having multiple tools that each solved a specific requirement would better serve our needs.We eventually landed on 3 open source tools: osquery, Santa, and the OpenBSM/Audit system; with each tool serving a specific purpose:osquery is an open source operating system instrumentation framework for Windows, macOS, Linux, and FreeBSD by Facebook. This tool allows users to query the state of their system via a SQL interface. Some of the useful features of this service are:Using osquery we can perform queries to search for IOCs (Indicators of Compromise) on a host such as the recent Proton malware:With osquery, we can get a lot of information about the current state and possibly the previous states of the machine. This still leaves us with a gap; what about events that occur between scheduled OSQuery queries?Here comes Santa Claus 🎵!Santa is an open source tool developed by Google specifically for macOS. It provides information on executed processes and some disk events. For processes, Santa can provide the following info:Another powerful feature that we won’t cover here is Santa’s ability to prevent execution of binaries (binary blacklisting and whitelisting).Using the data we collect from Santa we can investigate most execution actions performed on hosts. Interestingly, this lets us see execution events from the recent Proton malware such as the exfiltration (“exfil”) process:We can even see what was exfil’d from hosts, such as 1Password vaults, Chrome browser history, etc.Using the sha256 hashes provided in the Santa logs we can investigate the reputation of some of the files dropped by Proton.With osquery and Santa we have a really good picture of the executions that occur on a host. However, we are still missing some information about what actions are performed by specific applications with respect to network connections and filesystem interactions. osquery can give us some of this information querying the process_open_files table or the process_open_sockets table but there is still a chance we could miss events that happen between query intervals. Therefore, we need a real-time pipeline like the one that Santa gives us.To get this data we leverage the OpenBSM/Audit (or audit) system. This subsystem is built into the macOS kernel and is based on OpenBSM. OpenBSM/Audit provides a real-time stream of information about the host’s activities. During configuration of audit, we will tell audit which audit class of system calls you want it to monitor. For example, if you wanted to monitor network events you would utilize the nt audit class. The nt audit class will create a stream of data in a binary format where you can use another tool provided with audit called auditreduce. This gives the ability to filter out information to specific audit events from the class and convert the binary data to human readable XML-formatted logs.After setting up the appropriate logging services for audit, you can configure audit to produce events for the missing pieces of our puzzle. You can make it monitor for file read, file create, file write, and network events to get a better understanding of system activity that a process is making.All of the above interactions could be seen using individual events, which is great. However, what if we combine these events into something more?By using timestamps, PID, PPID, Network events, and File events we can create process trees. Each of these process trees can tell a story about what happened when this process was executed. The example above is a common attack technique using office documents with malicious macros to pull malware from the internet and compromise hosts. Once we have a clear picture of what happened via the process trees we can make judgment calls on actions performed by applications. These applications could look legitimate but, in the observed execution context, may be malicious.At Dropbox we are strong proponents for open source software and even stronger proponents for security. Being worthy of trust is our #1 cultural value and is core to our mission as a security team. If you’re interested in working on hard problems, our security team is always hiring talented people.Further Reading:",https://blogs.dropbox.com/tech/2018/04/4696/,0,dropbox,"docker,python",NULL,2018-04-26
Protecting Security Researchers,"At Dropbox, we encourage, support, and celebrate independent open security research.One way we do this is via our bug bounty program. We recently tripled our rewards to industry leading values. We also celebrated some of the amazing hacker community results with top-up bonuses, where we retroactively issued additional rewards for particularly unusual, clever, or high-impact findings.This post, however, is not about bug bounty programs. While a well-run bug bounty program is mandatory for maintaining top-tier security posture, this post is about the foundation on which bug bounty programs are built: the Vulnerability Disclosure Policy (VDP). It’s possible to have a great VDP without having a bug bounty program, and organizations should start their security journey there.Unfortunately, open security research, publication, and reporting has faced decades of abuse, threats, and bullying, such as:Anything that stifles open security research is problematic because many of the advances in security that we all enjoy come from the wonderful combined efforts of the security research community. Motivated by recent events and discussions, we’ve realized that too few companies formally commit to avoiding many of the above behaviors.Looking at our own VDP, we realized we could do better, and immediately committed to updating our VDP to be best-of-breed. Our updated VDP contains the following elements:And there’s one thing our VDP does not contain: we don’t gate researchers who wish to publish vulnerability details. Using policy or bug bounty payments to muzzle or curate scientific publication would be wrong.We’re also happy to announce that all of the text in our VDP is a freely copyable template. We’ve done this because we’d like to see others take a similar approach. We’ve put some effort in to this across our legal and security teams and if you like what you see, please use it. Similarly, if you have improvements to suggest, we’d love to hear from you.Of course, running a top-notch VDP isn’t just about the formal policy. It’s also about showing respect to researchers. We try and do this in various ways, including via prompt responses, fast payouts, transparency, and open conversations directly with our security engineers. For top bug bounty participants (for Dropbox or just generally), we invite them to visit our offices and give talks, and occasionally set up special internal contracts.We used some great references when refreshing our VDP and we’d like to give them a shout out here: HackerOne’s VDP guidelines and the US DoJ Cyber Security unit’s VDP framework. We also took into consideration recent Senate testimony of experts in vulnerability disclosure in the role hackers can play in strengthening security.In order to do our part to expand protections for researchers more broadly, we’re going to take an unfavorable view of potential suppliers who do not have VDPs protective of researchers, or do not have VDPs at all. A missing or restrictive VDP is often a sign of poor security. Conversely, a VDP welcoming arbitrary research and offering researcher protections is usually a sign of a mature security posture.We value the open security research community and have taken steps to protect researchers. We expect any company which has security as a priority will do the same. We invite the broader industry to join us in these protections and expectations.",https://blogs.dropbox.com/tech/2018/03/protecting-security-researchers/,0,dropbox,"frontend,react",NULL,2018-03-21
"Meet Bandaid, the Dropbox service proxy","With this post we begin a series of articles about our Service Oriented Architecture components at Dropbox, and the approaches we took in designing them. Bandaid, our service proxy, is one of these components. Follow along as we discuss Bandaid’s internal design and the approaches we chose for the implementation.Bandaid started as a reverse proxy that compensated for inefficiencies in our server-side services. Later we developed it into a service proxy that accelerated adoption of Service Oriented Architecture at Dropbox.A reverse proxy is a device or service that forwards requests from multiple clients to servers (i.e. backends). The most common use of reverse proxy is to provide load balancing for web applications. Additional uses for reverse proxies include web acceleration, SSL termination and various security features.Although there are many reverse proxy implementations available, companies with private clouds that manage significant volumes of traffic often build their own reverse proxy solutions. Here are some of the reasons why they build their own:Bandaid supports:Like many of the core infrastructure components at Dropbox, Bandaid is written in Go. Selecting Go allowed for tight integration with services and a shortened development cycle. Bandaid’s primary components are shown in the image below. The Request Handler sends requests to a Queue. The queues pop requests to Workers (goroutines) and the workers process them and send them one-by-one to sets of hosts we’ll refer to as Upstream/Upstreams. It is important to discuss how our queueing mechanism functions in order to understand the request handling workflow in Bandaid.Request queueing implemented inside the proxy allows for better management of overloaded backends. Bandaid always processes requests in Last In, First Out (LIFO) order. When the system is not overloaded, the queue will be empty (or almost empty). Thus there’s no real difference between popping requests from the front of the queue or the back of the queue. Where LIFO processing reduces overhead is when the system is overloaded. By processing the newest requests first—since the oldest requests are likely to time out soon—we avoid spending CPU cycles on expiring requests.Bandaid queueing can also support dropping requests once the queue reaches a configurable maximum capacity threshold. However, we don’t recommend this since it’s hard to distinguish whether a queue is full due to system overload or because of bursty traffic.Bandaid queues have two additional options that control how many requests from the same queue can be processed concurrently, and how quickly requests can leave the queue for processing.Because Bandaid always accepts TCP connections and pushes read requests into its own queues executed in the user-space, the kernel TCP accept queue [1] is always empty. One of the reasons for this decision is that clients may trigger a connection close unexpectedly, while the backend application is still processing the data. This consumes resources unnecessarily. Keeping the kernel accept queue empty and tracking the timing of connections in Bandaid queues allows to detect and propagate connection closure sooner, freeing up backend server resources. To do this, Bandaid simply fails those requests that are in the queue for more than the configurable timeout. Instead it forwards newly sent requests that have a lower probability of being closed by the client.We found it’s much easier to manage connections in the user-space queue; it gives us more control over both the queue and requests. See the scenario below for more details.Aggressive retries from the client side make the situation worse since the server is already overloaded and cannot process the extra requests. These requests will keep the kernel accept queue full, exhausting it with already closed connections.There are multiple ways to solve this problem; here are a few:At times it may be difficult to control timeouts between retry attempts on the client side (e.g. a third-party application accessing the server through an API).Bandaid implements connection management in the user-space LIFO queue by keeping the kernel queue empty. Old connections that sit in the queue for more than a specified amount of time are closed without being processed. New requests will go through for processing.Since Bandaid supports multiple queues it needs a mechanism to determine which queue to push requests to; this is done by the Request handler. Currently, the Request handler can only distinguish requests by their URL and hostname. It matches this information with a configurable list of URL patterns and hostnames that belongs to each queue. In the future we’ll have additional matching methods.Bandaid has a fixed-size pool of workers that process requests. This approach (as opposed to running an unlimited number of worker goroutines) makes it possible to precisely control upstream concurrency. The number of workers is configurable, but depends on the size of the serving set (number of healthy upstream hosts). Oversubscription occurs when this number is set much higher than number of healthy upstream hosts.In each worker loop iteration a worker pops a request from the queue, and calls the current request processor to handle the request. Since the number of workers controls how many concurrent requests can be processed, the number should be tuned so that there is enough workers to utilize the full upstream capacity. This configuration option should be chosen carefully because oversubscription will reduce the effectiveness of graceful degradation. When services in an upstream are overloaded sending more requests to that upstream will result in increased rate of failed requests or add latency. To mitigate this Bandaid drops these extra requests, keeping the load on the upstream at an appropriate level.Upstreams are composed of the following components: queues that receive incoming requests; a single dequeuer that serves as multiplexer; and a request processing work pool. Note that it’s possible to have multiple upstreams. The role of dequeuer is discussed in the section below where we talk about the various use cases made possible by Bandaid.This is an important Bandaid feature that enables canary deployments of services at Dropbox. This makes it possible to route a configurable percentage of traffic to a deployment with particular version of software (for example, send 10% of traffic to a new deployment).In the design of Bandaid multiple queues may belong to a single upstream. Each queue may have its own properties such as weight, queue size, rate limit, priority level and number of concurrent connections. There are also two extra enqueueing and dequeueing interfaces built on top of queues. These features enable weighted traffic management and prioritization functionalities in Bandaid.The enqueuer interface makes decisions about where to push requests based on queue weights. Queues with higher weights are more likely to take new requests. This allows us to implement traffic shifting as it’s shown in the next picture: 90% of traffic goes to one upstream (production) and 10% to another (canary). As mentioned earlier Bandaid supports hot config-reloading (dynamic reconfiguration without a restart). A new configuration can be applied without having to restart Bandaid. This simplifies development operations and allows us to see results within a few seconds from the push of a new configuration.The dequeuer determines dequeueing order based on the queue’s priority level. Requests are popped from higher priority queues sooner than from lower priority queues. Hence, when the system is overloaded, low priority requests are more likely to be slowed down than high priority requests.Multiple queues can share the same priority level. To ensure fairness among these equal priority queues, the priority dequeuer will semi-randomly shuffle them and will pop requests in the shuffled order.Strict priority-based dequeuing can result in starvation (i.e., requests with lower priority will never be served because there are always requests in higher priority queues). To combat this, Bandaid provides another option that controls how fairly queues are popped. At one extreme, queues are treated as if they all have the same priority; at the other extreme, the priority dequeuer will always favor high priority requests over low priority requests.Some backends may serve critical and non-critical routes from the same host. In this case performance degradation on non-critical routes may affect the responsiveness of critical routes. This is because the number of requests each individual host can handle is limited, so if it spends all its resources on serving non-critical routes, then it won’t be able to handle critical routes. One solution for this problem is to serve critical and non-critical routes from different hosts. Another approach involves performing isolation at the proxy level. This helps reduce operational overhead and minimizes the number of hardware instances.Bandaid allows configuring the following properties to control the behavior of critical and non-critical routes: rate limiting, number of concurrent connections, and queue priority.The image shows two queues in use, but there is no such limit in Bandaid—the same upstream may handle requests from multiple queues.This is a classic load-balancing use case. See the section on load balancing methods below for more details.Backend servers may have a limitation on the number of supported concurrent connections. In this case Bandaid handles all incoming connections (which can be a large number) and controls the number of connections (typically a much smaller number) forwarded to each backend process. Bandaid can be configured to reply with a specific status code when the limit of concurrent connections is reached. Below, outgoing TCP connections are being reduced by multiplexing them using Bandaid. Each host in the picture has multiple instances of clients and each client is establishing its own TCP connection with Bandaid. Bandaid reduces the number of concurrent connections when it communicates with the service by reusing inactive connections (keep-alive, http2).   Some of the services still use HTTP 1.0 and Bandaid can be used to translate the newest version of HTTP protocol to the oldest, or vice-versa.The current version of Bandaid supports multiple load balancing methods. Unfortunately, there is no perfect method that works equally efficiently in all cases. Different scenarios require different load-balancing approaches.This is a well known load-balancing method that is simple to implement. When configured, Bandaid will send the same number of connections to each host. This method doesn’t take into consideration hosts/services or connection slowness. It is likely to cause a slight imbalance when hosts perform differently. This situation is schematically shown in the image below. Red hosts perform more slowly than the green hosts. In the Round Robin implementation the number of connections processed by the red hosts will continue growing because Bandaid will send new requests to these hosts even if they are not done processing old requests.Let’s take a closer look at the following scenario: the round robin load-balancing method is used with various ratios between the number of slow and healthy backends. The probability that a worker will get stuck serving slow hosts can be found as V = KR/(1 + K*(R-1)) whereC – number of backends in bad state Lc – average latency across C machines (time between accepting the connection and finishing processing) P – number of backends in normal state T – total number of machines or T = C + P Lp – average latency across P machines. K – [0, 1] ratio between C (bad hosts) and T (total number of hosts). K = C/T, or C = KT, or P = (1-K)T R – ratio between Lc and Lp → R = Lc/Lp or Lc=R*LpThe graph at right shows a few examples of V for different ratios between slow backends and the total number of backends in upstream for various R values:You can see in the figure the rapid increase of the green line which means that slow hosts performing 50 times slower than others may consume ~70% of all capacity, even in cases when only 5% of these slow/bad machines are present.This is an effective method for backends that do not perform equally in terms of the time it takes them to process requests. Here, the load balancer needs to send a smaller number of requests to a slow host and a greater number of requests to faster/healthy hosts. The load balancing method that allows us to do so is least connections of N random choices [2].The image below shows the main principle and the steps for the method. In this example N=2 (i.e. two random choices).Least connections of two random choicesAn adverse situation can exist where an upstream host is failing requests at a high rate (especially in the case of a small serving set) and is selected as the one with the least connections. This is because the algorithm is not aware of server health or resource utilization and only cares about the number of concurrent connections.A derivative of this load-balancing technique is the Absolute Least Connections method. Because the number of connections could change while we search, we freeze (lock) the current state while Bandaid searches for the host with the least number of connections. Once found Bandaid will direct new connections to this host.In addition to this, Bandaid randomizes starting positions to add distribution across hosts that have the same least number of connections. This is done to avoid establishing all the connections with the first host from the serving set that has the smallest number of connections.For this method each worker (goroutine) belongs to a specific host in the upstream. The worker takes the next request from the queue only after the previous request has been processed by the host. This automatically reduces the load on slow hosts because each Bandaid worker is directly limited by the performance of the host.Seeing how each load-balancing method behaves and getting additional evidence that the theory works as expected is key before trying it in production. Testing requires an additional time investment in building a test environment that simulates sets of backends. Long term, testing allows us to validate code much faster and helps identify the right load-balancing method.Results of synthetic tests for load-balancing methods implemented in the current version of Bandaid are shown below. The test environment had the following conditions:Each backend host had its own processing latency shown on the following graph:The next graph shows the distribution of requests across backend servers in the upstream for each load-balancing method. As expected, the distribution of requests for the round-robin method is almost the same for each backend and doesn’t depend on their processing time. Other methods (absolute least connections, least connections of N random choices and pinning peer) send more requests to the backends with smaller processing times.Better distribution when using random choices and pinning-peer methods reduces total processing time and increases the total request rate across all backends.It is important to support the exclusion of backends that were previously tried during retry attempts. Otherwise there is some probability that future retry attempts could be made with the same bad/unhealthy hosts.The next graph shows the ratio of failed requests to total requests for each load-balancing method, with and without exclusions. In this scenario 20% of all hosts were failing hosts. A maximum of four retry attempts were configured. A failed/unhealthy host in this test environment was a host that immediately replied with an error status code.As you can see from the results in the graph above, enabling exclusions reduced the error rate for all tested methods except pinning peer. This was expected because retry attempts in the case of pinning peer won’t make much difference—the worker is host bound and will attempt all retries with one host.That’s all for this post! In the future, look out for further posts on Bandaid, including:[1] How TCP backlog works in Linux [2] The Power of Two Random Choices: A Survey of Techniques and Results [pdf]Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/,0,dropbox,"angular,frontend,php,python,react,docker",NULL,2018-03-01
Security at scale: the Dropbox approach,"The Dropbox Security Team is responsible for securing over 500 petabytes of data belonging to over half a billion registered users across hundreds of thousands of businesses. Securing data at this scale requires a security team that is not only well-resourced, but also one that can keep ahead of the expansion of our platform. We focus on scaling our own leverage, so each new security person we add multiplies the impact of our team.Over the course of this year—and beyond—we’ll go into more detail on how Dropbox approaches security and some of the projects we’ve tackled. Protecting Dropbox requires serious investments in security. We have a large number of security sub-teams and we’ll be hearing from many of them. Some of the themes we’ll look to explore in more detail include:Culture, not training It’s relatively easy to spin up an annual security training and declare your responsibility done. We do have security training, of course, but stopping there would be a mistake. A security team needs an open, ongoing relationship with the rest of the company, not a once-a-year checkpoint. At Dropbox, we’ve caught many attacks and internal problems early because of this partnership with Dropbox employees. By nurturing our culture of security, we’re scaling out our team’s instincts to the wider company.Independent validation The strongest defenses are the ones that are regularly tested and improved on an ongoing basis. I’ve been to dry board meetings in the past where I’ve been asked “do you get an annual pen test?,” as if an affirmative answer were all that’s needed for a robust security posture. We engage annually in not one, but multiple external paid product assessments: pen tests, product security assessments, and formal red team engagements. On top of these external tests, we have a dedicated internal Offensive Security team that performs adversarial testing day in, day out.And we still do more. On top of that, we invite the broader security community to participate via our Bug Bounty Program. The bounties have helped us close many important bugs, and build strong relationships with researchers. We’ve seen a lot of success and we recently increased our rewards to industry-leading levels. We’ve effectively scaled out security testing to the internet at large.Engineering advanced tooling One of the best ways to scale a team is to write code and tools and have computers do the work. We use in-house engineering to counter attempted abuse of our product in many different ways (for example, deflecting password brute forcing attempts). We also use engineering to help with internal security challenges. Last year, we open sourced SecurityBot, a way to automate certain components of internal alerting. For critical applications where commercial tools are lacking, we’re not afraid to engineer open source replacements, such as for Mac OS host monitoring.Closer to the product, we make sure to identify the most critical pieces of code and invest heavily in these places. An example here is the care we take in choosing how to hash and encrypt user passwords at rest. A further example is how we encrypt data in transit: we don’t just check the “SSL” box, we also use a range of industry-leading HTTP, SSL and cryptography best practices.Enabling users to self-serve Helping users to discover features that help them self-secure is a great way to scale. We’re an early supporter of multi-factor authentication and support very strong factors including U2F keys. But launching stronger authentication, ironically, may introduce scaling challenges in account recovery or risk more account lock outs. Our goal is to make strong security easy, so we launched linked mobile devices as a strong self-serve recovery option.And sometimes it’s simple things like making sure the technology behind “standard” features—such as the password strength estimator—is doing a great job at guiding users to good choices that can have a big impact.I’m excited for what the team will deliver in 2018. Follow our progress here.",https://blogs.dropbox.com/tech/2018/02/security-at-scale-the-dropbox-approach/,0,dropbox,"frontend,animation,docker,python",NULL,2018-02-13
Balancing open source and proprietary IP—they can co-exist,"Open source software can provide significant benefits to an organization—it can decrease product development time, distribute development across a community, and attract developers to your organization. It’s because of these benefits that we at Dropbox love open source. However, some organizations shy away from it due to perceived risks and fears around lost intellectual property (IP) rights. You’re not alone if you’re worried that once you’ve incorporated open source into your products or open sourced your own code that you’ve surrendered control over your most valuable assets, or worse, left your organization vulnerable to litigation with no defensive weapons to counter the threat. We too had that concern when we embarked on formalizing our open source program.Good news: it doesn’t have to be an either-or decision. It’s possible to simultaneously support open source while maintaining an active IP program. Smart organizations can avail themselves of the benefits of open source—decreased development time, community supported development and code review, platform adoption—and use various IP strategies to protect those aspects of its software that are unique to the business or provide a competitive advantage. We’ve taken a hybrid approach with several of our projects including Lepton, our streaming image compression format.When thinking about participating in open source the issues exist on a spectrum. There are 100% open sourced projects or products at one end, like the programming languages Python, Rust, and Google’s Go. In the middle are products that make use of a combination of open source and proprietary code. At the far end are proprietary products that are 100% closed source. Most businesses today are somewhere in the middle.Here are some tips for helping organizations participate in open source while maintaining their IP integrity.Identify your Motivation An important aspect in maintaining balance is identifying why you want to support an open source program. For example, is it because you don’t want to re-create the wheel, do you want to help maintain existing open source projects, do you want to use open source to attract and retain talent, do you want to promote your platform, do you want to be well-rounded open source participants, or is there some other reason? These are all legitimate motivators that can help you shape your open source program, identify risk, and chose the appropriate IP strategy.For us, it’s really all of the above, but sometimes only a subset of the factors apply to any individual open source opportunity. When we considered open sourcing Lepton at least two of the factors surfaced as key motivators: promoting our platform—we really wanted our compression technique to be adopted and incorporated in client applications like web browsers—and promoting the achievements of the engineers who developed the technique.Pick and Choose To strike an effective balance between open source and proprietary code, the key is to think strategically about the software, the value it can provide to the organization, and whether the technology should be developed internally. There are instances where software is actually more valuable because it includes open source components. For example, Dropbox encourages our engineers to think about open source when working on compilers, build tools, and analytics tools that enable our team to evaluate and assess our product. These tools support our development efforts. And by integrating open source components, Dropbox is engaging with a strong community to help us pinpoint weak spots, identify bugs, and maintain a steady pace of new feature releases. Not developing these capabilities exclusively in-house frees up our engineers to focus on projects that really drive the business.Contribute Back Something else we keep in mind is that open source is a two-way street. The community is stronger when everyone gives and takes, which is why Dropbox gives back to the community by contributing to other open source projects and open sourcing some of our own. With each contribution, we evaluate the IP rights that we may be granting to the community by asking a few simple questions. For example, will contributing to an existing open source project require us to grant licenses to patents in our portfolio? Will open sourcing a particular project achieve our goals? Or can we better achieve our goals by maintaining proprietary code that is protected through trade secrets or patents?Blended Protection Just as many organizations adopt a hybrid approach to open-sourcing, the same can also be true for specific products. There are certainly cases where there is value in open sourcing code, but there is additional value in preserving some of the IP rights. That’s a situation in which we might open source an implementation and file for a patent at the same time. In scoping the patent and the license terms, the open source community gets access to the software but the patent retains value. Apache 2.0 is an example of a license that delivers this layered protection. It allows other organizations to use open-sourced code for a specific use case, and receive a patent license for that code, but the granting organization retains the value for the rest of what the patent covers.Lepton is just one example of open sourcing our own projects, but it is one where we had serious discussions about our goals and how to achieve them. We can use the compression technique on our servers—and save storage space—and we could incorporate it into our client applications and compress the content before it is transferred to our servers—and save storage space and synchronization time. Because the compression technique provides such a significant savings it has clear business value and could provide a competitive advantage. So we saw a strong case for maintaining it as proprietary code and protecting through IP protections like patents.However, our client application isn’t the only way that files end up in a user’s Dropbox account. For example, some users upload content through the web. By open sourcing Lepton, it could be incorporated into web browsers, allowing files to be compressed client side. This provides a better user experience—decreased upload time—and space savings for us. So we also saw a strong case for open sourcing Lepton.After considering both sides it was clear to us that this was a prime scenario for a hybrid approach: open sourcing Lepton while simultaneously filing for patent protection. Ultimately, that is what we decided to do, and we open sourced it using the Apache 2.0 license. We get our compression technique out to the public, promote adoption, and benefit from insight from the community, but we protect some value for the company.Customized Agreements Another hybrid approach is to create customized open source licensing agreements. Say you have an open source license in place that doesn’t say anything about patent rights—an organization can add a custom patent license or a defensive termination clause that gives patent holders the ability to terminate a license if the licensee sues. Termination clauses can be customized to be stronger or weaker, broader or narrower, depending on the situation. For example, an organization could make a clause broader by adding provisions that allow licensors to terminate the license if the licensee sues at all—even if the lawsuit is unrelated to the open source project.When we open sourced Lepton we could’ve chosen a customized license or added a patent clause to an existing open source license. We opted for a traditional, permissive open source license that includes an explicit patent grant because it satisfied our needs. We want to promote adoption of Lepton, which we believe is achievable using an established permissive license. We also wanted clarity around which patent rights we’re granting (and what rights others are receiving), which the Apache 2.0 license provides.Due Diligence and Mutual Awareness Unfortunately, participation in open source isn’t without risk, so the push to explore open source options comes with guidelines. It’s important to vet all open source code thoroughly and keep clear documentation so, down the road, we know what’s in our code, what we’ve open sourced, and what responsibilities we bear. Additionally, it’s important that there’s mutual awareness about the company’s open source and IP strategies. True balancing can’t happen without strategy alignment and communication.Defensive Aggregators Finally, you can participate in open source and decrease risk by joining defensive aggregators, like LOT Network and Open Innovation Network—an organization designed to protect the linux kernel from patent litigation. In LOT Network, members sign agreements that include licensing terms which immunizes them from patent troll lawsuits, if (and only if) a fellow members’ patents are sold to a troll. Given that over half of companies sued by patent trolls make less than $10M in annual revenue, this type of agreement is especially beneficial to smaller companies in the open source community. And you don’t have to already have your own patents to participate in LOT Network.Whatever the case and whatever your goals, the first step is to figure out what kind of intellectual property you want to protect. The next step is to develop a system that ensures the vital elements remain under your company’s control and ownership. Open source doesn’t have to mean the end of ownership. If you’re smart about your open source/IP strategy, participating in the open source community can be an invaluable tool for building your own intellectual property more rapidly and securely.",https://blogs.dropbox.com/tech/2017/12/balancing-open-source-and-proprietary-ip-they-can-co-exist/,0,dropbox,,NULL,2017-12-13
Improving Document Preview Performance,"Ever open a file on dropbox.com, or click a shared link your coworker sent you? Chances are you didn’t need to download the file to see it—you saw it right in the browser. This is the work of the Previews team at Dropbox.Previews are part of the core Dropbox experience. They allow architects to access their entire portfolios on dropbox.com while at the job site to show their work. Designers can send work-in-progress to clients without worrying about whether they have the correct software installed. Office managers can review, comment, and annotate new office design proposals, regardless of the file format.For many users, a preview is their first interaction with Dropbox. Close to half of the previews we serve are documents (in formats including PDF, Microsoft Office, and Open Office). Unlike images, documents need to have a preview generated. This can take time. Our users want to see their content as soon as possible, so we have to provide great performance.Over the last year, the Previews team has been on a journey to make our document preview experience the fastest in the industry, and we’re happy to share what we learned.At Dropbox, all documents are converted to PDFs before being previewed. This preserves as much detail from original files as possible, while achieving compatibility on all clients. Our sibling team, Previews Infra, manages a large fleet of servers that handles file format conversion for us. Thanks to their work, our task is reduced to figuring out how to display PDF documents in the browser as fast as possible.Early versions of Dropbox directly embedded the PDF on the web page and relied on the browser to render the file. PDF renderers in browsers tend to have very good performance and high fidelity. However, the downsides were significant. We had very little control over the look and feel of PDF viewers. This made it hard for us to achieve a consistent user experience across browsers. More importantly, we had no ability to add collaborative features like comments, annotations or highlighting.To address these shortcomings, we replaced the direct embedding method with a JavaScript open-source PDF renderer, PDF.js. This change allowed us to build annotations on previews, and let browsers without PDF viewers—most notably Internet Explorer—see the preview. We also built a consistent user interface across all the file formats we support. However, moving to PDF.js led to significant performance problems. Not only did the client need to download and execute the entire JavaScript bundle, we also needed to download the entire PDF file to successfully preview it.Our File Preview is a monolithic Single Page Application, implemented in React. Most of our business logic is implemented client-side in JavaScript. As a consequence, seeing a document preview requires downloading and executing the entire JavaScript bundle, and then downloading and executing the PDF.js JavaScript code. Only then can we download the PDF and render it. This is highly inefficient. However, it is very hard to parallelize these operations, because JavaScript is single-threaded. Executing PDF.js code would block our executing code, and vice versa.For the user, previews aren’t just a core experience—they’re critical. This means it’s paramount for us to render them first, even at the expense of delaying collaborative features such as commenting.To speed up previews we implemented content prioritization: showing the preview first. Using Server-Side React, we render a skeleton web page on the server using an iframe that loads the document preview. Then on the client side, the JavaScript waits to execute until the document preview inside the iframe has successfully loaded. This approach, while simple, is highly effective. We saw a large drop in our Time to Interactive (TTI), a metric defined as when the user can interact with the document.While content prioritization was quite successful, it still took a long time to download the full document and PDF.js. Further performance improvement required us to explore approaches beyond PDF.js.For users to perceive a preview as fast, you have to show them something as quickly as possible. Taking as an example iOS apps that display a screenshot before actual functionalities are fully loaded, we wondered if it would be enough to simply display a high-resolution thumbnail of the first page until the document was downloaded and rendered. This idea turned out to perform very well in real world usage. It dramatically reduced the perceived slowness of the preview.Later, we began to automatically load and render thumbnails for subsequent pages as the user scrolls. Aside from not being interactive, thumbnails have very little difference from the actual previews rendered with PDF.js.Beyond performance improvements, our First Page View work gave us validation for another idea we had pondered for a long time: Server-Side Rendered Previews.Although using PDF.js represented a huge improvement over embedding the PDF directly, it also brought a number of challenges.First of all, integrating PDF.js with Dropbox was quite difficult, if not downright hacky. PDF.js was designed to be Firefox’s integrated PDF viewer, rather than a component of another product, so it provided limited support for our use case. Furthermore, PDF-based exploits are extremely common, so we decided to put PDF.js into an iframe on a separate domain, so that malicious PDFs could not access cookies on dropbox.com. This dramatically complicated our build and deploy process. It also necessitated a cumbersome postMessage call for communication between PDF.js and the main frame.More importantly, performance-wise, PDF.js left a lot to be desired. PDF is an incredibly complex file format—the specification is more than a thousand pages long, not including the extensions and supplements. Thus the PDF.js implementation is highly elaborate, which resulted in long download and execution time, and there is little we can do to improve it.For some time, the team had entertained the possibility of moving the render pipeline to the server side. This would allow us to transfer only the part of the document visible to the user, dramatically improving TTI.Because we had been using PDF.js for a long time, it was natural to first explore simply moving it to the server. Unfortunately, running PDF.js with Node results in poor image rendering quality. Not acceptable. The proposed alternative was running PDF.js in Chrome. At Dropbox, binaries are executed in a jailed environment and only whitelisted system calls are allowed. While borderline paranoid, this approach has saved us from zero-day vulnerabilities in third-party libraries we rely on. To move forward we needed to get hacking.Dropbox maintains a great tradition for getting back to our innovative roots. Every year we hold an in-house hackathon aptly named Hack Week. During Hack Week most company employees pause regular work and focus on any project they wish. However, projects that move the company forward are encouraged.During the 2017 Hack Week, I experimented with using PDFium as a Previews backend. PDFium is the engine that powers Chrome’s PDF Viewer, and is based on the battle-tested Foxit PDF SDK. However, since it is designed for client use, it wasn’t clear whether PDFium would be suitable for server-side usage. Ultimately, I put together a prototype that was much faster than our PDF.js based viewer. At the Hack Week presentation expo it earned the Cornerstone award for a project that makes significant contributions to Dropbox’s foundation, be it product stability, trustworthiness, or performance.The team then began comparing the benefits and risks of the two approaches. It soon became clear that PDFium allowed us to do a lot better than PDF.js. We could build a PDF viewer from scratch, designed specifically for Dropbox, and it would be secure, fast, and easy to develop. The render quality of PDFium surpasses PDF.js on many documents, especially those that use obscure PDF features. Extracting text as well as positioning it correctly is trickier than with PDF.js, but still feasible. We decided to go all in on PDFium with a project called QuickPDF.QuickPDF consists of two components: a server-side renderer that splits the PDF into parts, and a client-side viewer that reconstructs the file from those parts and displays them in the browser. The two parts are intentionally decoupled, in case we find a better solution than PDFium.On the server side, we wrote a statically-linked binary in C++ that uses a modified version of PDFium to render each page of the PDF as a PNG image. It parses the metadata, including the page number and page dimensions, and serializes them as JSON. It also extracts the text and positioning information by grouping adjacent characters with the same font and size on the same line into text boxes. Each text box is represented by an object that stores the text, position, width, font size, and font family. The binary is executed by our file conversion system inside the secure jail, and the results are cached.The client side is a React application. It fetches the document metadata. Using page count and page size, it draws a skeleton of the document on screen. As the user scrolls, it fetches the pages visible from the server. Each page consists of an image and a transparent layer of text that enables text selection. Hot Areas—implemented in PDF as Annotations—are also rendered, to enable clickable links.Drawing the text overlay accurately is the most difficult part. Without enough precision, the text selection can be flaky. To make matters worse, the user can freely zoom the page, complicating the positioning. After some careful study of the PDF Standard, we decided to draw the text layer at 72 DPI, the native resolution of PDF, and scale up or down as required. For each text box, we use the original font when available, and substitute a similar one otherwise. The text is then created at the specified size. After the box is drawn on screen, the width is measured and the entire box is stretched to the width specified. This also takes care of different kerning. The box is then rotated and translated to its position on the page.To make sure the application was fast enough, the team employed multiple optimization techniques. Requests for text and metadata are batched as much as possible. Pages are “over scanned,” i.e. we render more pages than are currently visible, so that the user doesn’t have to wait for a page to download. Since text overlay rendering is expensive, we defer it until the user is no longer scrolling. Using these techniques, we are able to achieve a butter-smooth experience across all supported browsers.QuickPDF proved to be a huge success. Our 75th-percentile Time to Interactive was reduced by half. The biggest improvement came with PowerPoint files. These files embedded with high-resolution graphics and video are very large. Previously, a significant percentage of our users would leave the preview before it could be rendered. After implementing QuickPDF our PowerPoint preview success rate dramatically improved through lower abandonment.The methods above are the ones that worked for us. Needless to say, we also experimented with several approaches that didn’t work so well. Through trial-and-error the team learned a lot of valuable lessons that we believe apply to everyone who cares about performance.Challenge Assumptions. Before Hack Week, nobody thought PDFium could be a viable alternative to PDF.js. This suspicion didn’t go away until the prototype demonstrated its potential. Assumptions are dangerous; they discourage radical ideas. Some of our most effective measures, including showing the thumbnail first and deferring JavaScript, sounded crazy on paper. Only metrics speak for themselves.Measure. Measure. Measure. In early stages of development we suffered from a lack of reliable metrics. Different loggers returned conflicting results, and we didn’t have a detailed breakdown to guide our optimization efforts. As we rolled out different experiments and fixed the logging, we realized that the lack of historical data made it very hard to measure effectiveness. After that discovery, logging became the top priority for each project.Define Metrics and Goals Carefully. While we decided early on to optimize for 75th-percentile Time to Interactive (p75 TTI), it took us some time to define, scientifically, what this metric would represent. For example, how would we define interactive? Should we measure Cold Starts, i.e. new users, and Warm Starts, i.e. those who had visited the site previously, and thus have most of our resources cached separately? What would the 75th-percentile cover, hours, days, or weeks? Due to limitations of our early logging, the number we reported was a weighted average of p75 across different file formats. Is this acceptable? Having exact agreement on what the different elements of each metric means is crucial, both to having engineers on the same page, and also for communicating with external stakeholders.Improving performance is a never-ending task, and it involves every part of the engineering stack, from JavaScript frontend to infrastructure and network. There are many coworkers whose advice along the way helped us achieve these amazing results. On behalf of the team I would like to express our most sincere gratitude.",https://blogs.dropbox.com/tech/2017/12/improving-document-preview-performance/,0,dropbox,,NULL,2017-12-01
Dropbox Paper: Emojis and Exformation,"Communication is hard 😖 (it’s ok, little buddy, we’re gonna talk about some tools to combat this). When it comes to conveying a message with other human beings you have to make sure to speak clearly, listen well, use unambiguous words, remember what the other person said, understand the context surrounding the conversation, read between the lines sometimes, pay attention to body language and intonation, comprehend cross-cultural differences, and so many more subtle intricacies. Now compound that problem with trying to communicate with someone over a digital medium. You have to do double the work in conveying and listening — you lose things like the benefit of body language, and intonation of voice. Was the other person angry when they told me “Ok”? Oh, I see that they added a period to “Ok.” — that definitely means that they’re angry! Does it?? If there was only some way that they could tell me how they felt… 🤔 (hmmmm)If information is the stuff we’re trying to convey, then exformation is the stuff that accidentally gets left out along the way — it’s the details you have to fill in as a listener to try to understand what the other person meant to say 🙊 (monkey says: “you figure out what I’m trying to say”). Ze Frank does a great (and hilarious) job of explaining this:So this is where emojis come in. Say what now? Oh yes, I totally mean it — emojis can help fill in the gaps in human communication where voice and facial expressions are missing. There’s still no substitute for true face-to-face or even just phone time but if you’re gonna be communicating over email, chat or Dropbox Paper (ahem) you might as well give in and join the emoji revolution 😄 (blissfully excited, like, omg).Four years ago when I joined the Paper team one of my (cupcake) things I did in my spare time was to add support for emojis to the editing surface. Try typing : in a Paper doc and you’ll see! I didn’t want our product to miss out on this sweeping new, disrupting technology 😜 (sarcasm / feeling chuffed with my cleverness). The problem was that back then browsers didn’t support emojis. You’d get this amazing little rectangle box instead:To work around this we made the backing text in your Paper doc be something like :sunglasses:. Then, using Paper’s multi-layered rendering system, we rendered that text on-the-fly in the browser (much like how we auto-link url’s, hashtags, and do code highlighting) into something more interesting like 😎 (captures the feeling of awesomeness). Since the browser couldn’t render the emoji character natively, we would just insert a tag with a background-image corresponding to what the emoji would be.That system worked well for a while but it did have its limitations. If you started to delete the emoji, what seemed like a single character would revert to something like :tada: and then you’d have to delete all of those backing characters. Plus, if you tried to copy that text out of Paper and paste it somewhere else you’d get :tada: which is pretty 😐 (meh). So, we got together in our “💩.txt” conference room (not making that up, btw) and decided to use our migration system to convert all existing docs to use the actual native character. We still would have to use the fallback images to render the emojis but at least the backing text would be something that would be ready for the native support that was being rapidly added to browsers. It also solved the problem of copying text out of Paper so that it would it be represented by the characters you intended.Paper has a system to mass-migrate documents from one format to another. Because we continually make changes to our document format over the years, we have this system to allow us to make otherwise not-backwards-compatible changes with older documents. Paper documents use something we call a changeset to transform a doc from one state to another one. Our migration system simply acts as a robot that adds a changeset transforming the underlying structure of our doc into the latest format we want. In our case, we had to convert text that matched our legacy :levitating_business_man: or :nerd_face: to be 🕴️ and 🤓 characters (some of my favorite emojis). Seems straightforward enough until you remember the axiom that nothing is ever as simple as it seems.Paper uses MySQL for its databases and it turns out that the default character encoding for MySQL is utf8. That sounds great — I mean, jeez, at least it isn’t latin1 anymore, right? Yeah, it’s great until you realize that utf8 in MySQL world doesn’t include emojis 😑 (wat). That’s right, if you want to store emojis in your database you’ll need to be using the newer utf8mb4 or otherwise you’ll have madness! (😺 and 🐶 living together, mass hysteria, you get the idea). Because of this, what should have been a simple document migration that can run in the background, required actually putting Paper into maintenance and read-only mode so that we could successfully migrate our databases over the course of a couple hours. 😤 (no, that emoji isn’t conveying anger, that’s triumph!)Great, with that hurdle aside, another challenge in converting our characters to be a ‘single character’ was that emojis don’t consist necessarily of one single character! For example, take the Spock “live long and prosper” emoji and combine it with the Fitzpatrick skin tone emoji scale:🖖 + 🏿 == 🖖🏿What looks rendered as one character (🖖🏿) is actually two characters (🖖 + 🏿) behind the scenes! Here’s another example where eight(!) separate characters get combined to end up as a single character of two people kissing:👩 + U+200D + ❤ + U+FE0F + U+200D + 💋 + U+200D + 👩 == 👩‍❤️‍💋‍👩It goes even further! Let’s take the example of the Spock emoji. Even just the original character (🖖 without an additional skin tone modifier) is actually two characters behind the scenes. In JavaScript:Regular expressions will start to fail in interesting ways as well:This difference in the apparent rendered length of characters vs. the actual character length behind-the-scenes would have messed a lot with Paper’s logic, since Paper is entirely Typescript on the frontend and backend. Syncing multiple characters in Paper of course works as you would expect. However, if we were applying a changeset to a document, say, to remove four characters from the beginning of “foo🖖bar”, we have to be careful where that 4th character starts and ends. The intent is to remove the 🖖 character entirely. However, without an understanding of how long emojis actually are it would have deleted only half of the emoji (e.g. the ""d83d"" part of the Spock emoji) leaving us with a � character and effectively corrupting the text. So, with a lot of work put in by Travis Hance, one of our expert Paper engineers, we were able to detect the start and end of an emoji so that we could treat it as a single character even though it was really far from being a single character at all. For a more in-depth and fantastic write-up about this problem, please check out Mathias Bynens’ article “JavaScript has a Unicode problem”.The next big shift was that emojis took off! Newer versions of emojis were being added all the time by the Unicode Consortium and when multiplied with the new skin tone and other modifiers we were talking about 1,000 new emojis! We had decided initially back during our migration that we would let the browser render natively where it was supported. However, depending on the combination of your browser, operating system, or phone you were using you either had support for the new emojis or you didn’t. On top of that, even where operating systems did support emojis, sometimes they did a terrible, terrible job of displaying those emojis 😨 (the horror, the horror):Linux: that camel though.Android: Oh, blobby.Windows: Color is for Mac users.Along came EmojiOne to save the day. They do an absolutely fantastic job of unifying the grand mess that is supporting emojis across different OSes and browsers. We use EmojiOne to make our emoji experience the seamless one that it is today.Along with switching to EmojiOne, we improved our design which had been originally just a short dropdown of emojis sorted alphabetically into a UI that currently lets you browse, jump by category, change skin tones, and add custom emojis. One interesting challenge was getting our browsable emoji UI to be performant — previously in our dropdown we would only display 20 or so emojis for you to search by keyword. But in the new UI we had 1500+ emojis to display which put a noticeable strain on React’s rendering system. Even despite the emoji UI and its components being pure rendered, it still manifested itself as a several second delay on page load. One of our engineers Sergio Almécija Rodríguez took on this challenge and was able to make a cache of our individual emoji components within our list UI to make sure we were rendering more efficiently. Now we were really 🤘 (rockin’)Sometimes even the rich (and occasionally bizarre) existing set of emojis isn’t enough to convey what you mean. In Paper, during one of Dropbox’s twice-a-year Hack Weeks, we decided to add support for custom emojis for all the little in-jokes and memes your team might have. In our case we could have waited until the Unicode Consortium had accepted our requests for      (cupcake, roly-poly cat, dancing wizard, trollface, and red panda, respectively) but instead we decided to just roll our own.First, I’ll describe some basics on how Paper works. A Paper doc consists of attributed text (atext for short) and an attribute pool (apool). Although it may seem that way, we don’t save HTML to our backend because that would be an absolute nightmare; instead, we take regular text and use attributes to describe the position of regions that have certain characteristics (hence, why string length is so important in our Spock emoji discussion above). We use attributes, for example, to indicate sections as being bold or containing a link to a website. Let’s introspectively look at how a part of this very paragraph is constructed in Paper!Let’s break down the operations of attributes above:Ok, now you have a baseline of how Paper saves its underlying data. We use attributes for much more than bolding text though. We put attributes on our richer objects to indicate things like the start of a list, or having an image associated with them. In Paper, these richer objects are called “Magic Objects” and they include things like lists, headers, checkboxes, images, Custom emojis are yet another example of a Magic Object in Paper’s ecosystem. In the case of custom emojis, we would add attributes saying:The renderer looks at the * character and instead paints an image indicated by the url. In addition, we instruct the editor to treat it much like it would a regular character.We actually had our original implementation creating custom emojis in the Private Use Area of Unicode. This worked ok but custom emojis are team-specific. This had the problem of not being able to be copy & paste the text across different teams without emojis turning out to be a � character. By converting the emoji to use Paper’s standard atext and Magic Object system (using our previously mentioned migration system) we were able to manipulate the data backing custom emojis much more easily.The success of emojis in Paper influenced our other designs in-turn. We added stickers to our commenting area not too long afterwards. On top of that, we added emoji reactions (“reacji”) to tell your colleagues how you felt in the comments section. Again, all these things might seem a little silly, but they do help people feel more at ease in a world where one can easily feel misunderstood online. Besides, they plain just make the day more fun. Emojis help you more fluidly communicate with your team, so that you can get things done with aplomb. 🤜 🤛 (fist-bump of accomplishment)In addition to stickers and emoji reactions, we recently launched a brand new feature! If you add an emoji to the start of your title of a Paper doc we’ll change the favicon in your browser’s tab for an easy-to-spot visual to your doc; in addition, we also set the emoji as the document icon on your list of docs on Paper’s homepage. Emojis everywhere!!!Now that we’re communicating clearly with each other and having fun while doing it, can we all get in a room and agree on what in the word (🙄, universal response to pun) some of these emojis mean? 💁 🙆 🙇 🙏Mime Čuvalo🕴️ Staff Engineer on Paper ",https://blogs.dropbox.com/tech/2017/11/dropbox-paper-emojis-and-exformation/,0,dropbox,"python,javascript,frontend,css,react",NULL,2017-11-15
Deploying IPv6 in Dropbox Edge Network,"In the past few months, we have gradually enabled IPv6 for all user-facing services in Dropbox edge network. We are serving about 15% of daily user requests in IPv6 globally. In this article, we share our experiences and lessons from enabling IPv6 in the edge network. We will cover the IPv6 design in the edge, the changes we made to support IPv6, how IPv6 was tested and rolled out to users, and issues we encountered. Note that this article is not about enabling IPv6 for internal services in our data centers, but rather focuses on making IPv6 available to users.The Internet Protocol (IP) has been a great success and powers the ever-growing Internet. However, it has long been known that the network address space in Internet Protocol version 4 (IPv4 or v4) would eventually be exhausted. Internet Protocol version 6 (IPv6 or v6) was proposed in the 1990s to address this. Even though the IPv6 transition is inevitable, only recently has IPv6 started to gain a considerable amount of adoption based on measurements from multiple companies and organizations. As announced earlier this year, Dropbox desktop client software already supports working in an IPv6-only environment. Enabling IPv6 for Dropbox services will benefit users who have IPv6 connectivity, prepare us for an IPv6 traffic-heavy network environment in the future, and contribute to the global efforts that promote IPv6 adoption.The Dropbox edge network consists of multiple Points of Presence (PoPs) distributed globally in order to reduce latency and increase throughput. As of today, the majority of our user traffic (such as web browsing, file uploading & downloading) is served through the edge network. Enabling IPv6 in the edge network will make most of our services operate in a dual-stack environment. We have also added IPv6 support in our data centers (DCs), though it will take more effort to enable IPv6 for all internal services.Each PoP consists of multiple layer-4 (L4) load balancers and layer-7 (L7) proxies, where our L4 load balancers are built on top of the IPVS kernel module and L7 proxies use the open-source Nginx software. For each service, such as www.dropbox.com, a Virtual IP (VIP) is announced via Border Gateway Protocol (BGP), and this VIP is also advertised in DNS. User traffic sent to this VIP is distributed to IPVS based on equal-cost multi-path routing by routers in the PoP. IPVS then forwards traffic to Nginx, which performs early TLS termination and proxies user traffic via secured HTTPs connections to DCs. We leverage Direct Server Reply (DSR) for the HTTPs responses (i.e., egress traffic) so that IPVS only needs to process ingress traffic. The figure above shows the IPv6 traffic flow in our edge network. IPv6 requests are forwarded by IPVS to Nginx via IPv6 in IPv6 (IPIPv6) tunnels. Nginx then proxies these requests to DCs in IPv4. Similar load balancing schemes apply when IPv4 requests reach DCs and then get delivered to application servers, though everything happens in IPv4. Essentially, IPv6 requests are terminated at PoPs.In Q4 2016 we started rolling out IPv6 across our entire network. We started first with dual-stacking all our links followed by making necessary changes to our routing protocols to support v6. Intermediate System to Intermediate System (IS-IS) which is a protocol-agnostic architecture and can easily support all address types including v4 and v6. This was our Interior Gateway Protocol (IGP) in the backbone, so there was no significant changes we had to make to support v6 for IS-IS. To have consistent routing for both v4 and v6, we chose IS-IS single-topology over multi-topology.After IS-IS, we had to make changes to BGP. We deployed separate BGP sessions for v4 and v6 but maintained the same routing policies across both of them to ensure routing symmetry. The third piece was Multi-Protocol Label Switch (MPLS-TE) that was deployed across our backbone to forward v4 traffic. We intended to use the same set of Label Switch Paths (LSPs) to forward v6 traffic. We could achieve that by using IGP short-cuts as defined in RFC3906. With the implementation of IGP-shortcuts, both v6 and v4 traffic were using the same set of MPLS-LSPs across the backbone. By the end of Q1 2017 we completed v6 roll-out across our data centers, backbone, and edge.As for public IPv6 address allocation, each PoP has a unique /48 address space which is advertised to external peers by the edge router in that PoP. Announcing the /48 v6 prefix to the external world only from that PoP guarantees that all user requests to an IPv6 VIP which belongs to this /48 address space enter Dropbox network via that PoP and terminate locally on the Ngnix machines in that PoP. Each /128 IPv6 VIP has a unique /64 prefix, and we announce this /64 prefix instead of the full /128 address from IPVS to routers in the PoP to be more memory-efficient (though only traffic sent to the /128 VIPs will be accepted in the PoPs). The IPv4 VIP is embedded as the last 32 bits of the IPv6 VIP to make it more operational friendly.The software stack in PoPs needed to be fully IPv6 compatible to handle IPv6 traffic. The applications running in our data centers also needed to be updated to properly work with client-side IPv6 addresses passed along in the X-Forwarded-For headers by Nginx.In our PoPs, both the IPVS kernel module and Nginx software support IPv6 natively, though we needed to update the in-house configuration management tools for them to work with IPv6. We use IPv6 in IPv6 tunneling between IPVS and Nginx so that the tunneled IPv6 user traffic can be correctly decapsulated on the Nginx side. Because IPv6 packet headers are longer than the ones in IPv4, the advertised TCP Maximum Segment Size (MSS) was reduced to 1400 bytes to work with tunneling.On the data center side, we updated our application servers to properly handle client-side IPv6 addresses embedded in the X-Forwarded-For header. The following lists some common IPv6 compatibility issues we saw when updating our code base:We first deployed IPv6 support in our infrastructure so that we could test without affecting production traffic. After that, we gradually enabled IPv6 for users, service by service. In this section, we present the rollout process in detail and the issues we encountered.IPv6 was first deployed to our network infrastructure so that we had IPv6 working at PoPs. After that, we deployed the updated IPVS and Nginx software, at which point we started to announce IPv6 VIPs via BGP. The deployment was done PoP by PoP to minimize risks. The IPv6 VIPs were not added to DNS, so they were not visible to users. In this step, we tested IPv6 reachability and performance. In the meantime, internal teams could leverage these VIPs to perform end-to-end IPv6 testing for their codes in the application servers.To enable IPv6 for a service, we need to add an AAAA record for the related domain(s), and users will receive the IPv6 VIP when performing AAAA DNS queries. To support a smooth transition from IPv4-only to dual-stack network environments, efforts such as Happy Eyeball have been proposed and implemented in many client-side software (such as browsers and the Dropbox desktop client). In these software, clients prefer IPv6 connections but are still able to fall back to IPv4 if the IPv6 connection is broken or not performing well. As for DNS, either parallel DNS queries will be made, or AAAA DNS queries will be sent ahead of A queries.In the rollout process, IPv6 was enabled service by service, so that we could monitor the status and performance, as well as isolate the impact if something went wrong. To enable a new service, we typically performed an office test, a user traffic test, and then rolled out to production. The figure above shows the increased IPv6 request percentage (each data point represents the average during a 4-hour time window) as we gradually enabled IPv6 for more services in the edge network. Note that the spike at the beginning of June is from a user traffic test.We ran into some initial hiccups during IPv6 deployment.One of them was whitelisting v6 /48 address from each PoP with our external peers. We learned that registering our v6 address space with RADb was not sufficient. We had to reach out to some of our providers to update their Access Control Lists (ACLs) to accept v6 routes. Unfortunately this occurrence seemed to be a lot more common than we anticipated which resulted in sub-optimal routing issues during our initial deployment. Luckily we could catch most of the anomalies during our internal testing before rolling it out to external users.We had to update our ACLs to accommodate for new functionality and roles that ICMPv6 has in the overall operation of IPv6, most common of which was Neighbor Discovery (ND). We had to reshuffle some of our ACL terms in accordance to RFC6192 to permit ICMPv6 above all other terms. Before doing that, we were very frequently running into issues while bringing up v6 eBGP peers as ND packets were getting blocked by our ACLs.DNS NXDOMAIN responses for AAAA queries are dangerous. Most dual-stack software will ensure a broken IPv6 connection has the opportunity to fall back to IPv4 to support a smooth transition from IPv4-only to dual-stack networks. However, DNS still represents a place where an IPv6-specific issue cannot easily fall back to IPv4 and may affect IPv4 as well. An NXDOMAIN response for AAAA queries essentially means there are not any records (neither A nor AAAA) exist. In this case, client software may not retry IPv4. Additionally, A queries could also be affected and receive NXDOMAIN if DNS resolvers cache the AAAA NXDOMAIN response. On a related note, Cloudflare has proposed an alternative DNS record type TYPE65536 that contains both A and AAAA answers, but for the purpose of reducing the overhead of additional DNS queries.After IPv6 is enabled for all services in the edge network, we see about 15% of daily user requests reach us in IPv6 globally. PoPs in the US receive the highest IPv6 request percentages, followed by PoPs in Europe and APAC regions.To understand the IPv6 deployment status, we have measured the percentage of IPv6 requests across all Dropbox services with sampled traffic (last 15 minutes of each hour) on September 20th, 2017 (Wednesday). We present the IPv6 statistics for different countries/regions and ISPs.Countries/Regions. The heat map below shows the average IPv6 traffic percentage for each country. In this map, a darker blue color means higher percentage of IPv6 requests.Similar as reported from others, we see higher IPv6 request rates in some European countries and the US. We also observe considerable IPv6 deployments in South America and the APAC region. The following figure lists the top 10 countries ranked by the IPv6 request percentage.ISPs. We have also looked into IPv6 statistics among ISPs. We selected the top 10 ISPs in terms of total number of IPv6 requests sent to our edge network during the measurement, and the following figure lists these ISPs ranked by IPv6 request percentage. We label each ISP with the continent code (i.e.., EU for Europe, SA for South America, and NA for North America) and a unique index within that continent. As can be seen, the top two ISPs (both are US mobile carriers) are getting close to 100% IPv6.The following is a list of IPv6 statistics reported from other organizations:It has been reported that IPv6 has better network performance compared to IPv4, especially in mobile networks. To understand IPv6 performance, we have measured the TCP Round Trip Time (RTT) on our API endpoint (api.dropbox.com), which is used mostly by our mobile clients. We have also looked into desktop client file download performance.TCP Round Trip Time. Many factors could contribute to the performance differences of IPv4 and IPv6, such as the additional delays introduced by NAT64/DNS64, performance advantages of newer hardware, client device performance, etc. Providing a fair IPv4 and IPv6 comparison is challenging. Ideally, the performance measurement should take place on the same client at the same time to minimize the impact of other factors. As we don’t have that capability from the server side, during our initial IPv6 deployment for the api endpoint (api.dropbox.com), we enabled IPv6 for only 50% of users via DNS for a few hours. This way, for an IPv6-heavy network (such as two of the US major mobile carriers), approximately 50% of users would connect to us via IPv6 while the other half via IPv4. We chose the api endpoint for this measurement because our mobile apps talk to this endpoint and thus most traffic will be from cellular networks.We compared the TCP RTT performance for IPv4 and IPv6 connections. The TCP round trip time was measured using the TCP_INFO socket option, and the results were reported via the tcpinfo_rtt variable provided by Nginx. We could also look into TCP retransmission stats using tcpi_total_retrans, but because we have enabled BBR in our edge network, packet losses would not affect the throughput as significantly as when other congestion algorithms, such as cubic, were used. The above graph shows the average RTT values (with 95% confidence intervals) reported for clients from two US cellular networks that we knew were close to 100% IPv6. As can be seen from the figure, IPv6 does show slightly better performance over IPv4. However, without detailed client-side and network information, it is hard to say definitely where the IPv6 performance gain is from. Additionally, the actual time needed for each request also depends on the performance of application servers.File download speeds. File syncing is one of the most important functions people use Dropbox for, thus we looked into desktop client file download speeds after IPv6 was enabled. We calculated the download speeds based on HTTP response body lengths and request time collected on Nginx proxies in our JFK PoP. Only files that were larger than 100KB were included in the study to exclude the potential variance introduced by small files. Since file download performance could differ based on user’s ISP networks, we focused on file download speeds for users from the ISP that sent the largest number of IPv6 requests to us. Because this ISP was not yet close to 100% IPv6 based on our stats, we compared week-over-week (Mondays) file download speeds after IPv6 was enabled.The figure above shows the file download speeds with different percentiles (pXX) for IPv4 and IPv6 after IPv6 was enabled, as well as the performance for IPv4 one week earlier (both were Mondays). From the figure, IPv6 file download speeds are faster than IPv4 after we enabled IPv6 at most percentiles, and the P90 performance (fastest download speeds) is comparable to IPv4. However, it is worth noting that this is not a strictly fair comparison because other factors could have contributed to the IPv6 performance gain. Comparing IPv6 performance with the IPv4 performance one week earlier, we could say that IPv6 performance is comparable to IPv4, and for lower percentiles, i.e., slower file download speeds, IPv6 also shows slightly better performance. Again, this is not a perfect comparison for IPv4 and IPv6, but we hope this provides some additional information for people who are interested in IPv6 performance.In this article, we’ve shared our experiences of enabling IPv6 in our network. The challenges of the overall process have been greatly reduced as hardware and protocol support for IPv6 become more mature. The majority of our efforts were on deploying IPv6 in our infrastructure, updating our software stack to be IPv6 compatible, testing, and gradually rolling out to users. We hope this article is helpful for those who are looking into enabling IPv6 for their front-end services and also those who are interested in IPv6 in general, and we look forward to hearing your feedback.Contributors to this article: Alexey Ivanov, Dzmitry Markovich, Haowei Yuan, Naveen Oblumpally, and Ross DelingerDo you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2017/11/deploying-ipv6-in-dropbox-edge-network/,0,dropbox,,NULL,2017-11-09
Handling system failures during payment communication,"Handling system failures during payment processing requires real-time identification of the issues in addition to offline detection, with the goal of eventual consistency. No matter what goes wrong, our top priority is to make sure that customers receive service for which they’ve been charged, and aren’t charged for service they haven’t received. Accurate payment processing is a crucial element in being worthy of trust, a core Dropbox company value.In a standard system of this kind, failures might result in page load errors or a failed database transaction. System failures during a charge request can result in uncertainty about where the money for that request ended up: is it in our company’s account or still in the customer’s account? These system failures are extremely rare, but when processing as many transactions a day as Dropbox does, even a small probability can lead to multiple occurrences a day. Designing payments infrastructure that can resolve issues such as these is vital to keeping our customers’ trust and providing our finance team with accurate information.In order to understand how system failures can disrupt payment processing, it’s important to understand each step involved in handling a customer’s purchase. When a customer visits the Dropbox website and elects to buy one of our products, we ask the customer to enter their payment information on the purchase form. After the customer submits the form, the system collects their payment information and securely sends it, as well as the amount we want to charge, to one of our external partners responsible for processing that type of payment information. For the purpose of this discussion, we’ll assume that the payment information in question is credit card information—not PayPal or other payment methods that Dropbox accepts. When our credit card partner receives the credit card information, they verify that the card is valid, store it for future charges (e.g. monthly recurring billing), and then attempt to charge the specified amount to the card. If the verification or charge fails, the credit card processor sends us a response containing a descriptive error code. In case of failure, we’ll refresh the purchase form, tell the customer that the charge attempt failed and ask the customer to try again. Otherwise, if the charge is successful, the credit card processor will respond with a success message as well as a token that we can use to reference the saved credit card for future charges. Upon receiving this success response, we store the payment result in our records. Finally, we will turn on the customer’s service—commonly called provisioning.As illustrated by the diagram above, we require communication with our external payment processor in order to complete the charge. This external communication involves side effects—changes in state as a result of the communication request. In particular, we care about whether money is moved from the customer’s account to the merchant’s account (Dropbox, in this case). In the presence of system failures, it can be unclear whether this occurred or not after making a request to an external system.There are three main failure points of this charging system described above:All of these failure scenarios result in one of two distinct situations:In both cases, the core scenario is the same: a charge request was made but never marked completed in our system and is thus in an unknown state. After detecting occurrences of this scenario, the solution is to discover whether the charge request actually went through or not, then address this charge appropriately.In order to detect incomplete charge requests, we record each charge request in our database before sending the information to our external partner. In this charge record, we store a customer identifier, the charge amount, as well as which payment processor the request will be sent to. The charge record also has a status attribute that tracks which part of the process the charge is in. Before we perform the charge, the charge record’s status is set to created. Next, we send the charge request to our external payment partner. When we receive the charge response from our partner, we update the charge record with a new status based on the response, normally either declined or successful.This status attribute of the charge request allows us to determine if a charge request was left in an unknown state. If the charge request has either the declined or successful status, then the charge response was correctly received and processed by our system. If the charge request has the created status, it’s necessary to look at the charge request’s creation time to figure out whether the request is in an unknown state or not. It’s possible that the charge request was only recently sent (milliseconds ago) and we could still get a charge response for it in the future. If the creation time is more than a couple minutes in the past (exact value depends on the timeout configurations) then we know that the charge request would’ve timed out by now so this request must be in an unknown state. To summarize, charge requests are in an unknown state if they have the created status and are more than a couple minutes old.A common way to solve a lost request is to simply reissue the request. However, this is not safe when the request has effects that should only happen one time. Each charge request could result in money being transferred out of a customer’s account. We never want to charge the customer multiple times for the same item so reissuing the charge request is dangerous. Even if we refund the extra charges later, the customer still sees the funds momentarily taken out of their account and this breaks the trust we want to establish with the customer. Since we don’t have an infallible detection system for the previous charge request’s state, it’s safer to abort the purchase attempt. Therefore, the system doesn’t grant a customer their Dropbox service until we have confirmation of a successful charge. The important result of this design decision is that if we discover a charge was successful but we have no record of it due to system failures, then the payment needs to be refunded since we would not have turned on the customer’s service in this case.Now that there is a way to identify the transactions in an unknown state and clear steps on how to handle them if the customer was charged, the next step in this solution is to discover whether the charge went through or not. The charge status can usually be discovered by communicating with the external payment processor. Most payment processors provide a convenient API to look up a charge’s status by either a merchant identifier or a transaction identifier. The merchant identifier, otherwise known as a merchant order number, is a unique identifier supplied by the merchant (Dropbox in this case) to reference this charge request. The transaction id, that we internally refer to as the external transaction id, is determined by the external partner at the time of the charge and referenced in the charge response. Thus, we will only know the external transaction id for a charge request if we received and processed the charge response. In the case of system failures, as discussed in the problem description, we do not receive a charge response so we do not have the external transaction id. That leaves the merchant order number as our only available option to perform an API lookup with. Since Dropbox formulates and sends the merchant identifier to the external payment processor, we have access to it at the time we’re making the charge request and store the value on the charge request record.Using this merchant identifier, we do a lookup for matching transactions using the payment processor’s API. If a matching transaction is found, we update the charge request record with either a declined or successful status as appropriate based on the transaction’s status. On the other hand, if a matching transaction could not be found, we need another way to resolve the transaction status. This case is possible if the external payment processor has a system failure after they perform the charge but before they are able to record the charge in their own system. In addition, this case is also caused by an internal error on our side if the merchant identifier is not correctly recorded for the transaction so we are unable to use the merchant identifier with the processor’s lookup API. In the case when the lookup API cannot be used, the transaction’s status can still be found in the processor’s settlement files. Every payment processor offers settlement files available for download for each merchant that they service. These settlement files contain a list of every successful transaction that was processed on behalf of that merchant in addition to other information, normally split into 24 hour time periods. Each settlement record includes the external transaction id and merchant identifier fields mentioned earlier, so if lookup through the processor’s API fails, a search through the settlement file for a matching record may be successful. If a match is found, then the charge record’s status is changed to successful. If a match is not found, then the charge record’s status is changed to error to acknowledge that something went wrong during the charge request and we are unable to determine what occurred.Additionally, the settlement file allows us to discover any successful charges which we have no record of due to internal bugs in our system or rare database failures. For this reason, we set up a background process which parses these settlement files and verifies that we have a charge record in our system for each settlement record. For any settlement record without a charge record, a charge record is created with information from the settlement file. With this process, we assert that all successful charges will have a corresponding record in our system. Note, this achieves the goal of eventual consistency since the settlement files arrive up to several days after the charge was performed and we don’t make the charge record until we have the settlement file.Occasionally, a charge is successfully applied for a user but we aren’t notified about it right away and thus don’t provision service for the user. In such cases, we need to return the customer’s money as soon as we are notified by the payment processor of the charge. There are two ways to reverse a payment: voiding or refunding.The decision of which method to use is influenced by many things. First, the cost of using an external payment processor involves fees that are assessed on each transaction that we perform through their platform. Voiding a charge, which is basically cancelling it, normally does not cost a fee. Refunding a charge, however, involves performing another payment in the opposite direction for which we need to pay a fee. Therefore, voiding a charge is cheaper than refunding the charge. Second, a voided charge will not show up on a customer’s end of month bank statement at all. Conversely, refunding the charge results in both the original charge and the refund payment being present on the bank statement. This could come as quite the surprise for the customer. From the customer’s perspective, the purchase form submission either returned an error or crashed with a 500 error (if an internal system failure occurred) and yet the customer sees evidence that we charged them. Even though we returned the money, this is still a negative experience for the customer. Third, if the charge request is successful and then we refund this charge later, there is a clear period of time between the charge and the refund during which the customer has less money in their account than they should have. Generally, this is a small amount of money but for some customers this could have a serious impact on their ability to complete other transactions while they are waiting for the refund. For these reasons, voiding is superior to refunding.Unfortunately, the ability to void a transaction depends on the how long it’s been since the charge was completed. To understand why the timing matters, it is necessary to know the steps of fulfilling a charge request. When the payment processor receives the charge request, they record the request in their system, verify the payment information and then ask the credit card company to perform the charge. The credit card company responds that they accept the charge and apply it to the card. At this point the payment processor responds to us, the merchant, to say that the charge was successful. However, at this point, the payment is only “submitted for settlement” and the charge may not have settled on the card yet. Settlement means that the funds have been transferred and the charge can no longer be cancelled. For this reason, voids can only occur during the time window when the payment is in the “submitted for settlement” state, but not yet settled. This time window generally lasts less than 24 hours. If this time window has passed, then a refund must be performed instead.Regardless of which method is used to reverse the transaction, once the reversal is complete, then our records and the customer’s account are now in the correct state. This combination of immediate mitigation and eventual consistency protects us from losing track of payments due to system failures which allows us to confidently assert that we are aware of all payments flowing through our system. This is just one of the ways that the monetization platform team makes sure that Dropbox is being worthy of trust.Thanks for reading our first blog post! The Monetization Platform team is based out of Dropbox’s Seattle office and we’re excited to share some of the things we work on. Learn more about the Monetization Platform team and why we exist in this feature story on The Muse.",https://blogs.dropbox.com/tech/2017/09/handling-system-failures-during-payment-communication/,0,dropbox,"javascript,frontend,react",NULL,2017-09-29
Updates on the Dropbox Bug Bounty Program,"We first launched our bug bounty program in 2014, with initial bounties for critical bugs in the range of $5,000, ramping up to (currently) over $10,000 for critical bugs. Over the past three years, leading security researchers from around the world have participated in our programs with some amazing, often original research. Beyond just the individual bugs, we have learned many a lesson, uncovering unique, interesting threats, exploit vectors, and new research as well as rejigged our priorities based on the bug bounty reports. From Dropbox and all our users, a big THANK YOU to all the researchers that help secure Dropbox for our users!Today, we’re excited to announce a number of improvements to the program, as well as highlight the progress we’ve made internally, in terms of both response and fix times.We know that researchers value quick response and rewards. We recently measured our response times since 2014 and learned that 75% of our responses were within 2 days and 2 hours, with the quickest response being around 50 minutes. We have been working hard to improve our responsiveness and our reward latency even more. Over the last 12 months, we’ve drastically reduced our 75th percentile response time to under 16 hours of the report. For high-quality reports, we usually reward as soon as we reproduce the bug. In fact, we have sometimes paid out within minutes of receipt of a bug.Through the bug bounty program, we have found a pool of incredible researchers who consistently do high-quality work. To further encourage such research, we’ve invited these researchers to a VIP program where we provide early access to upcoming features. Since the start of this program, 75% of our VIP reports got responses within 16 hours, and over the last year, we have reduced this time to 9 hours.Talking to the community, we also know that hackers really value quick resolution of reported bugs. We typically aim to resolve high and critical bugs as soon as possible. We have resolved some bugs in under an hour of the report; for reports with bounties of more than $1,000, we resolved (fixed and out to the world) more than half of them in under 16 days.Dropbox users trust us with some of their most sensitive data, and we work ceaselessly to provide the best possible security for our users. Security researchers participating in our bug bounty program are a critical partner in this effort, and we are excited to announce three new updates to our program.Starting right now, we are delighted to announce that we are more than tripling our bounties, with the reward for critical bugs — for example, bugs that could lead to remote code execution (RCE) on our servers — now topping out at $32,768 and bounties for RCE affecting our desktop/mobile clients at $18,564. To help kickstart this, we have also topped up any critical reports in the last 6 months with the equivalent increased bounty, paying out an additional bounty of over $28,000 for high/critical bugs reported this year.Additionally, we have instituted a process to review particularly novel, high-quality research submitted to our program. At least twice a year, Dropboxers will go through high-quality submissions and award bonuses. Typical factors going into a decision include quality of report/research, interaction with researcher, and so on. With these bonuses, we also aim to encourage novel research. We just went through submissions this year and awarded an additional $14,000 in bonuses. Here are some examples of interesting bugs that we rewarded:We have also started matching bounty donations to charity made through HackerOne. We recently matched a donation to Doctors Without Borders and look forward to supporting many a good cause with this matching.Dropbox loves partnering with the security researchers to protect our users. Thank you to all the researchers who help make Dropbox secure for everyone!Devdatta Akhawe manages the bug bounty program at Dropbox on top of his day job as engineering manager of the Product Safety team. If you’re a security researcher interested in participating in our bug bounty program, please contact us on HackerOne. We are also hiring.",https://blogs.dropbox.com/tech/2017/09/updates-on-the-dropbox-bug-bounty-program/,0,dropbox,"blockchain,bitcoin",NULL,2017-09-20
Infrastructure update: evolution of the Dropbox backbone network,"In our previous post, we provided an overview of the global edge network that we deployed to improve performance for our users around the world. We built this edge network over the last two years as part of a strategy to deliver the benefits of Magic Pocket.Alongside our edge network, we launched a global backbone network that connects our data centers in North America not only to each other, but also to the edge nodes around the world. In this blog, we’ll first review how we went about building out this backbone network and then discuss the benefits that it’s delivering for us and for our users.Over the last three years, our network has evolved significantly to keep up with user growth. We were an early adopter of cloud technology for all of our storage and infrastructure needs before we moved onto Magic Pocket, but the combined effect of migrating hundreds of petabytes of customer data into our own data centers while serving our growing customer base required us to grow our network significantly, and quickly.In early 2015, we began our network expansion initiative to accommodate for 10X scale, provide high (99.999%) reliability, and improve performance for our users. Our internal forecasts pointed to exponential growth in our network traffic as user adoption continued to grow. As we were planning to scale our network, we began to look at deploying technologies like Quality Of Service (QoS), Multi-Protocol Label Switch (MPLS), IPv6, and overhauling our routing architecture to support future growth.Routing Architecture: At that time, our routing architecture was primarily Open Shortest Path First (OSPF) as our Interior Gateway Protocol (IGP), and we had route reflectors (RR) for our Interior Border Gateway Protocol (iBGP) design. As we were planning for 10X scale and deploying new technologies, we were re-evaluating our routing architecture for both IGP and BGP design.IGP Migration: One of the biggest sticking points to continuing with OSPF was the complexity in rolling out IPv6. We were originally using OSPFv2, which only supports IPv4. IPv6—which we were upgrading to—requires OSPFv3. Multiple address families in OSPFv3 were not fully supported by all vendors nor widely deployed at that time. This meant we had to run two versions of OSPF to support v4 and v6, which was operationally more complex.We started to look at replacing OSPF with IS-IS, a protocol-agnostic architecture that runs at OSI layer-2, and can easily support all address types including v4 and v6. In addition, IS-IS uses Type Length Value (TLV) to carry information in Link State Packets. The TLVs make IS-IS easily extensible to carry different kind of information and support newer protocols in future. In Q2 2015 we successfully migrated from OSPF to IS-IS across the entire backbone.iBGP Design: Our initial iBGP design was based on a single hierarchy route-reflector (RR) model. But iBGP RRs have their own limitations, including the fact that they offer limited path diversity. After learning routes from clients, RRs advertise a single best path to their peers. This results in RR peers having visibility into only one path for every prefix, which potentially causes all traffic for that prefix to be sent only to one next-hop, instead of being distributed across several equal-cost next-hops.This results in an unequal load-balance of traffic across the network. We tried to mitigate that issue by using Add-Path, which provides the capability to announce multiple paths. Since Add-Path was still a new feature being developed by routing vendors at that time, we ran into multiple bugs when we tested it. At that point, we decided to come up with a new iBGP design and move away from route reflectors. We debated a couple of design choices, including:We ultimately decided on a hybrid approach of the two: full mesh iBGP that announces selective routes across regions. We now have full mesh iBGP across all routers, but also regionalize our backbone network into smaller groups that have different routing policies. Because transit-provider-routes constitute the bulk of our traffic, we confined routes from transit providers to the region where they originated. All other peering routes and internal traffic is announced across regions. This approach eliminated the limitations of RRs and also solves the route scaling issues due to full-mesh iBGP.MPLS-TE In early 2015, we started rolling out MPLS-TE. To meet and exceed customer expectations, our network must handle failures and rapidly respond to demand spikes. To address the challenge of adapting to dynamic changes in bandwidth capacity and demand, we implemented MPLS with RSVP.MPLS RSVP-TE has a mechanism to react to and adjust for sudden spikes in traffic without manual intervention. When there is sufficient bandwidth available, MPLS ensures traffic will follow the shortest path on the network between its source and destination by establishing a Label Switch Path (LSP) between those points. We deployed multiple LSPs with different priorities: user traffic always takes high-priority LSPs, whereas internal traffic takes low-priority LSPs.As traffic demand goes up (or network capacity goes down because of an outage) RSVP-TE will move LSPs to alternate higher metric paths which have sufficient bandwidth to handle that demand. Because we deploy multiple LSPs with different priorities, RSVP-TE can leave our user traffic on the shortest route and start by moving less critical internal traffic to the longer paths first as shown in the figure below. This allows the network to have redundancy as well as efficient utilization of our network resources to ensure the required level of service and avoid over-provisioning.Quality of Service Quality of service (QoS) is an industry-wide set of standards and mechanisms for ensuring high-quality performance for critical applications. Dropbox’s network carries a mix of latency-sensitive user traffic, and high-volume batch traffic—this includes traffic from data migrations and server provisioning. In 2015, we launched a Quality of Service (QoS) program to identify different traffic types and treat them accordingly, end-to-end. QoS gives us techniques necessary to manage network bandwidth, latency, jitter and packet loss, which helps us guarantee network resources to critical applications during congestion events.To build out this program, we worked with various application owners within Dropbox to mark their services on host machines based on the priority of their service. We classified all Dropbox traffic into four categories and assigned them into respective queues as shown below:We make sure we have enough bandwidth to support all traffic types at all times of the day, but we want to protect critical services against unplanned and unexpected network failure events. QoS helps us do that by allowing us to prioritize Premium (user) traffic over other lower-priority traffic.2016 was the year we re-architected the network and deployed new hardware to support future scalability.Types of routers The Dropbox backbone network consists of routers with three distinct roles:The Dropbox network has two types of traffic: “user traffic,” which flows between Dropbox and the open Internet, and “data center traffic,” which flows between Dropbox data centers. In the old architecture, there was a single network layer, and both traffic types were using the same architecture, passing through the same set of devices.Old Architecture:At first, we used the same hardware device for all three roles. But as we began to scale significantly, our existing designs and the platform that we were using reached their limits. We could have continued down the same path by growing horizontally, but that would have been expensive and operationally complex. We instead decided to re-think our architecture, which led to the evolution of our new, two-tier architecture.Two-Tier Architecture: In the new architecture, we created two network domains to handle each type of traffic independently. We also introduced a new set of routers called DCs to connect data centers. The new data center (DC) tier has full mesh MPLS (RSVP) LSPs between them, and is built on a new set of highly dense backbone routers that can easily scale to multi-terabit capacity. The new DC-tier carries the data center traffic, whereas the old DR-tier is used to transport user traffic, primarily from Dropbox to the Internet. Each tier has its own BGP and MPLS LSP mesh, but they connect to the same set of backbone (BB) routers, sharing the same physical transport network.We have about twice as much data center traffic as user traffic, and both traffic profiles have different characteristics. Data center traffic consists of internal services talking to each other, or copying data from one data center to another. User traffic is always transporting from DRs to a point of presence, and is treated as premium traffic. Peeling off Dropbox internal traffic to its own tier has enabled a clear separation between the two types of traffic, which helps in building traffic profiles and network topologies that are unique to each traffic type.Optical: To support the tremendous growth and maintain consistent service level agreements, we invested in dark fiber connecting our data centers to our PoPs. Leasing dark fiber and running our own optical systems gives us the flexibility to add capacity at a much faster pace, compared to purchasing bandwidth or leased line capacity from optical transport vendors. To build on this, we deployed the latest bleeding edge optical gear available, which gives us the ability to scale quickly and easily.Moving to 100G: In 2016, we started to qualify a next-generation backbone (BB) router with cutting-edge technology that has the scale and density to support tens of terabits of throughput capacity. We spent about eight months qualifying products from different vendors, and ultimately decided to utilize the latest technology product which could support our requirements. Dropbox was one of the first to qualify and deploy this platform in production infrastructure.Our initial deployment in the backbone was with 10G circuits. As the traffic on our network increased, we continued to add more 10G links to increase capacity, ultimately combining those 10G links into a single link aggregation bundle (LAG). By early 2016, we had multiple LAG bundles that each had more than ten 10G links each, which added complexity when it came to provisioning, managing and troubleshooting circuits. We decided to simplify our architecture by replacing multiple 10G circuits with 100G.With the roll-out of new BB routers across our network, we were able to migrate WAN links from multiple 10G LAG bundles to 100G. By June 2017 we migrated all our US and EU WAN links, including our transatlantic links, to 100G. This increased our cumulative WAN capacity by ~300%.IPv6 In Q4 2016 we started rolling out IPv6 across our entire network. One of our design goals was to have parity between IPv4 and IPv6 for both routing as well as forwarding. As part of this roll-out, and to have consistent routing for both v4 and v6, we chose IS-IS single-topology over multi-topology. For forwarding v6 traffic, we intend to use the same set of MPLS-TE LSPs as we used to tunnel v4 traffic. We could do that by using IGP short-cuts as defined in rfc3906. With the implementation of IGP-shortcuts, both v6 and v4 traffic were using the same MPLS-LSPs across the backbone. By end of Q1 2017 we completed v6 roll-out across our data centers, backbone and edge.Dropbox manages hundreds of gigabits of traffic, and we’re growing at a rapid pace. To keep up, one of the mantras the Dropbox Network Engineering team has adopted is to always “build for scale.” Building for scale isn’t about adding more network capacity, nodes, or devices. Instead, we’re periodically updating our architecture, and always thinking about how to grow and operate the network at ten times the scale we operate at today.This mindset means we’re always planning two to three years ahead, so that we have all of the tools, automation, and monitoring in place to support operating at a capacity that’s ten times greater than what we’re operating at now. The same principle applies across the entire network, whether it is Datacenter, Backbone, or Edge.We’re on the lookout for experienced network engineers to help scale the Dropbox edge network beyond terabits of traffic and 100G uplinks. Or how about our backbone network where we’re constantly improving the reliability and performance of our CLOS-based fabrics. We also have a hybrid security/networking position for a Network Security Engineer in San Francisco. Want something more high level? The traffic team is also hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2017/09/infrastructure-update-evolution-of-the-dropbox-backbone-network/,0,dropbox,"backend,cloud,docker",NULL,2017-09-15
Optimizing web servers for high throughput and low latency," This is an expanded version of my talk at NginxConf 2017 on September 6, 2017. As an SRE on the Dropbox Traffic Team, I’m responsible for our Edge network: its reliability, performance, and efficiency. The Dropbox edge network is an nginx-based proxy tier designed to handle both latency-sensitive metadata transactions and high-throughput data transfers. In a system that is handling tens of gigabits per second while simultaneously processing tens of thousands latency-sensitive transactions, there are efficiency/performance optimizations throughout the proxy stack, from drivers and interrupts, through TCP/IP and kernel, to library, and application level tunings.In this post we’ll be discussing lots of ways to tune web servers and proxies. Please do not cargo-cult them. For the sake of the scientific method, apply them one-by-one, measure their effect, and decide whether they are indeed useful in your environment.This is not a Linux performance post, even though I will make lots of references to bcc tools, eBPF, and perf, this is by no means the comprehensive guide to using performance profiling tools. If you want to learn more about them you may want to read through Brendan Gregg’s blog.This is not a browser-performance post either. I’ll be touching client-side performance when I cover latency-related optimizations, but only briefly. If you want to know more, you should read High Performance Browser Networking by Ilya Grigorik.And, this is also not the TLS best practices compilation. Though I’ll be mentioning TLS libraries and their settings a bunch of times, you and your security team, should evaluate the performance and security implications of each of them. You can use Qualys SSL Test, to verify your endpoint against the current set of best practices, and if you want to know more about TLS in general, consider subscribing to Feisty Duck Bulletproof TLS Newsletter.We are going to discuss efficiency/performance optimizations of different layers of the system. Starting from the lowest levels like hardware and drivers: these tunings can be applied to pretty much any high-load server. Then we’ll move to linux kernel and its TCP/IP stack: these are the knobs you want to try on any of your TCP-heavy boxes. Finally we’ll discuss library and application-level tunings, which are mostly applicable to web servers in general and nginx specifically.For each potential area of optimization I’ll try to give some background on latency/throughput tradeoffs (if any), monitoring guidelines, and, finally, suggest tunings for different workloads.For good asymmetric RSA/EC performance you are looking for processors with at least AVX2 (avx2 in /proc/cpuinfo) support and preferably for ones with large integer arithmetic capable hardware (bmi and adx). For the symmetric cases you should look for AES-NI for AES ciphers and AVX512 for ChaCha+Poly. Intel has a performance comparison of different hardware generations with OpenSSL 1.0.2, that illustrates effect of these hardware offloads.Latency sensitive use-cases, like routing, will benefit from fewer NUMA nodes and disabled HT. High-throughput tasks do better with more cores, and will benefit from Hyper-Threading (unless they are cache-bound), and generally won’t care about NUMA too much.Specifically, if you go the Intel path, you are looking for at least Haswell/Broadwell and ideally Skylake CPUs. If you are going with AMD, EPYC has quite impressive performance.Here you are looking for at least 10G, preferably even 25G. If you want to push more than that through a single server over TLS, the tuning described here will not be sufficient, and you may need to push TLS framing down to the kernel level (e.g. FreeBSD, Linux).On the software side, you should look for open source drivers with active mailing lists and user communities. This will be very important if (but most likely, when) you’ll be debugging driver-related problems.The rule of thumb here is that latency-sensitive tasks need faster memory, while throughput-sensitive tasks need more memory.It depends on your buffering/caching requirements, but if you are going to buffer or cache a lot you should go for flash-based storage. Some go as far as using a specialized flash-friendly filesystem (usually log-structured), but they do not always perform better than plain ext4/xfs.Anyway just be careful to not burn through your flash because you forgot to turn enable TRIM, or update the firmware.You should keep your firmware up-to-date to avoid painful and lengthy troubleshooting sessions. Try to stay recent with CPU Microcode, Motherboard, NICs, and SSDs firmwares. That does not mean you should always run bleeding edge—the rule of thumb here is to run the second to the latest firmware, unless it has critical bugs fixed in the latest version, but not run too far behind.The update rules here are pretty much the same as for firmware. Try staying close to current. One caveat here is to try to decoupling kernel upgrades from driver updates if possible. For example you can pack your drivers with DKMS, or pre-compile drivers for all the kernel versions you use. That way when you update the kernel and something does not work as expected there is one less thing to troubleshoot.Your best friend here is the kernel repo and tools that come with it. In Ubuntu/Debian you can install the linux-tools package, with handful of utils, but now we only use cpupower, turbostat, and x86_energy_perf_policy. To verify CPU-related optimizations you can stress-test your software with your favorite load-generating tool (for example, Yandex uses Yandex.Tank.) Here is a presentation from the last NginxConf from developers about nginx loadtesting best-practices: “NGINX Performance testing.”cpupowerUsing this tool is way easier than crawling /proc/. To see info about your processor and its frequency governor you should run:Check that Turbo Boost is enabled, and for Intel CPUs make sure that you are running with intel_pstate, not the acpi-cpufreq, or even pcc-cpufreq. If you still using acpi-cpufreq, then you should upgrade the kernel, or if that’s not possible, make sure you are using performance governor. When running with intel_pstate, even powersave governor should perform well, but you need to verify it yourself.And speaking about idling, to see what is really happening with your CPU, you can use turbostat to directly look into processor’s MSRs and fetch Power, Frequency, and Idle State information:Here you can see the actual CPU frequency (yes, /proc/cpuinfo is lying to you), and core/package idle states.If even with the intel_pstate driver the CPU spends more time in idle than you think it should, you can:Or, only for very latency critical tasks you can:You can learn more about processor power management in general and P-states specifically in the Intel OpenSource Technology Center presentation “Balancing Power and Performance in the Linux Kernel” from LinuxCon Europe 2015.You can additionally reduce latency by applying CPU affinity on each thread/process, e.g. nginx has worker_cpu_affinity directive, that can automatically bind each web server process to its own core. This should eliminate CPU migrations, reduce cache misses and pagefaults, and slightly increase instructions per cycle. All of this is verifiable through perf stat.Sadly, enabling affinity can also negatively affect performance by increasing the amount of time a process spends waiting for a free CPU. This can be monitored by running runqlat on one of your nginx worker’s PIDs:If you see multi-millisecond tail latencies there, then there is probably too much stuff going on on your servers besides nginx itself, and affinity will increase latency, instead of decreasing it.All mm/ tunings are usually very workflow specific, there are only a handful of things to recommend:Modern CPUs are actually multiple separate CPU dies connected by very fast interconnect and sharing various resources, starting from L1 cache on the HT cores, through L3 cache within the package, to Memory and PCIe links within sockets. This is basically what NUMA is: multiple execution and storage units with a fast interconnect.For the comprehensive overview of NUMA and its implications you can consult “NUMA Deep Dive Series” by Frank Denneman.But, long story short, you have a choice of:Let’s talk about the third option, since there is not much optimization needed for the first two.To utilize NUMA properly you need to treat each numa node as a separate server, for that you should first inspect the topology, which can be done with numactl --hardware:Things to look after:This is a particularly bad example since it has 4 nodes as well as nodes without memory attached. It is impossible to treat each node here as a separate server without sacrificing half of the cores on the system.We can verify that by using numastat:You can also ask numastat to output per-node memory usage statistics in the /proc/meminfo format:Now lets look at the example of a simpler topology.Since the nodes are mostly symmetrical we can bind an instance of our application to each NUMA node with numactl --cpunodebind=X --membind=X and then expose it on a different port, that way you can get better throughput by utilizing both nodes and better latency by preserving memory locality.You can verify NUMA placement efficiency by latency of your memory operations, e.g. by using bcc’s funclatency to measure latency of the memory-heavy operation, e.g. memmove.On the kernel side, you can observe efficiency by using perf stat and looking for corresponding memory and scheduler events:The last bit of NUMA-related optimizations for network-heavy workloads comes from the fact that a network card is a PCIe device and each device is bound to its own NUMA-node, therefore some CPUs will have lower latency when talking to the network. We’ll discuss optimizations that can be applied there when we discuss NIC→CPU affinity, but for now lets switch gears to PCI-Express…Normally you do not need to go too deep into PCIe troubleshooting unless you have some kind of hardware malfunction. Therefore it’s usually worth spending minimal effort there by just creating “link width”, “link speed”, and possibly RxErr/BadTLP alerts for your PCIe devices. This should save you troubleshooting hours because of broken hardware or failed PCIe negotiation. You can use lspci for that:PCIe may become a bottleneck though if you have multiple high-speed devices competing for the bandwidth (e.g. when you combine fast network with fast storage), therefore you may need to physically shard your PCIe devices across CPUs to get maximum throughput.source: https://en.wikipedia.org/wiki/PCI_Express#History_and_revisionsAlso see the article, “Understanding PCIe Configuration for Maximum Performance,” on the Mellanox website, that goes a bit deeper into PCIe configuration, which may be helpful at higher speeds if you observe packet loss between the card and the OS.Intel suggests that sometimes PCIe power management (ASPM) may lead to higher latencies and therefore higher packet loss. You can disable it by adding pcie_aspm=off to the kernel cmdline.Before we start, it worth mentioning that both Intel and Mellanox have their own performance tuning guides and regardless of the vendor you pick it’s beneficial to read both of them. Also drivers usually come with a README on their own and a set of useful utilities.Next place to check for the guidelines is your operating system’s manuals, e.g. Red Hat Enterprise Linux Network Performance Tuning Guide, which explains most of the optimizations mentioned below and even more.Cloudflare also has a good article about tuning that part of the network stack on their blog, though it is mostly aimed at low latency use-cases.When optimizing NICs ethtool will be your best friend.A small note here: if you are using a newer kernel (and you really should!) you should also bump some parts of your userland, e.g. for network operations you probably want newer versions of: ethtool, iproute2, and maybe iptables/nftables packages.Valuable insight into what is happening with you network card can be obtained via ethtool -S:Consult with your NIC manufacturer for detailed stats description, e.g. Mellanox have a dedicated wiki page for them.From the kernel side of things you’ll be looking at /proc/interrupts, /proc/softirqs, and /proc/net/softnet_stat. There are two useful bcc tools here: hardirqs and softirqs. Your goal in optimizing the network is to tune the system until you have minimal CPU usage while having no packet loss.Interrupt AffinityTunings here usually start with spreading interrupts across the processors. How specifically you should do that depends on your workload:Vendors usually provide scripts to do that, e.g. Intel has set_irq_affinity.Ring buffer sizesNetwork cards need to exchange information with the kernel. This is usually done through a data structure called a “ring”, current/maximum size of that ring viewed via ethtool -g:You can adjust these values within pre-set maximums with -G. Generally bigger is better here (esp. if you are using interrupt coalescing), since it will give you more protection against bursts and in-kernel hiccups, therefore reducing amount of dropped packets due to no buffer space/missed interrupt. But there are couple of caveats:CoalescingInterrupt coalescing allows you to delay notifying the kernel about new events by aggregating multiple events in a single interrupt. Current setting can be viewed via ethtool -c:You can either go with static limits, hard-limiting maximum number of interrupts per second per core, or depend on the hardware to automatically adjust the interrupt rate based on the throughput.Enabling coalescing (with -C) will increase latency and possibly introduce packet loss, so you may want to avoid it for latency sensitive. On the other hand, disabling it completely may lead to interrupt throttling and therefore limit your performance.OffloadsModern network cards are relatively smart and can offload a great deal of work to either hardware or emulate that offload in drivers themselves.All possible offloads can be obtained with ethtool -k:In the output all non-tunable offloads are marked with [fixed] suffix.There is a lot to say about all of them, but here are some rules of thumb:Here are couple of best practices from our production:Flow Director and ATREnabled flow director (or fdir in Intel terminology) operates by default in an Application Targeting Routing mode which implements aRFS by sampling packets and steering flows to the core where they presumably are being handled. Its stats are also accessible through ethtool -S:Though Intel claims that fdir increases performance in some cases, external research suggests that it can also introduce up to 1% of packet reordering, which can be quite damaging for TCP performance. Therefore try testing it for yourself and see if FD is useful for your workload, while keeping an eye for the TCPOFOQueue counter.There are countless books, videos, and tutorials for the tuning the Linux networking stack. And sadly tons of “sysctl.conf cargo-culting” that comes with them. Even though recent kernel versions do not require as much tuning as they used to 10 years ago and most of the new TCP/IP features are enabled and well-tuned by default, people are still copy-pasting their old sysctls.conf that they’ve used to tune 2.6.18/2.6.32 kernels.To verify effectiveness of network-related optimizations you should:For sources of information about network optimizations, I usually enjoy conference talks by CDN-folks since they generally know what they are doing, e.g. Fastly on LinuxCon Australia. Listening what Linux kernel devs say about networking is quite enlightening too, for example netdevconf talks and NETCONF transcripts.It worth highlighting good deep-dives into Linux networking stack by PackageCloud, especially since they put an accent on monitoring instead of blindly tuning things:Before we start, let me state it one more time: upgrade your kernel! There are tons of new network stack improvements, and I’m not even talking about IW10 (which is so 2010). I am talking about new hotness like: TSO autosizing, FQ, pacing, TLP, and RACK, but more on that later. As a bonus by upgrading to a new kernel you’ll get a bunch of scalability improvements, e.g.: removed routing cache, lockless listen sockets, SO_REUSEPORT, and many more.From the recent Linux networking papers the one that stands out is “Making Linux TCP Fast.” It manages to consolidate multiple years of Linux kernel improvements on 4 pages by breaking down Linux sender-side TCP stack into functional pieces:Fair Queueing is responsible for improving fairness and reducing head of line blocking between TCP flows, which positively affects packet drop rates. Pacing schedules packets at rate set by congestion control equally spaced over time, which reduces packet loss even further, therefore increasing throughput.As a side note: Fair Queueing and Pacing are available in linux via fq qdisc. Some of you may know that these are a requirement for BBR (not anymore though), but both of them can be used with CUBIC, yielding up to 15-20% reduction in packet loss and therefore better throughput on loss-based CCs. Just don’t use it in older kernels (< 3.19), since you will end up pacing pure ACKs and cripple your uploads/RPCs.Both of these are responsible for limiting buffering inside the TCP stack and hence reducing latency, without sacrificing throughput.CC algorithms are a huge subject by itself, and there was a lot of activity around them in recent years. Some of that activity was codified as: tcp_cdg (CAIA), tcp_nv (Facebook), and tcp_bbr (Google). We won’t go too deep into discussing their inner-workings, let’s just say that all of them rely more on delay increases than packet drops for a congestion indication.BBR is arguably the most well-documented, tested, and practical out of all new congestion controls. The basic idea is to create a model of the network path based on packet delivery rate and then execute control loops to maximize bandwidth while minimizing rtt. This is exactly what we are looking for in our proxy stack.Preliminary data from BBR experiments on our Edge PoPs shows an increase of file download speeds:6 hour TCP BBR experiment in Tokyo PoP: x-axis — time, y-axis — client download speedHere I want to stress out that we observe speed increase across all percentiles. That is not the case for backend changes. These usually only benefit p90+ users (the ones with the fastest internet connectivity), since we consider everyone else being bandwidth-limited already. Network-level tunings like changing congestion control or enabling FQ/pacing show that users are not being bandwidth-limited but, if I can say this, they are “TCP-limited.”If you want to know more about BBR, APNIC has a good entry-level overview of BBR (and its comparison to loss-based congestions controls). For more in-depth information on BBR you probably want to read through bbr-dev mailing list archives (it has a ton of useful links pinned at the top). For people interested in congestion control in general it may be fun to follow Internet Congestion Control Research Group activity.But enough about congestion control, let’s talk about let’s talk about loss detection, here once again running the latest kernel will help quite a bit. New heuristics like TLP and RACK are constantly being added to TCP, while the old stuff like FACK and ER is being retired. Once added, they are enabled by default so you do not need to tune any system settings after the upgrade.Userspace socket APIs provide implicit buffering and no way to re-order chunks once they are sent, therefore in multiplexed scenarios (e.g. HTTP/2) this may result in a HOL blocking, and inversion of h2 priorities. TCP_NOTSENT_LOWAT socket option (and corresponding net.ipv4.tcp_notsent_lowat sysctl) were designed to solve this problem by setting a threshold at which the socket considers itself writable (i.e. epoll will lie to your app). This can solve problems with HTTP/2 prioritization, but it can also potentially negatively affect throughput, so you know the drill—test it yourself.One does not simply give a networking optimization talk without mentioning sysctls that need to be tuned. But let me first start with the stuff you don’t want to touch:As for sysctls that you should be using:It also worth noting that there is an RFC draft (though a bit inactive) from the author of curl, Daniel Stenberg, named TCP Tuning for HTTP, that tries to aggregate all system tunings that may be beneficial to HTTP in a single place.Just like with the kernel, having up-to-date userspace is very important. You should start with upgrading your tools, for example you can package newer versions of perf, bcc, etc.Once you have new tooling you are ready to properly tune and observe the behavior of a system. Through out this part of the post we’ll be mostly relying on on-cpu profiling with perf top, on-CPU flamegraphs, and adhoc histograms from bcc’s funclatency.Having a modern compiler toolchain is essential if you want to compile hardware-optimized assembly, which is present in many libraries commonly used by web servers.Aside from the performance, newer compilers have new security features (e.g. -fstack-protector-strong or SafeStack) that you want to be applied on the edge. The other use case for modern toolchains is when you want to run your test harnesses against binaries compiled with sanitizers (e.g. AddressSanitizer, and friends).It’s also worth upgrading system libraries, like glibc, since otherwise you may be missing out on recent optimizations in low-level functions from -lc, -lm, -lrt, etc. Test-it-yourself warning also applies here, since occasional regressions creep in.Normally web server would be responsible for compression. Depending on how much data is going though that proxy, you may occasionally see zlib’s symbols in perf top, e.g.:There are ways of optimizing that on the lowest levels: both Intel and Cloudflare, as well as a standalone zlib-ng project, have their zlib forks which provide better performance by utilizing new instructions sets.We’ve been mostly CPU-oriented when discussing optimizations up until now, but let’s switch gears and discuss memory-related optimizations. If you use lots of Lua with FFI or heavy third party modules that do their own memory management, you may observe increased memory usage due to fragmentation. You can try solving that problem by switching to either jemalloc or tcmalloc.Using custom malloc also has the following benefits:If you use many complex regular expressions in your nginx configs or heavily rely on Lua, you may see pcre-related symbols in perf top. You can optimize that by compiling PCRE with JIT, and also enabling it in nginx via pcre_jit on;.You can check the result of optimization by either looking at flame graphs, or using funclatency:If you are terminating TLS on the edge w/o being fronted by a CDN, then TLS performance optimizations may be highly valuable. When discussing tunings we’ll be mostly focusing server-side efficiency.So, nowadays first thing you need to decide is which TLS library to use: Vanilla OpenSSL, OpenBSD’s LibreSSL, or Google’s BoringSSL. After picking the TLS library flavor, you need to properly build it: OpenSSL for example has a bunch of built-time heuristics that enable optimizations based on build environment; BoringSSL has deterministic builds, but sadly is way more conservative and just disables some optimizations by default. Anyway, here is where choosing a modern CPU should finally pay off: most TLS libraries can utilize everything from AES-NI and SSE to ADX and AVX512. You can use built-in performance tests that come with your TLS library, e.g. in BoringSSL case it’s the bssl speed.Most of performance comes not from the hardware you have, but from cipher-suites you are going to use, so you have to optimize them carefully. Also know that changes here can (and will!) affect security of your web server—the fastest ciphersuites are not necessarily the best. If unsure what encryption settings to use, Mozilla SSL Configuration Generator is a good place to start.Asymmetric EncryptionIf your service is on the edge, then you may observe a considerable amount of TLS handshakes and therefore have a good chunk of your CPU consumed by the asymmetric crypto, making it an obvious target for optimizations.To optimize server-side CPU usage you can switch to ECDSA certs, which are generally 10x faster than RSA. Also they are considerably smaller, so it may speedup handshake in presence of packet-loss. But ECDSA is also heavily dependent on the quality of your system’s random number generator, so if you are using OpenSSL, be sure to have enough entropy (with BoringSSL you do not need to worry about that).As a side note, it worth mentioning that bigger is not always better, e.g. using 4096 RSA certs will degrade your performance by 10x:To make it worse, smaller isn’t necessarily the best choice either: by using non-common p-224 field for ECDSA you’ll get 60% worse performance compared to a more common p-256:The rule of thumb here is that the most commonly used encryption is generally the most optimized one.When running properly optimized OpenTLS-based library using RSA certs, you should see the following traces in your perf top: AVX2-capable, but not ADX-capable boxes (e.g. Haswell) should use AVX2 codepath:While newer hardware should use a generic montgomery multiplication with ADX codepath:Symmetric Encryption If you have lot’s of bulk transfers like videos, photos, or more generically files, then you may start observing symmetric encryption symbols in profiler’s output. Here you just need to make sure that your CPU has AES-NI support and you set your server-side preferences for AES-GCM ciphers. Properly tuned hardware should have following in perf top:But it’s not only your servers that will need to deal with encryption/decryption—your clients will share the same burden on a way less capable CPU. Without hardware acceleration this may be quite challenging, therefore you may consider using an algorithm that was designed to be fast without hardware acceleration, e.g. ChaCha20-Poly1305. This will reduce TTLB for some of your mobile clients.ChaCha20-Poly1305 is supported in BoringSSL out of the box, for OpenSSL 1.0.2 you may consider using Cloudflare patches. BoringSSL also supports “equal preference cipher groups,” so you may use the following config to let clients decide what ciphers to use based on their hardware capabilities (shamelessly stolen from cloudflare/sslconfig):To analyze effectiveness of your optimizations on that level you will need to collect RUM data. In browsers you can use Navigation Timing APIs and Resource Timing APIs. Your main metrics are TTFB and TTV/TTI. Having that data in an easily queriable and graphable formats will greatly simplify iteration.Compression in nginx starts with mime.types file, which defines default correspondence between file extension and response MIME type. Then you need to define what types you want to pass to your compressor with e.g. gzip_types. If you want the complete list you can use mime-db to autogenerate your mime.types and to add those with .compressible == true to gzip_types.When enabling gzip, be careful about two aspects of it:As a side note, http compression is not limited to gzip exclusively: nginx has a third party ngx_brotli module that can improve compression ratio by up to 30% compared to gzip.As for compression settings themselves, let’s discuss two separate use-cases: static and dynamic data.Buffering inside the proxy can greatly affect web server performance, especially with respect to latency. The nginx proxy module has various buffering knobs that are togglable on a per-location basis, each of them is useful for its own purpose. You can separately control buffering in both directions via proxy_request_buffering and proxy_buffering. If buffering is enabled the upper limit on memory consumption is set by client_body_buffer_size and proxy_buffers, after hitting these thresholds request/response is buffered to disk. For responses this can be disabled by setting proxy_max_temp_file_size to 0.Most common approaches to buffering are:Whatever path you choose, do not forget to test its effect on both TTFB and TTLB. Also, as mentioned before, buffering can affect IO usage and even backend utilization, so keep an eye out for that too.Now we are going to talk about high-level aspects of TLS and latency improvements that could be done by properly configuring nginx. Most of the optimizations I’ll be mentioning are covered in the High Performance Browser Networking’s “Optimizing for TLS” section and Making HTTPS Fast(er) talk at nginx.conf 2014. Tunings mentioned in this part will affect both performance and security of your web server, if unsure, please consult with Mozilla’s Server Side TLS Guide and/or your Security Team.To verify the results of optimizations you can use:Session resumptionAs DBAs love to say “the fastest query is the one you never make.” The same goes for TLS—you can reduce latency by one RTT if you cache the result of the handshake. There are two ways of doing that:As a side note, if you go with session ticket approach, then it’s worth using 3 keys instead of one, e.g.:You will be always encrypting with the current key, but accepting sessions encrypted with both next and previous keys.OCSP StaplingYou should staple your OCSP responses, since otherwise:To staple the OCSP response you can periodically fetch it from your certificate authority, distribute the result to your web servers, and use it with the ssl_stapling_file directive:TLS record sizeTLS breaks data into chunks called records, which you can’t verify and decrypt until you receive it in its entirety. You can measure this latency as the difference between TTFB from the network stack and application points of view.By default nginx uses 16k chunks, which do not even fit into IW10 congestion window, therefore require an additional roundtrip. Out-of-the box nginx provides a way to set record sizes via ssl_buffer_size directive:There are two problems with static tuning:There is an alternative approach: dynamic record size tuning. There is an nginx patch from Cloudflare that adds support for dynamic record sizes. It may be a pain to initially configure it, but once you over with it, it works quite nicely.TLS 1.3TLS 1.3 features indeed sound very nice, but unless you have resources to be troubleshooting TLS full-time I would suggest not enabling it, since:Nginx is an eventloop-based web server, which means it can only do one thing at a time. Even though it seems that it does all of these things simultaneously, like in time-division multiplexing, all nginx does is just quickly switches between the events, handling one after another. It all works because handling each event takes only couple of microseconds. But if it starts taking too much time, e.g. because it requires going to a spinning disk, latency can skyrocket.If you start noticing that your nginx are spending too much time inside the ngx_process_events_and_timers function, and distribution is bimodal, then you probably are affected by eventloop stalls.AIO and ThreadpoolsSince the main source of eventloop stalls especially on spinning disks is IO, you should probably look there first. You can measure how much you are affected by it by running fileslower:To fix this, nginx has support for offloading IO to a threadpool (it also has support for AIO, but native AIO in Unixes have lots of quirks, so better to avoid it unless you know what you doing). A basic setup consists of simply:For more complicated cases you can set up custom thread_pool‘s, e.g. one per-disk, so that if one drive becomes wonky, it won’t affect the rest of the requests. Thread pools can greatly reduce the number of nginx processes stuck in D state, improving both latency and throughput. But it won’t eliminate eventloop stalls fully, since not all IO operations are currently offloaded to it.Logging Writing logs can also take a considerable amount of time, since it is hitting disks. You can check whether that’s that case by running ext4slower and looking for access/error log references:It is possible to workaround this by spooling access logs in memory before writing them by using buffer parameter for the access_log directive. By using gzip parameter you can also compress the logs before writing them to disk, reducing IO pressure even more.But to fully eliminate IO stalls on log writes you should just write logs via syslog, this way logs will be fully integrated with nginx eventloop.Open file cacheSince open(2) calls are inherently blocking and web servers are routinely opening/reading/closing files it may be beneficial to have a cache of open files. You can see how much benefit there is by looking at ngx_open_cached_file function latency:If you see that either there are too many open calls or there are some that take too much time, you can can look at enabling open file cache:After enabling open_file_cache you can observe all the cache misses by looking at opensnoop and deciding whether you need to tune the cache limits:All optimizations that were described in this post are local to a single web server box. Some of them improve scalability and performance. Others are relevant if you want to serve requests with minimal latency or deliver bytes faster to the client. But in our experience a huge chunk of user-visible performance comes from a more high-level optimizations that affect behavior of the Dropbox Edge Network as a whole, like ingress/egress traffic engineering and smarter Internal Load Balancing. These problems are on the edge (pun intended) of knowledge, and the industry has only just started approaching them.Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/,0,dropbox,"backend,angular,frontend,json,php,python,react,docker",NULL,2017-09-06
Evolution of Dropbox’s Edge Network,"Update (November 14, 2017): Miami, Sydney, Paris, Milan and Madrid have been added to the Dropbox Edge Network.Since launching Magic Pocket last year, we’ve been storing and serving more than 90 percent of our users’ data on our own custom-built infrastructure, which has helped us to be more efficient and improved performance for our users globally.But with about 75 percent of our users located outside of the United States, moving onto our own custom-built data center was just the first step in realizing these benefits. As our data centers grew, the rest of our network also expanded to serve our users — more than 500 million around the globe — at light-speed with a consistent level of reliability, whether they were in San Francisco or Singapore.To do that, we’ve built a network across 14 cities in seven countries on three continents. In doing so, we’ve added hundreds of gigabits of Internet connectivity with transit providers (regional and global ISPs), and hundreds of new peering partners (where we exchange traffic directly rather than through an ISP). We also designed a custom-built edge-proxy architecture into our network. The edge proxy is a stack of servers that act as the first gateway for TLS & TCP handshake for users and is deployed in PoPs (points of presence) to improve the performance for a user accessing Dropbox from any part of the globe. We evaluated some more standard offerings (CDNs and other “cloud” products) but for our specific needs this custom solution was best. Some users have seen and have increased sync speeds by as much as 300 percent, and performance has improved across the board.Dropbox Edge Network 2014Dropbox Edge Network 2015Dropbox Edge Network 2016Going back to 2014, our network presence was only in the US. With two data center regions (one on each coast) storing all our user data, and five network PoPs in major cities across the country where we saw the most traffic. This meant that users across the globe could only be served from the US, we were heavily reliant on transit, and often higher latency paths across the Internet limited performance for international users.Each PoP was also connected to the local Internet Exchange in the facility where it was located, which enabled us to peer with multiple end-user networks also connected to the exchange. At this time we peered with only about 100 networks, and traffic was unevenly spread across our PoPs, with some seeing much more ingress and egress traffic than others over both Peering and Transit links. Because of this traffic pattern, we relied mostly on Transit from tier-1 providers to guarantee reliable and comprehensive connectivity to end users and allow a single point of contact during outages.Our edge capacity was in the hundreds of gigabits, nearly all of which was with our transit providers and shifting traffic between PoPs was a challenge.In 2014 we were using Border Gateway Protocol (BGP) at the edge of the network to connect with the transit and fabric peers in our network, and within the backbone to connect to the data centers. We used Open Shortest Path First (OSPF) as the underlying protocol for resolving Network Layer Reachability Information (NLRI) required by BGP within the Dropbox network.Within the routing policies, we were using extended-BGP-communities which are tagged to prefixes within the network as well as prefixes learned from peers like transit and fabric. We also use various path attributes in the BGP protocol suite that are used for selecting an egress path for a prefix if more than one path exists.In early 2015, we overhauled our routing architecture, migrating from OSPF to IS-IS, changing our BGP communities, and implementing MPLS-TE to improve how we utilized our network backbone. The latter is an algorithm that provides an efficient way of forwarding traffic throughout the network, avoiding over-utilized and under-utilized links. This improved how our network handled dynamic changes in traffic flows between the growing number of network PoPs. More details on these changes will be covered in a future Backbone Blog.By mid-2015, we started thinking about how we could serve our users more efficiently, reduce round trip time and optimize the egress path from Dropbox to the end user.We were growing rapidly outside of the U.S., and started focusing our attention on the European market, specifically looking for locations where we could peer with more end user networks in the region. We selected three European PoPs which provided connectivity to the major peering exchanges and ambitiously expanded our peering edge in both North America and Europe. Our peering policy is open and can be referenced here: Dropbox Peering Policy.By the end of 2015, we added three new PoPs at Palo Alto, Dallas and New York, along with hundreds of gigabits of transit capacity, and we increased both the number of peer networks, and our traffic over peering connections substantially. Though we were still predominantly relying on our transit partners, our expanded peering footprint, geographically and in terms of capacity, allowed us to implement more extensive traffic engineering to improve user performance. It also laid the foundation for our new edge proxy design.As we started 2016, we sharpened our focus on three key areas:Based on the data collected for traffic flows, and a variety of other considerations, we narrowed our focus to London, Frankfurt, and Amsterdam, which offer the maximum number of eye-ball networks for cities across Europe. These were successfully deployed in 2016 in a ring topology via the backbone and were connected back to the US through New York and Ashburn as port of entries in the US.At the same time, we saw an increase in our traffic volumes from Asia in 2016, so we started a similar exercise to what we did for Europe. We decided to expand Dropbox’s edge network across Asia in Tokyo, Singapore, Hong Kong in Q3-2016. These locations were selected to serve local as well as other eyeball networks that use Dropbox within the Asia-pacific region.Once we had our PoP locations in place, we built out the new architecture to accelerate our network transfers.The edge proxy stack handles user facing SSL termination and maintains connectivity to our backend servers throughout the Dropbox network. Edge proxy stack comprises of IPVS and NGINX machines.More details on proxy stack architecture will be covered in a future blog post.With the proxy stack in place, we turned our attention to routing. Our original strategy was to advertise all our public prefixes from every PoP. This made sense when our front-ends were consolidated in our data centers. With our proxy stack coming online and new PoPs being deployed we needed to change this to avoid asymmetric or sub-optimal routing. Doing so allows us to ensure users are served from from the PoP closest to them. Factors that we considered for new routing policy design were:In the new design, we introduced the concept of “metro”, which meant breaking regions into individual metros. This design was validated based on Dropbox traffic flows and requirements. The idea behind a metro is:We also updated our BGP communities to support the new metro scope. Prefixes are now tagged with their Origin. Internally and externally learned routes are assigned the appropriate Route-Type. Prefix advertisements are limited (or summarized) to a Metro, Region, Continent, or Global as appropriate based on their Route-Scope. In addition we have defined a set of Actions which can be applied to a prefix which have internal meanings to the routers. The use of a Tag allows us to include other information (such as the community) with the prefix for special handling.Dropbox’s transit capacity until mid-2016 was more uneven and imbalanced than it is today. The IP transit ports in every metro had different capacity, so if we had to drain a metro, we wouldn’t necessarily be able to route traffic to the nearest PoP due to limited capacity. To fix this issue, we standardized the IP transit capacity across all the PoPs to ensure sufficient capacity is available in each PoP. Now, if a PoP goes down or if we have to do a Disaster Recovery Testing (DRT) exercise, we know that we will have enough capacity to move traffic between metros. Dropbox ingress traffic coming from the transit providers was also imbalanced. So we worked in collaboration with our tier-1 providers in implementing various solutions to fix the ingress imbalance into our ASN.We also re-designed our edge routing policies for IP transit so that a prefix now uses the shortest AS-PATH to exit our ASN between transit providers. If there is a tie between AS-PATH among multiple tier-1 transit providers, then one of the bgp attributes for path selection which is Multi-exit Discriminator (MED) would be used to break the tie.Up until Q1-2016 the majority of Dropbox traffic was egressing out via transit providers because our peering footprint was relatively small. We started identifying the top ASN’s behind transit providers in every metro and started collecting some data through netflow. By mid Q2, we had a list of certain ASNs (i.e., eye ball networks) with whom we could initiate some peering discussion. By the end of 2016, we shifted 30% of traffic to peering for better connectivity and to get closer to the user. Users across the globe saw significant performance improvements while accessing Dropbox.By executing the SSL handshake via our PoPs instead of sending them to our data centers, we’ve been able to significantly improve connection times and accelerate transfer speeds. We’ve tested and applied this configuration in various markets in Europe and Asia. For some users in Europe, for example, median download speeds are 40% faster after introducing edge proxy servers, while median upload speeds are approximately 90% faster. In Japan, median download speeds have doubled, while median upload speeds are three times as fast.The below graphs show major improvements in the TCP/SSL experience for users in Europe and Asia-pacific after the edge proxy stacks were deployed in every PoP within Dropbox. These graphs plot connect times for different countries (lower is better).We’ve also heard from several customers across Europe and Asia that their performance to Dropbox has significantly improved since we launched these PoPs. The below graph shows how latency dropped for one of these customers.By the end of 2016, we had added six new PoPs across Europe and Asia, giving us a total of 14 PoPs and bringing our edge capacity into terabits. We added hundreds of additional gigs of transit, fabric, private peer capacity based on metro/regional traffic ratios and standardized transit footprint across the network. We also added 200+ unique ASN’s via peering. Today, the majority of our Internet traffic goes from a user’s best/closest PoP directly over peering, improving performance for our users and improving our network efficiency.We’re on the lookout for experienced network engineers to help scale the Dropbox edge network beyond terabits of traffic and 100G uplinks. Or how about our backbone network where we’re constantly improving the reliability and performance of our CLOS-based fabrics. We also have a hybrid security/networking position for a Network Security Engineer in San Francisco. Want something more high level? The traffic team is also hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2017/06/evolution-of-dropboxs-edge-network/,0,dropbox,python,NULL,2017-06-19
Introducing Cape,"More than a billion files are saved to Dropbox every day, and we need to run many asynchronous jobs in response to these events to power various Dropbox features. Examples of these asynchronous jobs include indexing a file to enable search over its contents, generating previews of files to be displayed when the files are viewed on the Dropbox website, and delivering notifications of file changes to third-party apps using the Dropbox developer API. This is where Cape comes in — it’s a framework that enables real-time asynchronous processing of billions of events a day, powering many Dropbox features.We launched Cape towards the end of 2016 to replace Livefill, which we last talked about in 2014. In this post, we’ll give an overview of Cape and its key features, high-level architecture, and varied use cases at Dropbox.Cape is designed to be a generic framework that enables processing of event streams from varied sources, and allows developers to execute custom business logic in response to these events. There were many requirements we had to satisfy with it:Low latency Cape needs to enable processing within sub-second latencies to power content-based collaboration and sharing that increasingly happens in real-time and on-the-go through mobile devices. For example, we’d like a user to be able to search over or view a file that was just added to Dropbox and shared with them.Multiple event types Unlike its predecessor Livefill, Cape should generalize to processing different types of events in addition to Dropbox file events. This will enable the Dropbox product to react in real time to events that change non-file metadata, such as sharing a file, changing the permissions on a shared folder, or commenting on a file. Such capabilities have gained increased importance as Dropbox increasingly transforms from a pure file sync solution to a collaboration platform.Scale Cape must support the high throughput of tens of thousands of file and non-file events per second. In addition, many different workloads may result from the same event further amplifying the scale that needs to be supported.Variable workloads Workloads may vary in duration: from milliseconds, to tens of seconds, to minutes in some cases, based on the type of workload and the event being processed.Isolation Cape must provide reasonable isolation between different Cape users’ event processing code, so that an issue with one user’s processing doesn’t have a big adverse impact on all other users of the framework.At-least-once guarantee Cape must guarantee that each event is processed at-least-once since this is critical to many use cases for ensuring a correct and consistent product experience.Each event stream in Cape is called a Domain and consists of a particular type of events. Each Cape event has a Subject and a Sequence ID. The Subject is the entity on which an event occurs, the corresponding Subject ID serving as a key to identify the Subject. The Sequence ID is a monotonically increasing ID that provides a strict ordering of events within the scope of a Subject.There are two specific event sources supported in the first version of Cape that account for many of the important user events at Dropbox — we’ll take a quick detour to understand these sources before moving on.The first source is SFJ (Server File Journal), which is the metadata database for files in Dropbox. Every change to a file in a user’s Dropbox is associated with a Namespace ID (NSID) and a Journal ID (JID), which together uniquely identify each event in SFJ. The second supported source is Edgestore, which is a metadata store for non-file metadata powering many Dropbox services and products. All changes to a particular Edgestore Entity or Association type can be uniquely identified by a combination of a GID (global id) and a Revision ID.The following table describes how SFJ and Edgestore events fit into Cape’s event abstraction:In the future, Cape can support processing custom event streams in addition to SFJ and Edgestore events. For example, events flowing into a Kafka cluster could fit into Cape’s event stream abstraction as follows:Shown below is an overview of what Cape looks like:SFJ and Edgestore services send pings to a Cape Frontend via RPCs containing metadata from relevant events as they happen. These pings are not in the critical path for SFJ and Edgestore, and so are sent asynchronously instead. This setup minimizes the availability impact on critical Dropbox services as a whole (when Cape or a service it depends on is experiencing an issue) while enabling real-time processing of events in the normal case. The Cape Frontend publishes these pings to Kafka queues where they are persisted until they are picked up for processing.The Cape Dispatcher subscribes to the aforementioned Kafka queues to receive event pings and kick off the necessary processing. The Dispatcher contains all the intelligent business logic in Cape and dispatches particular events to the appropriate lambda workers based on how users have configured Cape. In addition, it’s responsible for ensuring other guarantees that Cape provides, notably around ordering between events and dependencies between lambdas.The Lambda Workers receive events from the Dispatcher via Redis, carry out the users’ business logic, and respond to the Cape Dispatcher with the status of this processing — if the processing is successful, this is the end of the processing for that particular event.As mentioned earlier, pings from SFJ and Edgestore are sent asynchronously and outside the critical path to the Cape Frontend, which of course means they are not guaranteed to be sent for every event. You may have realized that this makes it seemingly impossible for Cape to provide the guarantee that every event is processed at least once, e.g. it would be possible for a file to be synced to Dropbox but for us to miss all the asynchronous processing that should happen as a result. This is where Cape Refresh comes in — these workers continually scan the SFJ and Edgestore databases for recent events that may have been missed and send the necessary pings to the Cape Frontend to ensure they are processed. Additionally, this serves as a mechanism to detect permanent failures in users’ application code on any of the billions of Subjects processed by Cape.To integrate with Cape, these are the steps a developer needs to follow:After the above steps are followed, the newly deployed Cape workers will receive events related to the relevant lambda, can process these events appropriately, and respond back to the Cape Dispatcher with the status of this processing. Users also have access to automatically generated monitoring dashboards with basic metrics around the event processing being performed by this lambda.Cape already processes several billion events a day with 95th-percentile latencies of less than 100 milliseconds and less than 400 milliseconds, respectively, for both SFJ and Edgestore. These latencies are measured from the point when the Cape Frontend receives the event to when event processing starts at the lambda workers. For SFJ, end-to-end latencies (i.e., from when the change is committed to SFJ to when processing starts at the lambda workers) are higher: ~500 milliseconds & 1 second, respectively, due to batching within the SFJ service itself.Here are just a few examples of how Cape is used at Dropbox today:Given how generic the framework is and its support for the majority of Dropbox’s important events, we expect Cape to underpin many of Dropbox’s features in the future.The architecture overview we’ve included in this post obviously leaves out discussion of many details and design decisions. In future posts, we’ll dive deeper into further technical details of Cape and specific aspects of its architecture, design, and implementation.Thanks to the folks that helped build Cape: Bashar Al-Rawi, Daisy Zhou, Iulia Tamas, Jacob Reiff, Peng Kang, Rajiv Desai, and Sarah Tappon.",https://blogs.dropbox.com/tech/2017/05/introducing-cape/,0,dropbox,,NULL,2017-05-17
Augmented camera previews for the Dropbox Android document scanner,"With Dropbox’s document scanner, a user can take a photo of a document with their phone and convert it into a clean, rectangular PDF. In our previous blog posts (Part 1, Part 2), we presented an overview of document scanner’s machine learning backend, along with its iOS implementation. This post will describe some of technical challenges associated with implementing the document scanner on Android.We will specifically focus on all steps required to generate an augmented camera preview in order to achieve the following effect:This requires custom interaction with the Android camera and access to individual preview frames.Normally, when a third-party app requests a photo to be taken, it can be achieved easily in the following way:This delegates the task of taking a photo to the device’s native camera application. We receive the final image, with no control over intermediate steps.However, we want to augment the live preview, detecting the document and displaying its edges. To do this, we need to create a custom camera application, processing each individual frame to find the edges, and drawing a blue quadrilateral that symbolizes the document’s boundaries in the live preview.The whole cycle consists of the following steps:Needless to say, steps (2) – (7) must take as little time as possible so that the movement of the blue quadrilateral appears to be smooth and remains responsive to camera movements.It is believed than 10-12 frames per second is the minimum frequency required for the human brain to perceive motion. This means the whole cycle presented on the diagram should take no more than 80 ms. The Android hardware landscape is also very fragmented, which poses additional challenges. Cameras range from 0.3 to 24 megapixels, and unlike iPhones we can’t take the presence of any hardware feature (such as autofocus, back-facing camera or physical flash LED) for granted. The code needs to defensively check if each requested feature is there.In the rest of the post, we’ll discuss each of the steps presented in the diagram.The first step to the augmented reality preview is to create a custom camera preview without any augmented reality. For gaining access to the device’s camera, we will be using android.hardware.Camera object.Note: The android.hardware.Camera has been deprecated in version 5.0 (API Level 21) and replaced with much more powerful android.hardware.camera2 API. However, at the time of writing this post, roughly 50% of the active Android devices ran versions older than 5.0, so we were unable to avail of the improved camera API.The very first step before starting preview is to confirm whether a device has a rear-facing camera. Unlike iOS, we cannot assume it is true; the Nexus 7 tablet, for example, was equipped with a front-facing camera only.We can perform such a check using the following snippet:As per the documentation, PackageManager.FEATURE_CAMERA refers to the camera facing away from the screen. To check for the presence of a front camera, there is a separate flag available – FEATURE_CAMERA_FRONT. Hence, we are fine with the check above.Tip: Accessing device camera requires proper permissions. This includes both defining required permissions in AndroidManifest.xml: <uses-feature android:name=""android.hardware.camera"" android:required=""false"" /> <uses-feature android:name=""android.hardware.camera.autofocus"" android:required=""false"" /> <uses-feature android:name=""android.hardware.camera.flash"" android:required=""false"" /> and requesting permission.CAMERA permission at runtime so that it works on Android M and later versions.Another issue is that the camera sensor orientation that can vary depending on a specific device. The most common one is landscape, but so-called “reverse landscape orientation” used for the Nexus 5X camera sensor has caused a lot of problems to many apps that were unprepared. It is very important to set the display orientation correctly so that it works properly regardless of the device’s specific setup. The snippet below shows how to do it.Another very important thing to remember is the fact, that unlike iOS, there are multiple potential aspect ratios to support. On some devices, the camera capture screen has buttons that float over the preview, while on others there is a dedicated panel holding all the controls.Camera capture screen on the Samsung Galaxy S5Camera capture screen on the Xiaomi Mi4This is why we need to calculate the optimal preview size with the closest aspect ratio to our preview rectangle.The camera parameters object has a method called mCamera.getParameters().getSupportedPreviewSizes() that returns a list of preview dimensions supported by a given device. In order to find the best match, we iterate through the returned list and find the closest dimensions to the current preview size that match our aspect ratio (with some tolerance).This way, the document scanner will behave correctly even when unusual aspect ratio is needed due to e.g. operating in multi-window mode.There are several ways in which camera sensor data can be tied to an UI component. The oldest and arguably simplest way is using SurfaceView as shown in an official Google API demo example.However, SurfaceView comes with several limitations, as it’s just a drawing surface embedded inside the view hierarchy that is behind the window which contains all views. Two or more SurfaceViews cannot be overlaid, which is problematic for augmented reality use cases such as the document scanner, as issues with z-ordering may arise (and these issues will be likely device-specific).Another choice is a TextureView which is a first-class citizen in the view hierarchy. This means it can be transformed, scaled and animated like any other view.Once the camera object is acquired and parameters are set, we can start the preview by calling mCamera.startPreview() .Tip: It is very important to hold the camera object only when your app is in the foreground and release it immediately onPause . Otherwise, the camera may become unavailable to other apps (or our own app, if restarted).In order to place UI components on top of the live preview, it’s best to use FrameLayout . This way, vertical ordering will match the order in which components were defined in the layout file.(1) First, we define TextureView(2) On top of it, we place custom view for drawing quadrilateral(3) As a last component, we define the layout containing camera controls and last gallery photo thumbnailThis assumes that a TextureView is being used for the live preview. For SurfaceView , z-order can be adjusted with the setZOrderMediaOverlay method.In order to improve the user experience in low light conditions we offer both torch and flash toggles. These can be enabled via camera parameters Parameters.FLASH_MODE_TORCH and Parameters.FLASH_MODE_ON correspondingly. However, many Android devices (most commonly tablets) don’t have a physical LED flash, so we need to check for its presence before displaying the flash and torch icons. Once the user taps on the torch or flash icon, we change the flash mode by calling mCamera.getParameters().setFlashMode().It is important to remember that before changing camera parameters, we need to stop the preview, using mCamera.stopPreview(), and start it again when we are done, using mCamera.startPreview(). Not doing this can result in undefined behavior on some devices.On devices that support it, we use FOCUS_MODE_CONTINUOUS_PICTURE to make the camera refocus on the subject very aggressively in order to keep the subject sharp at all times. On devices that don’t support it, it can be emulated by requesting autofocus manually on each camera movement, which in turn can be detected using the accelerometer. The supported focus modes can be obtained by calling mCamera.getParameters().getSupportedFocusModes()In order to receive a callback each time a new frame is available, we need to register a listener. For TextureView , we can do this by calling mTextureView.setSurfaceTextureListenerDepending on whether a SurfaceView or TextureView has been used, the corresponding callback is either Camera.PreviewCallback with onPreviewFrame(byte[] data, Camera camera) invoked each time new frame is available or TextureView.SurfaceTextureListener with onSurfaceTextureUpdated(SurfaceTexture surface) method.Once a SurfaceView or TextureView is tied to the camera object, we can start preview by calling mCamera.startPreview() .Every time a new frame is available (for most devices, it occurs 20-30 times per second), the callback is invoked.When onPreviewFrame(byte[] data, Camera camera) is being used to listen for new frames, it’s important to remember that the new frame will not arrive until we call camera.addCallbackBuffer(mPreviewBuffer) in order to signal that we are done with processing the buffer and the camera is free to write to it again.If we use SurfaceTexture callbacks to receive new frames, onSurfaceTextureUpdated will be invoked every time new frame is available and it is up us whether it should be processed or discarded.Our document detector described in the previous blog posts requires the frame, which is later passed to C++ code, to be of specific dimensions and in a specific color space. Specifically, this should be a 200 x 200px frame in RGBA color space. For onPreviewFrame(byte[] data, Camera camera) , the data byte array is usually in NV21 format, which is a standard for Android camera preview.This NV21 frame can be converted to an RGBA bitmap using the following code:The bad news is, using this method, it takes 300-500 ms to process a 1920 x 1080 frame, which makes it absolutely unacceptable for real-time applications.Fortunately, there are several ways to do this conversion much faster such as using OpenGL/OpenCV or native code. However, there are two RenderScript intrinsic scripts that can provide the requested functionality without having to drop down to lower-level APIs — ScriptIntrinsicResize combined with ScriptIntrinsicYuvtoRGB. By applying these two, we were able to get the processing time down to 10-25 ms thanks to the hardware acceleration.Things look much simpler when the preview is implemented using TextureView and onSurfaceTextureUpdated(SurfaceTexture surface) callback. This way, we can get the bitmap straight from the TextureView once a new frame is available:TextureView#getBitmap is generally known to be slow; however, when the dimensions of the requested bitmap are small enough, the processing time is very reasonable (5-15ms for our 200×200 case). While this isn’t a universal solution, it turned out to be both the fastest and the simplest for our application.Moreover, as we mentioned earlier, the camera sensor orientation is usually either landscape (90 deg) or reverse landscape (270 deg), so the bitmap will most likely be rotated. However, instead of rotating the whole bitmap, it is much faster to rotate the quadrilateral returned by the document detector instead.On top of the scaled bitmap, our document detector requires passing a so called rotation matrix. Such matrix essentially provides information about phone movement direction (like tilting), which expedites calculating the next position of the quadrilateral. Knowing the coordinates of the quadrilateral at a given time, and the direction in which the device was moved, the document detector can estimate the anticipated future position of the quadrilateral, which speeds up computations.In order to calculate the rotation matrix, we need to listen for two types of sensor events — Sensor.TYPE_MAGNETIC_FIELD and Sensor.TYPE_ACCELEROMETER that represent magnetic and gravity data. Having these, the rotation matrix can be obtained by calling SensorManager.getRotationMatrix . The document detector is written in C++, hence we need to make the call using JNI.In case we cannot obtain sensor data, we pass an identity matrix.Tip: Since calls to the detector can take anywhere from 20-100ms depending on Android device, they cannot be executed in the UI thread. We run them sequentially in a separate thread with elevated priority.Once the call to document detector returns, we receive coordinates of the four points representing the quadrilateral that delimits the document edges. Understandably, these coordinates apply to the frame that was passed to the detector (e.g. 200×200 square that we mentioned), so we need to scale the coordinates to the original size of the preview. We also need to rotate the quadrilateral in case the camera orientation doesn’t match the orientation of the preview (see step (4) Converting frames, above).Having received frame coordinates, it is time to draw the quadrilateral over the camera preview (yet below camera controls). For simplicity and better control over z-ordering, we decided to create a custom View with an overriden onDraw() method that is responsible for drawing the quad on the canvas. Starting from Android 4.0 (Ice Cream Sandwich), drawing on a canvas is hardware-accelerated by default, which greatly improves performance.Each time we receive an updated frame, we need to call invalidate() on the View . The downside of such an approach is that we have no control over the real refresh rate. To be precise, we don’t know how much time will elapse between us calling invalidate() and the OS invoking onDraw() on our view. However, we have measured that this approach allows us to achieve at least 15 FPS on most devices.Tip: When implementing a custom view, it is very important to keep the onDraw() method as lightweight as possible and avoid any expensive operations, such as new object creation.If drawing using a custom view is too slow, there are many faster, yet more complex solutions such as having another TextureView or leveraging OpenGL.We measured the time consumed by each step (in milliseconds) on several Android devices. In each case, the Dropbox app was the only non-preinstalled app. However, since there are many different factors that influence the performance (e.g. phone movements), these results cannot be treated as a benchmark and are here solely for illustrative purposes.Note that faster devices usually have better cameras, so there is also more data to process. The worst case scenario for the document scanner would be a slow device with a very high resolution camera.The thumbnail we display in the lower left corner allows a user to preview the last gallery item. Tapping on it takes the user to the phone’s camera roll, where an existing photo can be selected for scanning.The last available thumbnail (if any) can be retrieved using the following query:Tip: To ensure proper orientation of the thumbnail (and a full-size photo), we need to read and interpret its ExifTags correctly. This can be achieved using android.media.ExifInterface class. There are 8 different tags representing orientation that need to be interpreted.If the cursor is empty (there are no photos in the gallery yet) or retrieving the bitmap threw an error (such as getting a null bitmap or exception), we simply hide the preview and make scanning from the gallery unavailable.Try out the Android Dropbox doc scanner today, and stay tuned for a future doc scanner post where we will describe the challenges in creating a multi-page PDF from a set of captured pages.",https://blogs.dropbox.com/tech/2017/05/augmented-camera-previews-for-the-dropbox-android-document-scanner/,0,dropbox,"java,python",NULL,2017-05-03
Creating a culture of accessibility,"At Dropbox, we strive to make products that are easy for everyone to use. As part of that mission, we’ve been improving product accessibility for users with disabilities, and building a collaborative culture in which our engineers understand and value accessibility best practices as part of their process.To create accessible products, you need to find opportunities to spread accessibility knowledge and enthusiasm in a sustainable way throughout your company. But awareness is one of the largest barriers to implementing these best practices into a product. Most computer science curriculums at colleges and universities don’t include in-depth coverage of accessibility (though organizations like Teach Access are working on changing that!). As a result, technology creators don’t always know much about accessibility unless it’s already had a direct impact on their life.There’s a lot of great information out there about how to make accessible products. The W3C’s Web Content Accessibility Guidelines (WCAG 2.0) and the BBC’s Mobile Accessibility Guidelines are both examples of great resources for understanding usability considerations, and how to implement them effectively. I’d like to share some techniques we’ve used within our engineering organization (and beyond), to keep accessibility top-of-mind for product creators.Building a robust accessibility program at Dropbox is an ongoing process, but I’m excited about how far these initiatives have spread over the years. I hope these tips can help you prioritize accessibility in your organization, too!How can you get your organization more engaged in accessibility? Here are some ways that have worked for us:Form a cross-functional working group or task force When we first started ramping up our accessibility efforts, we formed an accessibility working group comprised of representatives from various teams across Dropbox, including engineering, design, research, communications, and legal. Among its various projects, our working group helped create an opening for Dropbox’s first dedicated accessibility role. We also spread accessibility knowledge through field trips and an external speaker series. Cross-functional working groups like this are a great way to build up internal accessibility momentum, as they can organize events, start company-wide initiatives, and get conversations happening across disciplines.Run an Assistive Technology Lab Assistive technology, also referred to as access technology (AT), is anything that helps a person with an impairment complete a task more quickly, easily, or independently. In the digital space, assistive technology such as screen readers, speech recognition software, screen magnifiers, and switches help users navigate and interact with UIs. As you might imagine, the user experience of a product can change a lot depending on what type of assistive technology you’re using with it.We wanted to expose our product teams to the wide variety of ways that our users might navigate our products. In 2016, our Accessibility Working Group started running Assistive Technology Labs to give teams hands-on (or, in the case of speech recognition software and head mice, hands-off!) experience exploring their own features with these tools.Our pop-up labs have several stations, each with a particular type of assistive technology. At each station, we present Dropboxers with three tasks: an easy task to familiarize them with the basic mechanics of the station’s AT; a slightly more involved task; and an extremely challenging (if not impossible) task meant to highlight areas where our user experience could be improved. We want Dropboxers to get frustrated by the last task, so they can better understand our users’ pain points and channel that frustration into fixes. Many of the tasks from our first Assistive Technology Lab are no longer challenging, thanks to continuous UI improvements from our design and engineering teams.If you’re interested in running your own Assistive Technology Lab, here are a few tips:Reward your organization’s accessibility championsWe expect everyone on Dropbox’s engineering, product, and design teams to include accessibility in their process, but we like to recognize folks who put in extra effort to improve our internal accessibility processes or make our product experience more inclusive. One way we do this is through our badge system. Anyone can nominate a colleague (or themselves) for an “Accessibility Advocate” badge which, when awarded, appears on their profile in our employee directory:We decided to use the 👌🏿 emoji to represent “Accessibility Advocate.” In many cultures, this hand symbol represents “ok,” so we thought it’d be a great way to say “Good work!”There are a lot of ways you can show appreciation for colleagues’ awesome accessibility work within your organization: a physical or digital badge, a small gift on their desks, or an email recognition. Not only do these rewards show employees that their focus on digital inclusion is valuable, but they also encourage other employees to aim higher and work on more accessibility initiatives.When I joined Dropbox as its first full-time accessibility engineer, I wrote a ton of documentation, thinking it’d be a great way to get everyone around me to become accessibility experts. But I quickly realized that reading long documents wasn’t the most engaging way to start learning accessibility best practices, especially when those documents lived outside of engineers’ normal workflows. Now, in addition to this documentation, we’ve been spreading accessibility knowledge in more dynamic ways:Incorporate accessibility feedback into the development process To give accessibility advice relevant to colleagues’ current context, we built an accessibility debug tool and added it to the footer of our website, visible only to Dropboxers. The debug tool runs a series of accessibility checks on the current page.If any accessibility errors are found, the “AX” button turns red and displays the number of errors. Pressing the button opens up a dialog box that shows each accessibility check performed, with information about whether it passed or failed, a mechanism for highlighting its related errors (or listing them in the Debug Console), and a suggestion for how to fix the issue.Accessibility tool shows two error messages for image-alt and td-has-header, with descriptions of each error, recommended solutions, and a link to more information.Accessibility tool shows shows a list of passing rules, including dropbox_missingH1, dropbox_unclearLinkText, lowContrastElements, and aria-allowed-attr.Screenshot of the accessibility tool when it’s first expanded, showing 5 errors. The UI includes a button to “Run accessibility checks again” and two tabs for “Results” and “Settings.” The Results tab is selected and includes a filter for filtering results by type (Failure, Warning, Passing), as well as a list of results. Only the first result is visible in this UI, a single failure for “dropbox_anchorMissingHref.”For this tool’s first iteration, we included a small set of custom accessibility checks, looking for buttons and links without accessible names, keyboard-inaccessible links, unlabelled input fields, images without alternative text, and low contrast text. Since then, we’ve expanded the tool to include 51 accessibility checks, most of which come from Deque’s open-source aXe framework.What’s great about this tool is that it provides practical, relevant accessibility feedback without being too overwhelming or intrusive. As front-end developers work on new UIs, they can consult the tool to see if they are introducing any new errors on the page, get tips on how to fix them, and find contact information for the Accessibility Team if they still have questions.Automated accessibility checks like this can be incorporated into several stages of the engineering process, from on-page tools to code linters and unit tests.Do accessibility bug bashes Does your team ever get into a room together to test out user flows and try to break the product? At Dropbox we call these “bug bashes,” meetings during intermediate stages in UI development when engineers, quality engineers, designers, and product managers gather together to manually test a feature in a variety of scenarios and find as many bugs as we can. Bug bashes are the perfect opportunity to check for accessibility issues in in-progress features. During a team bug bash, get a few people in the room to try each core flow with just their keyboard, or with a free screen reader or their computer’s built-in zoom controls. Can they perform all expected tasks? Do they ever get stuck or lost?While you can schedule accessibility-specific bug bashes for thorough exploration of a product’s accessibility issues, it’s important to weave accessibility bug exploration into the regular bug bash meetings. This reinforces that accessibility is integral to usability, not a separate issue to address.Make an entire feature accessible All of a company’s products should be as accessible as possible. If that isn’t the case with your current suite of products, it can sometimes be hard to figure out where to start. One of the best ways to get going is to just pick one feature and commit to making it accessible. The best candidates for this project are brand new features you can build from the beginning with accessibility in mind, or a pre-existing, high-traffic area of your product. The team working on this feature will learn by doing, developing accessibility knowledge that will carry over to their next projects and set an example for other product teams.Accessibility improvements to a single feature can also help make other features more accessible with minimum effort. For example, during a recent refresh of our file browsing web UI, we paid special attention to the screen reader and keyboard experience. This led us to build a number of new React components that had accessibility baked into them, including dropdown menus and tree views. Because we architect our React components to be reusable, these accessibility enhancements have since trickled into other areas of our web UI.Here’s an animation of our reusable tree component in action:Host internal talks and invite guest speakers Tech talks are a great way to share accessibility knowledge throughout your team. Try to do one once a quarter to keep the information fresh, especially if you’re consistently hiring more UI engineers and designers.Looking for ideas? We’ve recently covered these topics in our internal talks:If you’re interested in seeing a blog post from us on one of these topics, let us know in the comments.Bring in guest speakers to talk about inclusive design concepts! Invite accessibility experts from other companies to talk about their processes and frameworks. Invite speakers with disabilities, who can speak from personal experience to the importance of accessible technology. External speakers tend to draw a good crowd and definitely bring new and engaging perspectives. Record these talks so colleagues can reference video archives later.Conduct user research Accessibility is about users. While you can follow all the accessibility guidelines out there and make UIs that pass every automated accessibility check, they may not actually be very efficient or enjoyable for your users. It’s important to listen to your users and turn their feedback into product improvements.Usability tests with screen reader users helped us find areas where we could improve the keyboard flow of dropbox.com. In another focus group, Dropbox users with low vision shared stories illustrating the impact color contrast has on how efficiently they can manage their files. With this user insight, we’re able to craft richer, higher quality experiences for all users.**If your organization is already doing research studies for new products, include people with disabilities as participants. It can also be helpful to lead focus groups to better understand a particular user group’s needs. Make sure to compensate research participants for their time. Invite product engineers to these sessions, or share video recordings with them, so they can understand the impact their code has — for better or worse! — on real users.Get involved in the community The digital accessibility community is a tremendous group of folks who are all driven to make technology better for people with disabilities. Staying connected to this community is a great way to keep up on current best practices, exchange advice on tricky accessibility design and development problems, and share ideas about how to advance technology further. I’ve found that this community is super generous about sharing what we’ve learned, as we all really want to provide users with quality, consistent patterns for navigation and communication.One way to get involved is through attending meet-ups and conferences. Most major cities have an accessibility interest group you can join. In San Francisco, for instance, Dropboxers attend Bay Area Accessibility Camp and some Bay Area Accessibility & Inclusive Design meetups. There are also major accessibility conferences around the country and the world, including the recent CSUN Assistive Technology Conference. These events are a great way to learn from industry leaders in accessibility. You can also get involved online by joining the Web Accessibility Slack community or following the #a11y hashtag on Twitter.We’ve found some of the best ways to get people engaged in accessibility are by hosting events that highlight usability impact, providing lightweight tools that point out easy-to-fix bugs, and celebrating the work of successful projects. There are still many more ways to weave accessibility into your team’s process. Try out some of these techniques for Global Accessibility Awareness Day (Thursday, May 18th) and see what sticks!Stay tuned to our blog for more accessibility tips from our team. And if you want to help make Dropbox more accessible for our 500 million users, we’re hiring.",https://blogs.dropbox.com/tech/2017/04/creating-a-culture-of-accessibility/,0,dropbox,,NULL,2017-04-27
Adding IPv6 connectivity support to the Dropbox desktop client,"Computers on the internet are uniquely identified by an IP address. For decades the world has used Internet Protocol version 4 (IPv4), which allows for about 4 billion unique addresses. As more of the world has come online, and we carry internet-capable devices in our pockets, we have run out of IPv4 addresses. Layers and layers of workarounds have been built to mitigate the problem. The current protocol—Internet Protocol version 6 (IPv6)—fixes various problems with IPv4; it has a significantly expanded address space that allows for the creation of many more unique IP addresses. Unfortunately, IPv6 has suffered from lack of adoption. This is finally changing.As of April 10, 2017, Google reported IPv6 adoption at 14%, with the United States at just over 30%. ISPs and private networks within enterprises are moving to IPv6-only or dual-stack networks (those that support both IPv6 and IPv4 connections). Cellular carriers are switching to IPv6-only networks, meaning devices have no IPv4 connectivity, but rely on a NAT64/DNS64 gateway to connect to legacy IPv4 internet networks.Given these trends, we recently took the initiative to add IPv6 support for the Dropbox desktop application. Version 24 of the Dropbox client, released April 17, 2017, supports IPv6-only and dual-stack networks.Adding IPv6 support involves making changes to address resolution and connection establishment. This has to be done in a cross-platform manner and support alternative mechanisms like proxies. Transferring files over the internet is one of Dropbox’s core tasks and users expect that to happen seamlessly and quickly. These changes needed to happen without affecting the user experience. Here is how we implemented IPv6 support while ensuring the client stayed functional for all existing users who are still on IPv4.Under IPv4, functions like gethostbyname() could be used for address lookup. In addition, functions like inet_aton() and inet_ntoa() could be used to convert between various IP address representations. None of these work with IPv6 addresses.The function getaddrinfo() has long been the recommended cross-platform way to look up IP addresses for a given host. It accepts a hostname and returns a list of addresses, both IPv6 and IPv4, ordered by the host’s preferred address family. Callers are then expected to iterate through these until a connection succeeds.Parameters that determine which address families are returned are accepted by getaddrinfo(). In the Berkeley sockets API, a family specifies the socket type. AF_INET (IPv4) and AF_INET6 (IPv6) are two valid socket types. There is an additional constant, AF_UNSPEC, that indicates getaddrinfo() should return all the families that it can. Under the hood, getaddrinfo() will attempt both an A and an AAAA query to the DNS server.Unfortunately getaddrinfo() has a couple of downsides. It is a blocking function and it does not support caller specified timeouts. Once getaddrinfo() has been called, there is not much the calling thread can do until it returns. The default timeouts implemented by operating systems are within the 30-90 second range. In a naive implementation, we may end up waiting several minutes for the AF_UNSPEC getaddrinfo(), then spend another few seconds falling back to resolving with AF_INET. This adds up to a lot of delay.To mitigate this, we use a thread pool (specifically Python’s concurrent.futures module) to perform the resolution. We concurrently start both AF_UNSPEC and AF_INET resolutions. Since we want to favor IPv6 connections, we wait a few seconds for AF_UNSPEC to succeed, and otherwise, we select the one that finishes first. Operating systems aggressively cache DNS lookups, so the lookup time and CPU penalty is paid very rarely.Based on our metrics, about 80% of connection attempts resolve successfully on the AF_UNSPEC call and we don’t need to bother with the result of the AF_INET call. But when the AF_UNSPEC call takes longer than a few seconds, we noticed that both the AF_UNSPEC and AF_INET calls will fail in >86% of cases. This usually indicates the user is on a bad network, or their computer suspended/shut down right when we were attempting to connect. In fact, the odds that only one of the calls will succeed is very low, representing about 0.3% of all connection attempts.The dual lookups introduce some complexity to our code, but there are no well designed, cross-platform DNS resolution alternatives. Third-party solutions like c-ares exist, but we did not want to introduce overhead for such a simple task.One interesting implementation detail we discovered is that Python’s non-blocking sockets can encounter delays similar to blocking sockets if the connect()method is passed a DNS hostname, instead of an IP address. This is because it uses getaddrinfo() under the hood. Be sure to perform lookup first if you intend to use non-blocking sockets.Once we have a list of IP addresses, which may be a mix of IPv6 and IPv4, we can attempt to connect to each of them in order and stop when a connection is successfully established, correct? Unfortunately things are not so easy.On an IPv4-only or IPv6-only network, if none of the addresses work, the user’s network has a problem. However, on a dual-stack network, it is possible for IPv4 to be functioning, and IPv6 to be down. Why is that?Among other reasons, IPv6 networks generally operate a NAT64/DNS64 gateway to allow IPv6 hosts to connect to the IPv4 internet. It is possible for this gateway to be down or slow. What would happen if getaddrinfo() had returned a list of 2 IPv6 addresses followed by 2 IPv4 addresses?We would have first spent several seconds (we use a 10 second timeout per attempt) trying to connect to 2001:DB8::1 , and another 10s connecting to 2001:DB8::2 , before finally connecting to 198.51.100.1.That does not sound appealing. There is a clever and simple-to-implement strategy—codified as Happy Eyeballs: Success with Dual-stack hosts—to deal with this situation. We pick the first IPv6 address (2001:DB8::1) and the first IPv4 address (198.51.100.1) from the results. Since we want to favor IPv6, we start the connection to IPv6. If that connection takes too long (the RFC recommends 300ms), we then start the IPv4 connection. Then we pick the winner of the two. In this case, since we have a functioning IPv4 network, we would pick 198.51.100.1. If this sounds similar to our approach for address resolution, it’s because Happy Eyeballs definitely served as inspiration for that solution.If we were unable to connect to either address, we would try the rest of the addresses—['2001:DB8::2', '198.51.100.2']—in order.Since Dropbox servers don’t advertise native AAAA records and dual-stack users may connect via IPv4, we can’t say how many Dropbox users actually connect via an IPv6 network; we are working to resolve this. In terms of resolving addresses to both IPv4 and IPv6 addresses, fewer than 0.5% of hosts were able to resolve Dropbox servers to a IPv6 address during connection attempts.Some Dropbox users connect via proxies and we wanted to support IPv6 wherever it was possible. Dropbox supports SOCKS4, SOCKS5 and HTTP(S) proxies. SOCKS4 and SOCKS5 use a binary protocol to request a connection from the proxy. SOCKS4, and the extension SOCKS4a, do not support IPv6 and will remain unusable on IPv6 networks. For SOCKS5, we added support for x03 ATYP to allow IPv6 addresses. HTTP(S) proxies require no negotiation, so they do not require any client-side changes. The client simply sends the request URL and the proxy uses a connection that that it supports.Finally, in order to connect to the proxy when both the client and proxy are on an IPv6-only network, we can re-use the logic for establishing connections.To make these critical changes with minimal hiccups, we spent several weeks with features enabled only for in-office and beta builds. This ability to deploy to the office every day allowed us to quickly detect and fix issues. We made extensive use of Google’s publicly available DNS64 servers during development. Combined with the Network Link Conditioner on MacOS, developers could quickly verify code changes.The safety net provided by out-of-process updaters ensured that if we ended up with a bug that prevented users from connecting to Dropbox, we could fix the bug, release a build, and update users without requiring manual intervention and without users even noticing. When one of the beta builds ended up performing local DNS resolution even for proxies, we were able to roll out a new build quickly and the affected users had functionality restored within days.Adding IPv6 support was an interesting technical challenge due to the variety of implementations required, and the need to be backwards compatible. There is wealth of information on the internet about migrating to IPv6. We hope this article adds to it and helps others transition to the modern protocol.",https://blogs.dropbox.com/tech/2017/04/adding-ipv6-connectivity-support-to-the-dropbox-desktop-client/,0,dropbox,"frontend,css",NULL,2017-04-25
"Accelerating Iteration Velocity on Dropbox’s Desktop Client, Part 2","In our previous blog post on investing in the Desktop Client platform at Dropbox, we discussed the challenges of trying to innovate and iterate on a product while maintaining high platform quality and low overhead. In 2016, Dropbox quadrupled the cadence at which we shipped the Desktop Client, releasing a new a major version every 2 weeks rather than every 8 weeks by investing in foundational improvements. These efforts tended to illustrate one or both of the following themes:The previous post described the improvements that we made to Continuous Integration, i.e., the tools that allow engineers to commit code frequently to the same codebase without breaking each others’ tests. This article will talk about the new (or significantly improved) processes and technologies that we implemented for the rest of code’s life cycle: building and testing new binaries, getting them out the door, and making sure they work in the wild.Enhancing Continuous Integration reduced a lot of developer agony. With a more reliable CI system, they could commit code more quickly, know that their tests pass across the wide number of platforms that Dropbox supports, and be safeguarded against future regressions. However, in early 2016, an on-call engineer still spent 8+ hours a week executing scripts, changing configurations, and writing emails to make and release new official Desktop Client builds. This was costly, and not much fun for the engineer. Meanwhile, thanks to the CI, we no longer required an engineer to track down test failures and decide which commits would make it into a release, because almost every commit had passing tests and was releasable. Build-making was thus a prime candidate for automation!To start, we began with the highest impact area: “internal alpha” builds, meaning binaries intended for internal “dogfooding” by Dropboxers. Internal alpha was the highest impact because we wanted to make builds for this group of users most frequently, as it was (and continues to be) the first destination for new code changes. It’s the first source of realistic feedback for developers. In addition, we were fine with shipping internal alpha builds that had passing unit tests but no manual quality assurance; if there were unexpected issues, Dropboxers could contact the responsible engineers directly. And if the new internal alpha build was completely unusable, IT could help us do complete reinstalls of Dropbox at the company — though the development of DropboxMacUpdate and its equivalent on Windows had drastically reduced this risk.Automating builds was not ground-breaking new technology, but it required pulling together a bunch of disparate parts of our ecosystem. For example, we added an authenticated API endpoint to configure the official build numbers served via autoupdate, which had previously only been accessible via a web interface. We hooked into Changes, our build-job orchestration framework, to check the status of tests and builds. We called the API for Phabricator, our task tracking and code review tool, to make and check tasks that humans could open or close to indicate blockers to release. We also wrote templated emails and Slack messages to communicate status.We were able to completely automate internal alpha builds by late March 2016, and in the process increase the release cadence to once a day! There were a few main takeaways from this project:After builds were automated, we had fixed two of the big manual parts of trying to get new versions out of the door: keeping tests green so that we could ship on any commit, and kicking off and deploying builds. However, in the old system, an on-call Desktop Client engineer still ran a basic test set of install, update, and sync — by hand. Note that this person was a Software Engineer, not a Quality Assurance Engineer who specialized in testing. We had made the decision to ship with only unit tests for internal users, but since there were large regressions sometimes, we weren’t willing to do the same for external “beta” users.Enter the Desktop Client Test Infrastructure team. Their mission was to automate end-to-end tests. Their challenge was that larger-scope tests have more opportunities for system flakiness.Here are a few examples of those challenges:To try and control, and then quash, flakiness, the Test Infra team strategically and incrementally stabilized the framework. First, they ran a basic sync test continuously, fixing issues as they came up, until there were 1000 consecutive passing test runs on each platform. Then, they gradually expanded the scope of tests, making sure that each new coverage area was stable before moving on.Once the first set of tests were written, the other big challenges were in process and culture:Once a new version of Dropbox is launched, we want to make sure that it’s working well on real user computers.Since at least 2008, Dropbox has used Community Forums to post new Desktop Client builds for enthusiasts to try and give feedback. Our beta testers are thoughtful and have surfaced many issues, but as of early 2016, investigating a specific reported issue is relatively difficult. For one, issues on the forums take the form of prose posts and tend to describe symptoms of the problem through product behavior. From only one report, the underlying issue is often difficult to isolate as it could have happened at any time and be caused by any component. In addition, forums posts are designed for answering questions and community discussion, so they are aggregated by user-defined topics, rather than timestamps or automatically detected content. To find all related reports of an issue, an engineer has to sort through several topics, find reports of similar application behavior, fill out an internal task, and do enough investigation to assign a severity.As we quadrupled the cadence at which we were releasing new official versions of Dropbox, we would potentially be quadrupling the overhead required to find and translate these user reports into bugs. Since, as of late 2015, ~60% of critical issues detected after feature freeze came from beta user reports, we couldn’t simply stop paying attention. Therefore, we needed a solution to these laborious but essential human-scale processes. As usual, this solution was to automate them!The goal of beta user bug reports is to get an idea of when things are going wrong, so that we can fix them before releasing to a wider audience. However, the Desktop Client itself has a lot more information than the end user about its internal state — it can catch and report exceptions, or report events based on when various code paths are executed. As of early 2016, Dropbox already had systems in place to collect tracebacks and aggregate event analytics, but they were primarily used to launch new features. For example, as an engineer iterated on a new widget, they would add logging to detect whether users were interacting with it and to ensure that it was operating correctly. They would also search for and fix tracebacks that indicate internal exceptions related to their new code. Once the feature was stable it was common that no one would look at those analytics again, except sometimes when there was a user-reported regression. This meant performance of the feature, or the entire Desktop Client, could slowly degrade over time and no one would notice.We realized we could be much more systematic about analytics by permanently tracking a set of metrics over time and by creating guidelines for analytics on new features. This had three big upsides: we could catch more issues before any external user noticed, engineers wouldn’t spend all their time sifting through posts, and the information would contain more context about internal application state.We wanted to track quality statistics over time. But how do we ensure that a regression in performance actually results in a fix? This is why we introduced a quality framework in the summer of 2016, followed by “Release Gating Metrics” in the autumn. We began by requiring each product team to implement a dashboard tracking the features they owned. The team could choose whatever metrics they believed to be most important but the metrics had to have a baseline that indicated quality, and not whether or not users simply liked the feature. A few of the most important are designated “release gating metrics”. These are always consulted before a release, and if they cross a pre-assigned threshold, the new version is delayed until the underlying issues are found and fixed.Let’s take account sign-up, for example. We could track the total number of Desktop Client-based sign-ups, but if we launched a promotion or redesigned the sign-up interface to be more attractive, the number of new accounts might spike and hide if the sign-up flow itself became glitchy or started to lag. Therefore, to capture quality rather than popularity, we might track the amount of time from application start to the appearance of the sign-up window and then define an acceptable duration. The team could create an alert in case it started taking too long or failed to launch too often.Robust monitoring is most useful when it captures information early, while there is still time to fix regressions ahead of the final release. However, the metrics and exception traceback volumes also need to be statistically significant to be actionable — giving five users hot-off-the-press versions of the Desktop Client would not catch many issues. This was particularly important as we began to move faster, and each version spent less time “baking” with beta users.Expanding the experimental user pool turned out to be surprisingly simple: the Dropbox Desktop Client already had an option to “Include me on early releases” in the settings panel. Previously, we released to these users after posting a version in forums for a while, right before rolling out to the rest of world. However integration tests bolstered the quality of the Desktop Client sufficiently that we no longer needed this step, and simply began including these users in the beta builds as soon as they were released. This expanded our pool of beta users by about 40-fold and diversified the number of configurations that the experimental code ran on so that exception tracebacks that might only be reported in specific edge cases were more likely to show up.Altogether, we still find issues through internal and external human reports, but the additional logging and analytics have made reports easier to debug, and the volume has remained manageable as overall development velocity increases.At this point in the story, we now had a thorough suite of indicators of how the Desktop Client could be going wrong. Metrics could dip below satisfactory levels, integration tests might fail, a manual bug report could be filed, or tracebacks may be reported when internal exceptions occurred. How would we translate these into actual actionable bugs and fixes? Improvements 1-4 unlocked the ability for many teams to develop on desktop, including teams with goals spanning the web, mobile, and desktop platforms, but there was no longer a single manager aware of all developments. The Desktop Platform team could investigate everything, but they would never have time for larger foundational projects and probably would lack context if a product team implemented a feature that was causing bugs. With the advent of distributed development, we now also had distributed ownership.The solution was to be really explicit about who was responsible for which component of the Desktop Client, and automate as much as possible about routing. Every part of the Desktop Client is given a “Desktop Component” tag on Phabricator, our bug tracking and code review platform. As teams shift and features are added, a central authority keeps track of ownership. If an issue can be clearly mapped to a specific component, it gets assigned the relevant tag and directly sent to the owning team. This way, the Desktop Platform team, instead of being responsible for investigating everything, is only responsible for doing any leftover first-level routing and for taking care of the things their team specifically owns.To assist in first-level routing, bugs are auto-assigned based on the code paths in the stack traces. When manual reports come in from our internal users, we make them as routable as possible by implementing an internal bug reporter, which prompts for information and adds some metadata. We have similar templates for problems bubbled up from external users. Generally speaking, if an issue is reported by hand that was not caught by our existing metrics, we strive to add monitoring and/or integration tests to catch similar problems in the future.Code flux tends to be proportional to product quality risk. In other words, the more that changes in a codebase, the more testing and verification is needed to be sure it works well. Our previous process put the onus on engineers to make only the changes that they deemed safe, with the guideline that no new features could be added after a build was made available externally. However, seemingly innocent changes that fixed a bug in one place could easily cause issues on another platform, especially without integration tests as guardrails to ensure that every commit preserved basic functionality.To replace these nebulous guidelines, we implemented a strict and objective set of requirements for allowing changes after “code freeze,” so that the testing and monitoring after freeze would accurately represent the quality of the final release. The primary benefit of this was predictability. Code could now go out on a regular cadence without too much time spent trying to re-establish test coverage and quality verification after last-minute commits. The downside was that teams had to accept missing a deadline if they could not stabilize their feature completely before code freeze.This was painful at first, especially as we first had to go through a transitional 4-week release cadence (instead of the 2-week one we have today). Engineers had to get used to bug bashing thoroughly ahead of code freeze and trust that if they missed one date, the next would come around in a few weeks as scheduled. The first time we implemented this safeguarding, I personally forgot to turn on a single feature flag (explained in more detail below) and had to wait a whole release cycle — and I had been working on the new release process!As for how this enforcement works: we gathered 5 senior technicians on the desktop client codebase to form the “Triage Council” and gave it a charter and explicit guidelines to accept or reject proposed changes once code freeze happened. (This is also when a release branch is created in the repository.) The Triage Council would have a lot of technical context, but be tasked only with upholding the existing rules. This had two advantages: these senior engineers weren’t at risk of burning out on playing “bad cop” or making difficult decisions (they could always just point to the charter); and other engineers would approach them with a good understanding of the requirements to make a last-minute change, or “cherry-pick”, to the release branch.So, what can be cherry-picked?We later added a fourth category as well:There is an explicit block on pushing code to a release branch without Triage Council approval, but there is also an escalation process. If someone wanting a cherry-pick — but rejected by the Triage Council — thinks that the rules were misapplied, or that another there should be an exception for another reason, such as hitting a product deadline, they can appeal further to our VP of Infrastructure.To keep improving our processes, each cherry-pick is followed up with a post-mortem review, which strives to identify what the root cause behind an issue is, why a solution was not found earlier, and how we can prevent similar issues from occurring again.One important way to make all of this possible was to boost cultural support for remote and binary code gating. There was already a robust set of tools for grouping users and hosts in order to show to A/B experiments on web (Stormcrow). These experiment flags can also can be passed down to the Desktop Client to fork logic. We now expect that any risky change is gated by Stormcrow flags so that they can be turned off safely without making a code change in the desktop codebase. Some changes, of course, happen when we cannot guarantee web connectivity. These are expected to be “feature-gated” within the Desktop Client codebase, meaning that changing a single variable from True to False would turn off an entire section of code. These feature gates can also be configured to turn on code for only certain types of builds, so that, for example, we could get feedback from Dropboxers for brand-new (but highly experimental) features a few months before we were ready to commit to that product strategy externally.All of the process changes and infrastructure and tooling investments built on top of one another to emphasize each others’ efficacy. They also tended to achieve one of the following ends:Reduce and automate “Keep The Lights On” work, operational work that often scales linearly with code change by:Automatically affirm quality, via testing and diagnostics such as:Together, they allowed us to accelerate our release cadence, from publishing a new major version every 8 weeks to every 2 weeks, in order to shorten the feedback cycle on new features and code changes, while sustaining high quality and avoiding undue operational burden on the engineers responsible for maintaining the platform.",https://blogs.dropbox.com/tech/2017/04/accelerating-iteration-velocity-on-dropboxs-desktop-client-part-2/,0,dropbox,"python,ruby,frontend,css",NULL,2017-04-21
Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning,"In this post we will take you behind the scenes on how we built a state-of-the-art Optical Character Recognition (OCR) pipeline for our mobile document scanner. We used computer vision and deep learning advances such as bi-directional Long Short Term Memory (LSTMs), Connectionist Temporal Classification (CTC), convolutional neural nets (CNNs), and more. In addition, we will also dive deep into what it took to actually make our OCR pipeline production-ready at Dropbox scale.In previous posts we have described how Dropbox’s mobile document scanner works. The document scanner makes it possible to use your mobile phone to take photos and “scan” items like receipts and invoices. Our mobile document scanner only outputs an image — any text in the image is just a set of pixels as far as the computer is concerned, and can’t be copy-pasted, searched for, or any of the other things you can do with text.Hence the need to apply Optical Character Recognition, or OCR. This process extracts actual text from our doc-scanned image. Once OCR is run, we can then enable the following features for our Dropbox Business users:When we built the first version of the mobile document scanner, we used a commercial off-the-shelf OCR library, in order to do product validation before diving too deep into creating our own machine learning-based OCR system. This meant integrating the commercial system into our scanning pipeline, offering both features above to our business users to see if they found sufficient use from the OCR. Once we confirmed that there was indeed strong user demand for the mobile document scanner and OCR, we decided to build our own in-house OCR system for several reasons.First, there was a cost consideration: having our own OCR system would save us significant money as the licensed commercial OCR SDK charged us based on the number of scans. Second, the commercial system was tuned for the traditional OCR world of images from flat bed scanners, whereas our operating scenario was much tougher, because mobile phone photos are far more unconstrained, with crinkled or curved documents, shadows and uneven lighting, blurriness and reflective highlights, etc. Thus, there might be an opportunity for us to improve recognition accuracy.In fact, a sea change has happened in the world of computer vision that gave us a unique opportunity. Traditionally, OCR systems were heavily pipelined, with hand-built and highly-tuned modules taking advantage of all kinds of conditions they could assume to be true for images captured using a flatbed scanner. For example, one module might find lines of text, then the next module would find words and segment letters, then another module might apply different techniques to each piece of a character to figure out what the character is, etc. Most methods rely on binarization of the input image as an early stage, and this can be brittle and discards important cues. The process to build these OCR systems was very specialized and labor intensive, and the systems could generally only work with fairly constrained imagery from flat bed scanners.The last few years has seen the successful application of deep learning to numerous problems in computer vision that have given us powerful new tools for tackling OCR without having to replicate the complex processing pipelines of the past, relying instead on large quantities of data to have the system automatically learn how to do many of the previously manually-designed steps.Perhaps the most important reason for building our own system is that it would give us more control over own destiny, and allow us to work on more innovative features in the future.In the rest of this blog post we will take you behind the scenes of how we built this pipeline at Dropbox scale. Most commercial machine learning projects follow three major steps:We will take you through each of these steps in turn.Our initial task was to see if we could even build a state of the art OCR system at all.We began by collecting a representative set of donated document images that match what users might upload, such as receipts, invoices, letters, etc. To gather this set, we asked a small percentage of users whether they would donate some of their image files for us to improve our algorithms. At Dropbox, we take user privacy very seriously and thus made it clear that this was completely optional, and if donated, the files would be kept private and secure. We use a wide variety of safety precautions with such user-donated data, including never keeping donated data on local machines in permanent storage, maintaining extensive auditing, requiring strong authentication to access any of it, and more.Another important, machine learning-specific component for user-donated data is how to label it. Most current machine learning techniques are strongly-supervised, meaning that they require explicit manual labeling of input data so that the algorithms can learn to make predictions themselves. Traditionally, this labeling is done by outside workers, often using a micro-work platform such as Amazon’s Mechanical Turk (MTurk). However, a downside to using MTurk is that each item might be seen and labeled by a different worker, and we certainly don’t want to expose user-donated data in the wild like this!Thus, our team at Dropbox created our own platform for data annotation, named DropTurk. DropTurk can submit labeling jobs either to MTurk (if we are dealing with public non-user data) or a small pool of hired contractors for user-donated data. These contractors are under a strict non-disclosure agreement (NDA) to ensure that they cannot keep or share any of the data they label. DropTurk contains a standard list of annotation task UI templates that we can rapidly assemble and customize for new datasets and labeling tasks, which enables us to annotate our datasets quite fast.For example, here is a DropTurk UI meant to provide ground truth data for individual word images, including one of the following options for the workers to complete:DropTurk UI for adding ground truth data for word imagesOur DropTurk platform includes dashboards to get an overview of past jobs, watch the progress of current jobs, and access the results securely. In addition, we can get analytics to assess workers’ performance, even getting worker-level graphical monitoring of annotations of ongoing jobs to catch potential issues early on:DropTurk DashboardUsing DropTurk, we collected both a word-level dataset, which has images of individual words and their annotated text, as well as a full document-level dataset, which has images of full documents (like receipts) and fully transcribed text. We used the latter to measure the accuracy of existing state-of-the-art OCR systems; this would then inform our efforts by telling us the score we would have to meet or beat for our own system. On this particular dataset, the accuracy percentage we had to achieve was in the mid-90s.Our first task was to determine if the OCR problem was even going to be solvable in a reasonable amount of time. So we broke the OCR problem into two pieces. First, we would use computer vision to take an image of a document and segment it into lines and words; we call that the Word Detector. Then, we would take each word and feed it into a deep net to turn the word image into actual text; we call that the Word Deep Net.We felt that the Word Detector would be relatively straightforward, and so focused our efforts first on the Word Deep Net, which we were less sure about.The Word Deep Net combines neural network architectures used in computer vision and automatic speech recognition systems. Images of cropped words are fed into a Convolutional Neural Net (CNN) with several convolutional layers. The visual features that are output by the CNN are then fed as a sequence to a Bidirectional LSTM (Long Short Term Memory) — common in speech recognition systems — which make sense of our word “pieces,” and finally arrives at a text prediction using a Connectionist Temporal Classification (CTC) layer. Batch Normalization is used where appropriate.OCR Word Deep NetOnce we had decided on this network architecture for turning an image of a single word into text, we then needed to figure out how to collect enough data to train it. Deep learning systems typically need huge amounts of training data to achieve good recognition performance; in fact, the amount of training data is often the most significant bottleneck in current systems. Normally, all this data has to be collected and then labeled manually, a time-consuming and expensive process.An alternative is to programmatically generate training data. However, in most computer vision problems it’s currently too difficult to generate realistic-enough images for training algorithms: the variety of imaging environments and transformations is too varied to effectively simulate. (One promising area of current research is Generative Adversarial Networks (GANs), which seem to be well-suited to generating realistic data.) Fortunately, our problem in this case is a perfect match for using synthetic data, since the types of images we need to generate are quite constrained and can thus be rendered automatically. Unlike images of natural or most manmade objects, documents and their text are synthetic and the variability of individual characters is relatively limited.Our synthetic data pipeline consists of three pieces:The generation algorithm simply samples from each of these to create a unique training example.Synthetically generated word imagesWe started simply with all three, with words coming from a collection of Project Gutenberg books from the 19th century, about a thousand fonts we collected, and some simple distortions like rotations, underlines, and blurs. We generated about a million synthetic words, trained our deep net, and then tested our accuracy, which was around 79%. That was okay, but not good enough.Through many iterations, we evolved each piece of our synthetic data pipeline in many ways to improve the recognition accuracy. Some highlights:Synthetically generated words using different thermal printer fonts, common in receiptsSynthetically generated negative training examplesFake shadow effectData is as important as the machine learning model used, so we spent a great deal of time refining this data generation pipeline. At some point, we will open source and release this synthetically generated data for others to train and validate their own systems and research on.We trained our network on Amazon EC2 G2 GPU instances, spinning up many experiments in parallel. All of our experiments went into a lab notebook that included everything necessary to replicate experiments so we could track unexpected accuracy bumps or losses.Our lab notebook contained numbered experiments, with the most recent experiment first. It tracked everything needed for machine learning reproducibility, such as a unique git hash for the code that was used, pointers to S3 with generated data sets and results, evaluation results, graphs, a high-level description of the goal of that experiment, and more. As we built our synthetic data pipeline and trained our network, we also built many special purpose tools to visualize fonts, debug network guesses, etc.Example early experiment tracking error rate vs. how long our Word Deep Net had trained, against an evaluation dataset that consisted of just single words (Single Word Accuracy)Our early experiments tracked how well Word Deep Net did on OCR-ing images of single words, which we called Single Word Accuracy (SWA). Accuracy in this context meant how many of the ground truth words the deep net got right. In addition, we tracked precision and recall for the network. Precision refers to the fraction of words returned by the deep net that were actually correct, while recall refers to the fraction of evaluation data that is correctly predicted by the deep net. There tends to be a tradeoff between precision and recall.For example, imagine we have a machine learning model that is designed to classify an email as spam or not. Precision would be whether all the things that were labeled as spam by the classifier, how many were actually spam? Recall, in contrast, would be whether of all the things that truly are spam, how many did we label? It is possible to correctly label spam emails (high precision) while not actually labeling all the true spam emails (low recall).Week over week, we tracked how well we were doing. We divided our dataset into different categories, such as register_tapes (receipts), screenshots, scanned_docs, etc., and computed accuracies both individually for each category and overall across all data. For example, the entry below shows early work in our lab notebook for our first full end-to-end test, with a real Word Detector coupled to our real Word Deep Net. You can see that we did pretty terribly at the start:Screenshot from early end-to-end experiments in our lab notebookAt a certain point our synthetic data pipeline was resulting in a Single Word Accuracy (SWA) percentage in the high-80s on our OCR benchmark set, and we decided we were done with that portion. We then collected about 20,000 real images of words (compared to our 1 million synthetically generated words) and used these to fine tune the Word Deep Net. This took us to an SWA in the mid-90s.We now had a system that could do very well on individual word images, but of course a real OCR system operates on images of entire documents. Our next step was to focus on the document-level Word Detector.For our Word Detector we decided to not use a deep net-based approach. The primary candidates for such approaches were object detection systems, like RCNN, that try to detect the locations (bounding boxes) of objects like dogs, cats, or plants from images. Most images only have perhaps one to five instances of a given object.However, most documents don’t just have a handful of words — they have hundreds or even thousands of them, i.e., a few orders of magnitude more objects than most neural network-based object detection systems were capable of finding at the time. We were thus not sure that such algorithms would scale up to the level our OCR system needed.Another important consideration was that traditional computer vision approaches using feature detectors might be easier to debug, as neural networks as notoriously opaque and have internal representations that are hard to understand and interpret.We ended up using a classic computer vision approach named Maximally Stable Extremal Regions (MSERs), using OpenCV’s implementation. The MSER algorithm finds connected regions at different thresholds, or levels, of the image. Essentially, they detect blobs in images, and are thus particularly good for text.Our Word Detector first detects MSER features in an image, then strings these together into word and line detections. One tricky aspect is that our word deep net accepts fixed size word image inputs. This requires the word detector to thus sometimes include more than one word in a single detection box, or chop a single word in half if it is too long to fit the deep net’s input size. Information on this chopping then has to be propagated through the entire pipeline, so that we can re-assemble it after the deep net has run. Another bit of trickiness is dealing with images with white text on dark backgrounds, as opposed to dark text on white backgrounds, forcing our MSER detector to be able to handle both scenarios.Once we had refined our Word Detector to an acceptable point, we chained it together with our Word Deep Net so that we could benchmark the entire combined system end-to-end against document-level images rather than our older Single Word Accuracy benchmarking suite. However, when we first measured the end-to-end accuracy, we found that we were performing around 44% — quite a bit worse than the competition.The primary issues were spacing and spurious garbage text from noise in the image. Sometimes we would incorrectly combine two words, such as “helloworld”, or incorrectly fragment a single word, such as “wo rld”.Our solution was to modify the Connectionist Temporal Classification (CTC) layer of the network to also give us a confidence score in addition to the predicted text. We then used this confidence score to bucket predictions in three ways:We also had to deal with issues caused by the previously mentioned fixed receptive image size of the Word Deep Net: namely, that a single “word” window might actually contain multiple words or only part of a very long word. We thus run these outputs along with the original outputs from the Word Detector through a module we call the Wordinator, which gives discrete bounding boxes for each individual OCRed word. This results in individual word coordinates along with their OCRed text.For example, in the following debug visualization from our system you can see boxes around detected words before the Wordinator:The Wordinator will break some of these boxes into individual word coordinate boxes, such as “of” and “Engineering”, which are currently part of the same box.Finally, now that we had a fully working end-to-end system, we generated more than ten million synthetic words and trained our neural net for a very large number of iterations to squeeze out as much accuracy as we could. All of this finally gave us the accuracy, precision, and recall numbers that all met or exceeded the OCR state-of-the-art.We briefly patted ourselves on the back, then began to prepare for the next tough stage: productionization.At this point, we had a collection of prototype Python and Lua scripts wrapping Torch — and a trained model, of course! — that showed that we could achieve state of the art OCR accuracy. However, this is a long way from a system an actual user can use in a distributed setting with reliability, performance, and solid engineering. We needed to create a distributed pipeline suitable for use by millions of users and a system replacing our prototype scripts. In addition, we had to do this without disrupting the existing OCR system using the commercial off the shelf SDK.Here’s a diagram of the productionized OCR pipeline:Overall Productionized OCR PipelineWe started by creating an abstraction for different OCR engines, including our own engine and the commercial one, and gated this using our in-house experiments framework, Stormcrow. This allowed us to introduce the skeleton of our new pipeline without disrupting the existing OCR system, which was already running in production for millions of our Business customers.We also ported our Torch based model, including the CTC layer, to TensorFlow for a few reasons. First, we’d already standardized on TensorFlow in production to make it easier to manage models and deployments. Second, we prefer to work with Python rather than Lua, and TensorFlow has excellent Python bindings.In the new pipeline, mobile clients upload scanned document images to our in-house asynchronous work queue. When the upload is finished, we then send the image via a Remote Procedure Call (RPC) to a cluster of servers running the OCR service.The actual OCR service uses OpenCV and TensorFlow, both written in C++ and with complicated library dependencies; so security exploits are a real concern. We’ve isolated the actual OCR portion into jails using technologies like LXC, CGroups, Linux Namespaces, and Seccomp to provide isolation and syscall whitelisting, using IPCs to talk into and out of the isolated container. If someone compromises the jail they will still be completely separated from the rest of our system.Our jail infrastructure allows us to efficiently set up expensive resources a single time at startup, such as loading our trained models, then have these resources be cloned into a jail to satisfy a single OCR request. The resources are cloned Copy-on-Write into the forked jail and are read-only for how we use our models so it’s quite efficient and fast. We had to patch TensorFlow to make it easier to do this kind of forking. (We submitted the patch upstream.)Once we get word bounding boxes and their OCRed text, we merge them back into the original PDF produced by the mobile document scanner as an OCR hidden layer. The user thus gets a PDF that has both the scanned image and the detected text. The OCRed text is also added to Dropbox’s search index. The user can now highlight and copy-paste text from the PDF, with the highlights going in the correct place due to our hidden word box coordinates. They can also search for the scanned PDF via its OCRed text on Dropbox.At this point, we now had an actual engineering pipeline (with unit tests and continual integration!), but still had performance issues.The first question was whether we would use CPUs or GPUs in production at inference time. Training a deep net takes much longer than using it at inference time. It is common to use GPUs during training (as we did), as they vastly decrease the amount of time it takes to train a deep net. However, using GPUs at inference time is a harder call to make currently.First, having high-end GPUs in a production data center such as Dropbox’s is still a bit exotic and different than the rest of the fleet. In addition, GPU-based machines are more expensive and configurations are churning faster based on rapid development. We did an extensive analysis of how our Word Detector and Word Deep Net performed on CPUs vs GPUs, assuming full use of all cores on each CPU and the characteristics of the CPU. After much analysis, we decided that we could hit our performance targets on just CPUs at similar or lower costs than with GPU machines.Once we decided on CPUs, we then needed to optimize our system BLAS libraries for the Word Deep Net, to tune our network a bit, and to configure TensorFlow to use available cores. Our Word Detector was also a significant bottleneck. We ended up essentially rewriting OpenCV’s C++ MSER implementation in a more modular way to avoid duplicating slow work when doing two passes (to be able to handle both black on white text as well as white on black text); to expose more to our Python layer (the underlying MSER tree hierarchy) for more efficient processing; and to make the code actually readable. We also had to optimize the post-MSER Word Detection pipeline to tune and vectorize certain slow portions of it.After all this work, we now had a productionized and highly-performant system that we could “shadow turn on” for a small number of users, leading us to the third phase: refinement.With our proposed system running silently in production side-by-side with the commercial OCR system, we needed to confirm that our system was truly better, as measured on real user data. We take user data privacy very seriously at Dropbox, so we couldn’t just view and test random mobile document scanned images. Instead, we used the user-image donation flow detailed earlier to get evaluation images. We then used these donated images, being very careful about their privacy, to do a qualitative blackbox test of both OCR systems end-to-end, and were elated to find that we indeed performed the same or better than the older commercial OCR SDK, allowing us to ramp up our system to 100% of Dropbox Business users.Next, we tested whether fine-tuning our trained deep net on these donated documents versus our hand chosen fine tuning image suite helped accuracy. Unfortunately, it didn’t move the needle.Another important refinement was doing orientation detection, which we had not done in the original pipeline. Images from the mobile document scanner can be rotated by 90° or even upside down. We built an orientation predictor using another deep net based on the Inception Resnet v2 architecture, changed the final layer to predict orientation, collected an orientation training and validation data set, and fine-tuned from an ImageNet-trained model biased towards our own needs. We put this orientation predictor into our pipeline, using its detected orientation to rotate the image to upright before doing word detection and OCRing.One tricky aspect of the orientation predictor was that only a small percentage of images are actually rotated; we needed to make sure our system didn’t inadvertently rotate upright images (the most common case) while trying to fix the orientation for the smaller number of non-upright images. In addition, we had to solve various tricky issues in combining our upright rotated images with the different ways the PDF file format can apply its own transformation matrices for rotation.Finally, we were surprised to find some tough issues with the PDF file format containing our scanned OCRed hidden layer in Apple’s native Preview application. Most PDF renderers respect spaces embedded in the text for copy and paste, but Apple’s Preview application performs its own heuristics to determine word boundaries based on text position. This resulted in unacceptable quality for copy and paste from this PDF renderer, causing most spaces to be dropped and all the words to be “glommed together”. We had to do extensive testing across a range of PDF renderers to find the right PDF tricks and workarounds that would solve this problem.In all, this entire round of researching, productionization, and refinement took about 8 months, at the end of which we had built and deployed a state-of-the-art OCR pipeline to millions of users using modern computer vision and deep neural network techniques. Our work also provides a solid foundation for future OCR-based products at Dropbox.Interested in applying the latest machine learning research to hard, real–world problems and shipping to millions of Dropbox users? Our team is hiring!",https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/,0,dropbox,"tensorflow,machinelearning,python,regression",NULL,2017-04-12
Deploying Brotli for static content,"Most representations of data contain a lot of redundancy, which provides an opportunity for greater communication efficiency by compressing the content. Compression is either built-in into the data format — like in the case of images, fonts, and videos — or provided by the transportation medium, e.g. the HTTP protocol has the Accept-Encoding / Content-Encoding header pair that allows clients and servers to agree on a preferred compression method. In practice though, most servers today only support gzip.In this blog post, we are going to share our experiences with rolling out Brotli encoding for static content used by dropbox.com, decreasing the size of our static assets by 20% on average. Brotli is a modern lossless compression algorithm based on the same foundations as gzip (LZ77 and Huffman encoding), but improves them with a static dictionary, larger matching windows, and extended context modeling for better compression ratios. We won’t go into much detail on how Brotli works here; if you want to dig deeper into the compression format, you can read a great introduction to Brotli internals by Cloudflare.Before starting any project, we need to ask ourselves whether it is worth the engineering resources involved. In this phase, it also makes sense to investigate alternatives. Here are the two most obvious ones for this project:We’ll only focus on static resources for now, and skip all of the on-the-fly/online/dynamic data compression. This gives us the following benefits:The main problem with rolling out new compression algorithms for the Web is that there are multiple dependencies besides the server itself: in particular, web browsers and all the agents in the middle need to support it, such as Content Delivery Networks (CDNs) and proxies. Let’s go over all of these components one-by-one.Webservers responsible for static content usually spend their time in a repeating loop consisting of the following elements:To prevent repeated compression steps for static content, nginx has a builtin gzip_static module, which will first try looking for pre-compressed versions of files and serve them instead of wasting CPU cycles on compression of the original data.As an additional optimization, it is also possible to only store compressed versions of the files by combining gzip_static with a gunzip module to save space on disk and in the page cache. Also, decompression is usually orders of magnitude faster than compression, so the CPU hit is almost non-existent.But let’s get back to Brotli. Piotr Sikora, well known to subscribers of nginx-devel mailing list, has written an ngx_brotli module that adds support for Brotli encoding to nginx, along with brotli_static directive that enables serving of pre-compressed .br files. This solves the server-side requirements.The biggest chunk of the actual work was adding a step to our static build pipeline that processes all the compressible MIME-types inside our static package and emits brotli-encoded versions of them.Brotli sources provide C, Java, and Python bindings. You can either use these bindings or just fork/exec the bro tool to directly compress files. The only caveat at this point is to balance the Brotli window size. Bigger compression windows lead to higher compression ratios but also have higher memory requirements during both compression and decompression, so if you are pre-compressing data for mobile devices, you may want to limit your window size.It is worth noting that there is no reason to set the compression window higher than the source file size. The window size in Brotli is specified in bits, from which the actual size is calculated via: win_size = (1 << wbits) - 16, so you need to take these 16 bytes into an account when computing your window size.The allowed range of window sizes is currently [1KB, 16MB], though there are some talks about using brotli with large windows (up to 1GB).As an alternative to Brotli, you could try to optimize gzip for pre-compression. There is not much difference in compression ratios for zlib levels higher than 6, so pre-compression with gzip -9 on average gives less than 1% improvement over our default gzip -6. But zlib is not the only implementation of the deflate format; luckily there are alternatives that offer various benefits:In theory, CDNs should support Brotli transparently, the same way they currently do with gzip: if the origin properly sets the Vary: Accept-Encoding header. In practice, though, CDNs are heavily optimized for the gzip/non-gzip use-case. For example:(By the way, if you are not familiar with Vary header, you probably want to read Fastly’s blog post about best practices of using Vary.)If you are going to add the Accept-Encoding header to your cache key, you should also be aware of all the downsides: values of that header are very diverse, and therefore your CDN cache hit rate will go down. Judging by the data provided by Fastly, Accept-Encoding has a really long tail of values, and so a naive implementation of a cache key may not always work. The solution here is to normalize the header before it is passed to the upstream. You can use this code as a reference example.Besides an explicit middleman like a CDN, there might also be implicit proxies between the client and server, any of which may misinterpret unknown content encodings. This is commonly referred as the “Middlebox problem.” Brotli solves it by advertising support for itself only over HTTPS, so that proxy servers can’t inspect/modify content. Sadly, though, there are still some issues with corporate proxies and Anti-Virus products that intercept-and-modify TLS. Fortunately, issues like these are quite rare, and vendors are usually quick to patch them.Client side support is, fortunately, the least problematic of all. Brotli was developed by Google and hence Chrome has supported it for quite a while. Since Brotli is an open standard, other browsers have support for it as well:The only straggler is Safari, which does not advertise Brotli support even in the latest Technological Preview.We did run into an unexpected issue with very old versions of Firefox 45 ESR (e.g. 45.0.*). Even though it advertises Brotli support, it couldn’t decode some files. Further digging revealed that it can’t unpack files encoded with windows smaller than original file size. As a workaround, you can either increase your window size or, if you are using nginx, implement the brotli_disable directive, which will mirror the gzip_disable behavior, but with a blacklist for brotli-misbehaving browsers.Enabling Brotli decreased the size of the payload fetched by users from CDN on average by 20%! Pretty pictures follow.(Oh, speaking of which, in case you’re wondering why all the graphs are in xkcd style: no reason really, it’s just fun. If your eyes bleed from the Comi^WHumorSans typeface there are links to “boring” SVGs at the bottom.)A small note about how to read the graphs. We’ve split all our static assets by type, divided each group into logarithmic size buckets from 32 bytes to 1MB, and then computed the average compression ratio win over gzip -9 for libdeflate and Brotli, respectively. Error bars represent the 95% confidence interval. For those of you who like to get all sciencey and stuff, boxplots are also attached (for the Brotli part of the dataset only) with IQR, median, min, and max stats.Let’s start with normal (unminified) JavaScript files. The results here are good; Brotli gives an astonishing 20-25% of improvement over zlib on both small files (where its static dictionary has a clear advantage), and on huge files (where now a bigger window size allows it to beat gzip quite easily). On medium-sized files Brotli “only” gives 13-17% of improvement.Libdeflate also looks quite good: getting additional 4-5% out of gzip is a low-hanging fruit that anyone can take advantage of.Compression improvements over `gzip -9` for Javascript files broken down by original file sizexkcd: PNG|SVG, normal: PNG|SVG, boxplot:PNG|SVGIf we now concentrate only on minified JavaScript, which is probably the bulk of any website right now, then Brotli maintains its compression ratios, except for the biggest filesizes. But as you can see from the wide confidence intervals in the box plots, we do not have enough samples there.Libdeflate drops to a stable ~3% of improvement over zlib.Compression improvements over `gzip -9` for minified Javascript files broken down by original file size xkcd: PNG|SVG, normal: PNG|SVG, boxplot: PNG|SVGStylesheets have even better compressibility in Brotli’s case: now it ranges between 20-30%, while libdeflate gets around 4%. This is most likely due to the huge redundancy of the CSS format.Compression improvements over `gzip -9` for CSS files broken down by original file size xkcd: PNG|SVG, normal: PNG|SVG, boxplot: PNG|SVGThe biggest benefits for Brotli are observed on Cyrillic and Asian languages — upto 31% (most likely due to their larger size, which benefit from Brotli’s larger compression window size). Other languages gets “only” around 25%.Libdeflate is now closer to 5%.Compression improvements over `gzip -9` for different languages xkcd: PNG|SVG, normal: PNG|SVGThe previous section was actually the second most important part of this post — this section is actually the most important! I’m personally a big fan of sharing negative results, so let me also discuss what we did not get from rolling out Brotli: a major web performance win.Internally, our main performance metric is Time-To-Interactive (TTI) — i.e., the amount of time it takes before the user can interact with a web page — and we saw only a minor benefit of rolling out Brotli on the 90th-percentile (p90) TTI stats, mainly because:All of this means that you won’t get a 20% TTI win from 20% smaller static files. But that does not mean Brotli was not worth it! Infrequent users of our site, especially those on lossy wireless links or costly mobile data plans will definitely appreciate the improved compression ratios.On the efficiency side, pre-compression also removed any CPU usage from our web tier serving static content, reducing overall CPU usage by ~15%.The next logical step is to start compressing non-static data with Brotli. This will require a bit more R&D since compression speed now becomes rather crucial. The Squash benchmark can give a ballpark estimation of the compression speed/ratio tradeoff:enwiki8 compression ratio vs compression speed (Here, we’d like to be as close to the top-right corner as possible — both fast compression speeds and high compression ratios.)However, it’s actually even more complicated that this chart suggests, since now we need to optimize for (compression_time + transfer_time + decompression_time) while also keeping internal buffering inside our proxy tier at a minimum. An example of bad internal buffering is zlib in the nginx gzip module, where enabling gzip without disabling proxy buffering may lead to excessive buffering. With all of that considered, we will keep a close eye on both Time-To-First-Byte (how fast the client starts to receive data) and Time-To-Last-Byte (how fast the client finishes receiving all data) changes during the Brotli rollout for dynamic content.It will take us a bit of time to collect enough data to fully explore this, but for now using Brotli with quality set to 5 seems like a good trade-off between compression speed and compression ratio for dynamic data, based on preliminary results.On average, only 10% of downloaded bytes are attributed to images, but there are outlier pages, such as landing pages which can be 80% images. Brotli won’t help with those, but that doesn’t mean we can’t do other things.There are many tools that you can plug into your static build pipeline to reduce the size of images. The specific tool to use depends on the image format:Currently we use pngquant to optimize our sprite files for our website, which shrinks their size to ~25% of the original size, cutting down each page size by almost 300kB. We can get an additional 5% out of deflate if we run ZopfliPNG/``advpng on top of it.A final fun fact: because of the rather limited back-reference distance of gzip, grouping visually similar sprites closer together will yield better compression ratios! The same trick can be generally applied to any deflate -compressed data; for example, we sort our langpacks so they compress better.It took us around 2 weeks to modify our static asset pipeline, add support for serving pre-compressed files to our webservers, and modify CDN cache behavior, test and deploy Brotli to production at Dropbox. This one-time effort enabled us to have 20% smaller static asset footprint from now on for all 500 million of our users!I mentioned that this post wouldn’t go too much into the Brotli internals, but if you want those nitty-gritty details, here are some WebFonts Working Group presentations:As usual, the most authoritative and full overview is the standard itself: RFC7932 (don’t worry that it is 132 pages long — two thirds of it is just a hex dump of Brotli’s static dictionary).Some time ago we’ve blogged about data compression using Brotli on our backend and some modifications we’ve made to it so it can compress quite a bit faster. This ended up as the “q9.5” branch on the Brotli’s official github. However, since this post was about using offline compression for static assets, we are using maximum quality settings here so that we can get the maximum benefits from the compression.Within the /research directory of the Brotli project there are many interesting tools, mostly aimed at exploring underlying file format and visualizing backreferences.One notable tool is brotlidump.py — a self-sufficient utility to parse Brotli-compessed files. I’ve used it frequently to look into resulting .br files, e.g. to check window size or inspect context maps.If you are looking at Brotli for compressing data internally, especially if you have a lot of small and similarly formatted files (e.g. a bunch of JSONs or protobufs from the same API), then you may increase your compression ratio by creating a custom dictionary for your data (the same way SDCH does it). That way you don’t have to rely on Brotli’s built-in static dictionary matching your dataset, and you also won’t have the overhead of constructing almost the same dynamic dictionary for each tiny file.Support for custom dictionaries exists in Brotli’s Python bindings if you want to play with it. As for constructing dictionaries from a sample of files, I would recommend Vlad Krasnov’s dictator tool.Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world.",https://blogs.dropbox.com/tech/2017/04/deploying-brotli-for-static-content/,0,dropbox,,NULL,2017-04-06
Memory-Efficient Image Passing in the Document Scanner,"In our previous blog posts on Dropbox’s document scanner (Part 1, Part 2 and Part 3), we focused on the algorithms that powered the scanner and on the optimizations that made them speedy. However, speed is not the only thing that matters in a mobile environment: what about memory? Bounding both peak memory usage and memory spikes is important, since the operating system may terminate the app outright when under memory pressure. In this blog post, we will discuss some tweaks we made to lower the memory usage of our iOS document scanner.In the iOS SDK, the image container used ubiquitously is UIImage. It is a general container for images that cover pretty much all the common use cases. It has the following nice properties:However, one thing that UIImage does not let you do is to directly access the raw pixels (unless you have created one from raw pixels yourself and keep around an independent reference.) This makes sense because if a developer could directly access the raw pixels of an arbitrary UIImage, then immutability immediately goes out the window. You can get a copy of the pixels via CGDataProviderCopyData, which is a part of the public API. This isn’t bad when the image is small, but given that latest iPhones capture 12-megapixel images (= 48MB rasterized), even having one extra copy of the data hurts.Image processing is often best expressed as a pipeline. For example, in our document scanner, the input image goes through resizing, document detection, rectification, enhancement and then compression, among other things. Consider the following toy example:In practice, we rely heavily on ObjC blocks in order to control whether the work happens synchronously or asynchronously. More specifically, we can chain the module—each module invokes its predecessor and schedules itself as a callback to the predecessor—as shown below:While ObjC blocks afford us flexibility, consider what happens in each of resize , rectify , enhance. In each method, before doing any useful work, we will have to read the pixels first, incurring a copy. As a result, we would end up doing a lot of memcpy operations. It seems silly to extract the pixels and then imprison them again in a UIImage each time.The teaser graph above shows a visualization created from Xcode’s Instruments app, as the document scanner flow is used to acquire a 3-page document. It is easy to see that there’s indeed a large spike in memory usage while the camera is actively running, and upon the user tapping the shutter button. We would often run into out-of-memory process deaths from these spikes, and this led to exploring an alternate design that could reduce both the spikes and the peak usage.We decided to roll our own image class—call it DBPixelBuffer—which is a thin wrapper around a block of memory, so that we could have read access of the pixels without incurring a copy each time. However, with great power comes great responsibility: doing so puts immutability at risk, so it is important to take proper care.Rolling out our own image class had some added benefits. Recall from the previous blog post that we are leveraging the onboard GPU to accelerate image processing. This requires passing the data to the GPU in a format that it can understand (typically 32-bit RGBA buffers), so using our own container gives us more flexibility to minimize the number of times the buffer may have to be converted when using the GPU.Also, while UIImage contains a flag for orientation and allows zero-cost rotation of images, computer vision algorithms often assume that the input buffer is upright, unless they have been trained on a dataset that is explicitly augmented with rotations. Hence it is generally a good idea to normalize the orientation when passing the image to computer vision algorithms. Since rotating images was a very common editing operation, and we decided to keep the orientation flag in DBPixelBuffer, so that we could lazily rotate the image.One complication is that the iOS camera gives us a UIImage, which is internally backed by a JPEG encoding. Reading the pixels triggers JPEG decompression, which is done in a temporary buffer managed by iOS. Because this temporary buffer is not directly accessible, we would need to copy the content of the buffer to a buffer we control. In other words, we will have two copies of the image in memory simultaneously, if we want to convert the given UIImage to DBPixelBuffer. To avoid this, we decided to do JPEG decompression with libjpeg, rather than using Apple’s SDK, so that we could dump the data directly into a buffer we control. As an added benefit, we could choose to decode the JPEG at a lower resolution, if only a thumbnail was needed.Note that even in the best case, converting from UIImage to DBPixelBuffer involves at least one extra buffer. Sometimes we want to defer this as much as possible—for many of the image processing operations we perform, we do not need to be at full resolution. If the result is going to be small, e.g. screen size, then we could do just enough compute (and not more!) to make sure the result looks good at the intended resolution. Hence we designed our image processing pipeline to take a DBPixelBufferProvider as the input, which is a common protocol implemented by both UIImage and DBPixelBuffer, so that the determination of the processing resolution could be deferred.Why is this helpful? Previously we would create a thumbnail from the captured image right away, as shown in the top half of the above figure. However, because we didn’t know a priori how big the detected document would be, the thumbnail had to be fairly large in order to ensure that the resulting crop was at least screen-sized. In the new implementation shown in the bottom half of the figure, we can avoid creating thumbnail up-front, and instead render the crop at screen resolution directly when needed.Note that moving to a lazy provider introduces additional complexity: for one, it potentially introduces latency at the call site. We should also carefully consider whether and how to cache the resulting pixel buffers. Nonetheless, moving to a lazy provider allowed us to reduce memory footprint across all code paths at once, which was crucial in reducing the peak memory usage.Transitioning from UIImage to our own image class with which we could precisely control when and how the pixels are transformed allowed us to reduce memory spikes from 60MB to 40MB, and peak memory usage by more than 50MB. (Check out the teaser graph above.) In this particular case, the complexity introduced by swapping out UIImage with our own was worth the reduction in resource utilization and increased stability.Try out the Dropbox doc scanner today, and stay tuned for our next blog post.",https://blogs.dropbox.com/tech/2017/03/memory-efficient-image-passing-in-document-scanner/,0,dropbox,"java,python",NULL,2017-03-30
"Accelerating Iteration Velocity on Dropbox’s Desktop Client, Part 1","Imagine you’re an engineer working on a new product feature that is going to have a high impact on the end user, like the Dropbox Badge. You want to get quick validation on the functionality and utility of the feature. Each individual change you make might be relatively simple, like a tweak to the CSS changing the size of a font, or more substantial, like enabling the Badge on a new file type. You could set up user studies, but these are relatively expensive and slow, and are a statistically small sample size. Ideally, you write some code, add it to the codebase, have it tested automatically, and then release it immediately to some users so you can gather feedback and iterate.Now imagine you’re the engineer responsible for maintaining the code platform. Every code change and every new version requires you or someone on your team to do a piece of work. You need to make sure that any random commit doesn’t break the development environment for everyone, that the machines that compile code are up and running, and that mysterious bugs are found and routed to the correct engineer to fix before a new version is released to the world. For the Dropbox Desktop Client, this also means keeping up lightning-fast sync rates, making sure core workflows (like sign-in) still function, and minimizing crashes. Ideally, you want to minimize manual operations work, and you want product quality to be high.This can create conflict. The product engineer wants to release as quickly as possible, with ambitious new ideas. The platform engineer doesn’t want to stay until 10pm everyday to make that happen, or to have to constantly say no to the product engineer because changes are too risky to product quality. How do we keep these two types of people in harmony?At Dropbox, the Desktop Client development and deployment schedule previously took 8 weeks per major release, with more than 3 full time engineers required to orchestrate everything. The experience was unpredictable for engineers, because there were frequent rollbacks and delays, and it took a lot of time to figure out and fix the source of new bugs and test failures. It would take up to 3 days before a product engineer’s code reached an internal user and 10-18 weeks before it was fully released.With improvements in our process and systems, we are now operating on a 2-week cadence. Most of the release process is handled by a Technical Project Manager with two engineers who assist part-time, and debugging work is quickly routed to the responsible team. Code is sent to internal users within one business day, and to everyone within 2-4 weeks. The rest of this post (and the next post in this series) talks about how we achieved this remarkable improvement.In 2015, the Dropbox Desktop Client targeted a cadence of releasing a new major version every 8 weeks. It took 10-18 weeks from when code was added to the main branch of the codebase until the feature was deployed to 100% of Desktop Client users. To quote from old documentation, the “long cycle time is to ensure that we are not putting insecure or very broken code on our users’ machines, because auto-update is a slow and imperfect process.” Here is a summary of that process:Integrating new codeThe Release Engineering team, responsible for the infrastructure on which test suites and build scripts ran, had at least one and up to three full-time people dedicated to operations work to keep the machines that allowed engineers to commit new code running.Making buildsCompiling a new version of Dropbox was the responsibility of a specific engineer, who was the “Primary On-Call” for the Desktop Client team. Their day looked like this:Troubleshooting issuesAnother engineer, the Sync On-Call, would specifically be in charge of possible sync-related issues. The Sync On-Call and Primary On-Call together would do an initial investigation to either solve the problem or assign it to a person who would. There were three versions of Dropbox being served at any given point of time: the “office” build, which was just for internal users; “forums” build, which external users could beta-test; and “stable”, the version currently given to the world. Keeping track of all three was assumed to take up both On-Calls’ entire weeks, but was more difficult for newer engineers without as much context. Issues could easily take several days until their root causes were uncovered and fixed, so each major version of Dropbox spent weeks in each stage.Rolling outA third engineer, called the Release Manager, would keep track of all issues that needed to be quashed before a specific major version could be released to the general public, stewarding it over the entire 8 week release cycle. They often had a big feature going out into that release, and had a perk of assigning a codename. (Crowd favorite: “The Kraken”).The Release Manager made sure all problems were resolved before rollout, and kept track of a complicated set of deadlines:Any of these phases could turn an issue big enough to halt the release train, and after a fix often required going through various parts of the process again. This meant high process complexity, and a possibly large matrix of code combinations that needed to be tested. Engineers were cautioned only to make code changes that would improve the quality of the product after feature freeze (for example, fixing small bugs), but if you missed a release, you would have to wait a full 8 weeks longer until your code shipped to the world. This created pressure to scramble to hit a specific version, and furthermore, even a seemingly safe change could have unexpected side effects.At this time, there were only about 30 engineers working on Desktop Client, meaning a tenth of the team was responsible for doing the often manual or organizational tasks necessary to keep the release trains running. We knew we wanted to speed up innovation on Desktop Client, but more engineers meant more changes per week, and more potential bugs that had to be tracked down per release. Further, the development environment required so much undocumented context that it was difficult for engineers working on other parts of the Dropbox to only work as a ‘part time’ Desktop Client engineer.These existing tools and processes were unscalable. Something had to change.As of this writing, new major versions of the Desktop Client are released every two weeks, with around 90 different engineers from dozens of teams contributing to each new release. An engineer working on a product feature usually sees their code in a Dropbox binary in front of some kind of user within 1 business day; their code is fully released within 28 days. Internal builds are automatically made and released daily, and external builds are automatically made and released as scheduled or necessary, stewarded by a single Technical Project Manager with two rotating engineers assisting part time, one to troubleshoot releases if necessary and one to keep an eye on the build system.Our efforts had two overarching themes:In these blog posts we’ll detail the steps Dropbox’s Desktop Platform team took to accelerate our release process, while maintaining high product quality. These “steps” were not discrete – often they had be worked on in parallel for maximum leverage, each multiplying the effectiveness of the other.When the project began in early 2016, Dropbox had Continuous Integration (CI) across the organization — engineers committed code to the same mainline branch in each codebase on a regular basis, and each commit kicked off a suite of test and builds. Generally speaking, as a software organization grows, CI and well-written test suites are the first line of defense for automatically maintaining product quality. They document and enforce what the expected behavior of code is, which prevents one engineer (who may not know or regularly interact with everyone who commits code) from unknowingly mucking up another’s work — or from regressing their own features, for that matter.Our test and build coordinator is a Dropbox open source project called Changes, which has an interface for each suite that looks this:Each bar represents a commit, in reverse chronological order. The result could be totally-passing (green) or have at least one test failure (red), with occasional system errors during the run (black). The time it took to run the job is represented by the height of the bar.Engineers were expected to commit only code that would pass the full test suite by testing locally. If a build went red, the On-Call for that area of code would play “build cop”, tracking down the failures and resolving the issue. This involved identifying the breaking change, asking the author to fix it quickly, or backing out the commit themselves. This troubleshooting could take a significant amount of time, while the test suite remained red and engineers committing new code would get emails notifying them of test failures. Errored builds were sometimes the fault of new code, and sometimes due to external problems, adding another layer of complexity. Engineers quickly learned that if you got a failure email, it likely wasn’t due to your change. They no longer trusted the system, and weren’t incentivized to investigate every failure in detail, so multiple failures could pile up before the On-Call untangled them.Untangling these build failures is KTLO work. To automate the job of ensuring that every single change passed the tests, Dropbox built a “Commit Queue” (CQ). Engineers submit new commits to the CQ, which run a suite of tests with the new commit incorporated into the codebase. If they passed, the CQ permanently adds the commit; if the tests failed, the commit is rejected and the author notified. The Commit Queue also ran tests on a greater set of environments than a single engineer could have on their laptop. An implementation of a commit queue had been running on the Server codebase since 2015, but using it for Desktop Client had two dependencies:A. Unifying technology across codebasesThe Server codebase had migrated to Git (from Mercurial) to reflect current trends in version control in 2014. Naturally, as they tackled similar issues and created new tools, those tools only explicitly supported Git. While we could have invested in improving the Server toolset to support Mercurial workflows, we ultimately decided instead to migrate the Desktop Client repo to Git. Not only would this enable us to leverage the work of our peer engineers, it also removed a point of daily friction faced by engineers committing code within both repos.This actually hints at a greater trend within Dropbox. The Dropbox EPD (Engineering, Product, and Design) organization had transitioned into explicit “Product” and “Product Platform” groups at the very beginning of 2016, rather than “Desktop”, “Web”, “Android”, etc. teams that did a combination of product and platform work. One benefit was obvious: it allowed us to specifically invest in platform goals like performance and ease of development, and free up product initiatives to be cross-platform. An additional side-benefit is that it put engineers with similar considerations across different codebases closer together organizationally, so that they could cross-pollinate and leverage the same set of tools.B. Reducing baseline flakinessBlocking developers from landing broken commits is great, but how do you know for certain a test failure is actually a real product bug, and not just noise? More trickily, what do you do about transient “flaky” failures that only occur infrequently when the test is run?There are two possible categories of reasons why a build would fail when the underlying code hadn’t regressed functionality: infrastructure flakiness (unreliability in the systems or code that run test jobs) and test flakiness (tests that fail some of the time, often non-deterministically). We had to hammer out a significant amount of both, or engineers would spend all their time waiting for their code to be merged. Or, they might retry preemptively, increase load on the infrastructure, and potentially cause cascading failures.Say you have a test that fails, non-deterministically, around 10% of the time, maybe due to a race condition. If you run it once in Commit Queue, most likely it will get through without a problem, and then fail every 10 builds or so there after. This will cause red builds in the post-commit runs, and occasionally block unrelated changes in Commit Queue. Both of these lead to a really bad developer experience, especially as flaky tests pile up and one test here or there fails, unrelated to your changeset, on every single run.Sometimes the flakiness is a badly written test, with a timeout that is too short and therefore triggers when the code being tested is doing just fine. Sometimes the flakiness is caused by state left over from a previous test that interferes with the current one, like a database entry. At Dropbox, tests are run in random order, so the same failure can show up as problems across the test suite. Sometimes the feature being tested is flaky, i.e., a window is supposed to open after a button click, but only does so some of the time. Categorizing these intermittent failure types is challenging for a human, let alone an automated tool.How do we identify flakiness? For one, we can re-run a test multiple times. The limiting factor is the total wait time for the test job, or if the the tests are sharded across multiple machines to reduce duration, the compute infrastructure costs of running the test suite. We configured Commit Queue to run a new or edited test many times as it is being committed, and reject a change if any of them fail. That should alert the authoring engineer that something needs to be fixed. From there, this engineer has the most context to figure out whether the product or the test (or both) is at fault. Once a test has passed Commit Queue, we run it up to three times post-commit and on unrelated changes, and count any success as “green”.However, because Commit Queue allows intermittent failures once a test is admitted to the general pool, we have to identify tests that recently started flaking and alert the corresponding engineer. For this, we have an additional tool called Quarantine Keeper that removes tests from the main pool if they fail too often, and files a bug against the engineer tagged as the owner of the test to fix and re-add to circulation. The overall goal is to try and keep signal to noise high; very unpredictable random one-off failures should not be alarming, but consistent occasional failures should be eliminated.Ironing out the build infrastructure flakiness meant systematically cataloging and fixing the types of failures — making sure that timeouts are set to appropriate intervals, adding retries where necessary, etc. The most impactful change we made was implementing a full-job retry to every job. If the network flaked momentarily in the beginning of the build, there was no reason to fail right off the bat — anything that failed for infrastructural reasons before 30 minutes were over was retried up to a total of 3 times, and it had a big impact on the greenness of our builds.Meanwhile, we had to get serious about running a distributed system, including measuring and anticipating the computational requirements to run the test and compilation jobs. The Dropbox Desktop Client is officially supported on over a dozen operating system versions, spread across Windows, Mac, and Linux. A lot of developer pain previously came from having to run tests across all these platforms by hand, so a coupled goal for all of this was increasing the number of OSes we had CQ coverage on. However, the more configurations we ran automatically, the more surface area for flakiness to manifest itself on any given commit, gradually eroding trust in the CI even as we worked to reduce many sources of flakiness since the beginning of the project. Further, we had to be careful because if we enabled more jobs types than we could support, we could easily push the machines that ran the jobs past their limits and cause across-the-board failures.One set of particularly interesting scaling-pains incidents occurred when rsync and git clone commands would mysteriously hang on Windows and Linux (but not Mac OS X) — and seemed to do so at a higher rate when more code was being committed. It turned out that the problem stemmed from the fact that our Windows and Linux Virtual Machines (VMs) shared the same network-attached storage device, while Mac OS X used different hardware. As we began supporting more test types, we were maxing out the disk I/O capacity of this machine, so rsync calls that simply copied files from one VM to the next would do both ends of the transfer on the same machine, overloading it, and fail! Thankfully we were able to fix it by removing some test types out of Commit Queue until we were able to upgrade our machine.In our next blog post on this topic, we will discuss the technological and process changes required to speed up making new builds and releasing with confidence.",https://blogs.dropbox.com/tech/2017/03/accelerating-iteration-velocity-on-dropboxs-desktop-client-part-1/,0,dropbox,"angular,html,frontend,ruby,css,python,bootstrap,react,javascript",NULL,2017-03-22
Preventing cross-site attacks using same-site cookies," Dropbox employs traditional cross-site attack defenses, but we also employ same-site cookies as a defense in depth on newer browsers. In this post, we describe how we rolled out same-site cookie based defenses on Dropbox, and offer some guidelines on how you can do the same on your website.Recently, the IETF released a new RFC introducing same-site cookies. Unlike traditional cookies, browsers will not send same-site cookies on cross-site requests. At Dropbox, we recently rolled out same-site cookies to defend against CSRF attacks and cross-site information leakage. We concluded that same-site cookies are a convenient way to reduce a website’s attack surface.Many attacks on the web involve cross-site requests, including the well-known cross-site request forgery (CSRF) attack. These attacks trick the victim’s browser into performing an unintended request to a trusted website. Because users trust Dropbox with their most sensitive data, it’s critical that we make our defenses against these attacks as strong as possible.What does a CSRF attack look like? As an example, let’s pretend Dropbox was naively not protecting against CSRF attacks. The attack starts when a victim visits an attacker-controlled website, say www.evil.com . The evil website then returns a page with a malicious payload. The browser executes this malicious payload, which makes a request to https://dropbox.com/cmd/delete and attempts to remove user data. A classic CSRF defense is to introduce a random value token — called the CSRF token — and store its value in a cookie, say csrf_cookie , on the first page load. The browser sends the CSRF token as a request parameter on every “unsafe” request (POSTs). The server then compares the value of csrf_cookie to the request parameter and throws a CSRF error if these values do not match.Even if a website has CSRF defenses, it could be vulnerable to cross-origin information leakage attacks like cross-site search attacks and JSON hijacking. For example, let’s assume www.dropbox.com has an AJAX route /get_files . A GET request to /get_files gets all of the logged in user’s filenames. and the size of the response can leak side channel information about how many files are there in a user’s Dropbox.We now describe how we designed and implemented defenses against cross-site attacks on Dropbox using same-site cookies.For reliability and security, our design for same-site cookie protections should have the following requirements:Cookies become same-site by setting the attribute SameSite to one of two values, strict or lax . When strict , browsers will not send the same-site cookie on any cross-site request. When it’s lax, the browser will only prevent sending the cookie on “unsafe” requests (POSTs) but will allow “safe” requests (GETs).Let’s say Dropbox stores two cookies: a session_cookie and a csrf_cookie (we’re simplifying a tad).Further, a POST request to https://dropbox.com/ajax_login on the Dropbox site takes as input the user’s credentials and logs the user in (or equivalently, writes session_cookie ). Dropbox also has many “shared links” on pages with the format https://dropbox.com/sh/…/filename . Users can share these links over email and restrict access to these links.While brainstorming on how to add same-site cookie protections to Dropbox, we came up with the following naïve designs but quickly figured out that they were flawed:   Instead, we opted to introduce a new cookie, __Host-samesite_cookie . This cookie is SameSite with enforcement mode as strict . We set this cookie on all browsers that support same-site cookies, and we validate this cookie on every relevant request on the same browsers.The value of this cookie is derived from the CSRF token. We validate the same-site cookie by checking for its presence as well as the correctness of its value. We check for the value to defend against session fixation in case a cookie with the same name got set by an attacker previously. __Host-samesite_cookie has an enforcement mode strict , which means it does not get sent on benign cross-site GET requests (e.g. visiting a Dropbox public link from an external page). This is fine, as we can control enforcement on server-side. If the request is a benign GET request but __Host-samesite_cookie is absent, we can still allow the request to pass through. However, if it’s a state-changing POST request and __Host-samesite_cookie is absent, we can treat this as a CSRF error. As an aside, we made __Host-samesite_cookie a __Host-prefix cookie. A __Host-prefix cookie is a cookie that must only be sent to the host that sent the cookie. JavaScript on a subdomain of www.dropbox.com cannot set this cookie. If both csrf_cookie and __Host-samesite_cookie are valid, we can be confident that no session fixation attacks have occurred.Dealing with cookie authentication is very risky. It could create many availability and security issues. In the worst cases, it could lock many users out (and force them to manually reset their cookies), log users into another user’s account, or completely disable our CSRF defenses!Therefore, we decided to roll out same-site cookie defenses in two stages: first in “warnings-only” mode, where we log all errors, and later in “enforcement” mode when we see no unexpected errors. Further, we would want to be flexible in terms of what kinds of requests we would like to enforce the same-site check for. Enabling the same-site check for POST requests only would be equivalent to our current CSRF check, but wouldn’t necessarily be a defense against cross-origin information leakage or be helpful with entry-point investigation.To recap, we added a new cookie __Host-samesite_cookie for browsers that support the cookie. The cookie is SameSite with enforcement mode strict . Its value is derived from csrf_cookie . For the following requests, we check for presence of __Host-samesite_cookie and validate:In case (1), we noticed minimal false alarms, so we switched from warnings to enforcement mode, raising an HTTP 403 in case of a violation.For case (2), we noticed that a few AJAX GET requests, such as the ones used by the Saver are cross-site by design. We whitelisted these few endpoints from the same-site check. Further, we noticed on service worker AJAX GET fetches, __Host-samesite_cookie wasn’t sent. We filed a bug report on Chrome. For those service worker routes, instead of relying on the cookie check, we added an additional header to block cross-site requests. After this, we were confident that we could roll this out in enforcement mode.For case (3), most websites have very few “toplevel” pages like https://www.dropbox.com and https://www.dropbox.com/help , and many pages users navigate to from these base pages, e.g. https://www.dropbox.com/team/admin/members , but do not visit directly. We call these toplevel pages entry points. Enforcing that users can visit non-entry points only by navigating to from entry points can reduce the attack surface of a website.By leveraging same-site checks for all non-AJAX GET routes, we found a few non-entry points, such as:However, we noticed that our website has much fewer non-entry points than we expected. We suspect that modern web applications might not have many non-entry points, but we’d love to hear your thoughts.Same-site cookies are a convenient way to defend against a variety of attacks using cross-origin requests. Because it’s not supported on all browsers, it’s best as a defense in depth measure. We hope that other browsers will implement this feature in the future.Rolling out out same-site cookie defenses on top of existing CSRF defenses should give added security benefits without disrupting availability. For reasons outlined in this post, we recommend adding a new cookie, with SameSite in strict enforcement mode, and controlling the actual same-site enforcement on the server side. This cookie should preferably be a __Host-prefix cookie and its value should preferably be derived from the CSRF token.Dropbox is leveraging the security benefits of same-site cookies made possible by new browsers. Security is core to our company, and we’re excited to add another layer of protection for our users and their data.",https://blogs.dropbox.com/tech/2017/03/preventing-cross-site-attacks-using-same-site-cookies/,0,dropbox,,NULL,2017-03-16
DropboxMacUpdate: Making automatic updates on macOS safer and more reliable,"Keeping users on the latest version of the Dropbox desktop app is critical. It allows our developers to rapidly innovate, showcase new features to our users, maintain compatibility with server endpoints, and mitigate risk of incompatibilities that may creep in with platform/OS changes.Our auto-update system, as originally designed, was written as a feature of the desktop client. Basically, as part of regular file syncing, the server can send down an entry in the metadata that says, “Please update to version X with checksum Y.” The client would then download the file, verify the checksum, open the payload, replace the files on disk, restart the app and boom! It would be running version X. This meant that the client had to be running in order to update itself. More importantly, it also meant that small bugs in other parts of the client could affect auto-update. Eliminating these potential failures was crucial to maintain continuity of Dropbox’s value to its users. So we decided it was time to move our auto-update mechanism out of the main app.Back in 2014, we accomplished this on Windows by taking Google’s Omaha project and adapting it to our needs. Since Omaha is an out-of-process updater, if we shipped a completely broken client we could still update it. This project took a while to finish since Omaha is also an installer/meta-installer and we had to rework several of our installation flows to make it all work well. But we were happy with the end result.Last year, we decided we wanted to do the same for macOS. Usually we like to start projects like this by doing lots of research. Why reinvent the wheel if you don’t have to? Google did have an open source project called UpdateEngine which was essentially “Omaha for Mac,” but the last code drop was back in 2008 and it wouldn’t compile cleanly with modern XCode, so we decided not to use it. Other options we looked at had other difficulties. Some were in-process only, or supported only one app, or only supported Sierra (we support Dropbox on some pretty old versions of OS X) so we couldn’t use them.So we decided to write our own auto-update system. This gave us a lot of flexibility in the feature set, and rather than bolting stuff on after the fact, as we did with Omaha, we could build the exact system we needed. (We also didn’t have to use XML for the API 😃.)We built an “app” called DropboxMacUpdate. Because we needed to support old systems (Mac OS 10.7+) we wrote it in ObjC rather than using Swift. Picking one of Apple’s languages let us leverage many of the OS features without too much trouble. (In comparison, the client is written in Python, and when we need to do some OS-specific thing we have to write lots of bridge code.)Upon installation, DropboxMacUpdate.app will register itself with launchd . This is a well known technique that will allow the app to periodically check for updates. Any Dropbox app can register with DropboxMacUpdate by giving it the path of where it’s installed. Every five hours, DropboxMacUpdate will check its registration database for apps, then check the paths of those apps for the installed version and send them to the server. The server will check if an update is needed and will reply with the version, the URL and the hash of the payload. DropboxMacUpdate then downloads the payload and uses it to update the app. This all happens without the need for any user interaction.At a minimum, the payload is a DMG with an executable file called .dbx_install at the root. Those of you familiar with Google’s UpdateEngine may see some similarities here. The executable is in charge of doing all the work needed to install the new version of the app. In practice, the DMG will also include the .app that needs to be installed; however, this format allows us to (someday) create a DMG that can update the app using diffs, for example. Notice that DropboxMacUpdate doesn’t have to know the details of how to update the app. This means that if your payload won’t install for some reason, it’s not a problem; just have the server give it a different payload next time (one that actually installs), and you’ll be out of your predicament in no time.Shipping updates is no trivial matter. We have to ensure that there’s no way for a user to get a “bad” version of the client. So we employ multiple layers of security checks.But most importantly, being able to deliver secure, reliable, and rapid updates to our users is the biggest security improvement.Because DropboxMacUpdate can pretty much run at any time, it might try to update a running application. For the Dropbox client we wanted to be able to ask the app to quit so the update could be done as soon as possible. So .dbx_install will find the running app and ask it to exit, using a Darwin notification. The client receives the notification, ensures that the app isn’t showing UI, finishes the current sync operations, and exits cleanly. .dbx_install will then swap out the Dropbox.app atomically and restart it. If the client is busy showing UI then we’ll wait for some time before quitting to make sure the user experience is not jarring.Many of our beta users have been running DropboxMacUpdate for the last months and have benefited from an increased speed in how fast they receive the latest version. In fact, we can update about 3000 clients/sec at peak! We’re excited about shipping this to all our macOS users with version 21 of the desktop client. Happy updating!",https://blogs.dropbox.com/tech/2017/03/dropboxmacupdate-making-automatic-updates-on-macos-safer-and-more-reliable/,0,dropbox,"python,html,frontend,css,react",NULL,2017-03-08
Introducing Stormcrow,"A SaaS company like Dropbox needs to update our systems constantly, at all levels of the stack. When it comes time to tune some piece of infrastructure, roll out a new feature, or set up an A/B test, it’s important that we can make changes and have them hit production fast.Making a change to our code and then “simply” pushing it is not an option: completing a push to our web servers can take hours, and shipping a new mobile or desktop platform release takes even longer. In any case, a full code deployment can be dangerous because it could introduce new bugs: what we really want is a way to put some configurable “knobs” into our products, which a) give us the flexibility we need and b) can be safely tweaked in near real-time.To satisfy this need, we built a system called Stormcrow, which allows us to edit and deploy “feature gates.” A feature gate is a configurable code path that calls out to Stormcrow to determine how to proceed. Typical code usage looks like this:Stormcrow feature gatesBuilding a one-size-fits-all feature gating system like this is tricky, because it needs to be expressive enough to handle all the different use-cases we throw at it on a daily basis yet robust enough to handle Dropbox-scale traffic. The rest of this blog post will describe how the system works and some of our lessons from building it.Suppose we wanted to run an A/B test to see what button colors are preferred by German users. And further suppose we already know that English speakers prefer blue buttons. In the Stormcrow UI, we might configure the feature like this:This shows that “German locale users” will be exposed at a rate of 33% RED_BUTTON , 33% BLUE_BUTTON , and 34% CONTROL , and English sessions are set to 100% BLUE_BUTTON. But all other users will receive OFF .Notice that you can use heterogeneous population types in a given feature: the example uses both a “user” population and a “session” population—the former represents logged-in users only, while the latter represents any visit to our site.Stormcrow features are built using a sequence of populations which are matched using a fall-through system: first we try to match population 1, and if we fail we fall through to population 2, and so on. As soon as we match a population, we pick a variant to show the user according to the chosen variant mix for that population.It’s important to note that the variant assignment is stateless. It is randomized by hashing the user’s ID with a seed (the small gray box in the top right). Advanced Stormcrow users can even manipulate the seed to accomplish special behaviors. For example, if you want two different features to assign users the exact same way, you can give them the same seed.To understand how populations work, we need two pieces of vocabulary:Here’s an example of a real datafield, user_email , taken straight from the code:The @dataField decorator specifies that this datafield requires a USER object, and will produce a STRING . It also includes a help string so we can make autogenerated documentation. The actual body of the function simply pulls the user’s email out of the object.Once a datafield is defined, you can use it in a population. Here’s a population which matches users at Gmail and Yahoo domains, except for a couple of excluded addresses, plus tomm@dropbox.com :Datafields are powerful since they can run arbitrary code in order to fetch a value. Dropbox has a lot of them to support all of our targeting use-cases, and new ones are being added all the time by different teams who need new capabilities.Even with the ability to create arbitrary datafields, we face one limitation: we can only gate on information that’s accessible to our server code in some way, i.e., present in an already loaded model or in a database we can query efficiently. But there’s another big source of data at Dropbox: our Hive analytics warehouse. Sometimes a Dropboxer wants to select an arbitrary set of users by writing a HiveQL query, which can draw on all kinds of historical logging and analytics data.Defining a population in this way is an exercise in moving data around. In order for the population definition to be accessible to Stormcrow, we need to move it out of our analytics warehouse and into a scalable datastore that’s query-able from production code. To do this, we built a data pipeline that runs every day and exports the full set of Hive-based populations for that day into production.The main disadvantage of this kind of approach is data lag. Unlike a datafield, which always produces up-to-the-minute data, populations based on Hive exports only update on a daily basis. (And sometimes slower, if anything goes wrong with the pipeline.) While this is unacceptable for some kinds of gating, it works great for feature gates where populations change slowly. For example, a product launch to a predefined set of beta users is a good candidate for a Hive-based population.Hive-based populations represent a fundamental trade-off between expressive power and data freshness: performing feature gating on complex analytics data incurs more lag and data engineering work than gating on commonly accessed data.One of Stormcrow’s most powerful features is its ability to define populations not only in terms of rules or queries like above, but also in terms of other populations and features. We call these derived populations. For example, here’s a population that is only matched when a) we match the “Android devices” population and b) we receive variant OFF for the feature recents_web_comments.This capability solves the problem of complicated rule configurations being copied and pasted again and again throughout the tool. Instead, feature gating at Dropbox aims to build a core set of basic populations, which can be mixed and matched to produce arbitrarily complex targeting. We’ve found in practice that designing derived population hierarchies is very similar to refactoring code.In fact, you can look at derived populations as a way to replace coded “if” statements to choose between experiments. Rather than write logic of the form “if user is in Experiment A show them thing A, otherwise if they’re in not in Experiment A but are in Experiment B show them thing B…” you can express these relationships directly in the Stormcrow UI.Like any complicated software system, Dropbox has a number of internal models used in our code. For example, the user model represents a single user account, and the team model represents a Dropbox Business team. The identity model is how we represent paired accounts: it ties together a personal plus a business user model into a single object. All of our models are connected via various one-to-many and many-to-one relationships.In Dropbox product code, we typically have access to one or more of these models. For developer convenience, it’s nice if Stormcrow understands our model relationships well enough to “infer” extra selectors automatically. For example, a developer may have access to a user object u and want to query some feature which is gated on team information. While they could writeit is much more convenient if Stormcrow can fill in the details automatically, so the developer only needs to writeIn Stormcrow we represent Dropbox’s model relationships as a graph which we call the selector inferring graph. In this graph, each node is a model type, and an edge from node A to node B means that we can infer model B from model A. When a Stormcrow call happens, the first thing we do is take the selectors we were given and compute their transitive closure in this graph.Of course, inferring may introduce a performance cost in the form of extra computation or network calls. To make it more efficient, inferring produces thunks, which are lazily evaluated so that we only compute them if a selector is actually needed to make a gating decision. (See the “Performance dangers” section below for more on the risks of Stormcrow making network calls.)Here’s our actual selector inferring graph. Each node represents a selector type that Stormcrow knows about. For example, we can see that viewer is a very handy model to have, because we can use it to infer session , team , user , and identity . In addition, the special node (none) represents selectors that can be auto-inferred from “thin air”: for example, the session is always auto-inferred in our server code, so there’s no need to pass any selectors to use it.We’ve found selector inferring to be a big win for developer convenience, while at the same time being easy to understand. We also have tooling to check that developers don’t make mistakes with which selectors they pass in; see the “Auditing challenges” section.If you have a large fleet of production servers, how should the feature gating configuration be deployed to them? Keeping feature gating information in a database is the obvious answer, but then you need a network call to retrieve it. Given that there may be a large number of feature gates evaluated on a typical page load on dropbox.com, this can result in a huge numbers of configuration fetches against the database. Even if you mitigate these problems with a carefully designed caching system (using local caching + memcached, for example), the database becomes a single point of failure for the system.Instead, we deploy a JSON file called stormcrow_config.json to all of our production servers. This deployment simply uses our internal push system and is pushed every time a change is made to Stormcrow configuration.All of our servers run a background thread called the “Stormcrow loader” which constantly watches the stormcrow_config.json copy on disk, reloading it when it changes. This allows Stormcrow to reload without interruption to the server.If the configuration file is not found for some reason, Stormcrow has the ability to fall back to direct database access—but this is highly frowned upon for any system that might produce nontrivial amounts of traffic.Feature gating on the desktop and mobile platforms is a little different. For these clients, it makes more sense for them to batch request feature and variant information. When they request Stormcrow information from our backend, they receive information like the following:Clients on both kinds of platforms also pass up one or more special selectors containing platform-specific information. Mobile clients pass up a selector providing information on the app being used and on the device itself, and desktop clients pass up a selector with information on the desktop host. As with other selectors, Stormcrow has datafields that can be used to write rules based on these characteristics.Every Stormcrow feature assignment and exposure is logged to our real-time monitoring system, Vortex. The Stormcrow UI has embedded graphs in it, where users can track the rate of assignments and exposures. For example, the graph below shows three different variants (yellow, blue, and green) and how many users have been exposed to each variant over time. These graphs are also annotated with a vertical line for every time a feature (or a population that the feature depends on) is edited. This allows us to easily see how our edits affect the rates at which different variants are assigned. In this graph, for instance, we can see that the rates of the green and blue variants converged after the first edit (vertical line), and the yellow variant went up. Interestingly, we can also see usage effects not caused by Stormcrow changes, such as the gradual increase of the yellow variant in the middle of the graph.Users can also click the links at the bottom to drill into the data in more detail, using our in-house tool Vortex or other data exploration tools.Because of Stormcrow’s modular datafield design, it’s possible for people at Dropbox to write datafields that are not performant. Often this is done with the best of intentions: someone creates a new datafield which is perfectly safe for their small use-case, but could be used by someone else to send huge amounts of traffic toward a fragile system.This has taught us an important lesson: avoid database calls or other I/O in the feature gating system!Instead, one should pass as much information into the system from the caller as possible. This puts the performance onus on the caller, and makes I/O more predictable: if the caller always does the I/O no matter what, then a Stormcrow edit can’t change the performance characteristics of the code.In an ideal world, Stormcrow would be completely “pure” (in the functional programming sense) and would not perform any I/O at all. We haven’t been able to do this yet for practical reasons: sometimes the necessity of providing a convenient API for the caller means that Stormcrow needs to do some of its own heavy lifting. Sometimes you want to gate on information that lives a database call away, so it makes sense to have the capability to (safely) do this. It helps to have a highly scalable data store like Edgestore around for such tasks.Feature gates are awkward because they aren’t checked into version control: instead, they are a separate piece of state which can change independently of your code. Code pushes at Dropbox happen in a predictable fashion via our “daily push” system (for our backend), or via the release processes for the desktop or mobile platforms. But feature gate edits, by their very nature, can happen at any hour of the day or night. So, it’s important to have good auditing tools so we can track feature-gating related regressions down as fast as possible.Stormcrow tackles this in the following ways: providing full audit history and by tracking features in our codebase with static analysis.Audit history is simple enough: we just show a “news feed” style view of all edits to a given feature and population. This feed shows all edits that could affect the given item, including edits to transitive dependencies (which can arise through derived populations).Static analysis of our codebase is a little more interesting. For this, we run a special service called the Stormcrow Static Analyzer. The static analyzer knows how to clone our code and scan over it, searching for Stormcrow feature usages. For a given feature, it produces two outputs:For example, here’s what the static analyzer has to say about a feature called can_see_weathervane :The static analyzer also performs the important task of making sure the most common variant found in our production code matches up with what our unit tests are testing. It knows how to send “nag” emails to feature owners about this and other issues, such as stale features that aren’t used anymore and should be removed from the codebase.These tools make it straightforward to track down how a given feature affects our code.For manual testing of our features, Stormcrow supports “overrides.” Overrides allow Dropboxers to temporarily put themselves into any feature or population. We also have a notion of “datafield overrides,” which allow you to change a single datafield value. For example, you can force your locale to be German in order to test the German experience.For unit tests, we run a mock Stormcrow where every feature is given a “default” variant to use in tests. Stormcrow variants can also be overridden by any test. We even have special decorators to say “make sure this test passes under all possible variants.”Providing a unified feature gating service at Dropbox’s scale involves lots of considerations, from infrastructure issues like data fetching and configuration management all the way up to UI and tooling. We hope this post is useful to people working on their own feature gating systems.Does your company’s handle feature gating differently? Please let us know in the comments!Thanks to the following people for help on this post: Mor Katz, Christopher Park, Lee Sheng, Kevin Zhou, Taylor McIntyre.P.S. Why is this system called Stormcrow? Because this system replaced our previous feature-gating system, which was called Gandalf (“You shall not pass!”). The Lord of the Rings fans will recognize “Stormcrow” as one of Gandalf’s many names. Plus we had a bird thing going on for internal project names at the time.",https://blogs.dropbox.com/tech/2017/03/introducing-stormcrow/,0,dropbox,"react,frontend,css,javascript",NULL,2017-03-06
Meet Securitybot: Open Sourcing Automated Security at Scale,"Security incidents happen. And when they do, they need to be dealt with—quickly. That’s where detection comes into play. The faster incidents are detected, the faster they can be handed off to the security team and resolved. To make detection as fast as possible, teams are usually aided by monitoring infrastructure that fires off an alert any time something even slightly questionable occurs. These alerts can lead to a deluge of information, making it difficult for engineers to sift through. Even worse, a large number of these alerts are false positives, caused by engineers arbitrarily running sudo -i or nmap.Ignoring some of these alerts is tempting. After all, for every alert that involves a person, a member of the security team needs to manually reach out to them. More alerts means more work: we all know that Chris runs nmap about six times a day, and the SREs need to run sudo fairly often. So we can just ignore those alerts, right? Wrong. This sets a dangerous precedent that never ends well. There’s a clear need for a system that can reduce the burden of alerts for the security team.A year ago, Slack set out to tackle this very issue. Instead of manually reaching out to employees to verify their actions, they built an automated system designed to reach out and send aggregate results back to the security team. We were inspired: what if our team at Dropbox created an automated, distributed alerting bot of our own. Could we reduce the burden of alerts for our security team, and help them sort through alerts faster than ever before? To answer that question, we developed and deployed Securitybot, and found out that yes, we could.But we didn’t want to stop there. As a founding member of the TODO Group (short for Talk Openly, Develop Openly), we are committed to sharing our knowledge with the greater tech community through support of open source projects. So, today we are also open sourcing our implementation in the hopes that other companies can benefit from what we’ve built.One of the hardest, most time-consuming parts of security monitoring is manually reaching out to employees to confirm their actions. Despite already spending a significant amount of time on reach-outs, there were still alerts that we didn’t have time to follow up on. We wanted to implement a system that would reach more users while allowing us to spend more time on other things, like building better detection tools and proactively hunting for bad actors.Securitybot now finds its place in our alert detection chain. Soon after an alert is fired, an employee receives a message asking them to confirm whether they performed a potentially malicious action. Their response is then stored and later sent to the security team. Alert rollups are later augmented with employees’ responses to the bot. In the event where an employee reports that they did not perform an action, the security team is alerted immediately. This is meant to keep most alerting in the background but to surface the alerts that truly require prompt attention and follow-up. Rather than spending their time repeatedly reaching out, our security engineers now have more time to work on foundational projects that improve our overall security posture.When designing Securitybot, we wanted to hit on all the key points from Slack’s post. And the core ideas are retained: Securitybot is tied into our detection and alerting system and our company-wide Slack instance. Upon getting an alert, the bot contacts whoever triggered the alert and logs the response for the security team. However, we also wanted to extend the design to make it more useful to Dropbox and ideally the community at large. The goal was to make our implementation modular and reusable. For instance, if we shift chat platforms or monitoring systems, we wanted to be able to do so without rewriting the core code. Securitybot was designed around a set of core functions that reach out to monitoring and communication systems via a set of simple, composable plugins.Securitybot moves between grabbing new alerts from our monitoring tools and communicating with employees. Whenever a new alert is encountered, it’s logged and a message is queued for whomever triggered it. Regular polling ensures that we get alerts promptly and can deal with them as soon as possible. Later, when responses are collected, they’re brought back into our monitoring system to be available alongside the rest of our alerts.Securitybot ensures that user interaction is prompt and streamlined. For each alert, we simply ask an employee whether they triggered it and for a brief explanation. These are then aggregated back into our monitoring infra so that when we review hourly or daily aggregations all of the responses are right there for review. Responses are secured via 2FA, so even if an attacker managed to compromise Slack as well, they couldn’t fool the bot.Finally, we’ve added a bit of user friendliness. Rather than bombard employees with messages, we let most alerts “snooze.” If we ping you for using sudo , there’s a good chance you may be using it again in the future. So, we don’t bother you for some period of time, because we can be pretty sure three sudos in a row, in the same context, are all you.First and foremost, Securitybot helps the security team sort through alerts faster than ever before. False positives are resolved without needing to reach out to employees, and possible incidents are immediately escalated.Securitybot not only helps the security team, but all Dropbox employees. Responding to a polite chat bot is much easier than responding, in full sentences at that, to a member of the security team. It not only saves our security engineers time but also all of our employees. (After all, it’s not just production engineers — with the bot we can alert on anomalous events within employees’ e-mail and Dropbox accounts as well unusual activities on their laptops.)We understand the annoyance of having to respond to a nagging security team, and having an unfeeling bot that doesn’t understand “I’m busy, I’ll get to it later” doesn’t make things much better. So, we devoted some time to workshop the interaction between bot and user to ensure that it would be sufficiently pleasant to deal with. We wanted to make our bot polite and cordial rather than blunt and robotic. It turned out that giving a bit of personality to our interactions moved the bot from “annoying” to “adorable.”Finally, we’re excited to share that we’ve open sourced Securitybot. As far as we’re aware, this will be the only open source project to automatically confirm and aggregate suspicious behavior with employees on a distributed scale. We hope that by putting forward an initial open source implementation, we can help others to improve their internal detection and easily get their distributed security up and running. We also hope the security community will share and improve the code. And while we use Securitybot for internal monitoring, the same system could conceivably be used for external, user-facing detection. Lastly, this can hopefully give other teams a starting point for creating their own systems.",https://blogs.dropbox.com/tech/2017/02/meet-securitybot-open-sourcing-automated-security-at-scale/,0,dropbox,"backend,frontend,tensorflow,python,cloud,machinelearning,docker,animation",NULL,2017-02-22
Annotations on Document Previews,"Location-specific feedback has always been fundamental to collaboration. At Dropbox, we’ve recognized this need and implemented annotations on document previews. Our goal was to allow users to provide focused and clear feedback by drawing rectangles and highlighting text on their documents. We ran into a few main challenges along the way: How do we ensure annotations can be drawn and rendered accurately on any kind of document, with any viewport size, and using any platform? How can we maintain isolation of user documents for security? How can we keep performance smooth and snappy? Below, I’m going to answer these questions and dive a bit deeper into how annotations work at Dropbox.Before jumping into the annotations library, let’s take a look at our existing file preview architecture. On the web, files are previewed by our FileViewer module, which behaves differently depending on file type. Images and text files are relatively simple, and can be inserted directly into the DOM. Previewing more complicated files (e.g. PDFs, Microsoft Office files, and Adobe Illustrator files), requires first generating a PDF preview on the back-end and then displaying that preview in an iframe within the FileViewer.User documents are incredibly variable and could potentially contain malicious content. Therefore, complicated filetypes with generated previews are shown in an iframe. Since the iframe’s source comes from a different domain, its context doesn’t have direct access to the main site’s DOM, CSS styles, JavaScript functions, cookies, or local storage. Thus, the user-generated content is effectively isolated. Within that iframe, Dropbox uses PDF.js to display the generated previews. To maintain isolation, PDF.js knows nothing about the user and has the sole purpose of rendering a PDF at a given URL. Using PDF.js in an iframe has some additional benefits besides increased security: it keeps the code simple and allows us to benefit from an existing technology with a large user base and continuous upgrades.While this structure worked very well for vanilla read-only document previews, it provided some substantial challenges when it came time to enhance previews with inline annotations. We now needed to do more than simply view a document, so we had to establish communication between PDF.js and our FileViewer. This communication happens with FrameMessenger, a Dropbox proprietary message-passing module which sends information in JSON. Although annotations must work for arbitrary file types and platforms, the following discussion will use PDF previews on the web as an illustrative example.At Dropbox we use the React JavaScript library for our front end. Annotations have two main React components, which can easily be reused on arbitrary document types: the inline markup itself (the Annotation) and the corresponding comment bubble (the AnnotationBubble ). The Annotation is a yellow overlay positioned within the document itself that refers to part of its content. Currently, this could be a text highlight or a rectangle. In the future we may add other types, such as freehand shapes or pointers. The Annotation is placed and sized based on user mouse events and must react smoothly while being created. Annotations also must move with the document when it’s scrolled or resized.The second component is the corresponding AnnotationBubble, which has comment text contained in a popup “bubble” which floats near the Annotation. The AnnotationBubble will contain the original comment, along with any replies and a list of users relevant to the conversation. When a user @mentions someone, our CommentComposer React component brings the feedback directly to the attention of a recipient via an email and popup notification. This AnnotationBubble must be visually attached to its Annotation, but must also connect with other Dropbox components, such as the User object, the contact list popup, and the comments list side panel.One of the most challenging aspects of designing the architecture for annotations was deciding how to bridge the divide between the disjointed ecosystems of the Dropbox FileViewer and the PDF.js iframe. Where should we get mouse events from? Where should we draw the Annotation and AnnotationBubble? Ideally, the Annotation and AnnotationBubble should move smoothly while the mouse interacts with them or the document scrolls or resizes. Also, AnnotationBubble should be able to float over the edge of the document, but the Annotation should be clipped at the edge. For implementation simplicity, we wanted to limit modifications to the third-party PDF.js and do most of our development in Dropbox’s FileViewer. Finally, we needed to be careful about what data we’re sending to and from the iframe. If we relied on too much data flow, performance could be adversely affected. More importantly, we didn’t want to compromise the security provided by the iframe’s encapsulation by sending sensitive user data across to the document.One option would have been to customize PDF.js and implement everything within the iframe. The annotation components could be, in every sense, “inside” the document. This means that resizes and scrolls could immediately and seamlessly update the Annotation’s position, no calculations needed. Also, the Annotation would never overflow the bounds of the document, since it would be automatically clipped by the iframe. Although this has huge performance and simplicity benefits, it also has some serious drawbacks:The opposite approach would have been to implement everything in FileViewer, in an overlay on “top” of the iframe. Advantages would include aligning the development process more with the rest of the Dropbox website and allowing for easier code reuse between other Dropbox systems and between document types. Also, information passing between the AnnotationBubble and FileViewer would be trivial and would have no security implications. However, with this approach it becomes very hard to make the Annotation look like a part of the document. Instead of having to transmit bulky user information in JSON via the FrameMessenger as before, we’d have to send streams of fast-moving mouse, scroll, and resize events. The time required for this cross-document communication, along with translation between coordinate systems and manual repaints of the Annotation would cause the Annotation to perceptibly lag behind a user’s mouse or the document’s scroll. Annotations could also flow outside the document’s edges, and the illusion that the Annotation was attached to the document would be impossible to maintain.We found that a compromise between these two options was the best solution, both for code quality and performance. The code for the Annotation is integrated into PDF.js so that relevant mouse events are captured and used right away. Since the Annotation is inside the iframe and attached to the document as a child div, it moves smoothly along with the document when it’s scrolled or resized. The Annotation is also automatically clipped when it overflows the iframe. The AnnotationBubble, however, is in the parent FileViewer, and benefits greatly from direct access to other Dropbox components and data. It also can easily overflow the iframe window, allowing for a better use of viewport space. However, since its position needs to follow the Annotation in the iframe, any movements of the Annotation are sent up through the FrameMessenger and then translated to the viewport’s coordinates. This does introduce a delay in the AnnotationBubble’s movements, which we mitigate by hiding it when its Annotation is moving. There is also some necessary algorithmic complexity involved in translating positions between the iframe and FileViewer, which we describe in the appendix at the bottom of the post. (In fact, every different type of preview has its own interface for accepting and translating movement events sent from the Annotation to the AnnotationBubble.)This table summarizes the three options above:The following example shows how we isolate the preview and how we deal with communication across the iframe. In this scenario, the user has decided to place an annotation on a PDF and has already begun drawing a rectangle by clicking and dragging her mouse across a part of the screen. Now, the user releases the mouse, starting a flurry of events, summarized in the diagram below the animation.(Click to zoom into the diagram)The following is a simplified version of the coffeescript code in AnnotationRegion’s onMouseUp callback (the code path specific to this example is bold):This is an example of the actual JSON that gets sent across the iframe boundary:The information cascade in the above example was started by a single user mouse action, and multitudes of other events are fired continuously as the user interacts with the preview. Events also go in the reverse direction, as FileViewer needs to inform the iframe of higher-level actions such as the user turning commenting on or off. To make the annotations system react smoothly and sensibly to all of this input, we needed to bridge the gap between our intentionally isolated document preview and the broader Dropbox environment. As explained above, we kept the purely visual Annotation simple and attached it directly to the document to maximize its performance. The information-heavy AnnotationBubble was kept outside and a flexible interface was made to connect them. This separation of components and use of interfaces made it easy to gracefully extend this implementation for image files, and will make annotations possible on many more file types in the future.Try out annotations on a sample file today!For PDFs, the decision outlined above to split annotations between the iframe and FileViewer meant that coordinates would have to be translated between two different systems: PDF points and viewport pixels.On PDFs, positions are expressed in relation to a physical printed document. Each position is measured from the bottom left corner of a page and expressed in “points” (one of which equals 1/72 of an inch on a printed page). Conversely, positions in the viewport are measured from the top left of the viewer’s viewport and expressed in pixels. When translating from PDF points in the iframe to pixels in the viewport, the current page and scroll position of the document both need to be taken into account to calculate an offset. Also, the vertical component of the point needs to be reversed. Finally, the zoom level of the document is used to determine the multiplier required to complete the translation to viewport pixels.All of this translation is required every time the Annotation moves, whether the movement is caused by the user drawing the Annotation, scrolling/resizing the document, etc. This position information is sent as a stream of information from the iframe to the FileViewer. Information is also passed in the other direction, from the FileViewer to the iframe. For example, a message is passed down when a user changes the visibility of all comments on a document, or when the user interacts with the AnnotationBubble to post or delete a specific comment. Fortunately, all these simple messages are fast to transmit, resulting in no performance issues.",https://blogs.dropbox.com/tech/2016/11/annotations-on-document-previews/,0,dropbox,,NULL,2016-11-30
Infrastructure Update: Pushing the edges of our global performance,"Dropbox has hundreds of millions of registered users, and we’re always hard at work to ensure our customers have a speedy, reliable experience, wherever they are. Today, I am excited to announce an expansion to our global infrastructure that will deliver faster transfer speeds and improved performance for our customers around the world.To give all of our users fast, reliable network performance, we’ve launched new Points of Presence (PoPs) across Europe, Asia, and parts of the US. We’ve coupled these PoPs with an open-peering policy, and as a result have seen consistent speed improvements. In fact, we’ve already doubled the transfer speeds for some Dropbox clients around the world.To upload a file to Dropbox, a user’s client needs to establish a secure connection with our servers. Before we launched these PoPs, a user that lives across the Pacific Ocean could expect this process to take as much as 450 milliseconds—half a second gone, and the client hasn’t even begun sending data.It can take up to 180 milliseconds for data traveling by undersea cables at nearly the speed of light to cross the Pacific Ocean. Data traveling across the Atlantic can take up to 90 milliseconds. This travel time is compounded by the way TCP works. To establish a reliable connection for uploads, the client initiates what’s called a slow start. It sends a few packets of data, then waits for an ACK (or acknowledgement), confirming that the data has been received. The client will then send a larger group of packets and await confirmation, repeating this process until ultimately transmitting data at the user’s full available link capacity. Given the limitations we encounter here—the distance across the Pacific Ocean, and the speed of light—there are only so many optimizations we can make before physics stands in the way.So we’ve established proxy servers at the network edge, giving us accelerators in California, Texas, Virginia, New York, Washington, the UK, the Netherlands, Germany, Japan, Singapore and Hong Kong. A user’s client connects to these edge proxies, completing the initial TLS and TCP handshakes quickly—the proxies have enough buffer space to let the client get through the slow start without having to wait for an ACK from our data centers. This gets that connection between clients and our data centers (via those edge proxy servers) started quickly.We also wanted to minimize the average Round Trip Time (RTT) per HTTPS request to our data centers. To do this, we connected our PoP to our data centers via our private Backbone links for increased stability and control, and also configured our proxies to keep the connections to our data centers alive. This helps us avoid the latency cost of opening a new connection when you want your data synced and start the transfer immediately.At the same time, we are committed to ensuring your data remains secure. We use TLS 1.2 and a PFS cipher suite at both our origin data centers and proxies. Additionally, we’ve enabled upstream certificate validation and certificate pinning on our proxy servers. This helps ensure that the edge proxy server knows it’s talking to our upstream server, and not someone attempting a man-in-the-middle attack.We’ve tested and applied this configuration in various markets in Europe and Asia. In France, for example, median download speeds are 40% faster after introducing proxy servers, while median upload speeds are approximately 90% faster. In Japan, median download speeds have doubled, while median upload speeds are three times as fast.As part of this expansion we also offer an open-peering policy, free of charge. Our open-peering agreements help us provide faster connections and improved network performance, to better serve local populations. With open-peering, major ISPs can route Dropbox traffic directly to and through our networks, improving transfer speeds for our users, and reducing bandwidth costs for Dropbox and our ISP partners. More than 400 ISPs are participating in the program globally, including BT in England, Hetzner Online in Germany and Vocus Communications in New Zealand, through their Orcon, Slingshot and Flip brands. Open-peering helps large companies like Google and LinkedIn manage their networks, and it’s helping Dropbox improve network performance too.Earlier this year we introduced Magic Pocket, our in-house multi-exabyte storage system. We’re now storing over 90% of our users’ data on this custom-built architecture, which allows us to customize the entire stack end-to-end and improve performance. We plan to continue this expansion in new regions over the next six to twelve months, and will continue to make infrastructure investments as the needs of our customers evolve and change. This expansion we’re announcing today is another part of that ongoing investment in our infrastructure, as we strive to offer the best possible experience for all of our users.",https://blogs.dropbox.com/tech/2016/11/infrastructure-update-pushing-the-edges-of-our-global-performance/,0,dropbox,"backend,cloud,docker,python",NULL,2016-11-16
Improving the Responsiveness of the Document Detector,"In our previous blog posts (Part 1, Part 2), we presented an overview of various parts of Dropbox’s document scanner, which helps users digitize their physical documents by automatically detecting them from photos and enhancing them. In this post, we will delve into the problem of maintaining a real-time frame rate in the document scanner even in the presence of camera movement, and share some lessons learned.Dropbox’s document scanner shows an overlay of the detected document over the incoming image stream from the camera. In some sense, this is a rudimentary form of augmented reality. Of course, this isn’t revolutionary; many apps have the same form of visualization. For instance, many camera apps will show a bounding box around detected faces; other apps show the world through a color filter, a virtual picture frame, geometric distortions, and so on.One constraint is that the necessary processing (e.g., detecting documents, detecting and recognizing faces, localizing and classifying objects, and so on) does not happen instantaneously. In fact, the fancier one’s algorithm is, the more computations it needs and the slower it gets. On the other hand, the camera pumps out images at 30 frames per second (fps) continuously, and it can be difficult to keep up. Exacerbating this is the fact that not everyone is sporting the latest, shiniest flagship device; algorithms that run briskly on the new iPhone 7 will be sluggish on an iPhone 5.We ran into this very issue ourselves: the document detection algorithm described in our earlier blog post could run in real-time on the more recent iPhones, but struggled on older devices, even after leveraging vectorization (performing many operations simultaneously using specialized hardware instructions) and GPGPU (offloading some computations to the graphics processor available on phones). In the remaining sections, we discuss various approaches for reconciling the slowness of algorithms with the frame rate of the incoming images.Let’s assume from here on that our document detection algorithm requires 100ms per frame on a particular device, and the camera yields an image every 33 ms (i.e., 30 fps). One straightforward approach is to run the algorithm on a “best effort” basis while displaying all the images, as shown in the diagram below.The diagram shows the relative timings of various events associated with a particular image from the camera, corresponding to the “Capture Event” marked in gray. As you can see, the image is displayed for 33 ms (“Image Display”) until the next image arrives. Once the document boundary quadrilateral is detected (“Quad Detection”), which happens 100 ms after the image is received, the detected quad is displayed for the next 100 ms (“Quad Display”) until the next quad is available. Note that in the time the detection algorithm is running, two more images are going to be captured and displayed to the user, but their quads are never computed, since the quad-computing thread is busy.The major benefit of this approach is that the camera itself runs at its native speed—with no external latency and at 30 fps. Unfortunately, the quad on the screen only updates at 10 fps, and even worse, is offset from the image from which it is computed! That is, by the time the relevant quad has been computed, the corresponding image is no longer on screen. This results in laggy, choppy quads on screen, even though the images themselves are buttery smooth, as shown in the animated GIF below.Another approach is to serialize the processing and to skip displaying images altogether when we are backed up, as shown in the next diagram. Once the camera captures an image and sends it to our app (“Capture Event”), we can run the algorithm (“Quad Detection”), and when the result is ready, display it on the screen (“Quad Display”) along with the source image (“Image Display”). While the algorithm is busy, additional images that arrive from the camera are dropped.In contrast to the first approach, the major benefit here is that the quad will always be synced to the imagery being displayed on the screen, as shown in the first animated GIF below.Unfortunately, the camera now runs at reduced frame rate (10 fps). What’s more disruptive, however, is the large latency (100 ms) between the physical reality and the viewfinder. This is not visible in the GIF alone, but to a user who is looking at both the screen and the physical document, this temporal misalignment will be jarring and is a well-known issue for VR headsets.The two approaches described thus far have complementary strengths and weaknesses: it seems like you can either get smooth images OR correct quads, but not both. Is that true, though? Perhaps we can get the best of both worlds?A good rule of thumb in performance is to not do the same thing twice, and this adage applies aptly in video processing. In most cases, camera frames that are adjacent temporally will contain very similar data, and this prior can be exploited as follows:While this is a promising simplification that turns our original detection problem into a tracking problem, robustly computing the transformation between two images is a nontrivial and slow exercise on its own. We experimented with various approaches (brute-forcing, keypoint-based alignment with RANSAC, digest-based alignment), but did not find a satisfactory solution that was fast enough.In fact, there is an even stronger prior than what we claimed above; the two images we are analyzing are not just any two images! Each of these images, by stipulation, contains a quad, and we already have the quad for the first image. Therefore, it suffices to figure out where in the second image this particular quad ends up. More formally, we try to find the transform of this quad such that the edge response of the hypothetical new quad, defined to be the line integral of the gradient of the image measured perpendicular to the perimeter of the quad, is maximized. This measure optimizes for strong edges across the boundaries of the document.See the appendix below for a discussion on how to solve this efficiently.Theoretically, we could now run detection only once and then track from there on out. However, this would cause any error in the tracking algorithm to accumulate over time. So instead, we continue to run the quad detector as before, in a loop—it will now take slightly over 100 ms, given the extra compute we are performing—to provide the latest accurate estimate of the quad, but also perform quad tracking at the same time. The image is held until this (quick) tracking process is done, and is displayed along with the quad on the screen. Refer to the diagram below for details.In summary, this hybrid processing mode combines the best of both asynchronous and synchronous modes, yielding a smooth viewfinder with quads that are synced to the viewfinder, at the cost of a little bit of latency. The table below compares the three methods:The GIF below compares the hybrid processing (in blue) and the asynchronous processing (in green) on an iPhone 5. Notice how the quad from the hybrid processing is both correct and fast.In practice, we observed that the most common camera motions in the viewfinder are panning (movement parallel to the document surface), zooming (movement perpendicular to the document surface), and rolling (rotating on a plane parallel to the document surface.) We rely on the onboard gyroscope to compute the roll of the camera between consecutive frames, which can then be factored out, so the problem is reduced to that of finding a scaled and translated version of a particular quadrilateral.In order to localize the quadrilateral in the current frame, we need to evaluate the aforementioned objective function on each hypothesis. This involves computing a line integral along the perimeter, which can be quite expensive! However, as shown in the figure below, the edges in all hypotheses can have only one of four possible slopes, defined by the four edges of the previous quad.Exploiting this pattern, we precompute a sheared running sum across the entire image, for each of the four slopes. The diagram below shows two of the running sum tables, with each color indicating the set of pixel locations that are summed together. (Recall that we sum the gradient perpendicular to the edge, not the pixel values.)Once we have the four tables, the line integral along the perimeter of any hypothesis can be computed in O(1): for each edge, look up the running sums at the endpoints in the corresponding table, and calculate the difference in order to get the line integral over the edge, and then sum up the differences for four edges to yield the desired response. In this manner, we can evaluate the corresponding hypotheses for all possible translations and a discretized set of scales, and identify the one with the highest response. (This idea is similar to the integral images used in the Viola-Jones face detector.)Try out the Dropbox doc scanner today, and stay tuned for our next blog post.",https://blogs.dropbox.com/tech/2016/10/improving-the-responsiveness-of-the-document-detector/,0,dropbox,"java,python",NULL,2016-10-19
NetFlash: Tracking Dropbox network traffic in real-time with Elasticsearch,"Large-scale networks are complex, dynamic systems with many parts, managed by many different teams. Each team has tools they use to monitor their part of the system, but they measure very different things. Before we built our own infrastructure, Magic Pocket, we didn’t have a global view of our production network, and we didn’t have a way to look at the interactions between different parts in real time. Most of the logs from our production network have semi-structured or unstructured data formats, which makes it very difficult to track a large amount of log data in real-time. Relational database models do not support these logs very well, and while NoSQL solutions such as HBase or Hive can store large amounts of logs easily, they aren’t readily stored in a form that can be indexed in real-time.The real-time view was particularly critical when we moved 500 petabytes of data—in network terms more than 4 exabits of traffic—into our exabyte-scale Magic Pocket infrastructure in less than six months. This aggressive goal required our network infrastructure to support high traffic volume over a long period of time without failure. Knowing the traffic profile on the production backbone between AWS and our datacenter helped us detect anomalies. Mapping NetFlow data to our production infrastructure made it much faster to recognize the root source of problems. We developed the NetFlash system to answer the scale and real-time challenges.NetFlash collects large volumes of NetFlow data from our network that we enhance with information specific to our infrastructure. We then use Elasticsearch (ES) to query these enhanced logs, and Kibana as a web UI for creating dashboards to monitor the queries in real-time.Here’s an example so you can see the insights we are able to surface with this system. In the image below, you can see that the network traffic in several clusters momentarily drops without warning at nine in the morning:In the next image, we see that outbound traffic to AWS also took a dive:Finally, in the third image, we’ve drilled down to the individual team that made the change that affected network performance:Before NetFlash, on-call engineers would have a much harder time diagnosing issues as they were happening. In our previous system, it could take many hours to query the network data from an event like the one we’ve just seen. Now, thanks to this chart, it’s just a matter of sending a quick message directly to the right team, and getting things back on track.Simple, right? Actually there was quite a bit we had to do to make this all work in production.First, a few definitions so you understand the different pieces of the system and how they are affected by the massive scale of our infrastructure.NetFlow is an industry-standard datagram format proposed by Cisco. A NetFlow datagram contains information like source IP addresses, destination IP addresses, the source and destination ports, IP protocol, and the next hop IP address. In principle, if we gather NetFlow logs from all of our routers, we have a concise view of where our network traffic data is coming from, where it’s going, and any hurdles it encounters along the way.Elasticsearch is an open source distributed search service. It is the most popular enterprise search engine that powers well-known services from Microsoft Azure Search to the full archive of The New York Times. In large deployments like ours, ES is running on multiple clusters to enable real-time querying of large amounts of data.Kibana is an open source project that enables data visualizations (like the samples above) from the content indexed by an Elasticsearch cluster.We found this combination would allow us to monitor our network data in real-time at scale. Dropbox generates roughly 260 Billion NetFlow datagram records every day. That’s terabytes of aggregated data about our data flows. In our first implementation of NetFlow collection, we stored the logs to our data infrastructure in Hive/Hadoop clusters, and analyzed them with HiveSQL queries. We still use this data pipeline for permanent storage, but it’s slow. New data isn’t available to query for somewhere between two and twelve hours after it’s collected, due to the nature of our data ingestion pipeline. That’s fine for long-term or historical data analysis, but makes real time monitoring impossible.To enable real-time queries, we built a solution in parallel with our existing data pipeline. Before we dive into the details, here’s a look at the architecture below:Dropbox collects NetFlow datagram records every day, from production backbone, edge, and data center routers. Each of these routers sends NetFlow datagrams to two different collectors which are distributed geographically. The copy serves as a backup to the original packet, just in case a collector fails.In the NetFlash system, the collectors now send the processed datagrams to two data pipelines: the HDFS/Hive clusters for permanent data storage, and the new Kafka/Elasticsearch cluster that gives us a near real-time view.Let’s drill down a bit further into the details of our data pipeline:We worked on three key components: the collector, the search backend, and the Kafka pipeline.There are about six collectors at each data center, and each collector receives billions of log entries a day. So we optimized the performance of our collectors in a few ways. We chose high-performance servers with lots of RAM. The collector code is written in Go and designed to make use of multi-core processors. And we found that increasing the I/O buffer size is really helpful for reducing packet loss at the collectors.The raw datagrams themselves aren’t all that useful unless you can associate the IP addresses with meaningful information. The IP is the key to these other attributes, but with so many queries per second it is impossible to query the production database each time. So we built a local DB cache, where the IP is linked to machine, router, load balancer, and GeoIP attributes. The processors inside a collector annotate each raw diagram with a lookup to this local cache. After annotation, each data record contains server names, project names, server owners, roles, locations and more. Those attributes will be the index tokens on the search backend. We send these enriched logs to both the Hive and Elasticsearch pipelines.As we tested the system, we observed that the ES cluster reduced its throughput when it was unhealthy, so we added a Kafka cluster as a buffer between the collectors and ES cluster to improve fault tolerance. Adding Kafka as an intermediate pipeline increased end to end latency but not by a perceptible amount. (Normally, the latency is on the order of <1 sec.) In return, Kafka’s durability guarantees that undelivered messages are kept for a certain period. This means that a recovered ES cluster can consume all unprocessed messages from Kafka.Once we added the Kafka cluster we needed a way to transfer the Kafka data to Elasticsearch. LogHub has the advantage of being a general solution that contains a Kafka consumer, an ES encoder and an ES connection pool. This gives us a lot of flexibility to adapt new Kafka topics into the Elasticsearch index.On the backend, our best solution for real-time document search is Elasticsearch(ES), but deploying it at scale proved to be the challenge. At first, we set up one master node and three data node clusters for prototyping. It crashed almost immediately under our data load!In order to clear this hurdle we’ve made many improvements:We’re very happy with the current production setup for ES. The failure of data nodes are now invisible to internal users, and this configuration requires no extra effort or maintenance from us.The ES cluster was a bit fragile at first, so we deployed a few tricks to improve stability. We observed that ES masters could be too busy to re-allocate indexing shards among data nodes, causing some data nodes to lose their connections from the master. The masters would then attempt to re-allocate the lost shards from lost nodes to the rest of their live nodes, a cycle that would repeat until the entire cluster crashed.To solve this, we deployed more than twice as many data nodes as shards. So for 10 shards of indexed data, we’d allocate 22 nodes—two copies of each shard, and then two free nodes. If there are any node failures, there will always be an available node with a copy of the shard from the failed node that can replace it seamlessly. At the point of failure, the duplicate node replaces the failed node, the shard from this node is copied to one of the free nodes as a new backup, and then the failed node is released from the cluster. None of the other data nodes are affected by the failure. We upgraded ES to 2.0.0, and plan to use HDFS as an index backup in case the cluster enters a turbulent state. This will also give users the ability to search time period data without that eight day limit.Our small team wouldn’t have been able to complete this project without working together closely with the people using the system. Kibana’s query format is not the SQL-like format that network engineers are used to working with. Our partners in neteng facilitated adoption of the tool by saving the most useful queries, graphs, and dashboards for their teammates. To make the data relevant to the network engineers, they also re-indexed tokens so that the frequency of tokens is calculated as a bps (bit per second) rate for every 5 min., to match network terminologies.Thanks to these efforts, we can offer instantaneous access to our network data, complete with informative dashboards that show the traffic matrix at the cluster and metropolitan levels. The graphs at the beginning of this post are examples of how we monitor backbone flows for our block storage system, one of our largest traffic producers.We also leverage our NetFlow data stream to help us make intelligent business decisions. By watching the flow of data on our networks across the globe, our physical infrastructure team can more easily understand where to deploy new hardware where it’ll have the most impact. We can also see the markets that are potentially underserved, and begin planning expansions in ways that are efficient, and cost effective.Without our NetFlash pipeline, gathering this data takes a lot of time and effort. Now, this kind of intelligence is more readily available to decision makers at Dropbox. This means that capacity planning is always informed with a bird’s eye view of our network landscape, helping us deploy our resources effectively. The real-time aspect of our implementation isn’t required here, but the speed helps our production engineers and capacity planners get immediate answers to many of their questions. Knowing where our traffic spikes and when is also helpful for selecting other networks to establish peer relationships with.The monumental task of data migration is behind us, but NetFlash continues to help us monitor high-volume projects. What’s exciting now is to think about what else we can do with this pipeline in the future.Solving the scaling problems was the hard part—building new applications is easy. And one of the advantages of building tools at scale is the ability to adapt those tools for other purposes. We’re now using NetFlash to monitor ongoing operations, including marketing analytic logs, smart sensor data for analysis, and production operation logs. We can now adapt what we’ve built to collect, enrich, search, monitor, and analyze any kind of log data on our systems in real-time.",https://blogs.dropbox.com/tech/2016/10/netflash-tracking-dropbox-network-traffic-in-real-time-with-elasticsearch/,0,dropbox,"database,frontend,mongodb,python,react,webpack,docker",NULL,2016-10-06
How Dropbox securely stores your passwords,"It’s universally acknowledged that it’s a bad idea to store plain-text passwords. If a database containing plain-text passwords is compromised, user accounts are in immediate danger. For this reason, as early as 1976, the industry standardized on storing passwords using secure, one-way hashing mechanisms (starting with Unix Crypt). Unfortunately, while this prevents the direct reading of passwords in case of a compromise, all hashing mechanisms necessarily allow attackers to brute force the hash offline, by going through lists of possible passwords, hashing them, and comparing the result. In this context, secure hashing functions like SHA have a critical flaw for password hashing: they are designed to be fast. A modern commodity CPU can generate millions of SHA256 hashes per second. Specialized GPU clusters allow for calculating hashes at a rate of billions per second.Over the years, we’ve quietly upgraded our password hashing approach multiple times in an ongoing effort to stay ahead of the bad guys. In this post, we want to share more details of our current password storage mechanism and our reasoning behind it. Our password storage scheme relies on three different layers of cryptographic protections, as the figure below illustrates. For ease of elucidation, in the figure and below we omit any mention of binary encoding (base64).Multiple layers of protection for passwordsWe rely on bcrypt as our core hashing algorithm with a per-user salt and an encryption key (or global pepper), stored separately. Our approach differs from basic bcrypt in a few significant ways.First, the plaintext password is transformed into a hash value using SHA512. This addresses two particular issues with bcrypt. Some implementations of bcrypt truncate the input to 72 bytes, which reduces the entropy of the passwords. Other implementations don’t truncate the input and are therefore vulnerable to DoS attacks because they allow the input of arbitrarily long passwords. By applying SHA, we can quickly convert really long passwords into a fixed length 512 bit value, solving both problems.Next, this SHA512 hash is hashed again using bcrypt with a cost of 10, and a unique, per-user salt. Unlike cryptographic hash functions like SHA, bcrypt is designed to be slow and hard to speed up via custom hardware and GPUs. A work factor of 10 translates into roughly 100ms for all these steps on our servers.Finally, the resulting bcrypt hash is encrypted with AES256 using a secret key (common to all hashes) that we refer to as a pepper. The pepper is a defense in depth measure. The pepper value is stored separately in a manner that makes it difficult to discover by an attacker (i.e. not in a database table). As a result, if only the password storage is compromised, the password hashes are encrypted and of no use to an attacker.We considered using scrypt, but we had more experience using bcrypt. The debate over which algorithm is better is still open, and most security experts agree that scrypt and bcrypt provide similar protections.We’re considering argon2 for our next upgrade: when we moved to our current scheme, argon2 hadn’t (yet) won the Password Hashing Competition. Additionally, while we believe argon2 is a fantastic password hashing function, we like that bcrypt has been around since 1999 without any significant vulnerabilities found.Recall that the global pepper is a defense in depth measure and we store it separately. But storing it separately also means that we have to include the possibility of the pepper (and not the password hashes) being compromised. If we use the global pepper for hashing, we can’t easily rotate it. Instead, using it for encryption gives us similar security but with the added ability to rotate. The input to this encryption function is randomized, but we also include a random initialization vector (IV).Going forward, we’re considering storing the global pepper in a hardware security module (HSM). At our scale, this is an undertaking with considerable complexity, but would significantly reduce the chances of a pepper compromise. We also plan to increase our bcrypt strength in our next update.We believe this use of SHA512, plus bcrypt, and AES256 is currently among the strongest and most future-proof methods to protect passwords. At the same time, we know that attackers are continuously evolving—and our defenses will too. Our password hashing procedure is just one of many measures we use to secure Dropbox. We’ve deployed additional safeguards against online brute-force attacks like rate-limiting password attempts, captchas, and a range of abuse mitigations. Like the diagram above, there are many layers to maintaining robust security, and we’re actively investing in all of them. We’d love to hear your thoughts.",https://blogs.dropbox.com/tech/2016/09/how-dropbox-securely-stores-your-passwords/,0,dropbox,"backend,database,frontend,sql,ruby,php,db,mysql,react",NULL,2016-09-21
Improving the performance of full-text search,"For Firefly, Dropbox’s full-text search engine, speed has always been a priority. (For more background on Firefly, check out our blog post). When our team saw search latency deteriorate from 250 ms to 1000 ms (95th percentile), we knew what to do—we measured, we analyzed, we fixed.In order to create a good user experience for Firefly, we strive to keep our query latency under 250 ms (at 95th percentile). We noticed that our latency had deteriorated quite a bit since we started adding users to the system.Change in 95th percentile latency for Firefly backendOne aspect of Firefly that allows us to support search over a very large corpus (hundreds of billions of documents) with a relatively small machine footprint is that we don’t load the search index in RAM — we serve it from solid-state drives (SSDs). In our investigation, we found that the deterioration in latency was caused by an increase in I/O operations per sec (IOPS) on the index servers. This increase caused the index servers to be I/O bound.In Firefly, each document update mutates our inverted index. To reduce the I/O, we needed to reduce the amount of data read and written during an update to our index. This motivated us to take a closer look at the encoding we were using in our index. We started by instrumenting our code to collect more stats around I/O, then we implemented two different encoding schemes and looked at their impact on the I/O.Conceptually, a search index contains the mapping:Examples of attributes are: whether the token appeared in the text of the document, if it was part of the filename, or if the token was an extension.For compactness, we encode the list of document IDs separately from the list of attributes. These are then laid out one after another:The header contains meta information like the byte length of each encoded part and the version of the encoding scheme used. This ended up being very helpful for this project, as it allowed us to introduce new ways of encoding this list without breaking backwards compatibility for old index entries.For example, if the token “JPG” is an extension for documents 10001, 10002 and 10005, and it appears in the text of document 10007, then the mapping for “JPG” would look like the following:In order to make the encoding more compact, we made two changes — we switched to using delta encoding for the list of document IDs, and using run-length encoding for the list of attributes. With delta encoding the list of Document IDs, the above posting list becomes:We’re using Varint encoding for the document IDs, so smaller values translate into fewer bytes in encoded form.When we use run-length encoding of attributes, we get an even more compact representation:These encoding changes reduced the total size of the encoded search index by 33%.The above graph shows two large reductions in index size. The first one was a result of deploying delta encoding, and the second one was a result of deploying run-length encoding. Both of these greatly reduced the I/O we do on the index servers. This is illustrated by the following graph:Interestingly, the run-length encoding resulted in a larger improvement than the delta length encoding. We had expected this to only reduce the posting-lists for certain filename tokens (e.g., “jpeg” was usually an extension) — these frequently had the same attributes associated with them. To our surprise, this also gave us a large reduction in the posting-list size for full-text tokens. Our theory is that some users may have the same word a single time in a large number of documents, so all of these hits share the same attributes.These changes resulted in a significant improvements in the 95th percentile latency of our system, as shown by the following graph—we’re back at 250 milliseconds!It is important to track the vital stats of the system as it’s scaled up, and follow up on any degradation. Sometimes it may reveal unexpected behavior. Also, having extensible structures is pretty helpful — it would have been very hard to introduce alternative encodings without the use of the header in the mapping.As a result of these changes, our index servers were no longer I/O bound, leading to a better experience for our users.",https://blogs.dropbox.com/tech/2016/09/improving-the-performance-of-full-text-search/,0,dropbox,,NULL,2016-09-07
(Re)Introducing Edgestore,"Edgestore is the metadata store that powers many internal and external Dropbox services and products. We first talked about Edgestore in late 2013 and needless to say, much has happened since.In this post, we give a high-level overview of the motivation behind Edgestore, its architecture, salient features and how it’s being used at Dropbox. We’ll be doing a deep-dive on various aspects of Edgestore in subsequent posts.Like so many startups, Dropbox started with vanilla MySQL databases for our metadata needs. As we rapidly added both users and features, we soon ended up with multiple, independent databases; some databases grew so large that we had to split them into multiple shards. And before long, unsurprisingly, we started hitting challenges of such an architecture, such as:In late 2012, we began building a system that would address these challenges. In addition, we wanted the following characteristics to meet future needs:At the time, no off-the-shelf solution met all our requirements. Given our in-house MySQL expertise and similar systems at other companies (notably, Facebook’s TAO), we decided to build our own system that would abstract away the database by providing higher-level abstractions and use MySQL (InnoDB) as the storage engine.Edgestore started out as a simple client-side ORM wrapper, but has over time evolved into a sophisticated service with features like caching, geo-replication and multi-tenancy. Edgestore has been running in production at Dropbox for almost four years, on thousands of machines across multiple data centers, storing several trillion entries and servicing millions of queries per second with 5–9s of availability!Users interact with Edgestore via language-specific SDKs that implement the Edgestore API. The API allows developers to easily describe (and evolve) their data model without worrying about how or where data gets stored. We currently provide SDKs for Go and Python.Objects in Edgestore can be Entities or Associations, each of which can have user-defined attributes (roughly analogous to columns in a traditional database table). Associations describe how different Entities relate to each other: for instance, users/group membership might be described using a UserEntity , TeamEntity and a UserTeamAssoc .The SDK provides Edgestore Clients that connect to any one of Edgestore Cores. Cores comprise a stateless layer responsible for routing (or forwarding) the request to the correct “shard” (or region).Since our workload is read-heavy, Cores use a Caching Layer to speed lookups. The caches are also partitioned and replicated for high-availability. Edgestore provides strong-consistency by default, which requires invalidating caches on writes. If the workload can tolerate stale reads, clients can request eventual consistency.For writes (or on a cache-miss), Cores send the request to the Engine where data is “mastered”, as determined by our partitioning scheme. Engines abstract away the Storage layer from Cores, so our design is not MySQL specific. Engines translate Edgestore APIs to MySQL commands and track resource consumption by traffic-source. This forms the foundation of isolation and multi-tenancy in Edgestore.For brevity, we’re glossing over many technical details here. Please check back for our follow-up posts where we’ll deep-dive into the special topics listed under “Coming Up” below!Given its continued dominance, it should be no surprise that email remains one of the most powerful ways for Dropbox to connect with our user base. To make it easy for any team in Dropbox to set up an email campaign, we have built an internal email management system that leverages Edgestore’s ability to function as a queue. Every email message that is stored and sent through our email service maintains an Assoc with the current status of the message (is the message pending, is it scheduled to go out, was it sent and returned with an error) and a timestamp that is used to order the messages. A periodic job is used to scan the timestamps and status to see which messages require sending; they are sent and removed from the queue (or re-enqueued, as needed). This system stores all of its metadata in Edgestore and relies on Edgestore’s strong consistency guarantees to provide exactly–once delivery semantics. To recap, Edgestore is a strongly consistent, read-optimized, horizontally scalable, geo-distributed metadata store that powers many internal and external Dropbox services and products. At some point as a company evolves, the need arises for a flexible, generic metadata store. Rather than continue bolting more features and tooling to our MySQL infrastructure, we built Edgestore to abstract away the database altogether.We are excited to share our learnings with the community! This is the first of a series of blog posts about Edgestore. Over the next few months, we plan to cover more technical details on some of the more interesting aspects of Edgestore. Some examples:This is a tentative list — if there are specific areas you’d like to see covered, please leave a comment!Thanks to: Alex Degtiar, Adil Hafeez, Bogdan Munteanu, Chris Roberson, Daniel Tahara, Kerry Xing, Maxim Bublis, Mehant Baid, Michelle Chesley, Mihnea Giurgea, Rajat Goel, Renjish Abraham, Samir Goel, Tom Manville and Zviad Metreveli",https://blogs.dropbox.com/tech/2016/08/reintroducing-edgestore/,0,dropbox,"backend,frontend,java,json,php,spring,css,python,docker,react,javascript",NULL,2016-08-30
Fast Document Rectification and Enhancement,"Dropbox’s document scanner lets users capture a photo of a document with their phone and convert it into a clean, rectangular PDF. It works even if the input is rotated, slightly crumpled, or partially in shadow—but how?Here’s an example input and output:On the flip side, the x- and y-coordinates of the four detected document corners gives us effectively eight constraints. While there are seemingly more unknowns (9) than constraints (8), the unknowns are not entirely free variables—one could imagine scaling the document physically and placing it further from the camera, to obtain an identical photo. This relation places an additional constraint, so we have a fully constrained system to be solved. (The actual system of equations we solve involves a few other considerations; the relevant Wikipedia article gives a good summary.)Once the parameters have been recovered, we can undo the geometric transform applied by the capture process to obtain a nice rectangular image. However, this is potentially a time-consuming process: one would look up, for each output pixel, the value of the corresponding input pixel in the source image. Of course, GPUs are specifically designed for tasks like this: rendering a texture in a virtual space. There exists a view transform—which happens to be the inverse of the camera transform we just solved for!—with which one can render the full input image and obtain the rectified document. (An easy way to see this is to note that once you have the full input image on the screen of your phone, you can tilt and translate the phone such that the projection of the document region on the screen appears rectilinear to you.)Lastly, recall that there was an ambiguity with respect to scale: we can’t tell whether the document was a letter-sized paper (8.5” x 11”) or a poster board (17” x 22”), for instance. What should the dimensions of the output image be? To resolve this ambiguity, we count the number of pixels within the quadrilateral in the input image, and set the output resolution as to match this pixel count. The idea is that we don’t want to upsample or downsample the image too much.If we could tell whether a given pixel belongs to the foreground or to the background, this task would be straightforward. However, assigning a binary label leads to aliasing, especially for text with small font. A simple linear transform based on the pixel value is not sufficient, either, because there are often shadows or other lighting variations across the image. Hence, we will try to compute the final output image J without explicitly solving the foreground/background classification problem.We achieve the above requirements by writing a cost function that penalizes things we don’t want, and then running it through a standard optimization procedure to arrive at the solution with the lowest cost possible; hopefully, this will correspond to the best possible output image.So now, the degree to which a potential solution J adheres to the first requirement is fairly straightforward to write as a cost: where 255 denotes white pixels and the indices x, y range over the extent of the image. If the output image is mostly white, this measure would be minimized.For the second requirement, we’d like to ensure that the foreground has a crisp contrast against the background for ease of reading, despite changes in brightness throughout the image. Since we are not explicitly assigning foreground labels, what we need is a way to preserve local structure while factoring out global brightness changes. One common measure of the local structure within an image is its gradient, which denotes the difference between neighboring pixels. Hence, to preserve the local structure, we can use as our cost the degree to which the output gradient deviates from that of the original image:Solving Poisson’s equation on a full-resolution image (8–12 megapixels on the latest iPhones) is still computationally demanding, and can take several seconds on older devices. If the user is creating a multi-page PDF, the wait time increases commensurately. To provide a smoother user experience, we would like to reduce the processing time by an order of magnitude.One observation is that the output is generally linearly correlated with the input, at least locally—if one were to apply some gain to the input and add an offset, it would be a reasonably good solution locally, i.e.,Of course, the rationale behind using a mathematical machinery like the Poisson’s equation in the first place was that there is no single gain and offset that works for the whole image. In order to handle uneven illuminations and shadows, however, we could allow the gain and the offset to vary across the image:While this new formulation is more flexible than before, it has twice as many unknowns (the gain and the offset at each pixel, rather than simply the final output value), making it trickier and more expensive to solve.The key insight for reducing the computational cost and further constraining the problem is that the gain and the offset should vary relatively slowly across the image—we’re aiming to deal with illumination changes, not rainbow-colored paper! This allows us to solve the optimization problem at a much lower resolution compared to the input image, and therefore much faster. This also implicitly forces the unknown values to correlate locally, because we then upsample the gain and offset back to the original resolution. Once the gain and the offset are known across the image, we can plug them back into the above equation to obtain the final output image.So far our derivations have ignored color, even though most photos come in the form of an RGB image. The simplest way to deal with this is to apply the above algorithm to each of the R, G, B channels independently, but this can result in color shifts, since the channels are no longer constrained together.In our initial effort to combat this, we tried to substitute in the original RGB values for the output pixels that are not close to white. However, when we tried this, we encountered the effect of color constancy. Here’s a great illustration of this “illusion,” in which the two tiles marked A and B have the same pixel values, but appear to be very different:",https://blogs.dropbox.com/tech/2016/08/fast-document-rectification-and-enhancement/,0,dropbox,"python,java,frontend,css",NULL,2016-08-16
Fast and Accurate Document Detection for Scanning,"A few weeks ago, Dropbox launched a set of new productivity tools including document scanning on iOS. This new feature allows users to scan documents with their smartphone camera and store those scans directly in their Dropbox. The feature automatically detects the document in the frame, extracts it from the background, fits it to a rectangular shape, removes shadows and adjusts the contrast, and finally saves it to a PDF file. For Dropbox Business users, we also run Optical Character Recognition (OCR) to recognize the text in the document for search and copy-pasting.Beginning today, we will present a series of technical blog posts describing the computer vision and machine learning technologies that make Dropbox’s document scanning possible. In this post, we’ll focus on the first part of the pipeline: document detection.The goal of document detection is to find the corners and edges of a document in the image, so that it can be cropped out from the background. Ideally, detection should happen in real time, so that the user can interactively move the camera to capture the best image possible. This requires the detector to run really fast (100ms per frame or less) on a tight CPU and memory budget.A common approach to solving problems like this is to train a deep neural network (DNN). DNNs are algorithms that take a large amount of labeled data and automatically learn to predict labels for new inputs. These have proved to be tremendously successful for a variety of computer vision applications, including image classification, image captioning, and face detection. However, DNNs are quite expensive, both in terms of computation time and memory usage. Therefore, they are usually difficult to deploy on mobile devices.Another potential solution is to use Apple’s rectangle detection SDK, which provides an easy-to-use API that can identify rectangles in still images or video sequences in near-realtime. The algorithm works very well in simple scenes with a single prominent rectangle in a clean background, but is less accurate in more complicated scenes, such as capturing small receipts or business cards in cluttered backgrounds, which are essential use-cases for our scanning feature.We decided to develop a customized computer vision algorithm that relies on a series of well-studied fundamental components, rather than the “black box” of machine learning algorithms such as DNNs. The advantages of this approach are that it is easier to understand and debug, needs much less labeled training data, runs very fast and uses less memory at run time. It is also more accurate than Apple’s SDK for the kinds of usage scenarios we care about; in an A/B test evaluation, the detections found by our algorithm are 60% less likely to be manually corrected by users than those found by Apple’s API.Our first observation is that documents are usually rectangular-shaped in physical space, and turn into convex quadrilaterals when projected onto 2D images. Therefore, our goal turns into finding the “best” quadrilateral from the image, and use that as our proxy for the document boundary. In order to find the quadrilateral, we need to find straight lines and their intersections. Finally, to find straight lines, we need to detect strong edges in the image. This gives us the outline of our detection algorithm, as shown below. We will discuss each component in more detail next.Finding edges in an image is a classic problem in image processing and computer vision. It has decades of history, and saw early success already in the ’80s. One of the best known methods is the Canny edge detector, named after its inventor, John Canny. It dates back to 1986 but is still widely used today.We applied the Canny Detector to our input image, as shown below, but the results were not very promising. The main problem is that the sections of text inside the document are strongly amplified, whereas the document edges—what we’re interested in—show up very weakly.Left: the input image. Right: the output of the Canny edge detector.To overcome these shortcomings, we used a modern machine learning-based algorithm. The algorithm is trained on images where humans annotate the most significant edges and object boundaries. Given this labeled dataset, a machine learning model is trained to predict the probability of each pixel in an image belonging to an object boundary.The result of this learning-based edge detector is shown below. It’s much better at focusing on the document edges that we care about.Left: the input image. Right: the output of the machine learning-based edge detector.Once we have an accurate edge map, we’d like to find straight lines in it. For this, we use the venerable Hough transform, a technique that lets individual data points “vote” for likely solutions to a set of equations. In our case, each detected edge pixel votes for all lines passing through that point; the hope is that by adding up the votes across all edge pixels, the true document boundaries will emerge with the most votes.More formally, here’s how it works: The slope-intercept form of a line is y = mx + b. If we detect an edge pixel at a particular (x,y) point, we want to vote for all lines that pass through the point. This corresponds to all slopes m and intercepts b that satisfy the line equation for that point. So we set up a “Hough Space” with m and b axes. Here, a single point (m,b) corresponds to a line in the original image; conversely, a point in the original image space corresponds to a line in the Hough Space. (This is called a duality in mathematics.) For every edge pixel in the original image, we increment a count for all corresponding points in the Hough Space. Finally, we simply look for the points with most votes in the Hough Space, and convert those back into lines in the original space.In the figure below, you can see the detected edge pixels on the left and the corresponding Hough Space in the middle. We’ve circled the points with the most votes in the Hough Space, and then converted them back into lines (overlaid onto the original image) on the right. Note that although we described the Hough Transform above in terms of the slope-intercept form of a line, in practice we use a polar parameterization, r=x·sinθ+y·cosθ, that is more robust and easier to work with.Left: detected edges. Middle: the Hough Transform of the edges, with local maxima marked in red. Right: the lines corresponding to the local maxima overlaid onto the original image.After finding straight lines, the rest of the work is relatively simple. We compute the intersections between the lines as potential document corners, with some simple geometric constraints. For example, intersections with very acute angles are unlikely to be document corners. We next iterate through potential document corners, and enumerate all possible quadrilaterals, each of which is scored by adding up the probability predicted by the edge detector over pixels along its perimeter. The quadrilateral with highest score is output as the detected document.Left: intersections of detected lines are potential document corners, although the red ones are filtered out by using geometric constraints. Middle: one possible quadrilateral formed by the potential corners. Right: the quadrilateral with the highest score, which is the output of our algorithm.Finally, we show a video below demonstrating each step of the pipeline. The video is generated with a standalone iOS app we built to develop, visualize and debug our algorithm. The full pipeline runs near realtime at about 8–10 frames per second.Visualization of all steps in the detection algorithm.Try out the Dropbox doc scanner today, and stay tuned for our next blog post, where we’ll describe how we turn the detected document outline into an enhanced rectangular image.",https://blogs.dropbox.com/tech/2016/08/fast-and-accurate-document-detection-for-scanning/,0,dropbox,"tensorflow,machinelearning,python",NULL,2016-08-09
Lepton image compression: saving 22% losslessly from images at 15MB/s,"We are pleased to announce the open source release of Lepton, our new streaming image compression format, under the Apache license.Lepton achieves a 22% savings reduction for existing JPEG images, by predicting coefficients in JPEG blocks and feeding those predictions as context into an arithmetic coder. Lepton preserves the original file bit-for-bit perfectly. It compresses JPEG files at a rate of 5 megabytes per second and decodes them back to the original bits at 15 megabytes per second, securely, deterministically, and in under 24 megabytes of memory.We have used Lepton to encode 16 billion images saved to Dropbox, and are rapidly recoding our older images. Lepton has already saved Dropbox multiple petabytes of space.Community participation and improvement to this new compression algorithm is welcome and encouraged!At Dropbox, the security and durability of your data are our highest priorities. As an added security layer, Lepton runs within seccomp to disable all system calls except read and write of already-open file descriptors. Lepton has gone through a rigorous automated testing process demonstrating determinism on over 4 billion photos and counting. This means that once we verify an image decodes back to its original bits the first time, we can always get back to the original file in future decodes.All of our compression algorithms, including Lepton, decode every compressed file at least once and compare the result to the input, bit-for-bit, before persisting that file. Compressed files are placed into kernel-protected, read-only, memory before the bit-for-bit comparison to guarantee they are immutable during the full verification process.The JPEG format encodes an image by dividing it into a series of 8×8 pixel blocks, represented as 64 signed 10-bit coefficients. Thus the following 16×16 image would be encoded as 4 JPEG blocks.  Instead of encoding pixel values directly, the signed 10-bit coefficients are the result of a reversible transformation called the Discrete Cosine Transform, or DCT for short.One of the 64 coefficients in a block, known as the DC, represents the brightness of the entire block of 8×8 pixels. The collection of all DC values in a whole JPEG can be viewed as a thumbnail that is a factor of 8 smaller than the original image in each dimension. The other 63 values, known as the AC coefficients, describe the fine detail going on in the 8×8 pixel block: for example, the texture of pebbles on a beach, or the pattern on a plaid shirt.Here’s an animation of the letter A becoming ever clearer as its AC coefficients are added one by one. The animation starts with the DC and adds each AC in turn which brings out the detail of the A.By Hanakus, CC BY-SA 3.0, via Wikimedia CommonsTo encode the 63 AC coefficients, Lepton first serializes the number of non-zeros in the block, and then zigzags through the 8×8 block of coefficients, writing out each value using the efficient representation described below.Instead of writing bits as zeros and ones, Lepton encodes the data using the VP8 arithmetic coder, which can be very efficient, if supplied with carefully chosen context information from previous sections of the image. Stay tuned to future blog posts for more about the context Lepton feeds into the arithmetic coder.To encode an AC coefficient, first Lepton writes how long that coefficient is in binary representation, by using unary. Unary is a numerical representation that is as simple as counting off on your fingers. For example, three would be 1110.  The extra zero at the end is the signal to stop counting. So, five would be 111110 and zero would just be 0.Next Lepton writes a 1 if the coefficient is positive or 0 if it is negative. Finally, Lepton writes the the absolute value of the coefficient in standard binary. Lepton saves a bit of space by omitting the leading 1, since any number greater than zero doesn’t start with zero.For example, here’s 47 represented in Lepton:For the coefficients we observed in JPEG files, the Lepton representation results in significantly fewer symbols than other encodings, such as pure unary, or fixed length two’s complement.The DC coefficient (brightness in each 8×8 block) takes up a lot of room (over 8%) in a typical iPhone photograph so it’s important to compress it well. Most image formats put the DC coefficients before any AC coefficients in the file format. Lepton gets a compression advantage by coding the DC as the last value in each block.Since the DCs are serialized last, there is a wealth of information from the AC coefficients available to predict the DC coefficient. By defining a good and reproducible prediction, we can subtract the actual DC coefficient from the predicted DC coefficient, and only encode the delta. Then in the future we can use the prediction along with the saved delta to get the original DC coefficient. In almost all cases, this technique results in a significantly reduced number of symbols to feed into our arithmetic coder.Let’s assume we are in the middle of decoding a JPEG file using Lepton, row by row.Here’s an example of a block in the middle of the image without a known brightness, only having the AC coefficients (the details) so far:Since we are decoding from left-to-right and top-to-bottom and have already decoded some blocks, we know all the pixels of the block directly above and the block to the left:One approach to predict the DC value could be to compute the overall brightness that minimizes the differences between all 16 pairs of pixels at the border of the current 8×8 block and both the left and top neighbors. This can be used as a first cut at the prediction.If we average based on the median 8 pixels and ignore the 8 of the outlier pixels, the above technique shrinks the DC by roughly 30% over baseline JPEG. Applying this technique to the example above results in this complete icon of a Dropbox:However, in reality, images tend to have smooth gradients, in the same way as the sky fades from blue to orange towards the horizon during a sunset. If we simply tried to reduce the difference between neighbor pixels, then we would be essentially predicting that any smooth gradients abruptly stop at 8×8 boundaries.A better prediction is to actually continue all gradients smoothly as in this illustration: Since we are encoding the DC after all the AC coefficients, and since the difference between a pair of pixels (the gradient) does not depend on the brightness (DC), we can utilize the gradients in the current block to help compute a prediction.  We also have the gradients from all pairs of pixels in the neighbor blocks, because they have been completely decoded.Thus, we can compute the gradient from the second row of the current block to the edge, and from the neighbors back to the edge, meeting in the middle, as illustrated:Where these two gradients meet in the middle, between these two pixels is the prediction point that Lepton uses to predict the DC of the current 8×8 block. The delta of this prediction is written in the same manner as the AC’s, using length, followed by sign and residual.For those familiar with Season 1 of Silicon Valley, this is essentially a “middle-out” algorithm.Pied Piper presents their “MIDDLE OUT!!’ algorithm at TechCrunch Disrupt, photo from HBOThe overall savings from only encoding the delta reduces the stored size of the DC coefficients all the way down to 61% of their original size, saving 39%. DC coefficients occupy just 8% of an average JPEG file, but this middle-out algorithm still manages to contribute a significant reduction to the overall file size.This is just one of the many techniques we use to save 22% on each JPEG file. The savings are surprisingly consistent over a wide range of images captured by modern cameras and cell phones.Lepton compression applied to 10,000 imagesLepton can decompress significantly faster than line-speed for typical consumer and business connections. Lepton is a fully streamable format, meaning the decompression can be applied to any file as that file is being transferred over the network. Hence, streaming overlaps the computational work of the decompression with the file transfer itself, hiding latency from the user.Lepton decode rate when decoding 10,000 images on an Intel Xeon E5 2650 v2 at 2.6GHzLepton encode rate when encoding 10,000 images on an Intel Xeon E5 2650 v2 at 2.6GHzThe code for Lepton is open source and available today. It provides lossless, bit-exact storage for any type of photo, whether it be for archival purposes, or for serving live. Stay tuned for more details about how Lepton works and the challenges of compressing images at Dropbox scale.",https://blogs.dropbox.com/tech/2016/07/lepton-image-compression-saving-22-losslessly-from-images-at-15mbs/,0,dropbox,"css,frontend,python",NULL,2016-07-14
Pocket watch: Verifying exabytes of data," There is nothing more important to Dropbox than the safety of our user data. When we set out to build Magic Pocket, our in-house multi-exabyte storage system, durability was the requirement that underscored all aspects of the design and implementation. In this post we’ll discuss the mechanisms we use to ensure that Magic Pocket constantly maintains its extremely high level of durability.This post is the second in a multi-part series on the design and implementation of Magic Pocket. If you haven’t already read the Magic Pocket design overview go do so now; it’s a little long but provides an overview of the architectural features we’ll reference within this post. If you don’t have time for that then keep on reading, we’ll make this post as accessible as possible to those who are new to the system.When most good engineers hear “durability” they think “replication”. Hardware can fail, so you need to store multiple copies of your data on physically isolated hardware. Replication can be tricky from a mathematical or distributed-systems perspective, but from an operational perspective is the easiest to get right.In the case of Magic Pocket (MP) we use a variant on Reed-Solomon erasure coding that is similar to Local Reconstruction Codes, which allows us to encode and replicate our data for high durability with low storage overhead and network demands. If we use a Markov model to compute our durability given the expected worst-case disk failure rates and repair times, we end up with an astonishingly-high 27 nines of durability. That means that according to this model, a given block in Magic Pocket is safe with 99.9999999999999999999999999% probability!Does that mean we should trust this model and call it a day? Of course not. Replication is a necessary ingredient for durability, but by no means is it sufficient. There are a lot more challenging failure modes to contend with than just random disk failures: stuff like natural disasters, software bugs, operator error, or bad configuration changes. True real-world durability requires investing heavily in preventing these less-frequent but wider-reaching events from impacting data safety.We recently presented a talk on Durability Theater, highlighting the challenges in building a system for real-world durability. Take a look at the video to find out more about about the breadth of defenses we employ against data loss, which we categorize across four dimensions: Isolation, Protection, Verification and Automation. Each of these would be a great topic for future blog posts but today we want to focus on one of our favorite areas of investment: Verification.The most important question we ask ourselves every day is “is this system correct?” This is an easy question to ask, but a surprisingly difficult question to answer authoritatively. Many systems are “basically correct”, but the word “basically” can contain a lot of assumptions. Is there a hidden bug that hasn’t been detected yet? Is there a disk corruption that the system hasn’t stumbled across? Is there bad data in there from years and years ago that is used as a scapegoat whenever the system exhibits unexpected behavior? A durability-centric engineering culture requires rooting out any potential issues like this and establishing an obsessive focus on correctness.A large fraction of the Magic Pocket codebase is devoted purely to verification mechanisms that confirm that the system continually maintains our high level of correctness and durability. This is a significant investment from the perspective of engineering time, but it’s also a huge hardware and resource-utilization investment: more than 50% of the workload on our disks and databases is actually our own internal verification traffic.Let’s take a look at the stack of verifiers that we run continually in production…One quick definition before we get started: an extent is the physical manifestation of a data volume on a given storage node. Each storage node stores thousands of 1GB extents which are full of blocks of user data.There are a lot of scanners in here so we’ll go through them one-by-one. Let’s start with the lowest level of the storage stack: the Disk Scrubber.This will be no surprise to those working in large-scale storage: your disks are lying to you. Hard drives are an amazing and reliable technology, but when you have over half a million disks in production they’re going to fail in all manner of weird and wonderful ways: bad sectors, silent disk corruption and bit-flips, fsyncs that don’t fsync . Many of these errors also slip through S.M.A.R.T. monitoring on the disks and lie there dormant, waiting to be discovered… or worse, not discovered at all.When we talked about our durability model we mentioned how it depends on the time taken to repair a disk failure. When we detect a bad disk in MP we quickly re-replicate the data to ensure the volumes aren’t vulnerable to a subsequent disk failures, usually in less than an hour. If a failure were to go undetected however, the window of vulnerability would expand from hours to potentially months, exposing the system to data loss.The disk scrubber runs continually on our storage nodes, reading back every bit on disk and validating it against checksums. If the scrubber detects bad data on disk then it automatically schedules that data to be re-replicated and for the disk to enter our disk remediation workflow, which we’ll discuss in a future post.We perform a full sweep of each disk approximately every 1–2 weeks. This requires reading terabytes of data off the disk, but these are sequential scans which minimizes disk seeks. We also scan the recently-modified areas of the disk more frequently to catch any fresh issues.If Magic Pocket needs to move a volume between storage nodes, or rewrite a volume after garbage collecting it, then it writes the volume to a new set of storage nodes before deleting it from the old nodes. This is an obviously-dangerous transition: what if a software bug caused us to erroneously delete an extent that hadn’t yet been written stably to a new location?We adopt a protection in MP called trash. When the Master in a cell instructs a storage node to delete an extent the node actually just moves the extent to a temporary storage location on the disk. This “trash” data sits there until we can be sure this data was deleted correctly.The Trash Inspector iterates over all the blocks in trash extents and checks the Block Index to determine that either:Once a trash extent passes inspection it is kept on-disk for an additional day (to protect against potential bugs in the trash inspector) before being unlinked from the filesystem.The Trash Inspector does a good job of ensuring that we only delete data as intended, but what if a bad script or rogue process attempts to delete an extent before it has passed inspection? This is where the Extent Referee comes in. This process watches each filesystem transition and ensures that any move or unlink event corresponds to a successful trash inspection pass and corresponding instruction from the Master to remove the extent. The Extent Referee will alert on any transition that doesn’t conform to these requirements. We also employ extensive unix access controls along with TOMOYO mandatory access control to guard against operators or unintended processes interacting with data on storage nodes. More on these in future posts.One advantage of storing our Block Index in a database like MySQL is that it’s really easy to run a table scan to validate that this data is correct. The Metadata Scanner does exactly this, iterating over the Magic Pocket Block Index at around a million blocks per second (seriously!), determining which storage nodes should hold each block, and then querying these storage nodes to make sure the blocks are actually there.A million checks per second sounds like a lot of activity, and it is, but we have many hundreds of billions of blocks to check. Our goal is to perform a full scan over our metadata approximately once per week to give us confidence that data is entirely correct in one storage zone before we advance our code release process and deploy new code in the next zone.The storage nodes keep enough metadata in-memory to be able to answer queries from the Metadata Scanner without performing a disk seek. The Metadata Scanner and Disk Scrubber thus work together to ensure both that the data on the storage nodes matches the data in the Block Index, and also that the data on the disks themselves is actually correct.The most insidious bugs in a large distributed system are logic errors on the boundaries between modules: an engineer misunderstanding an API, or the semantics of an RPC call, or the meaning of a field in a database. These are hard to catch in unit testing, and typically require comprehensive integration testing or verification mechanisms to detect. But what if the engineer writing the code is the same person writing the tests or the verifier? It’s very likely that the same broken assumptions will make their way into the verification mechanism itself.The Watcher was designed to avoid any such “broken verifiers” and was written as an end-to-end black box checker, implemented by someone who wasn’t involved in building the storage system. We sample 1% of all blocks written to Magic Pocket and record their corresponding storage keys (hashes) in queues in Kafka. The Watcher then iterates over these queues and makes sure it can correctly fetch these blocks from Magic Pocket after one minute, one hour, one day, one week and one month. This allows us to verify that MP is indeed still serving blocks correctly from an end-to-end perspective, and that there are no errors introduced over time that may prevent a block from being accessed.This is the last one so we’ll make it quick. MP contains multiple storage zones that store data for different users and we need to maintain some careful invariants when moving users between these zones. We also need to make sure that if a zone goes down for maintenance or because of an outage that we recover quickly and transfer all data that may have been missed during this downtime.The Cross-zone Verifier lives outside of MP and scans through the Dropbox filesystem maintained in a separate system called File Journal. This verifier walks this filesystem and checks that all files in Dropbox are correctly stored in all zones corresponding to the given user. While the other verifiers confirm that MP is correctly storing the blocks that we know we should have, the Cross-zone Verifier ensures that there’s agreement between what MP is storing and what our clients (the rest of Dropbox) think we should be storing.If it felt like a lot of work reading that long list of verifiers, rest assured that it was a lot more work to actually build them. Building these verifiers was a really valuable investment for us however, and greatly improved our execution speed. This is because a comprehensive verification stack doesn’t just ensure that the system is correct in production, it also provides a highly-valuable testing mechanism for new code.Unit tests are great but there’s no substitute for comprehensive integration testing. We run a broad suite of integration tests on simulated workloads, including failure injection and multiple simultaneous code versions, but the final gateway to production is always our Staging cluster. This cluster is tens of petabytes in size and stores a geographically-distributed mirror of a subset of production data. We actually serve live traffic out of Staging and fall back to production MP if there’s a problem. We run these verifiers for at least a week on Staging before team members can sign off to release a code version from stage into production. The verification mechanisms provide us a very comprehensive view into the correctness of the system and ensure that any bugs are caught before they make it to an actual production cluster.One last remark before we go: how do we verify the verifiers themselves?Here’s a screenshot of the primary metric generated by the Metadata Scanner. Take a look and see if you notice anything interesting:That’s right, it’s the most boring graph in the world!MP is designed for extremely high levels of durability so the graph of missing hashes is just a zero line. Apart from the Disk Scrubber which finds regular disk errors, and occasional timeouts from our other verifiers, all our verification graphs look like this. How do we know that the scanners are actually doing something useful rather than just spitting out zeros all day? What we really need to do is create some problems and make sure they get detected.This dovetails into a larger discussion about DRTs (Disaster Recovery Training events) that we’ll cover another time. These are induced failures to test that a system is able to recover from disaster, and that the team is trained to respond to an incident. The other important and sometimes-overlooked aspect to a DRT however is ensuring that all the verification systems are actually doing their job.For us this means constructing failure tests that trigger our verification mechanisms without potential for actually impacting user data. Typically this means running tests in our staging cluster, which is safely backed up in our production clusters.The details of one interesting test are as follows:In this test one of our engineers got permission to secretly corrupt data in our staging cluster, flipping bits, truncating extents, and inserting random data. We then waited for the rest of the team to detect the full set of errors. In this particular test all errors were quickly detected, except for two which were automatically repaired by MP before the verifiers got to them. Importantly however, there was no data loss in our staging cluster because of this test; MP contains enough redundancy to automatically detect and recover from all these failures without any operator intervention.It was certainly a lot of work to build all the verification systems in MP; there probably aren’t too many large-scale systems in the world subject to more scrutiny. Every project will involve a tradeoff on the spectrum of effort vs correctness, and for Dropbox data-safety is of paramount importance.Regardless of where you land on this spectrum, the main lesson from this post is that confidence in correctness is a very empowering concept. Being able to reason about the current state of a storage system means that you’re able to move fast on future development, respond to operational issues with confidence, and to build reliable client applications without confusion about the source of errors or inconsistencies. It also lets us sleep a little better every night, which is always a valuable investment.Stay tuned for more posts about Magic Pocket. Next post will likely cover some of the operational issues involved in running a system at this scale, followed by a post discussing our Diskotech architecture to support append-only SMR storage. We’ll catch you then!",https://blogs.dropbox.com/tech/2016/07/pocket-watch/,0,dropbox,python,NULL,2016-07-06
Lossless compression with Brotli in Rust for a bit of Pied Piper on the backend,"Written by Daniel Reiter Horn and Mehant Baid, Serving Infrastructure team at Dropbox.In HBO’s Silicon Valley, lossless video compression plays a pivotal role for Pied Piper as they struggle to stream HD content at high speed.John P. Johnson/HBOInspired by Pied Piper, we created our own version of their algorithm Pied Piper at Hack Week. In fact, we’ve extended that work and have a bit-exact, lossless media compression algorithm that achieves extremely good results on a wide array of images. (Stay tuned for more on that!)However, to help our users sync and collaborate faster, we also need to work with a standardized compression format that already ships with most browsers. In that vein, we’ve been working on open source improvements to the Brotli codec, which will make it possible to ship bits to our business customers using 4.4% less of their bandwidth than through gzip.Brotli is a Google open source project that includes a versatile encoder with a range of time/space settings. It’s already supported as an encoding format in Mozilla Firefox, Google Chrome, Android Browser, and Opera.In the diagram below, we’ve depicted Brotli’s impact as applied to typical business use-case files (excluding photographs and video). For a bit of fun, we’ve also included the Weissman score—a metric created for the TV show Silicon Valley for evaluating compression algorithms. Note that the Weissman score favors compression speed over ratio in most cases.We have two main requirements to implement Brotli in our storage pipeline: 1. We need to be able to rapidly ingest bytes in the Brotli format, meaning we need compression that runs significantly faster than line speed. 2. We need to be able to decompress any stream of bytes safely and repeatably.The main challenge for compression on external traffic is the tradeoff between compression speed and compression ratio. On average, each file in Dropbox is written once and read just a handful of times in its lifetime. This means that if our desktop client or mobile client can comfortably compress the data faster than line speed, we save our users’ bandwidth and reduce the time to sync documents.Unfortunately, with the default settings, which use the Zopfli algorithm, Brotli can’t quite keep up with faster internet connections. For instance, a fast internet connection can upload several megabytes per second, but Brotli may require up to 20 seconds to compress just 4 megabytes of data. As an alternative to the Zopfli compression, using a greedy algorithm like gzip -9 to do the compression can waste up to 10% of the space but can keep up with almost any line speed.To understand tradeoffs in making a Brotli compressor, it will help to give an overview of Brotli. Brotli files have a metablock header which encodes a set of named Huffman tables. Each Huffman table describes shorthand for bytes that may appear in a file. For example, in a Huffman table used to compress an English text file, commonly used letter like a e i o and u would probably be described with fewer bits than z or x, and the bytes describing the letter ø might not have an entry in the table at all.Here is an example Huffman table that might be helpful in encoding text with lots of h, u and m characters as in the long, Hawaiian word humuhumunukunukuapua'a:What you see is that the most common letter in the data we are trying to encode, u, is represented high up in the tree with a one-bit code of 0. H, the second most common, is one node down with a two-bit code 10. In this way, the more common letters are represented by fewer bits than the less common ones.After the metablock header, the rest of the file is simply a repeated set of command blocks. Each command block consists of three things:1. the index of the Huffman table to use, 2. the amount of data to copy from other parts of the file and where to find that data, 3. new data encoded using the selected Huffman table.The challenge for the compressor is to figure out when to start a new command block and switch Huffman tables, versus chugging along with the current command block. Each switch costs precious bits, so the tradeoff is a complex optimization. In the greedy mode, Brotli does some trial block splits, then merges with the last or second-to-last block only if the file size would go down. In Zopfli mode, Brotli tries to find the optimal block splits and uses those for the final file.Here is an example of encoding a long word using the Huffman table above as Table A, with a choice of using a new Huffman Table B.Our insight was a simple one: optimal may be the enemy of good. In working with compressed data, we noticed that often several distinct ways of representing the data had very similar sizes.Our approach was simply to exclude 95% of the possible splits from the search. This still allowed for an occasional “creative” split to happen, since we were not restricted to a purely greedy approach, but it avoided most of the overhead with the search approach.The approach adds an additional 0.45% to the file size when it misses a few optimal block splits, but the compression speed more than doubles, bringing it in line with upload speeds. Including videos and photographs that comprise the majority of Dropbox storage, the final percent savings in bandwidth with the modified Brotli compressor is 4.4%.As an added bonus, the Brotli Weissman score goes up by 6.5%. I’m sure Richard Hendricks would be gleeful!Once the files have been uploaded, they need to be durably persisted as long as the user wishes, and at a moment’s notice they may need to be restored to their original bits exactly in a repeatable, secure way.For Dropbox, any decompressor must exhibit three properties:1. it must be safe and secure, even against bytes crafted by modified or hostile clients, 2. it must be deterministic—the same bytes must result in the same output, 3. it must be fast.With these properties we can accept any arbitrary bytes from a client and have full knowledge that those bytes factually represent the file data.Unfortunately, the compressor supplied by the Brotli project only has the third property: it is very fast. Since the Brotli decompressor consists of a substantial amount of C code written by human beings, it is possibly neither deterministic nor safe and secure against carefully crafted hostile data. It could be both secure and deterministic, but there is simply too much code to reason through a mathematical proof of this hypothesis.Operating at Dropbox scale, we need to guarantee the security of our data, so our approach was to break down the problem into components. By writing a new Brotli decompressor in a language that is safe and deterministic, we only needed to analyze the language, not all the code written in it. This is because such a language would prevent us from executing unsafe code (eg. array out of bounds access) or nondeterministic code (eg reading uninitialized memory), so therefore we can trust the code to repeatably produce the same output without any security risks.The Rust programing language fits the bill perfectly: it’s a language that promises memory safety without garbage collection, concurrency without data races, and abstractions without overhead. It also has sufficient performance for our needs. That means that code written in Rust has the same memory requirements as the equivalent code written in C. At Dropbox, many of our services are actually memory bound, so this is a key advantage over a garbage collected language.We created rust-brotli, a direct port of the C decompressor into safe Rust. We also went one step further and wrote our own Rust memory allocator that can be used to allocate memory in the standard way using Boxes, or from a fixed size allocation on the heap, or even a pool on the stack.This allows us to put an upper bound on the memory we would allow for the decode of a single 4MB block. After the virtual memory is allocated, we enable a timer using the alarm syscall, to avoid a runaway process that never returns control. Finally, we enter the process into the secure computing (SECCOMP) mode, disabling any system calls except for read, write, sigreturn and exit.Even if there were a hypothetical gap in the safety of the Rust runtime, no hostile process could escape the sandbox and do harm to any Dropbox servers because system calls are blocked by the kernel’s SECCOMP filter before the call is ever executed.Porting Brotli from C to Rust required a couple of steps to ensure it met our goals:1. A foundational memory manager with a malloc-like interface needed to be created to abstract away any stdlib access. This ensured we could add SECCOMP support later and could operate without the stdlib. We created rust-alloc-no-stdlib for this purpose. 2. The bit_reader and Huffman modules were ported directly using the provided memory manager. Unit tests were created by running the C Brotli decompressor on various inputs and recording all data that entered or left the bit_reader or Huffman modules respectively. This made sure that all modules were dependable and could be expected to be correct when debugging the main decoder. 3. The BrotliState continuation structure was ported and lifetimes were mapped. This required discerning which pointers were statically sized data, owned data, or aliases to other data. 4. The main decode loop was ported, with stubbed out helpers. A common pattern in the decode.c was to pass the state struct into helpers alongside a bit_reader or a pointer to a Huffman table. The main loop had to be modified to pull those structures out of the main state struct. Sometimes the state struct could be broken down into related modules that could be independently borrowed by helper functions. 5. The control flow of the decode loop had to be modified to remove gotos and fallthroughs in case statements. 6. Integration tests needed to pass. The major bugs encountered centered around a handful of negative array indices being used to access slices of referenced arrays, the -- operator being interpreted as a no-op (repeated negation) rather than predecrement, and an errors where C performed a shift operation on an integer rather than a byte.So given the safety, determinism and correctness, the final cornerstone is the speed of the Rust-based decompressor. Currently the decompressor runs at 72% of the speed of the vanilla -O3 optimized Brotli decompressor in gcc-4.9 for decompressing 4 megabyte blocks of data. This means it is able to safely decompress Brotli data at 217 MB/s on a Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz.Where is the 28% of the time going?a) Zeroing memory: Telling the Rust allocator to avoid zeroing the memory when allocating for Brotli improves the speed to 224 MB/s.b) Bounds checks: decompressing a Huffman coded file with backwards references requires a lot of table lookups.Sometimes code can have unexpected bounds checks:The above code counter-intuitively requires not one, but two array bounds checks.The reason for this is that offset + 1 may inadvertently wrap in release mode (it would panic in debug). This could cause the first overflow check to succeed where the second could fail.The fix in this case could be to restrict the range of the index variable so it will not wrap:On many machines, the usize would give 64 bits of computational range to the addition. On those machines, the Rust compiler is intelligent enough to only insert a single bounds check in the above function since the 64 bit add will not wrap in any case where two 32 bit integers are added.However, many of the bounds checks cannot be elided like one of the two in the above example. To measure their effect, we made a macro fast!((array)[index]) to toggle between the safe operator[] on slices or else the unsafe get_unchecked() method, depending on whether a --features=unsafe flag was passed to the rust-brotli build. Activating unsafe mode results in another gain, bringing the total speed up to 249MB/s, bringing Brotli to within 82% of the C code.c) Offsets: The Brotli C code caches direct pointers to buffers and passes those to helper functions or saves them for future Brotli decode calls. Sometimes it even uses negative offsets on those pointers. Rust discourages a class of aliasing errors by enforcing that a single mutable pointer to a given object exists at a time and bars negative slice indices altogether. To operate properly, the Rust decompressor has to track the base slice plus an offset to avoid any negative array accesses and also to avoid two mutable borrows of a single buffer.d) Control flow: the original codebase had a number of gotos and case statements that would explicitly fall through. Neither of these features are present in Rust and they were emulated using a while loop encompassing a match statement, so that a fallthrough could be translated into a continue, and a break retains the semantics of a C break.These slight performance tradeoffs are easily mitigated by the fact that Brotli decompression actually is a fully streamable. That means that irrespective of the total time of the decompression, it’s possible to simply decompress the stream of bytes as they are downloaded and to overlap the cost of decompression with the byte download. Thus, only the first or last packets add to the time of the complete download. And since the data can traverse the network compressed, fewer bytes have to be downloaded, resulting in users being able to sync their Dropbox faster.The security and safety of Rust in a SECCOMP sandbox without the overhead of a garbage collector is a big win. It is worth the minor computational overhead of decompressing in a safe language with bounds checked arrays and determinism guarantees.Sure, that was a mouthful. But as they say on Silicon Valley, “most importantly we’re making the world a better place through constructing elegant hierarchies for maximum code reuse and extensibility.” More Pied Piper inside jokes and updates coming soon!",https://blogs.dropbox.com/tech/2016/06/lossless-compression-with-brotli/,0,dropbox,"backend,html,frontend,java,php,python,css,cloud,docker",NULL,2016-06-29
Going deeper with Project Infinite,"Last month at Dropbox Open London, we unveiled a new technology preview: Project Infinite. Project Infinite is designed to enable you to access all of the content in your Dropbox—no matter how small the hard disk on your machine or how much stuff you have in your Dropbox. Today, we’d like to tell you more—from a technical perspective—about what this evolution means for the Dropbox desktop client.Traditionally, Dropbox operated entirely in user space as a program just like any other on your machine. With Dropbox Infinite, we’re going deeper: into the kernel—the core of the operating system. With Project Infinite, Dropbox is evolving from a process that passively watches what happens on your local disk to one that actively plays a role in your filesystem. We have invested the better part of two years making all the pieces fit together seamlessly. This post is a glimpse into our journey.Our earlier prototypes around solving the “limited disk-space problem” used something called FUSE or Filesystems in Userspace. FUSE is a software interface that lets non-privileged users create their own filesystems without needing to write a kernel extension. It is part of the kernel itself on some Unix-like operating systems and OS X has a port that is available as a dedicated kernel extension and a libfuse library that needs to be linked by a program in user space.We take security seriously. We do everything we can to protect our users and their data. This includes having internal Red Teams, running a bug-bounty program, and hiring external pen-testers on a regular basis to help us discover vulnerabilities in our products.As we’ve been building out our kernel extension, we have also begun to look at what other long-standing user problems we can solve. It turns out there’s a lot we can do.We’ve seen the number of companies that rely on Dropbox Business soar past 150,000 since we launched it just three years ago. With so many teams on Dropbox, we increasingly hear about a scenario we call the “untrained intern problem.” Imagine you are working with a bunch of other people on a project and collaborating through a Team folder on Dropbox. Summer is quickly approaching and you’ve brought on an intern. The intern, never having used Dropbox before, moves a folder from inside their Team folder to their Desktop, not realizing that they’ve simultaneously removed access to this folder for everyone else in the company. Now of course this folder could be restored, but don’t you wish there was a better way so this could have been prevented from even happening?So if you’re someone who compulsively monitors the list of loaded kernel extensions on your system (there are dozens of us, dozens!) and you see com.getdropbox.dropbox.kext  you now know why!We wanted to address some comments about Project Infinite and the kernel. It’s important to understand that many pieces of everyday software load components in the kernel, from simple device drivers for your mouse to highly complex anti-virus programs. We approach the kernel with extreme caution and respect. Because the kernel connects applications to the physical memory, CPU, and external devices, any bug introduced to the kernel can adversely affect the whole machine. We’ve been running this kernel extension internally at Dropbox for almost a year and have battle-tested its stability and integrity. File systems exist in the kernel, so if you are going to extend the file system itself, you need to interface with the kernel. In order to innovate on the user’s experience of the file system, as we are with Project Infinite, we need to catch file operation events on Dropbox files before other applications try to act on those files. After careful design and consideration, we concluded that this kernel extension is the smallest and therefore most secure surface through which we can deliver Project Infinite. By focusing exclusively on Dropbox file actions in the kernel, we can ensure the best combination of privacy and usability.We understand the concerns around this type of implementation, and our solution takes into consideration the security and stability of our users’ experience, while providing what we believe will be a really useful feature.",https://blogs.dropbox.com/tech/2016/05/going-deeper-with-project-infinite/,0,dropbox,"javascript,frontend,css,react",NULL,2016-05-24
Enabling HTTP/2 for Dropbox web services: experiences and observations,"At Dropbox, our traffic team recently upgraded the front-end Nginx servers to enable HTTP/2 for our web services. In this article, we would like to share our experiences and findings during the HTTP/2 transition. The overall upgrade was smooth for us, although there are also a couple of caveats that might be helpful to others.HTTP/2 (RFC 7540) is the new major version of the HTTP protocol. It is based on SPDY and provides several performance optimizations compared to HTTP/1.1. These optimizations include more efficient header compression, server push, stream multiplexing over the same connection, etc. As of today, HTTP/2 is supported by major browsers.Dropbox uses the open-source Nginx to terminate SSL connections and perform layer-7 load balancing for web traffic. Before the upgrade, our front-end servers ran Nginx 1.7-based software and supported SPDY. Another motivation for the upgrade is that Chrome currently supports SPDY and HTTP/2 but they will be dropping SPDY support on May 15th. If we don’t support HTTP/2 at that time, our Chrome clients would go from using SPDY back to HTTP/1.1.The HTTP/2 upgrade was a straightforward and smooth transition for us. Nginx 1.9.5 added the HTTP/2 module (co-sponsored by Dropbox) and dropped SPDY support by default. In our case, we decided to upgrade to Nginx 1.9.15, which was then the latest stable version.The Nginx upgrade involves making simple changes in configuration files. To enable HTTP/2, the http2 modifier needs to be added to the listen directive. In our case, because SPDY was previously enabled, we simply replaced spdy with http2.Before  (SPDY): listen A.B.C.D:443 ssl spdy; After (HTTP/2): listen A.B.C.D:443 ssl http2;Of course, you probably want to go through the complete Nginx HTTP/2 configuration options to optimize for the specific use cases.As for deployments, we first enabled HTTP/2 on canary machines for about a week while we were still using SPDY in production. After verifying the correctness and evaluating the performance, HTTP/2 was enabled across the fleet for our web services.The figure above shows the smooth transition from SPDY to HTTP/2. The remaining HTTP/1.1 connections are not shown in this figure. We gradually enabled HTTP/2 across all front-end web servers around minute 23, 36, and 50. Before that, the connections include both HTTP/2 traffic in the canary machines and SPDY traffic in production machines. As you can see, roughly all the SPDY clients eventually migrated to HTTP/2.We have closely monitored the performance after we enabled HTTP/2 on canary machines. Our observations include performance data that demonstrates the effectiveness of HTTP/2 as well as a couple of caveats as most HTTP/2 implementations are still relatively new.We have seen a significant reduction in the ingress traffic bandwidth, which is due to more efficient header compression (HPACK).The figure above shows the ratio of average (per machine) traffic bandwidth between the canary and production machines, where HTTP/2 was enabled only on canary machines. Every canary or production machine received approximately the same amount of traffic from load balancers. As can be seen, the ingress traffic bandwidth was reduced significantly (close to 50%) after we enabled HTTP/2. It is worth noting that although we enabled SPDY previously in all canary and production machines, we did not turn on SPDY header compression due to the related security issues (CVE-2012-4929 aka CRIME). As for egress traffic, there was no significant change because headers typically contributed to a small fraction of the response traffic.Update: the issues with the increased POST request latency and refused stream errors were resolved in the recent Nginx 1.11.0 release. The figure of the updated P50 request latency after applying this change is at the end of this post.Increased latency for POST requests. When we enabled HTTP/2 on the canary machines, we noticed an increase in median latency. The figure below shows the ratio of P50 request latencies between canary and production machines. We investigated this issue and found that the increased latency was introduced by POST requests. After further study, this behavior appeared to be due to the specific implementation in Nginx 1.9.15. Related discussions can be found in the Nginx mailing list thread.Note that the increased P50 request latency ratio we see here (approximately 1.5x) depends on the specific traffic workload. In most cases, the overhead was about one additional round trip time for us, and it did not impact our key performance much. However, if your workload consists of many small and latency-sensitive POST requests, then the increased latency is an important factor to consider when upgrading to Nginx 1.9.15.Be careful with enabling HTTP/2 for everything, especially when you do not control the clients. As HTTP/2 is still relatively new, from our experience, some clients/libraries and server implementations are not fully compatible yet. For example:Because our API users may employ various of third-party HTTP libraries, we need to perform more extensive testing before enabling HTTP/2 support for our APIs.CloudFlare has presented a nice summary of HTTP/2 debugging tools. In addition, we found the Chrome net-internals tool (available at chrome://net-internals/#http2 in Chrome) to be helpful. The figure below is a screenshot of frame exchanges reported by net-internals when opening a new HTTP/2 session to www.dropbox.com. Overall, we made a smooth transition to HTTP/2. The following are a few takeaways from this post.We hope this post is helpful for those who are interested in enabling HTTP/2 for their services or those interested in networking in general. We would also like to hear your feedback in the comments below.Update: the issues with the increased POST request latency and refused stream errors were resolved in the recent Nginx 1.11.0 release. The figure below shows that the P50 request latency ratio (canary vs. production) decreased after applying the change in canary machines. Note that in this figure, the P50 request latency in production was increased previously when we upgraded to nginx 1.9.15 to support HTTP/2. Contributors: Alexey Ivanov, Dmitry Kopytkov, Dzmitry Markovich, Eduard Snesarev, Haowei Yuan, and Kannan Goundan",https://blogs.dropbox.com/tech/2016/05/enabling-http2-for-dropbox-web-services-experiences-and-observations/,0,dropbox,"backend,database,django,php,mysql,python,docker",NULL,2016-05-11
Inside the Magic Pocket,"We’ve received a lot of positive feedback since announcing Magic Pocket, our in-house multi-exabyte storage system. We’re going to follow that announcement with a series of technical blog posts that offer a look behind the scenes at interesting aspects of the system, including our protection mechanisms, operational tooling, and innovations on the boundary between hardware and software. But first, we’ll need some context: in this post, we’ll give a high level architectural overview of Magic Pocket and the criteria it was designed to meet.As we explained in our introductory post, Dropbox stores two kinds of data: file content and metadata about files and users. Magic Pocket is the system we use to store the file content. These files are split up into blocks, replicated for durability, and distributed across our infrastructure in multiple geographic regions.Magic Pocket is based on a rather simple set of core protocols, but it’s also a big, complicated system, so we’ll necessarily need to gloss over some details. Feel free to add feedback in the comments below; we’ll do our best to delve further in future posts.Note: Internally we just call the system “MP” so that we don’t have to feel silly saying the word “Magic” all the time. We’ll do that in this post as well.Magic Pocket is an immutable block storage system. It stores encrypted chunks of files up to 4 megabytes in size, and once a block is written to the system it never changes. Immutability makes our lives a lot easier.When a user makes changes to a file on Dropbox we record all of the alterations in a separate system called FileJournal. This enables us to have the simplicity of storing immutable blocks while moving the logic that supports mutability higher up in the stack. There are plenty of large-scale storage systems that provide native support for mutable blocks, but they’re typically based on immutable storage primitives once you get down to the lower layers. Dropbox has a lot of data and a high degree of temporal locality. Much of that data is accessed very frequently within an hour of being uploaded and increasingly less frequently afterwards. This pattern makes sense: our users collaborate heavily within Dropbox, so a file is likely to be synced to other devices soon after upload. But we still need reliably fast access: you probably don’t look at your tax records from 1997 too often, but when you do, you want them immediately. We have a fairly “cold” storage system but with the requirement of low-latency reads for all blocks. To tackle this workload, we’ve built a system based on spinning media (a fancy way of saying “hard drives”), which has the advantage of being durable, cheap, storage-dense and fairly low latency—we save the solid-state drives (SSDs) for our databases and caches. We use a high degree of initial replication and caching for recent uploads, alongside a more efficient storage encoding for the rest of our data.Durability is non-negotiable in Magic Pocket. Our theoretical durability has to be effectively infinite, to the point where loss due to an apocalyptic asteroid impact is more likely than random disk failures—at that stage, we’ll probably have bigger problems to worry about. This data is erasure-coded for efficiency and stored across multiple geographic regions with a wide degree of replication to ensure protection against calamities and natural disasters.As an engineer, this is the fun part. Magic Pocket had to grow from our initial double-digit-petabyte prototypes to a multi-exabyte behemoth within the span of around 6 months—a fairly unprecedented transition. This required us to spend a lot of time thinking, designing, and prototyping to eliminate the bottlenecks we could foresee. This process also helped us to ensure that the architecture was sufficiently extensible, so we could change it as unforeseen requirements arose.There were plenty of examples of unforeseen requirements along the way. In one case, traffic grew suddenly and we started saturating the routers between our network clusters. This required us to change our data placement algorithms and our request routing to better reflect cluster affinity (along with available storage capacity, cluster growth schedules, etc) and eventually to change our inter-cluster network architecture altogether.As engineers we know that complexity is usually antithetical to reliability. Many of us have spent enough time writing complex consensus protocols to know that spending all day reimplementing Paxos is usually a bad idea. MP eschews quorum-style consensus or distributed coordination as much as possible, and heavily leverages points of centralized coordination when performed in a fault-tolerant and scalable manner.  There were times when we could have opted for a distributed hash table or trie for our Block Index and instead just opted for a giant sharded MySQL cluster; this turned out to be a really great decision in terms of simplifying development and minimizing unknowns.Before we get to the architecture itself, first let’s work out what we’re storing.MP stores blocks, which are opaque chunks of files, up to 4MB in size:These blocks are compressed and encrypted and then passed to MP for storage. Each block needs a key or name, which for most of our use-cases is a SHA-256 hash of the block.4MB is a pretty small amount of data in a multi-exabyte storage system however and too small a unit of granularity to move around whenever we need to replace a disk or erasure code some data. To make this problem tractable, we aggregate these blocks into 1GB logical storage containers called buckets. The blocks within a given bucket don’t necessarily have anything in common; they’re just blocks that happened to be uploaded around the same time.Buckets need to be replicated across multiple physical machines for reliability. Recently uploaded blocks get replicated directly onto multiple machines, and then eventually the buckets containing the blocks are aggregated together and erasure coded for storage efficiency. We use the term volume to refer to one or more buckets replicated onto a set of physical storage nodes.To summarize: A block, identified by its hash, gets written to a bucket. Each bucket is stored in a volume across multiple machines, in either replicated or erasure coded form.So now that we know our requirements and data model, what does Magic Pocket actually look like? Well, something like this:That might not look like much, but it’s important. MP is a multi-zone architecture, with server clusters in western, central and eastern United States. Each block in MP is stored independently in at least two separate zones and then replicated reliably within these zones. This redundancy is great for avoiding natural disasters and large-scale outages but also allows us to establish very clear administrative domains and abstraction boundaries to avoid a misconfiguration or congestion collapse from cascading across zones.[We have some extensions in the works for less-frequently accessed (“colder”) data that adopts a different multi-zone architecture than this.]Most of the magic happens inside a zone however, so let’s dive in:We’ll go through these components one by one.These nodes accept storage requests from outside the system, and are the gateway to Magic Pocket. They determine where a block should be stored and issue commands inside MP to read or write the block.This is the service that maps each block to the bucket where it’s stored. You can think of this as a giant database with the following schema:(Our real schema is a little more complicated than this to support things like deletes, cross-zone replication, etc.)The Block Index is a giant sharded MySQL cluster, fronted by an RPC service layer, plus a lot of tooling for database operations and reliability. We’d originally planned on building a dedicated key-value store for this purpose but MySQL turned out to be more than capable. We already had thousands of database nodes in service across the Dropbox stack, so this allowed us to leverage the operational competency we’ve built up around managing MySQL at scale.We might build a more sophisticated system eventually, but we’re happy with this for now. Key-value stores are fashionable and offer high performance, but databases are highly reliable and provide an expressive data model which has allowed us to easily expand our schema and functionality over time.The cross-zone replication daemon is responsible for asynchronously replicating all block puts from one zone to the other. We write each block to a remote zone within one second of it being uploaded locally. We factor this replication delay into our durability models and ensure that the data is replicated sufficiently widely in the local zone.Cells are self-contained logical storage clusters that store around 50PB of raw data. Whenever we want to add more space to MP we typically bring up a new cell. While the cells are completely logically independent, we stripe each cell across our racks to ensure maximal physical diversity within a cell.Let’s dive inside a cell to see how it works:The most important characters in a cell are the OSDs, storage boxes full of disks that can store over a petabyte of data in a single machine, or over 8 PB per rack. There’s some very complex logic on these devices for managing caching, disk scheduling, and data validation, but from the perspective of the rest of the system these are “dumb” nodes: they store blocks but don’t understand the cell topology or participate in distributed protocols.The Replication Table is the index into the cell which maps each logical bucket of data to the volume and OSDs that bucket is stored on. Like the Block Index, the Replication Table is stored as a MySQL database but is much smaller and updated far less frequently. The working set for the Replication Table fits entirely in memory on these databases which gives us very high read throughput on a small number of physical machines.The schema on the Replication Table looks something like this:One important concept here is the open flag, which dictates whether the volume is “open” or “closed”. An open volume is open for writing new data but nothing else. A closed volume is immutable and may be safely moved around the cell. Only a small number of volumes are open at any point in time.The type  specifies the type of volume: a replicated volume or encoded with one of our erasure coding schemes. The generation number is used to ensure consistency when moving volumes around to recover from a disk failure or to optimize storage layout.The Master is best thought of as the janitor or coordinator for the cell. It contains most of the complex protocol logic in the system, and its main job is to watch the OSDs and trigger data repair operations whenever one fails. It also coordinates background operations like creating new storage buckets when they get full, triggering garbage collection when data is deleted, or merging buckets together when they become too small after garbage collection.The Replication Table stores the authoritative volume state so that the Master itself is entirely soft-state. Note that the Master is not on the data plane: no live traffic flows through it, and the cell can continue to serve reads if the Master is down. The cell can even receive writes without the Master, although it will eventually run out of available storage buckets without the Master creating new ones as they fill up. There are always plenty of other cells to write to if the Master isn’t around to create these new buckets.We run a single Master per cell, which provides us a centralized point of coordination for complex data-placement decisions without the significant complexity of a distributed protocol. This centralized model does impose a limit on the size of each cell: we can support around a hundred petabytes before the memory and CPU overhead becomes a bottleneck. Fortunately, having multiple cells also happens to be very convenient from a deployment perspective and provides greater isolation to avoid cascading failures.The Volume Managers are the heavy lifters of the cell. They respond to requests from the Master to move volumes around, or to erasure code volumes. This typically means reading from a bunch of OSDs, writing to other OSDs, and then handing control back to the Master to complete the operation.The Volume Manager processes run on the same physical hardware as the OSDs since this allows us to amortize their heavy network-capacity demands across idle storage hardware in the cell.Phew! You’ve made it this far, and hopefully have a reasonable understanding of the high-level Magic Pocket architecture. We’ll wrap up with a very cursory overview of some core MP protocols, which we can expound upon in future posts. Fortunately these protocols are already quite simple.The Frontends are armed with a few pieces of information in advance of receiving a Put request: they periodically contact each cell to determine how much available space it has, along with a list of open volumes that can receive new writes.When a Put request arrives, the Frontend first checks if the block already exists (via the Block Index) and then chooses a target volume to store the block. The volume is chosen from the cells in such a way as to evenly distribute cell load and minimize network traffic between storage clusters. The Frontend then consults the Replication Table to determine the OSDs that are currently storing the volume.The Frontend issues store commands to these OSDs, which all fsync the blocks to disk (or on-board SSD) before responding. If this was successful then the Frontend adds a new entry to the Block Index and can return successfully to the client. If any OSDs fail along the way then the Frontend just retries with another volume, potentially in another cell. If the Block Index fails then the Frontend forwards the request to the other zone. The Master periodically runs background tasks to clean up from any partial writes for failed operations.There are some subtle details behind the scenes, but ultimately it’s rather simple. If we adopted a quorum-based protocol where the Frontend was only required to write to a subset of the OSDs in a volume then we would avoid some of these retries and potentially achieve lower tail latency but at the expense of greater complexity. Judicious management of timeouts in a retry-based scheme already results in low tail latencies and gives us performance that we’re very happy with.Once we know the Put protocol, the process for serving a Get should be self-explanatory. The Frontend looks up the cell and bucket from the Block Index, then looks up the volume and OSDs from the Replication Table, and then fetches the block from one of the OSDs, retrying if one is unavailable.As mentioned, we store both replicated data and erasure coded data in MP. Reading from a replicated volume is easy because each OSD in the volume stores all the blocks.Reading from an erasure coded volume can be a little more tricky. We encode in such a way that each block can be read in entirety from a single given OSD, so most reads only hit a single disk spindle; this is important in reducing load on our hardware. If that OSD is unavailable then the Frontend needs to reconstruct the block by reading encoded data from the other OSDs. It performs this reconstruction with the aid of the Volume Manager.In the encoding scheme above, the Frontend can read Block A from OSD 1, highlighted in green. If that read fails it can reconstruct Block A by reading from a sufficient number of blocks on the other OSDs, highlighted in red. Our actual encoding is a little more complicated than this and is optimized to allow reconstruction from a smaller subset of OSDs under most failure scenarios.The Master runs a number of different protocols to manage the volumes in a cell and to clean up after failed operations. But the most important operation the Master performs is Repair.Repair is the operation used to re-replicate volumes whenever a disk fails. The Master continually monitors OSD health via our service discovery system and triggers a repair operation once an OSD has been offline for 15 minutes — long enough to restart a node without triggering unnecessary repairs, but short enough to provide rapid recovery and minimize any window of vulnerability.Volumes are spread somewhat-randomly throughout a cell, and each OSD holds several thousand volumes. This means that if we lose a single OSD we can reconstruct the full set of volumes from hundreds of other OSDs simultaneously:In the diagram above we’ve lost OSD 3, but can recover volumes A, B and C from OSDs 1, 2, 4 and 5. In practice there are thousands of volumes per OSD, and hundreds of other OSDs they share this data with. This allows us to amortize the reconstruction traffic across hundreds of network cards and thousands of disk spindles to minimize recovery time.The first thing the Master does when an OSD fails is to close all the volumes that were on that OSD and instruct the other OSDs to reflect this change locally. Now that the volumes are closed, we know that they won’t accept any future writes and are thus safe to move around.The Master then builds a reconstruction plan, where it chooses a set of OSDs to copy from and a set of OSDs to replicate to, in such a way as to evenly spread load across as many OSDs as possible. This step allows us to avoid traffic spikes on particular disks or machines. The reconstruction plan allows us to provision far fewer hardware resources per OSD, and would be difficult to produce without having the Master as a central point of coordination.We’ll gloss over the data transfer process, but it involves the volume managers copying data from the sources to the destinations, erasure coding where necessary, and then handing control back to the Master.The final step is fairly simple, but critical: At this point the volume exists on both the source and destination OSDs, but the move hasn’t been committed yet. If the Master fails at this point, the volume will just stay in the old location and get repaired again by the new Master. To commit the repair operation, the Master first increments the generation number on the volumes on the new OSDs, and then updates the Replication Table to store the new volume-to-OSD mapping with the new generation (the commit point). Now that we’ve incremented the generation number we know that there’ll be no confusion about which OSDs hold the volume, even if the failed OSD comes back to life.This protocol ensures that any node can fail at any time without leaving the system in an inconsistent state. We’ve seen all sorts of crazy stuff in production. In one instance, a database frontend froze for a full hour before springing back to life and forwarding a request to the Replication Table, during which time the Master had also failed and restarted, issuing an entirely different set of repair operations. Our consistency protocols need to be completely solid in the face of arbitrary failures like these. The Master also runs a number of other background processes such as Reconcile, which validates OSD state and rolls back failed repairs or incomplete operations.The open/closed volume model is key for ensuring that live traffic doesn’t interfere with background operations, and allows us to use far simpler consistency protocols than if we didn’t enforce this dichotomy.Thanks for making it this far! Hopefully this post gives some context for how Magic Pocket works and for some of our motivations.The primary design principle here is keep it simple! Designing a distributed storage system is a big challenge, but it’s much harder to build one that operates reliably at scale, and supports all of the monitoring and verification systems and tooling that will ensure it’s running correctly. It’s also incredibly important to make technical decisions that are the right solution to the right problem, not just because they’re cool and novel. Most of MP was built by a team of less than half a dozen people, which required us to focus on the things that mattered, and played a big part in the success of the project.There are obviously a lot of details that we’ve left out. (Just in case you’re about to respond, “Wait! this doesn’t work when X, Y and Z happens!”—we’ve thought about that, I promise.) Stay tuned for future blog posts where we’ll go into more detail on specific aspects about building and operating a system at this scale.",https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/,0,dropbox,"python,php",NULL,2016-05-06
Scaling to exabytes and beyond,"Years ago, we called Dropbox a “Magic Pocket” because it was designed to keep all your files in one convenient place. Dropbox has evolved from that simple beginning to become one of the most powerful and ubiquitous collaboration platforms in the world. And when our scale required building our own dedicated storage infrastructure, we named the project “Magic Pocket.” Two and a half years later, we’re excited to announce that we’re now storing and serving over 90% of our users’ data on our custom-built infrastructure.Dropbox was founded by engineers, and the ethos of technical innovation is fundamental to our culture. For our users, this means that we’ve created a product that just works. But there’s a lot that happens behind the scenes to create that simple user experience.We’ve grown enormously since launching in 2008, surpassing 500 million signups and 500 petabytes (i.e., 5 followed by 17 zeroes!) of user data. That’s almost 14,000 times the text of all the books in the Library of Congress. To give you a sense of the incredible growth we’ve experienced, we had only about 40 petabytes of user data when I joined in 2012. In the 4 years since, we’ve seen over 12x growth.Dropbox stores two kinds of data: file content and metadata about files and users. We’ve always had a hybrid cloud architecture, hosting metadata and our web servers in data centers we manage, and storing file content on Amazon. We were an early adopter of Amazon S3, which provided us with the ability to scale our operations rapidly and reliably. Amazon Web Services has, and continues to be, an invaluable partner—we couldn’t have grown as fast as we did without a service like AWS.As the needs of our users and customers kept growing, we decided to invest seriously in building our own in-house storage system. There were a couple reasons behind this decision. First, one of our key product differentiators is performance. Bringing storage in-house allows us to customize the entire stack end-to-end and improve performance for our particular use case. Second, as one of the world’s leading providers of cloud services, our use case for block storage is unique. We can leverage our scale and particular use case to customize both the hardware and software, resulting in better unit economics.We knew we’d be building one of only a handful of exabyte-scale storage systems in the world. It was clear to us from the beginning that we’d have to build everything from scratch, since there’s nothing in the open source community that’s proven to work reliably at our scale. Few companies in the world have the same requirements for scale of storage as we do. And even fewer have higher standards for safety and security. We built reliability and security into our design from the start, ensuring that the system stores the data in a safe and secure manner, and is highly available. The data is encrypted at rest, and the system is designed to provide annual data durability of over 99.9999999999%, and availability of over 99.99%.Code development: Magic Pocket became a major initiative in the summer of 2013. We’d built a small prototype as a proof of concept prior to this to get a sense of our workloads and file distributions. Software was a big part of the project, and we iterated on how to build this in production while validating rigorously at every stage. The innovation focused on creating a clean design that would scale from zero to one of the largest storage systems in the world, and automation that would allow our small team to maintain an enormous amount of hardware. We needed to test and audit the reliability of the system for the highest levels of data durability and availability.Dark launch: In August 2014, we embarked on our “dark launch,” at which point we were mirroring data between two regional locations and considered the system ready to store user data. Of course, being Dropbox, we kept extra backups for another 6 months after this date just in case.Launch day: February 27, 2015 was D-day. For the first time in the history of Dropbox, we began storing and serving user files exclusively in-house. Once we validated our new infrastructure, we set an aggressive goal of scaling the system to more than 500PB in six months.BASE Jump: On April 30, 2015, we began the race to install additional servers in three regional locations fast enough to keep up with the flow of data. To make this all work, we built a high-performance network from our servers which allowed us to transfer data at a peak rate of over half a terabit per second. Because our schedule left so little time to “open the parachute,” we called this part of the project BASE Jump. On the hardware side, we were pushing up against the limitations of how many racks of hardware could fit in the loading dock at one time.Successful landing: Our goal was to serve 90% of our data from in-house infrastructure by October 30, 2015. We actually hit the mark almost a month early, on October 7, 2015. The team not only delivered on time, but also achieved this significant technical undertaking without any major service disruptions or any loss of data.This is an exciting milestone and the start of more innovation to come. We’ll continue to invest in our own infrastructure as well as partner with Amazon where it makes sense for our users, particularly globally. Later this year, we’ll expand our relationship with AWS to store data in Germany for European business customers that request it. Protecting and preserving the data our users entrust us with is our top priority at all times.This is Dropbox engineering—always aiming higher!This is the first of a series of blog posts about the Magic Pocket. Over the next month we’ll share a lot of the technical details around what we learned from building our own high-performance cloud infrastructure. Stay tuned.",https://blogs.dropbox.com/tech/2016/03/magic-pocket-infrastructure/,0,dropbox,"frontend,animation,docker,python",NULL,2016-03-14
Open Sourcing Pytest Tools,"At Dropbox, we made the switch from testing with unittest to pytest. We love the features, fixtures, plugins, and customizability of pytest. To further improve our experience, we built a couple of tools (pytest-flakefinder, unittest2pytest) for working with pytest and released them as open source.We developed the pytest-flakefinder plugin to help with a common problem, flaky tests. Tests that involve multiple threads, or that depend on certain ordering can often fail at a fairly low rate. A few flaky tests aren’t a big deal, but with thousands of tests, they become a huge issue. We used to literally run pytest in a loop or sometimes just copy paste the test code multiple times to see if we could reproduce the failure. With this plugin we can easily have pytest run the test multiple times in a row. And if you combine it with -x --pdb you can just run it until it fails and get yourself a debugger ready to find out what happened. We now run flakefinder on updated tests in CI to detect flakiness proactively.Another one of the key pytest features we love is assertion rewriting. This pytest feature allows us to see significantly more detailed error messages when asserts fire. In order to take advantage of assert rewriting, the tests must use a raw assert a == b rather than the unittest library’s idiomatic self.assertEqual(a, b).Test output using unittest style assertions:pytest output with raw Python asserts:When we made the switch to pytest, we had thousands of existing tests generally using the latter form. Fortunately, pytest is compatible with unittest asserts, so our test suite still passed. However, we weren’t getting the benefits of assertion rewriting everywhere. On top of that, we had different testing practices in our codebase, leading to some confusion.We developed unittest2pytest to convert our existing unittest asserts to pytest rewrite-compatible raw asserts. It’s built on top of the lib2to3 library for automatic code rewriting. This library was able to safely convert most of our code automatically. There were a few hiccups with certain kinds of whitespace and inline commenting, which you can see in our issue reporter. unittest2pytest simply skips over converting things it doesn’t understand.At Dropbox, we developed pytest-flakefinder and unittest2pytest to improve our experience with pytest. These tools are both open source now, so check them out if you use pytest or are considering switching.",https://blogs.dropbox.com/tech/2016/03/open-sourcing-pytest-tools/,0,dropbox,"django,flask,python",NULL,2016-03-03
What do you mean ‘we need more time’??,"In tech, we spend little time talking about the softer skills like communication, project management, and prioritization. These are the skills that elevate someone from a good programmer to a great software engineer. Today, I’m going to focus on one aspect of project management that we’re famously bad at — the art of estimating a project schedule.If there’s any doubt that this is a necessary skill, just consider that dreaded but frequently-asked question “How long will it take?” Even if you’re uber-Agile and don’t believe in far-off project deadlines, rest assured that somebody will crack under the pressure and give a date, which your team will be held to. When that date hits and you are not ready to launch, your manager will be angry at you because you made her look like a fool, sales will be angry at you because they promised your most important customers that they could have it today, and your team will be angry at you because they’ve worked five weekends in a row trying to hit an impossible deadline. So let’s just dodge that whole mess and create a schedule that you can live up to.To demonstrate how this works, I’d like to try an exercise that I’ve shamelessly lifted from an Intro to Development class from Microsoft. The goal is to estimate how long it will take to paint a room. It’s a lesson in the challenges of estimation that doesn’t require specific knowledge about any particular software system.Take some time now before scrolling below to write down your estimate for how long it will take to paint a room. Don’t skip this part — it’s important to record your thoughts as you go to see how they evolve.Done?I hope not, because you barely even know the assignment yet! Start by demanding a spec and then asking some clarifying questions.Just goes to show that even this seemingly simple task can require a lot of up-front clarification. All right, let’s try this again. Write down your best estimate before reading on.Ready? Here we go.OK, let’s take a pause from painting for a minute and step back into the world of software engineering process to note some similarities.Some of these steps may seem silly. Double check the color? But failing to nail down all details of the spec before implementing is a very common mistake, and you might spend a lot of time making something nobody wants. The tiniest difference in the spec (“Oh, you wanted waterproof paint in a different gloss, and only on one wall? Well [a-z]{4}.”) can be very costly to fix afterwards, in this case almost doubling your initial estimate.Without research or a prototype, you could spend a lot of time going down rabbit holes. With a bit of digging, you might find that there’s already a framework that does exactly what you want. A basic prototype might then reveal that the framework documentation was lying or doesn’t cover one crucial scenario, and it doesn’t actually do what you want, and you’ll have to do it yourself after all! If your prototype was very cheap, you just saved yourself the wasted work of integrating the framework into your production system.Then, if you don’t break the tasks down into small enough pieces, you might fail to notice some very important points. For example, if you had forgotten about moving the furniture, you might not have had a second person on hand to move those giant bookshelves (good luck doing that on your own).  The more you break things down, the more you realize you overlooked.More importantly, the biggest factor in the accuracy of your estimate is whether or not you’ve done something like this before. Even with extensive research, it’s hard to know how many coats you’ll need in order to paint over that particular color, what your personal painting speed is, or how the humidity in your area affects drying time. In fact, if you’ve done this exact project before, you can skip steps 1 through 4 altogether. But if you haven’t, you were probably repeatedly surprised by steps you’d forgotten to account for, and the prep work will give you a far more accurate estimate than your initial guess. And it does mean that you won’t have a real estimate until you’re done with step 4. Anything you say before that will be a wild guess that you’ll probably have cause to regret later, so it’s safest to say “I don’t know but I can probably tell you in a few days” until then.OK, back to painting. We’ve costed it out and it’s about 12 hours. Are we done?Well…the paint and prime steps have way less detail than the other tasks. Face it, you still don’t actually know how that part will be done, so your estimates are wild guesses. Recursively applying the principles above, let’s break it down more.You’ll also realize that priming is not very different than painting. So double this estimate for priming.All tallied up, assuming one coat, that’s about 15 hours for the whole shebang. Phew, that’s a lot longer than we initially thought! And just to be safe, let’s leave a bit of buffer time for unexpected setbacks, like needing to jerry-rig a strainer to remove chunks from your paint. So we’ll make it an even 17 hours. Final answer, let’s get painting, right?Nope, still not there yet!Yeah, you’ve estimated how long it’ll take to paint the room. But that’s not what anyone wants to know. They want to know how long it’ll be before the room is painted. It’s a subtle but important distinction. When I ask about a bug, it’s nice to hear that you can code up a fix in an hour, but what I actually needed to know is that you won’t get around to it until next week, so it’ll be done in a week and an hour! The fact that I technically only asked how long the fix would take is something only an engineer would bother pointing out. -_-So what are we still missing in this estimate? Bathroom and meal breaks, random interruptions, and competing priorities. Your work could be delayed by all kinds of incidents, expected and unexpected. Maybe you had to stop early because it’s laundry day, or some emergency came up. How can you even factor in that kind of unpredictability?The answer is lots of buffering, based on past experience. You can track a multiplier for your estimates by logging how long it actually took you to do each task and comparing your initial schedule with the real time elapsed for the project. Since every project tends to be quite different, you won’t be able to reach fantastic accuracy by refining individual task estimates over time. But a multiplier applied to the whole project will account for everything from your natural optimism, to the fact that you have more meetings than you realized, to the time you spend procrastinating by surfing the internet.I won’t get into greater detail here, because there is already a terrific article on this subject by Joel Spolsky called Evidence Based Scheduling. While his method may sound time-intensive, tracking time sheets for just a couple of projects can help refine your estimates a lot. Like all skills worth improving, this one will take time and effort to hone.The above is all fine and dandy if you’re asked to give estimates. But as a software engineer, you’re often handed a schedule along with your project. Maybe the deadline was set by marketing because they want it to be available in time for Christmas, or by a manager who needs a date to coordinate with other teams who have their own deadlines. Or maybe they haven’t given you a schedule, but you can tell from the gleam in their eyes that they have Certain Expectations. The important thing is, if you think the dates are unrealistic, you need to speak up.Ideally, each engineer should each be able to estimate his or her part of the project from scratch, rather than anchoring on a schedule someone else has given. It can be easy to convince yourself that you can do a project in two weeks, or at least not question that number too much, if that’s how long your manager or tech lead says. Only when you sit down and do the real work of estimation will you find out how wrong it is.It’s really important to actually do that estimation due diligence early and discuss any unrealistic deadlines. Remember that when you push back against a bad schedule, you are not a Debbie Downer fighting against a magical world where you ship the project by Christmas and everything is awesome. That world doesn’t exist. You simply prefer a world where everyone compromises on the date and feature set and then meets those goals, rather than a world where the deadline gets missed, or gets met by last-minute corner cutting and quality sacrifices, with a generous side of reproach and finger pointing. If your lead or PM really isn’t buying it, maybe point them to the article on Evidence Based Scheduling (really I can’t recommend it enough).Yes, all this sounds like a lot of work. But I assure you, for any project sufficiently important, an accurate schedule will save you a lot of grief. Hopefully, you now have more tools in your toolbelt to create more accurate schedules for future projects!",https://blogs.dropbox.com/tech/2015/10/what-do-you-mean-you-need-more-time/,0,dropbox,,NULL,2015-10-25
Inside LAN Sync,"Dropbox LAN Sync is a feature that allows you to download files from other computers on your network, saving time and bandwidth compared to downloading them from Dropbox servers.Imagine that you are at home or at your office, and someone on the same network as you adds a file to a shared folder that you are a part of. Without LAN Sync, their computer would upload the file to Dropbox, and then you would download the file from Dropbox. With LAN Sync, you can download the file straight from their computer. Since you are on the same network, this transfer will be much faster and will save you bandwidth.As the number of companies and offices using Dropbox has increased, the use cases for LAN Sync have grown, and the feature was recently rewritten and improved. Here’s a look inside how it works.First, we need to talk about some of the abstractions that Dropbox uses internally, namely blocks and namespaces. If you’ve read other articles on the tech blog, you may already be familiar with these.Files in Dropbox are split up into 4MB blocks, where the last block is smaller when the file size is not evenly divisible by 4MB. Each block is addressed by the SHA-256 hash of its contents. A file, then, can be described by the list of hashes of blocks that make it up.Here, the file could be described as ‘h1,h2,h3,h4’, where h1, h2, etc. are the hashes for their respective blocks.Namespaces are the primitive behind Dropbox’s permissions model. They can be thought of as a directory with specific permissions. Every account has a namespace which represents its personal Dropbox account and all the files in it. In addition, shared folders are namespaces which multiple accounts can have access to.The key takeaway from the last section is that file download requests can be thought of a series of (hash, namespace) requests, indicating to download the given block, authenticated as the namespace. Without LAN Sync, these requests would be queued up and sent to the block server, which would return block data. This is a nice abstraction, because this way the block download pipeline does not need to know about filenames or how the blocks fit together.This also means that LAN Sync only syncs actual file data, and not file metadata, such as filenames. By ensuring that all metadata is received from Dropbox’s servers, we can ensure that everyone is always in a consistent state.With LAN Sync, we try to download blocks directly from peers on the LAN first, using block server only if that fails. Given a block and a namespace, the interface to LAN Sync either returns the block or an indicator that it was not found.ARCHITECTUREThere are three main components of the LAN Sync system that run on the desktop app: the discovery engine, the server, and the client. The discovery engine is responsible for finding machines on the network that we can sync with (i.e., machines which have access to namespaces in common with ours). The server handles requests from other machines on the network, serving requested block data. The client is responsible for trying to request blocks from the network.DISCOVERY ENGINEThe first challenge about LAN Sync is finding other machines on the LAN to sync with. To do this, each machine periodically sends and listens for UDP broadcast packets over port 17500 (which is reserved by IANA for LAN Sync). These packets contain:When a packet is seen, we add the IP address to a list for each namespace, indicating a potential target.PROTOCOLThe actual block transfer is done over HTTPS. Each computer runs an HTTPS server with endpoints of the form '/blocks/[namespace_id]/[block_hash]'. It supports the methods GET and HEAD. HEAD is used for checking if the block exists (200 means yes, 404 means no), and GET will actually retrieve the block. HEAD is useful in that it allows us to poll multiple peers to see if they have the block, but only download it from one of them.Because Dropbox aims to keep all of your data safe, we want to make sure that only clients authenticated for a given namespace can request blocks. We also want to make sure that computers cannot pretend to be servers for namespaces that they do not control. The concern here is not that they might try to give you bad data (we can check for that by ensuring that it hashes to the right thing) but rather that they might be able to learn something by watching which block hashes you request.The solution to this is to use SSL in a creative way. We generate SSL an key/certificate pairs for every namespace. These are distributed from Dropbox servers to user’s computers which are authenticated for the namespace. These are rotated any time membership changes e.g., when someone is removed from a shared folder. We can require both ends of the HTTPS connection to authenticate with the same certificate (the certificate for the namespace). This proves that both ends of the connection are authenticated.One interesting problem: when making a connection, how do we tell the server which namespace we are trying to connect for? For this, we use [Server Name Indication (SNI)], so that the server knows which certificate to use.Note that in this diagram, the HEAD request seems useless, but when there is more than one peer, we need to avoid downloading block data twice. Also, the key distribution happens when the computer comes online, not only when using LAN Sync.SERVER/CLIENTGiven this protocol, the server is not complicated. It just needs to know which blocks are present and where to find them.The client maintains a list of peers for each namespace (this list comes from the discovery engine). When the LAN Sync system gets a request to download a block, it sends a HEAD request to a random sample of the peers that it has discovered for the namespace, and then requests the block from the first one that responds saying it has the block.One important optimization to avoid the latency of an SSL handshake each time we need a block is to use connection pools to allow us to reuse already-started connections. We don’t open a connection until it is needed, and once it is open we keep it alive in case we need it again. Designing these pools was a good exercise in concurrency, since they needed to be able to give out connections or have them released back into the pool or be shut down when the connection died, all while being accessed from multiple threads.Of course, the most important thing is getting your files to you quickly, and we wouldn’t want a slow computer or connection on your network to slow things down. To this end, we have a fairly aggressive timeout on how long we are willing to wait before falling back to the block server. In addition, we limit the number of connections we are willing to make to any single peer, and how many peers we are willing to ask for a block. We made an effort to tune these parameters to work well, and they can be controlled by Dropbox’s servers if needed.Assuming that the block was found and downloaded successfully, we have successfully downloaded the block! Otherwise, we will try getting the block from Dropbox’s block server, as we normally do without LAN Sync.LAN Sync is currently a part of the Dropbox app! It can be toggled from the Network tab of the preferences page. Because it is designed to be transparent to the user, you may have not even noticed it while it was handling your syncing.If you want to force a LAN Sync, you’ll need two computers on the network with either the same account or a shared folder in common. Add a file to one of the computers, and the other computer should attempt a LAN Sync.If you enjoyed this post, follow our tech blog! And if you’re passionate about sync internals and performance you should check out our jobs page. We’re hiring.",https://blogs.dropbox.com/tech/2015/10/inside-lan-sync/,0,dropbox,"docker,python",NULL,2015-10-13
Open Sourcing Zulip – a Dropbox Hack Week Project,"This year’s Dropbox Hack Week saw some incredible projects take shape – from the talented team that visited Baltimore to research food deserts, to a project to recreate the fictional Pied Piper algorithm from HBO’s Silicon Valley. One of the most special elements of Hack Week, though, is that often times we’re able to share these exciting projects openly with our users and our community.At Dropbox, we love and depend on numerous excellent open source projects, and we consider contributing back to the open source community to be vitally important. Popular open source projects that Dropbox has released include the zxcvbn password strength estimator, the Djinni cross-language bridging library, the Hackpad codebase, and the Pyston JIT for Python.During this year’s Hack Week, we reassembled the original team from Zulip (a group chat application optimized for software development teams that was acquired by Dropbox in 2014) to tackle open sourcing Zulip on an ambitious timeline. Today, on behalf of the Zulip team, I’m very excited to announce that we have released Zulip as open source software!We took on this project during Hack Week in order to enable Zulip’s users to enjoy and improve a product they love. Zulip’s users are passionate about the product, and are eager to make their own improvements, and we’re excited to be able to offer them that opportunity. In particular, the Recurse Center has announced plans to work on the Zulip open source project.To make Zulip maximally useful to the world, we have released it under the Apache license, and we’ve released everything, including the server, Android and iOS mobile apps, desktop apps for Mac, Linux and Windows, and the complete Puppet configuration needed to run the Zulip server in production.The world of open source chat has for a long been dominated by IRC and XMPP, both of which are very old and haven’t advanced materially in the last decade. In comparison, Zulip starts with many useful features and integrations expected by software development teams today and has a well-engineered, maintainable codebase for those that are missing. We’re very excited to see what people build on top of Zulip.",https://blogs.dropbox.com/tech/2015/09/open-sourcing-zulip-a-dropbox-hack-week-project/,0,dropbox,"html,frontend,php,python,css,java",NULL,2015-09-25
[CSP] Third Party Integrations and Privilege Separation," This is the fourth of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring! We will also be at AppSec USA this week. Come say hi! In previous blog posts, we discussed our experience deploying CSP at Dropbox, with a particular focus on the script-src directive that allows us to control script sources.  With a locked down script-src whitelist, a nonce source, and mitigations to unsafe-eval, our CSP policy provided strong mitigations against XSS via injection attacks in our web application. In supported browsers, the only code allowed to run in our web application would be from our website, CDNs, and trusted third-party integrations. Unfortunately, deploying CSP with third-party integrations has its own risks and challenges. In this post, I will discuss how we handled these challenges using HTML5 privilege separation. For an example of a third-party integration code running on our website, consider the reactive chat widget from SnapEngage running on our business and marketing pages. The recommended mechanism for integrating with SnapEngage with a web application involves inserting the following code snippet into a script node in the HTML page: The above code creates a script node pointing to the SnapEngage library, sets up onload handlers, and inserts the node into the page. The loaded JavaScript library then creates the markup for showing the chat widget, the associated event handlers, and inserts them into the page.  There are two key problems with this approach. First, the chat widget markup and event handlers are not aware of CSP policy. So, depending on the widget’s use of eval and inline event handlers, our CSP policy breaks the chat widget. When we first deployed nonces for script-src on the Dropbox website, we had to disable it on pages relying on the widget (thus, increasing the XSS risk on the page).  A second, more subtle threat, is that this increases the trusted computing base of the Dropbox website to also include the SnapEngage servers. Recall that the same-origin policy means that all code running on www.dropbox.com origin gets the same privileges. Upcoming improvements to the web platform can mitigate this risk, but these are still under discussion at the W3C.  At Dropbox, we have been well aware of this risk and all our integration providers go through thorough reviews and mandatory security requirements. But, compromise of third-party providers is not unheard of and we deemed it an unacceptably high risk for the data that our customers trust us with.  One possible solution is to just copy the integration code to our servers and also modify the widget to not use inline event handlers. Unfortunately, this is an expensive and extremely intrusive solution. Worse, it means that any improvement to the widget must involve a manual check and code commit at Dropbox. Note that automating this has many of the same security concerns that we originally had.  Given these issues, we instead investigated privilege separation for mitigating the issues posed by such third party integration. The core idea of privilege separation is simple: instead of running third-party code directly in the Dropbox origin, we run it in an unprivileged origin and include it on our website via an iframe. Using postMessages between the iframes, the Dropbox origin provides a smaller, trusted API for ensuring the relevant functionality while not compromising security.  This is similar to the privilege separated architecture of OpenSSH and Google Chrome; the unprivileged child processes correspond to unprivileged origins while the privileged process corresponds to the privileged (Dropbox) origin. Instead of the IPC mechanisms used in binary applications, we use postMessage to communicate between iframes.  Concretely, lets do a deeper dive into how the SnapEngage integration looks with a privilege separated design. On loading a page that needs the SnapEngage chat widget, code on https://www.dropbox.com origin creates an iframe pointing to https://www.dbxsnapengage.com. The code on dbxsnapengage.com, in turn, loads the SnapEngage chat widget per the sample code above. The CSS on the iframe hides the border and the iframe’d chat widget looks exactly the same as it would have if included directly.  Of course, this is not sufficient. For maintaining the functionality, the chat widget needs to integrate with the main page. For example, we want to show the chat widget only when users click the “chat” button at the top of the page. Previously, the JavaScript code would listen for a click on the chat button and execute startSupportChat function. startSupportChat is a function that calls the relevant SnapEngage code that initiates the chat. Now that SnapEngage code is running on dbxsnapengage.com, this function does not do anything (nor does it exist on www.dropbox.com). Instead, we modify the code on www.dropbox.com to send a message to the iframe. This sends a message to the iframe on dbxsnapengage.com. This page, in turn, defines the following postMessage event handler that calls startSupportChat in turn. The end result is that clicking the “Chat” button shows the iframe and sends the chat widget code the appropriate message to start the chat. But, all the chat widget code is now running on the unprivileged dbxsnapengage.com origin.  Note that this is just an example: we use privilege separation in multiple places on the Dropbox website to reduce risk from third party integrations (e.g., our integration with our payments provider). Another subtle but important feature of this design is that SnapEngage did not have to make any changes and can continue using even inline event handlers. We own and operate the dbxsnapengage.com domain and also implemented the postMessage API that crosses the privilege boundary.   Special thanks to Mark Gilbert and Brad Stronger who took the lead on implementing this integration with SnapEngage. This wraps up our blog post series on CSP. If this sort of work interests you, we are hiring! Come talk to us at OWASP AppSec USA, if you are around! Cross shard transactions at 10 million requests per secondCrash reporting in desktop Python applicationsWhat we learned at our first JS Guild Summit",https://blogs.dropbox.com/tech/2015/09/csp-third-party-integrations-and-privilege-separation/,0,dropbox,,NULL,2015-09-24
[CSP] The Unexpected Eval," This is the third of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring! We will also be at AppSec USA this week. Come say hi! Previously, we discussed how at Dropbox we have deployed CSP at scale to protect against injection attacks. First, we discussed how we extract signal from violation reports to help create a host whitelist and restrict the sources of code running in our application. We also discussed how nonce sources allow us to mitigate XSS attacks due to content injections. Unfortunately, our CSP policy still has 'unsafe-eval'. This allows the use of string → code constructs like eval, new Function, setTimeout (with a string argument), and thus leaves an XSS risk open.Needless to say, this is not great. Unfortunately, due to wide use of  legacy JS templates in our client-side code, it is not easy to disable eval. While we migrate away from these legacy templates to React, we were also wondering what the exact risk from including unsafe-eval in a page’s CSP policy is, and how to mitigate it.At first glance, unsafe-eval does not seem like a terribly insecure directive. Unsafe eval only controls whether the browser allows 'eval' (and its variants like new Function); but if an attacker is able to call eval, the attacker has already achieved code execution and we have lost. An exploit would require the attacker to inject into strings that then flow into an eval “sink.” This is in contrast to unsafe-inline, which allows an attacker to convert a simple HTML injection vulnerability into a code injection vulnerability.Unfortunately, on further investigation, we realized that this simple reasoning was not true. The main reason for this was our use of libraries such as jQuery and (on legacy pages) Prototype. In fact, the presence of unsafe-eval, while using jQuery or Prototype, negated the advantages of removing 'unsafe-inline' from our policy. Lets dive into more detail for jQuery, but note that similar bugs exist in Prototype and possibly other libraries too.Consider the following two lines of HTML. At a glance, it seems like they will achieve the same result:With a CSP policy that disallows inline scripts, untrusted_input can have all the onclicks in the world, the browser will not execute them. This is true for both lines of code. But, if untrusted_input contains an inline script tag (e.g., alert(1)), the two lines of code diverge. In the first case, innerHTML does not support inline script tags and the alert will fail to execute. In the second case, jQuery will parse out the script tag, realize that directly setting untrusted_input via innerHTML won’t work. Instead, jQuery will parse out the contents of the script tag and directly eval the code inside the script tag. Worse, if untrusted_input is https://attacker.com/foo.js then jQuery will XHR that foo.js file and eval it (thus, even bypassing the content source restrictions on scripts). The code to do this is in the core domManip function in jQuery. The jQuery code calls this function for nearly all dom manipulation operations (insert, append, html, etc.)Another example of this is the jQuery.ajax function. At a glance, this function looks like a simple function to make XHR requests. Unfortunately, jQuery, by design, provides extra powers to its ajax function. In particular, if the response of an XHR request has the content-type script, jQuery will eval the response (Github issue). This means that any place where the attacker is able to control the target URI of an ajax call becomes a code injection vulnerability. In the presence of a CSP policy disallowing eval, the browser would block both these cases. Unfortunately, enabling such a policy was an expensive option for us. Instead, to mitigate this risk, we implemented a “security patch” on top of jQuery that blocks these unsafe behaviors. We are now happy to share our jQuery patches for securing against such ‘unexpected evals’. We hope that the broader community finds these patches useful. And, if you find other places needing a patch, please share them with us!There are two key components of the patch. First, we remove the implicit eval in ajax with the line below that replaces the default handler for script responses (set here in the jQuery code to an eval) with a no-op:Second, we override the default domManip with our own implementation that checks the script tag for the presence of the right nonce value before executing it. The patch just reimplements the domManip function (copied verbatim from jQuery) but the key patch is in line 183 of the domManip function:Another option is to just disable this behavior outright by deleting these lines or use something like jPurify that sanitizes all jQuery DOM operations. The important point here, though,  is that if you are deploying a CSP policy with ‘unsafe-eval’, it is important to mitigate this risk in some manner to protect against XSS attacks.As I mentioned earlier, we cannot remove unsafe-eval from our policy because our legacy code still requires the use of unsafe eval. In particular, we need unsafe-eval because of our use of JavaScript Microtemplates. The template library essentially evals (using new Function) the template (stored in a script tag marked with a content-type of ('text/template'). For example, here’s a template from John’s original blogpost:The templating code looks up the template using the id parameter and then calls (in essence) new Function on the contents of the script tag above. Unfortunately, this also means that an attacker can exploit an HTML injection vulnerability to insert a malicious template that is eval’ed by our template library. This is not great. To mitigate this risk, we inserted nonce attributes in all template script tags and modified the template library to also check the nonce attribute of template nodes. This is similar to how the browser checks the nonce attribute for script nodes.One issue we hit is that sometimes our client-side code downloads the template after the page load. Since the server-side generates a new nonce each time, the nonce on the template downloaded after page load would be different from the nonce for the main page. To mitigate this issue, we modified our code at the server side. Instead of creating a random nonce on each load, the script nonce for our pages is the hash of the CSRF tokens (note that the CSRF tokens are already random, unguessable values). This reduces the security of the nonce to the security of the CSRF token, but if an attacker knows the CSRF token, they can already take arbitrary actions for the user via CSRF attacks.Finally, this is another reminder that CSP is a mitigation and defense-in-depth feature and is not intended to be the first line of defense. The correct defense for XSS is to construct HTML securely in a framework that automatically escapes untrusted data followed by a good DOM sanitizer as a second layer of defense.Thanks to all the Dropboxers who helped explain the intricacies of the Dropbox website to me. Special thanks to David Goldstein who implemented the fix to jQuery.ajax. Up next, we will talk about issues with CSP and third-party integrations as well as risks associated with them.",https://blogs.dropbox.com/tech/2015/09/csp-the-unexpected-eval/,0,dropbox,"matlab,animation,docker,python,frontend",NULL,2015-09-23
[CSP] Unsafe-inline and nonce deployment," This is the second  of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring! We will also be at AppSec USA this week. Come say hi!  In the previous post, we discussed how to filter reports and deploy content source whitelists using CSP for the website. Typically, the most important content sources to whitelist are the source of your code, as defined by the script-src (and the object-src directive). A standard content-security-policy deployment will typically include a list of allowed domains like the main website and trusted CDNs in script-src as well as directives like 'unsafe-inline' and 'unsafe-eval'.  While this policy prevents arbitrary inclusion of third party scripts, this does not provide protection against XSS attacks due to the presence of the 'unsafe-inline' directive. If you are not familiar with the 'unsafe-inline' and nonce-src directives, please take a look at the CSP primer on html5rocks; but here’s a quick recap: by default, CSP blocks all inline script blocks ( tags) and inline event handlers (<div onclick=""somescript"">). This enforces code-data separation: all code running in the page has to come from script files in a whitelist of sources. This significantly reduce XSS risk, but it is difficult without a huge migration effort.  As a way to ease migration, the specification accepts a special source syntax for the script-src and style-src directives that allows inline content if it has a matching nonce attribute. Thus, a policy with nonce-randomnumber in the source list for script-src will allow script tags that have ""randomnumber"" as the value of their nonce attribute. The nonce syntax is part of CSP2 and is currently only supported by Chrome and Firefox. On other browsers, using inline script tags requires enabling all inline scripts via 'unsafe-inline'.  This post will discuss our experience deploying nonces on the Dropbox website. Before we go into details, let me stress that CSP is only a mitigation and is not a replacement for robust validation and sanitization. At Dropbox our preferred libraries are Pyxl and React at the server and client side respectively, while we use DOMPurify for client-side HTML sanitization.  Deploying script nonces involves two steps: including the right nonce with all inline script tags and removing all inline event handlers. At Dropbox, we use Pyxl for server-side HTML generation. Pyxl converts HTML tags into Python objects and automatically escapes untrusted data during serialization. For inserting script nonces, we modified the Pyxl serialization code to insert the right nonce element into the script tag.  The next step, removing inline event handlers, is difficult. For a while now, we have deprecated the use of inline event handlers and new code written at Dropbox does not use them. But this still left us with a big chunk of old code that had not been migrated. To ease the transition, we decided to automatically rewrite inline event handlers to use inline script tags instead. Essentially, the code <div id=""foo” onload=""somescript""> gets converted to:  A few things stand out in the example above. First, we use an immediately invoked function expression to not pollute the global namespace with our modifications. Second, we insert a script tag that adds this onload event handlers right after the original tag is opened, instead of after the original div element ends or after the DOMContentLoaded event. While the latter two are probably fine, the behavior above closely matches the browser’s original behavior, and thus, is apt for an automatic transform.  If you are wondering: yes, this transformation does not fix existing XSS attacks in the onload code and, thus, isn’t a completely secure transform. However, we deemed this an acceptable risk because systems like Pyxl have gotten reasonably good at identifying and preventing XSS in our server-side code. Further, over time, we plan to deprecate all inline event handlers which should take away this risk.  With this change, only inline scripts and event handlers we know about will execute. If an attacker manages to insert an event handler due to, say, a DOMXSS, browsers supporting the nonce attribute will block it.  Like all CSP deployments, the typical way to roll out the new nonce attribute is to roll it out in report-only mode. At Dropbox, we were in report-only mode with the nonce for nearly a month. Similar to violation reports for content sources, it is important to filter out noise even for inline script violations.  In addition to the tricks specified in the previous post, Chrome sends two additional fields to help understand an inline script violation: script sample and source file. The former is extremely valuable to quickly grep through the code base, in case reproducing the issue locally is difficult. The latter points to the source JavaScript file that inserted the inline script via DOM APIs.  During the filtering phase for inline script violations, we filter out all reports where the source file field is a URI that does not belong to our application (ad injectors and extensions are a common source of violations). Similarly, per Neil’s excellent advice, we also filter out reports with script samples containing code clearly not from our application (e.g., scripts that include the string ""lastPass""). Firefox currently has had a bug where even if a nonce src allows execution of inline script, Firefox reports a violation, while still executing the code. As a result, to reduce noise, I recommend, for now, deleting the report-uri for Firefox if you are deploying nonce support.  Update: This bug was fixed! But, the fix will go live in Firefox 43, scheduled for release in Dec 2015, so I still suggest not sending report-uri for Firefox till late January, 2016.  With the proper filtering in place, we deployed our policy with nonce sources and started a process of looking for violations and fixing them. This is a tedious process particularly at our scale and large code base. But, the rewards are worth it and progress is measurable. After a couple of weeks, we reached a place where we were comfortable deploying nonce-src in enforcement mode. Unfortunately, nonce sources are a feature of CSP2. While Chrome and Firefox have supported this for a while, Safari and Edge do not. Both browsers ignore the nonce source syntax and enforce the 'unsafe-inline' source expression instead. To further harden our web application, we relied on another trick: Just to recap: the DOMContentLoaded event is fired after the browser executes all the HTML and synchronous scripts (including inline scripts). After that, any other JavaScript tasks, events queued up, or other onload handlers fire. Following performance best practices, we already do not execute remote scripts synchronously, so the vast majority of our JavaScript code executes after DOMContentLoaded.  The code outlined above inserts a second CSP policy after the DOMContentLoaded event fires. Importantly, the second CSP policy does not include ‘unsafe-inline’. Browsers handle multiple CSP policies by enforcing all of them, so only code permitted by all is executed. The net result of this is that Safari and Edge parse and support inline script only in the initial HTML until the DOMContentLoaded event fires. After that, these browsers stop supporting inline event handlers.  Imagine that an attacker is able to insert a malicious payload (<div onclick=alert(1)>) via, say, innerHTML. In Chrome and Firefox, the presence of nonces in the original policy delivered via a header blocks the inline onclick. In Safari and Edge, while the first policy allows the onclick handler, the second policy, inserted after the DOMContentLoaded event, blocks it. While this is a weaker protection than on browsers supporting nonce sources, it does protect against a large class of DOMXSS attacks.  We have found the techniques outlined above to be an effective mitigation against XSS attacks in our web application. Between Chrome, Firefox, Safari, and Edge, a huge chunk of our users have a strong mitigation in place. While we fix all injection vulnerabilities, it is a welcome relief to know that even successful injection attempts have a second barrier in place for the vast majority of our users.  Deploying something as major as CSP, particularly deprecating 'unsafe-inline' requires support from the whole company. Special thanks to all the Dropboxers involved in this project, particularly all the members of the security engineering team. This is the second of a series of blog posts detailing our experience deploying CSP. Up next, we will talk about the impact of including ‘unsafe-eval’ in our policy and how to mitigate the risk. ",https://blogs.dropbox.com/tech/2015/09/unsafe-inline-and-nonce-deployment/,0,dropbox,"frontend,angular,java,react",NULL,2015-09-22
[CSP] On Reporting and Filtering,"This is the first of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring! We will also be at AppSec USA this week. Come say hi! At Dropbox, we are big fans of Content Security Policy or CSP. For those not familiar with the specification, I recommend reading Mike West’s excellent introduction to CSP. A quick recap: at its core, CSP is a declarative mechanism to whitelist content sources (such as sources for scripts, objects, images) in a web application.Setting a CSP policy mitigates injection risk in the web application by limiting content sources. For example, here’s the script-src directive in the content security policy for a request I made to the Dropbox homepage:The directive lists all the trusted URIs (including the full path for browsers supporting it) where we could possibly load script code from. When a web browser supporting CSP sees a script tag, it checks the src attribute and matches it against the whitelist provided by the script-src directive of the CSP policy. If the script source is not included in the whitelist (maybe because of HTML injection), the browser will block the request.The Dropbox CSP policy provides a strong mitigation against XSS and content injection attacks. But deploying a strong CSP policy at scale has a number of challenges. We hope that this four-part series sharing lessons we learnt provides value to the broader community. Today’s post discusses how to setup a report filtering pipeline to identify errors in the policy; in the second post, we will discuss how we deployed nonces and mitigated the ‘unsafe-inline’ in the policy above. In the third post, we will discuss our efforts to mitigate the risk from ‘unsafe-eval’, including open-sourcing patches we wrote. Finally, we will discuss how we reduced the risk of third-party integrations with privilege separation.Identifying and enforcing a CSP header for a modern, complex web application is a difficult task. Thankfully, Content-Security Policy supports a trick to help you roll it out: report-only mode. The key trick behind report-only mode is allowing a website to test out policies and see their impact via violation reports sent to an endpoint of the policy author’s choosing. For example, you could just set a report-only policy of script-src ‘none’ to learn all the places you include scripts from.Report-only mode holds great promise for deploying CSP: you keep iterating on the policy in report-only mode till you hit a point of no violation reports and then flip the switch to enforcement. This is often the recommended first step before turning on CSP in enforcement mode. Similarly, at a recent event I attended, the panel on adopting modern security mechanisms stressed how the CSP report-only mode can provide a useful crutch to deploying CSP, allowing you to evaluate possible policies before deploying them in enforcement mode.This is true: CSP reporting is an irreplaceable tool for getting actionable feedback on deployed policies. At Dropbox, we deployed CSP in report-only mode for months before flipping the switch and going to “block” mode. But, at scale, one of the first lessons of deploying CSP is the sheer noise in the reports that make the default report mechanism unusable.We found the biggest source of noise to be browser extensions that insert scripts into the page and/or other programs that might modify the HTML of your page. Recall that CSP blocks any unknown content from running on your page, so content injected into the page will likely get blocked by the browser too. If we just log all the reports that reach us, the logs will contain these errors too. Since you don’t have any control over these extensions, the end goal of “no more violation reports” mentioned above is unreachable.Given our experience deploying CSP at scale, we have over the last year fine-tuned a filtering mechanism to ignore common false-positive violation reports. Our reporting pipeline filters out these reports before logging them to our analytics backend. In the spirit of encouraging adoption of CSP, we are sharing these filtering techniques and hope that you find them useful. The list started off from Neil Matatall’s brilliant, detailed list that we strongly recommend reading too.At first glance, filtering violation reports sounds weird. Why would you not want to know when ad-injectors and spammers are modifying your web application? But, recall that we are talking about the pre-rollout phase of CSP. At this stage, the focus is on making sure that the CSP content whitelist isn’t breaking the web application. Filtering out the noise lets you focus on places where CSP enforcement might be a breaking change and fix appropriately. Once you enable CSP enforcement, the browser will block all the loads in the filtered list anyhow.The filtering is two fold: first, we filter based on the URI scheme of the blocked URIs.If the scheme of the blocked URI starts with any of the members of this list, we ignore it. The second step of the filtering looks at the host component of blocked URIsIf the host component of a blocked URI contains any of the keywords above, our filtering code would not log the violation report. Of course, before using this list, you should confirm that you are not legitimately using any of the domains in the list on your own website.Another source of noise we observed was extensions modifying the CSP policy we deliver. To ignore such errors, we also filter based on the violated directive field. If the violated directive field contains either “http:” or “:443″, we filter the report since we never include these in our policy.One trick we have considered doing is adding a hash of the current policy in the report URI and then only accepting the report if the policy in the violation report matches the hash in the URI. But, we haven’t felt the need to try this yet. Thanks to all the Dropboxers, in particular members of the security engineering team who helped me through this process of CSP deployment and report filtering. Additionally, thanks to everyone who reviewed an early version of this blog post. ",https://blogs.dropbox.com/tech/2015/09/on-csp-reporting-and-filtering/,0,dropbox,,NULL,2015-09-21
Dropbox Bug Bounty Program: Best Practices,"Dropbox is recognizing security researchers for submitting security bugs through a bug bounty program with HackerOne and Bugcrowd. Whether you’re a security bug guru or a complete newbie, we want to make it as easy as possible to submit any bugs you find!To this end, we’ve compiled the top 5 security bug report tips from our very own Security Engineers:If you’re wondering what a good bug report looks like, here’s an example:https://hackerone.com/reports/56828This report has a clear and concise bug description. The impact of the bug is highlighted and includes actual/potential impact, and it has step-by-step instructions on how to reproduce the vulnerability. Including these details will help make your bug report as useful as possible to us, and increase the chances of us using your report.",https://blogs.dropbox.com/tech/2015/08/dropbox-bug-bounty-program-best-practices-2/,0,dropbox,,NULL,2015-08-31
How to Write a Better Scribe,"Like many companies, Dropbox uses scribe to aggregate log data into our analytics pipeline. After a recent scribe pipeline outage, we decided to rewrite scribe with the goals of reducing operational overhead, reducing data loss, and adding enhancements that are missing from the original scribe. This blog post describes some of the design choices we made for the rewrite.This section describes the scribe pipeline with respect to how it is setup at Dropbox (we suspect most companies deploy/use scribe in similar fashion).  Feel free to skip this section if you are already familiar with scribe.A scribe server can be thought of as an input-queued network switch which performs routing based on the message’s category.  The server’s configurations, including its upstream locations, are loaded on startup from an xml-like config file.  The upstreams may be either other scribe nodes, hdfs (or local file), or a sink which drops all messages.Scribe’s primary client interface isLog(list of (category, message)-tuples) -> statusWhen a client sends a batch of messages to a scribe server, the scribe server forwards each message in the batch to the corresponding upstream and returns “ok” to the client to indicate success.  If the message batch is throttled, the server returns “try again” to client and the client should retry sending at a later time.Scribe nodes are typically configured into a fan-in tree topology.  At Dropbox, we deploy scribe nodes in a 3-tier (leaf, remote hub and central hub) fan-in tree configuration:Application (production services, scripts, etc.) scribe clients are topology-agnostic and can only talk to the local leaf node running on the same machine (i.e., Leaf nodes run on every machine in our fleet).  Each leaf scribe node is configured to randomly forward log messages to a small subset of remote hub scribe upstreams.The remote hub tier is our primary message buffering tier.  We want to minimize buffering on the leaf tier since leaf nodes runs on the serving machines and thus compete with serving systems for memory and disk IO.  Remote hub scribe nodes will forward messages to the correct central node based on a hash of the message’s category.The central hub tier is responsible for appending message into HDFS files; messages for the same category are appended to the same file.  HDFS does not support multiple writers appending to the same file. This implies that each category can only be written by exactly one central hub node.  This limitation also means that every central node is a single point of failure and is a bottleneck for categories with high write throughput.  To reduce potential data loss due to central hub node crashes, our central hub tier is configured with a very small memory buffer and no disk buffer.  To improve write throughput, our application scribe client shards known high-throughput categories by appending shard suffixes to these categories.  Our analytics pipeline then merges the sharded categories back into a single category.Since parts of Dropbox’s serving stack runs on EC2, while our analytics pipeline runs entirely within our datacenter, we have to transfer EC2 logs into our datacenter. For this purpose, an extra EC2 remote hub tier is setup to forward logs into our standard scribe fan-in tree. Note that all traffic between EC2 and our datacenter goes through secure tunnels.With the basics out of the way, let us dive into some key issues with the original scribe.  We will henceforth refer to the original scribe as OldScribe and the rewrite as NewScribe wherever the distinction matters, and simply scribe if there is no distinction.Problem: OldScribe’s configurations are stored in xml-like config files.  Whenever configuration changes, we need to restart nodes to pick up the changes.  While this approach works well for tens, maybe even hundreds, of nodes, this approach is unsustainable for tens of thousands of nodes.  In particular, it is difficult to ensure the entire fleet is running on the most up-to-date configurations, and it is unpleasant to restart nodes on the entire fleet.  This is especially problematic for leaf nodes since adding remote hub nodes requires manually modifying and syncing the leaf’s configuration before restarting all of them.Solution: NewScribe solves these issues by storing the configurations in zookeeper.  Whenever configurations change in zookeeper, NewScribe will pick up the changes and reconfigure itself automatically (without restarting).  NewScribe also supports service discovery; this eliminates the need to manually update configuration whenever the scribe upstream changes.Problem: OldScribe’s throttling occurs on ingress.  When a message batch comes into OldScribe, OldScribe will look for category queues associated to the message batch; if any of the queues is full, then the entire message batch is rejected and the OldScribe will ask the downstream client to “try again”.  This scheme works well when the message batch contains only a single category.  However, when the message batch contains multiple categories, this scheme can cause unintentional head of line blocking (assuming the downstream client retries).  Worst yet, if the downstream client ignores the “try again” directive, the messages are lost forever.  Also, note that this scheme is very wasteful since a lot of network bandwidth and cpu are wasted on retries.Solution: NewScribe’s throttling occurs on egress.  It always accepts incoming messages and attempts to buffer as much as it can until the category’s queue is full; this eliminates accidental head of line blocking.  To ensure upstreams are not overwhelmed, each category’s upstream writing is rate limited by a leaky bucket.Problem: OldScribe treats each category’s memory buffer and disk buffer as a single logical queue.Solution: NewScribe treats the memory buffer and disk buffer as individual queues.  Message ordering is not preserved.When a message enters the system, NewScribe will try to put the message into the memory queue first.  If the memory queue is full, then NewScribe will try to put the message into the disk queue (if that fails, the message is dropped). The upstream writer will grab messages from both queues and will send messages upstream when at least one queue has messages.  This allows some messages to bypass the disk queue which reduces disk IO stress, and allows the upstream writer to continue processing the memory queue at full speed while the disk queue pauses to load more messages.   NOTE: message ordering does not matter too much since the analytics pipeline must handle message reordering regardless of how scribe writes the messages.Problem: Currently, our application scribe client assigns each message’s category with a shard suffix. The OldScribe remote hub will route the message to a specific OldScribe central hub based on the hash(category with suffix).  Since the routing is predetermined by the application scribe client, whenever an OldScribe central hub dies, messages destined for that central hub will get backlogged until we replace that node.(Partial) Solution: For sharded categories, NewScribe ignores the shard suffix provided by downstream and assigns each upstream message batch with a new shard suffix; the shard suffixes are round robin, hence we’re rotating upstreams at the same time (and thus ensuring progress in face of single upstream node failure).  For unsharded categories, single point of failure remains an issue (This is partly the motivation for migrating to Kafka; see below).  NOTE: we can “fix” the single point of failure by forcing each category to have at least 2 shards.Problem: OldScribe creates a new file every time it flushes to disk.  The un-checksummed files are all written into the same directory.  When it tries to upstream the disk buffer, it loads the entire file and sends the entire file as a single message batch.  This is problematic since it creates a ton of log files in a single directory, which makes the os unhappy.  Whenever file corruption occurs, OldScribe may enter into an endless crash loop.Solution: NewScribe uses checksummed logs with rotations/checkpoints to handle writing to/reading from disk queues.  Each disk queue is composed of a reader thread and a writer thread.  The reader thread will only operate on immutable log files, while the writer thread will operate on a single mutable log file at any given time.  Mutable log files are rotated (and become immutable) on one of two conditions:Each category’s logs are written into a different subdirectories.Problem: Our analytics team want to replace HDFS with Kafka to take advantage of newer analytics tool such as Storm.  To support that in OldScribe, we will need either a scribe-to-kafka shim server or a server that tails HDFS; both solutions are unattractive since they both introduce yet another server into our already complex ecosystem.Solution: NewScribe natively supports writing to kafka upstreams.For comparison, here are the architecture block diagrams for OldScribe and NewScribe.As mentioned previously, OldScribe’s architecture resembles a simple network switch.  Its (logical) architecture is as follow:Each category runs in its own thread (The individual components within a category are just library calls to the common abstract Store interface).  Notice the lack of control plane since routing configuration are loaded during startup.NewScribe’s architecture also resembles a network switch.  Its architecture is as follows: Control Plane:Data Plane: While we cannot open-source our rewrite at current time because it is tightly coupled to Dropbox’s internal infrastructure, we hope this post provides enough details for you to re-implement your own.Per usual, we’re looking for awesome engineers to join our team in San Francisco, Seattle and New York, especially Infrastructure Software Engineers and Site Reliability Engineers!Contributors: John Watson, Patrick Lee, and Sean Fellows",https://blogs.dropbox.com/tech/2015/05/how-to-write-a-better-scribe/,0,dropbox,,NULL,2015-05-20
Introducing the Dropbox bug bounty program,"Protecting the privacy and security of our users’ information is a top priority for us at Dropbox. In addition to hiring world class experts, we believe it’s important to get all the help we can from the security research community, too. That’s why we’re excited to announce that starting today, we’ll be recognizing security researchers for their effort through a bug bounty program with HackerOne.Bug bounties (or vulnerability rewards programs) are used by many leading companies to improve the security of their products. These programs provide an incentive for researchers to responsibly disclose software bugs, centralize reporting streams, and ultimately allow security teams to leverage the external community to help keep users safe (something I’ve advocated for in previous research).While we work with professional firms for pentesting engagements and do our own testing in-house, the independent scrutiny of our applications has been an invaluable resource for our team — allowing our team to tap into the expertise of the broader security community. We’ve recognized the contributions of the researchers we’ve worked with in a public hall of fame, and now we’re very excited to be one of several companies that provide monetary rewards, too. In fact, we’ll be retroactively rewarding researchers who’ve reported critical bugs in our applications through our existing program, paying out $10475 today.Here are some additional details about the program:This is another step in our commitment to security and privacy, which has already been reflected in the recognition and ranking by external organizations like EFF and SSLLabs, as well as our participation and support of organizations like SimplySecure. We look forward to working with security researchers and awarding them for their contributions to the security of all Dropbox users.",https://blogs.dropbox.com/tech/2015/04/introducing-our-bug-bounty-program/,0,dropbox,"blockchain,bitcoin",NULL,2015-04-15
"Firefly: Instant, Full-Text Search Engine for Dropbox (Part 1)","Like any serious practitioners of large distributed systems our first order of business was clear: come up with a name for the project! We settled on Firefly. Today, Firefly powers search for all Dropbox for Business customers. These are the power users of Dropbox that collectively own tens of billions of documents.This blog post is the first in a series that will touch upon the most important aspects of Firefly. We will start by covering its distinguishing characteristics and explain why these lead to a non-trivial distributed system. In subsequent blog posts, we will explain the high-level architecture of the system and dive into specific details of our implementation.There are three dimensions that determine the complexity of a search engine:We want Firefly to be the one system that powers searches for all our users and we want it to be blazing fast — our goal is to have serving latency under 250 msecs at the 95th percentile (i.e., 95 percent of all searches should take less than 250 msecs). For a compelling user experience we also want it to index a document “instantly”. That is, additions or modifications to a Dropbox should be reflected in search results in under 10 secs (95th percentile). In essence: we set a goal to build a search engine that performed well in all three dimensions. What could possibly go wrong? 🙂At first glance, this might seem like a simple problem for Dropbox. Unlike a web search engine (such as Google), every search query only needs to cover the set of documents a user has access to. So why not just build and maintain a separate index for each Dropbox user with each stored in a separate file on disk? After all, one would rightly expect the distribution of Dropbox sizes across our user base to follow a Zipf distribution — a large fraction of users would have a small number of documents in their Dropbox. For these users, the corresponding search index would be relatively small and therefore easy to build and update as documents are added or modified.There are two main drawbacks of this approach. Firstly, we expect some users to have a large number of documents in their Dropbox, making it non-trivial to update their corresponding index “instantly”. Secondly, this approach requires the system to maintain as many indices as there are users with each stored in a separate file. With over 300 million users, keeping track of so many indices in production would be an operational nightmare. We like to sweat the details here at Dropbox and would hate for even one of our customers to have problems searching because of an issue on our side. Having such a large number of index files would impair our ability to effectively and precisely monitor issues affecting a small fraction of our users.Clearly we need a design that will create fewer search indices but now we have another problem: if we slice the complete search index into 1000 pieces each piece becomes pretty large (reflecting the Dropbox content for over 100 thousand users). How do we update such a large index “instantly”? To achieve this goal we need a system that supports incrementally updating the search index: a challenging task with billions of documents and millions of users.In the discussion above we concluded that for the system to be both fast and operationally viable we needed a relatively small number of slices. In this section we take up a related question: what is a good way to slice the search index? In other words, how do we assign documents to a given slice? In distributed systems parlance the slices are commonly called shards and the deterministic function which maps a document to a shard is called a sharding function.One additional dimension of complexity is that a Dropbox user may choose to share a folder with multiple other users. Files in a shared folder appear in each member’s Dropbox. So if we picked a sharding function based on user-id, a shared file would appear in the index multiple times, one for each user that has access to it. For efficiency, we wanted each user file to appear exactly once in the index.As a result, we chose a sharding function based on “namespace”. A namespace is a widely used concept in our production systems. Internally, we represent a user’s Dropbox as a collection of namespaces. Each namespace consists of files and directories, along with a directory structure, and is mounted at a certain directory path within a user’s Dropbox. In the simplest case, a user’s Dropbox consists of just one namespace mounted at “/”, which is called the “Root” namespace.The concept of a namespace makes it easy to support the notion of shared folders. Each shared folder in Dropbox is represented by a separate namespace. It is mounted at a certain directory path within the “Root” namespace for all users with whom the folder has been shared.Dropbox manages billions of namespaces. We use a standard hash function as our sharding function to divide them into a relatively small set of shards. By pseudo-randomly distributing namespaces across shards, we expect the shards to be roughly similar in terms of properties such as number of documents, average document size, etc.Since namespaces are already divided into shards, we can take advantage of this structure and generate a search index per shard. To process a given user’s query, we first determine the namespaces a user is able to access. These namespaces may belong to different shards. Then, we use the sharding function to find these namespace shards and use the corresponding search indices to answer the query.Conceptually, the search index contains the mapping: {token => list of document IDs}. If a user with access to namespace with ID ns1 issues the query “san francisco”, we tokenize it into tokens: ""san"" and ""francisco"", and process it by intersecting the corresponding list of document IDs. We would then discard all document IDs that do not belong to namespace ns1.This is still inefficient, as a shard typically contains a large number of namespaces (in the millions), while a user typically has access to a handful of namespaces. To make the query processing even faster, we prefix each token in the search index with the ID of its namespace: {namespace-id:token => list of document IDs}. This corresponding list of documents contains only those that contain the token and are also present in the namespace. This allows us to focus our processing on the subset of the search index that is relevant to the set of namespaces that belong to the user. For example, if a user with access to the namespace with id ns1 issues the query “san francisco”, we process it by intersecting the list of document IDs for tokens: ""ns1:san"" and ""ns1:francisco"".Before we embarked on building Firefly, we considered whether we should leverage an off-the-shelf open-source search engine (SolrCloud, ElasticSearch, etc). We evaluated many of these solutions and decided to build our own for two main reasons. Firstly, none of these solutions is currently deployed at a scale comparable to ours. And secondly, a system built from scratch gives us control over design aspects that have significant impact on the machine footprint, performance, and operational overhead. Also, search is a foundational feature for many of our products, current and planned. Instead of setting up and maintaining a separate search system for each of these, over time we intend to extend Firefly into a “search service”. This will allow us to quickly enable search over new corpora.Having said that, we do leverage a number of open-source components in the implementation of Firefly (e.g., LevelDB, HDFS,  HBase, RabbitMQ, Apache Tika). We will go into the details of our use of these in subsequent blog posts.Today, Firefly has been in production for several months and powers search for all Dropbox for Business users.  We have designed it to be a horizontally scalable system. It is able to comfortably meet the goals for serving and indexing latency that we set for ourselves.In this post, we discussed the key requirements for Firefly and the motivations behind them. We also explained why meeting these requirements was not easy and described our sharding strategy in some detail.In subsequent posts, we will cover the overall design of Firefly and detail the components that enable it to be a scalable, robust and instant search system. We will describe how Firefly scales horizontally to support growth as well as gracefully handle different types of failures.There is always more to be done to support Dropbox’s growth and optimize the performance of our systems — if these type of problems are your cup of tea, join us!Firefly was built by a very small team — Firefly infrastructure was built by the two of us with help from Adam Faulkner (our intern, who recently joined us full-time!). If you are interested in working in a small team and making a large impact come talk to us.Contributors:  Abhishek Agrawal, Adam Faulkner, Franck Chastagnol,  Lilian Weng, Mike Lyons,  Rasmus Andersson, Samir Goel",https://blogs.dropbox.com/tech/2015/03/firefly-instant-full-text-search-engine/,0,dropbox,python,NULL,2015-03-23
Dropbox at AWS re:Invent 2014,"Dropbox is an active customer of Amazon Web Services, currently operating one of the largest global deployments into S3, tens of thousands of EC2 instances, and heavily utilizing other services like SQS and Route 53. Pushing hundreds of gigabits per second through EC2/S3 is an everyday occurrence for us, and conducting massively parallel operations across our over one trillion objects in S3 happens on an ongoing basis.However, that’s just half the story. We also have large physical datacenters split between two geographical regions, running tens of thousands of servers responsible for storing and serving the metadata for every file in Dropbox. Managing the metadata for the one billion files saved every day means that these servers have to be extremely fast and reliable.Orchestrating the hundreds of backend services that comprise Dropbox is an ongoing challenge for our infrastructure team, along with keeping up with growth and reliability.Two weeks ago, at the AWS re:Invent conference, we presented a deep-dive into one of the systems that allows us to perform operations for every Dropbox file update in near-realtime. This enables features like fast photo thumbnails and video previews for Carousel, multi-platform Microsoft Office document previews, and realtime full-text search. The system bridges our physical datacenters and AWS using SQS as a fast, reliable, no-maintenance message bus that sustains ~20,000 requests per second on average, but often bursts to ~300,000 per second.Another part of Dropbox is Mailbox, a completely reimagined email client for iOS, Android, and Mac OS X. Mailbox operates wholly on AWS and heavily leverages additional AWS services to keep their team lean and iterate quickly. Simple Notification Service‘s (SNS) support for mobile push has enabled the team to quickly support new platforms and scale their push notification pipeline with almost no effort.Sean Beausoleil, an engineering lead on the Mailbox team, spoke at re:Invent about why they chose SNS and how it’s since become a foundation component of the Mailbox service (starts at 36:49).Dropbox’s infrastructure and usage has grown exponentially since our launch in 2008, yet we still run everything with a small team of fewer than 100 infrastructure engineers in San Francisco, Seattle, and New York City. We’re always looking for driven, creative engineers to join the team and to push both AWS and our physical infrastructure to their limits, either as Site Reliability Engineers or Infrastructure Software Engineers.",https://blogs.dropbox.com/tech/2014/12/aws-reinvent-2014/,0,dropbox,"java,python",NULL,2014-12-04
"Building Carousel, Part III: Drawing Images on Screen","Making Carousel highly responsive was a critical part of providing an awesome user experience. Carousel wouldn’t be as usable or effective if the app stuttered or frequently caused users to wait while content loaded. In our last post, Drew discussed how we optimized our metadata loading pipeline to respond to data model changes quickly, while still providing fast lookups at UI bind time. With photo metadata in memory, our next challenge was drawing images to the screen. Dealing with tens of thousands of images while rendering at 60 frames per second was a challenge, especially in the mobile environments of iOS and Android. Today we are going to take a look at our image decoding and rendering pipeline to provide some insight into the solutions we’ve built.BACKGROUND: When work on Carousel first started, we set three key implementation goals for ourselves:It was incredibly important to us that we meet these goals in the main Carousel views as the user scrolls through their photos. The task at hand will be familiar to those who have worked with drawing before: decode the thumbnails, which we store as JPEGs for data compression purposes, and display them in the UI. In general we lay out images three in a row in the main Carousel view. To determine what thumbnail size to use, we ran some UI experiments on modern phones such as the iPhone 5 or the Nexus 5, and decided that the cutoff resolution for a high fidelity thumbnail would be around 250px by 250px – anything lower resolution would look degraded in quality to the eye. Given the fact that Dropbox always pre-generates 256px by 256px thumbnails for all uploaded photos and videos, we were leaning toward using 256px by 256px thumbnails. To further validate this choice, we tested the network time needed to download such a thumbnail (~0.1s in batch over wifi), size on disk (~28KB), time to decode such a thumbnail (9ms on iPhone 5), and memory consumption after being decoded into a bitmap (0.2MB). All numbers looked reasonable, and we decided to go with these 256px by 256px thumbnails. Those who have worked with intensive image drawing might have predicted that image decoding would be a problem for us. And sure enough! While JPEGs are efficient for data compression, they are also expensive to decode into pixel data. As a data point, decoding a 256px by 256px thumbnail on a Nexus 4 takes about 10ms. For a 512px by 512px thumbnail, this increases to 50ms. A naive implementation might try to draw 256px by 256px thumbnails on the main thread synchronously. But in order to render at 60 frames per second, each frame needs to be rendered in 16ms. In a single frame, when a row of three thumbnails appears on screen, we must decode 3 thumbnails. With the naive implementation at 10ms per thumbnail, it would take 30ms to render that frame. You could see immediately that such an approach wouldn’t work without dropping frames and losing smooth scrolling.Naive ApproachFIRST SOLUTION A fairly standard approach to the problem above is to offload the decoding of the 256px by 256px thumbnails to a background thread. This frees up the main thread for drawing and preserves smooth scrolling. However, this yields a separate problem of not having content to display to the users as they scroll. Have you ever scrolled really quickly in an app and only seen placeholder squares where you should see images? We call this the “gray squares” problem and we wanted to avoid it in Carousel.First Solution: Background queueSECOND SOLUTION It became clear that if we wanted scrolling to be smooth we had to render in the background, but latency was an issue with that approach. What could we do to hide this? One idea was that if we couldn’t decode fast enough, we could decode less. Again we ran some UI experiments to find the lowest resolution thumbnails that looked degraded to the eye but still gave a high enough level of detail for a user to be able to understand the content of the photo. Turns out this is about 75px by 75px. We wanted these to be square thumbnails because they were displayed as square thumbnails in most Carousel views, and we didn’t want to decode any more than what needed to be displayed. Another advantage of having small thumbnails is that the variance of JPEG file size is smaller, so every image takes roughly the same amount of time to decode. Furthermore, we already pre-generated 75px by 75px size thumbnails on the server. Thus we decided to download and cache a 75px by 75px thumbnail along with a 256px by 256px thumbnail for each image.The 75px by 75px thumbnails takes roughly 1/5 of the time to render compared to 256px by 256px thumbnails, a big performance win gained at the cost of image quality. Here was the dilemma: just using those small thumbnails alone would go against our goal of data fidelity, but rendering big thumbnails would be too slow when the user scrolls quickly. We intuited that a user scrolling quickly would prefer to see a preview of each thumbnail, rather than nothing at all. So, what if we detect when the user scrolls quickly, and render 75px by 75px thumbs on the main thread on demand? Since it’s blazingly fast to render these low-resolution thumbnails (~2.7ms on iPhone 5), we could still preserve smooth scrolling. As soon as we detect the user is scrolling slowly, we add a rendering operation for 256px by 256px thumbnails onto a queue which is processed by a background thread. Decoding work is processed one by one from the beginning of the queue. As the user scrolls, new thumbnails will queue at the beginning since it’s most urgent to decode them. In order to only render relevant 256px by 256px thumbnails, we dequeue the stale requests as images go off the screen. This tight connection with UI ensures that no extra work is done to process offscreen thumbnails To further ensure no extra work is done and reduce CPU resource utilization, we only render the larger thumbnails when the user is likely to see them. We check if the user is scrolling too quickly by listening to draw callbacks (CADisplayLink on iOS) and measuring the difference in scroll offset by time.Second Solution: Background queue + low resolution thumbs on main threadWHAT WE BUILT We ran with the last approach and built an image renderer that contains a queue of 256px by 256px rendering jobs. After experimentation we settled on caching the resulting bitmaps, with a configurable cache size, which allows us to hold on to the most recently decoded thumbnails. In case the user scrolls back and forth, we don’t need to render the same thing again. As the diagram below indicates, when the user scrolls an image onto the screen, we check if we have the high-resolution bitmap already rendered first. If we do, we just display that already rendered image. If not, we render the 75px by 75px thumbnail on the main thread synchronously, and only queue the 256px by 256px thumbnail in the background if the user is scrolling slowly. If the user scrolls fast, we don’t queue the rendering jobs associated with 256px by 256px thumbnails until the scrolling slows down. As the user scrolls slowly, the background render queue doesn’t have much work to do, so the low-resolution to high-resolution swapping happens almost immediately. As the user flings really fast, nothing gets into the render queue, since we only display low-resolution thumbnails as they fly by quickly. As the user scrolling slows down, the background render queue starts to be fed with relevant on-screen thumbnails, so low-resolution to high-resolution thumbnail swapping is almost seamless. Rendering jobs are also dequeued as the associated thumbnails go off the screen, so the render queue only has a maximum of a screen-full of decode jobs.There are of course a few additional enhancements we made along the way. For example, we prefetch offscreen thumbnails around the user’s viewport so we already have a window of pre-rendered thumbnails ready to go. Also, for events with a lot of photos, we show a blurred view with “+n” to indicate that the event is expandable – we don’t need to render these images with high-resolution before applying the blur effect. Performing the CPU-intensive task of decoding images on a background thread is a pretty standard practice in mobile engineering. However, that practice alone is not sufficient for our needs in Carousel, where we need to provide data availability and smooth scrolling for users with tens of thousands of photos. We hope this post, as well as the two preceding posts on Carousel performance, have given you some insight into the challenges we faced moving large amounts of data from our servers, to device local storage, to pixels on the screen.",https://blogs.dropbox.com/tech/2014/10/building-carousel-part-iii-drawing-images-on-screen/,0,dropbox,"backend,json,python",NULL,2014-10-27
The Tech Behind Dropbox’s New User Experience on Mobile (Part 2),"In last week’s post, Kat described how we redesigned our new user experience from the ground up to make it a delight for users to get started on Dropbox from our mobile apps. In this post, I’ll go into more detail about everything we did to make the mobile-to-desktop transition simple for users. To recap the previous post, here’s a summary of the flow:  How Desktop Connect WorksThe desktop connect flow begins on the phone. It instructs the user to go to dropbox.com/connect on their computer (we added a picture of a computer too, just to make things extra clear). Our goal in this flow is to get the user set up with Dropbox on their desktop, minimizing any risk of losing them along the way. Trying to go to the connect website from the phone’s browser is one possible failure mode. On our servers, we generate a QR code with a unique token and display it on the website. To make the flow more fun, we display the QR codes inside a Dropbox. Here’s what the background image looks like. We place identical copies of the QR codes (embedded as inline images with data urls) in each of the empty boxes.When the user taps ‘Next’ on their phone, a camera appears with instructions to point it at the computer. The phone auto-focuses and captures the QR code. It knows to ignore any QR codes not generated by Dropbox. Once it successfully captures the QR code, it extracts the unique token and communicates it to the server, along with the user ID of the user currently signed in on the phone. The browser, meanwhile, is polling the server for new information about the unique token. When the server hears from the phone, it can tell the browser that the QR code’s unique token is now associated with a user. The browser then authenticates the users and logs them in, without requiring their username or password at all. To convey that this transition has occurred, the browser greets the user by name: The browser immediately starts downloading the meta-installer. The full Dropbox installer is huge—it’s about 35MB. The Dropbox client that runs on Windows, Mac OS, and Linux is built using Python, so the installer includes the entire (slightly modified) Python runtime that must be installed on every machine in order for Dropbox to run successfully. With such long download times, users can get distracted easily and totally forget about Dropbox by the time the installer finishes downloading. To solve this problem, we created what we call the ‘meta-installer.’ The meta-installer is a very small executable that’s just a placeholder to grab the full installer from a CDN. This executable preps the machine for installation (it handles acquiring permissions to complete the install, for instance) while it fetches the payload from the server in parallel.The meta-installer has another huge advantage. One big problem for us on the product/engineering teams was that we had almost no insight into what was going on for a particular user after they initiated the download. Did the user launch the installer? Did the user’s installation complete successfully, or did it fail at a particular spot? How did this behavior differ with speed of internet connections? Which download route was the most successful, and which wasn’t? One of the keys to solving this problem was to be able to generate some kind of token that helps us associate an instance of a download with the installation, embed it in the installer, and trace it through the funnel. The meta-installer gave us an opportunity to do this. Because it’s such a small download, we can serve it ourselves instead of through a CDN. We can choose to serve a unique binary for every request. We tag the meta-installer with an ID while the larger full installer is still a static resource that we can host on a CDN.How The Meta-Installer and Auto-Signin WorkTo generate the meta-installer, we start with a standard template installer binary that contains a 1K buffer (known as the tag buffer) somewhere in the binary. The exact location (and implementation) is platform-specific. For every installer request, we clone the template, generate a token, and then replace the empty tag buffer with this token.There was another interesting challenge here, though. Modern OSes require that each binary is signed by the publisher. The template installer was signed using Dropbox’s key, but modifying it with the token would invalidate the signature. One option to fix this was to defer signing the installer until the point where we had embedded the token, but that posed some performance problems. Instead, we created a custom version of the signing tools which complied with the Authenticode spec (for Windows) while letting us safely modify content for each binary. Our custom tool allows us to create an unverified section of the binary in a way that is compliant with the Authenticode spec. We make the tag buffer an unverified section so that the tag buffer can change without having to re-sign the binary.So finally when the full installer runs, it knows where to look for the token in the tag buffer. It can use the token to report progress or report any failures along the way. This enables us to get a holistic view into user behavior and issues in Dropbox’s install funnel.At this point, the Dropbox desktop client is running on a user’s machine. However, there’s still a lot of room for failure. Users might be busy with other applications and forget to log in. Some might have forgotten their username or password. They may not feel like looking it up, changing it, or just typing it in.To fix this, we leveraged our solution to the meta-installer tagging problem above to create a secure means of authenticating the user on the desktop client using their credentials from the web session. The meta-installer solution above enables us to embed arbitrary tokens in the 1k tag buffer, so we embedded a token affiliated with the user’s identity. When the user installs and runs the desktop client, the client can look at that buffer and use the token to log in as the user automatically.Of course, we needed to address security aspects for this solution to be broadly deployed. For instance, user A shouldn’t be able to download an installer, send it to user B, and have user B’s computer linked to user A’s account. Similarly, user X should not be able to generate arbitrary links that can trick user Y into downloading an installer that links their machine to user X’s account. We needed to be conservative and follow smart heuristics to determine when not to automatically sign in a user and fall back to requiring an email and password. In order to solve these and some other security concerns, we leveraged the tag buffer, browser cookies, and a client-side nonce. The browser, desktop client, and server work together to verify the user’s identify. It works like this:By leveraging the browser cookie, we ensure that the desktop client only logs in if the same user who downloaded it is logged in on the browser for that particular machine. You can’t force somebody else to log in to your Dropbox by sending them your binary. Browser tag validation and cross-checking it with the client nonce helps prevent attacks where the flow is interrupted and continued on another device. We also have a few other security and reporting measures implemented to ensure that things work smoothly. If any of our conditions isn’t met, we abort auto sign-in and ask the user to log in with an email and password. Additionally, for users who have enabled 2-factor authentication or use SSO to log in, we don’t do auto sign-in.By the end of this flow, the user has a fully functioning, logged in, and ready-to-use desktop version of Dropbox up and running. They never had to type in their email or password. Even though there’s a lot going on behind the scenes, to the user everything appears so smooth that it’s expected. When we first tested this flow in user studies, almost nobody noticed that anything unusual had happened. That’s when we felt we had done our job well. To the user, it just works. The flow is now live for Android, iOS, Mac, and Windows.If you’re excited about building product-driven technology, come join us!",https://blogs.dropbox.com/tech/2014/08/tech-behind-dropboxs-new-user-experience-for-mobile/,0,dropbox,"frontend,css",NULL,2014-08-20
"Building Dropbox’s New User Experience for Mobile, Part 1","At Dropbox, we treat growth as an integral part of the product experience. We look at major holes in user experience that slow growth, and we try to be creative in addressing the big picture, rather than trying to “growth hack.” We look for solutions that enable users to experience the full value of Dropbox.Dropbox has always been about accessing your stuff anywhere. Back when Dropbox launched six years ago, that meant installing Dropbox on your desktop and then accessing photos and docs on the web or on a smartphone, for some. Our smartphone apps were a way of helping users who already had Dropbox on their desktop view docs on the go, and they were designed as such. When users signed up on mobile devices, our apps assumed they already knew what Dropbox was and how to use it.On the growth team, we realized that we needed to redesign our mobile apps for a mobile-first world. We needed an experience that could get users up and running from their phones, even if they’d never touched Dropbox before.We spent a few months designing an end-to-end experience to educate and activate mobile users. After brainstorming sessions, prototypes, user studies, and A/B tests, we arrived at a new user experience that we think is simple and delightful. In this article, I’ll talk about the vision we had for new users to easily get started on Dropbox. Stay tuned for a blog post with more detail on the technical stack to power the new flows.The first barrier for many of our mobile-first users was simply having no idea what Dropbox is all about. Maybe that user’s phone already had Dropbox pre-installed, or their friend told them they should download it. Users opened the app with no idea about what it did, and all they saw was an account creation screen. That experience made sense for the old world where almost all users who opened the mobile app already had a Dropbox account on their desktop, but it doesn’t make sense today. We wanted to fix that.We landed on adding an introductory flow for new users before they create an account. Our ideas ranged from elaborate (an animated story showing a user taking a photo on her mobile device, the photo syncing automatically to her Dropbox, a bird swooping in and grabbing the device away, and then the user realizing that her photo was safe forever in Dropbox) to simple (a one-page list of how Dropbox was useful). The design that won was a simple, interactive flow with fun animations that succinctly conveyed the value proposition of Dropbox without too much distraction.We experimented with putting the intro flow before and after login. Even though it takes longer to get to the initial account creation stage, the percentage of users who logged in or created an account and the engagement of those users was higher with the tour before login. In this case, giving the users more information is better than getting them to log in as quickly as possible.We then designed a step-by-step flow that activated users by guiding them through a two-way sync. After completing the flow, users will have synced files from their phone to their desktop and back from the desktop to the phone. By backing up photos, files go from their phone to their computer. By installing the desktop client and adding a file from their computer, users can now see their files on their phone too. The two-way sync gives users a much deeper understanding of how Dropbox works.Importantly, each step in the checklist is clickable and guides users to complete the task. For instance, clicking on “upload a photo” will do something different based on the user’s state. If the user doesn’t have Camera Upload on, it will show a dialog that allows them to turn on camera upload. If Camera Upload is on but their wifi is off, it will take them to their wifi settings. By carefully guiding the user through each of these steps, we made sure nobody would quit out of frustration or confusion.At the “Install Dropbox on your computer” step, we ran into a roadblock with our goal of carefully guiding users through each step. It’s hard to hold a user’s hand when they venture into the world of their desktop. There are a lot of potential places where they could get lost. They could fail to find the download link, they could wander off while our ~40MB installer (it includes the Python runtime) is downloading and never return, they could open Dropbox but then forget their password, etc. We eventually came up with a flow that takes users from the mobile app to the signed-in desktop app with very little risk of attrition.Using a personalized QR code, the desktop connect flow allows users to securely log in to the website without typing in credentials and initiates an installer download. The “meta-installer” downloads almost instantly because of its small file size. When launched, it sets up permissions for Dropbox and then starts downloading the full installer and completes installation automatically.When Dropbox is fully installed, the server, the browser, and the desktop client work together to validate the client and allow it to safely log the user in without requiring the user to enter their email and password again.  By the end of this flow, the user has a fully functioning, logged in, and ready-to-use desktop version of Dropbox up and running, and they never had to enter in their email address and password. This eliminates the possibility of a user mistakenly using a different email address or forgetting their password.The onboarding flow described above goes a long way in helping new users set up Dropbox on their computers, but there are some more interesting challenges left to solve that would help more users get started. For instance, it would be great to help users get the most important documents and photos from their computer into Dropbox automatically. Our experiments showed that users are much less likely to complete the last step of the Get Started flow: adding files to their Dropbox. If we help users more with this step, we could likely increase that number.The introductory flow and the desktop connect flow are both shipped on Android and iOS. The Get Started flow is still in its experimentation phase on Android. We think we have solved some fun design, product, and engineering challenges in making simple user experiences, but there’s still a lot left to do. If you’re into engineering delightful experiences for users, we’re hiring!",https://blogs.dropbox.com/tech/2014/08/building-dropboxs-new-user-experience-for-mobile-part-1/,0,dropbox,,NULL,2014-08-12
"Building Carousel, Part II: Speeding Up the Data Model","In the last post, Stephen explored the asynchronous client-server communication model we use in Carousel to provide a fast user experience, where interactions aren’t blocked on network calls. But network latency was not our only hurdle. In order to make the app feel fast and responsive, we also needed to minimize user-visible disk latency. One of Carousel’s primary goals was to make all of the user’s photos and videos accessible in one continuous view. We have users with over 100,000 photos in Dropbox, and we needed to build a metadata-loading pipeline that could accommodate them. This meant building a metadata-loading system that can show photos on-screen within a second of opening the app and provides smooth scrolling even as the user navigates back in time through their entire history of photos. Simply reading the metadata off of disk is too slow to do on demand while scrolling, so we keep a model of the user’s photo metadata in memory that reflects the metadata we have in our cache on disk.Keeping the in-memory model correct with respect to the on-disk model is tricky because there are many threads that want to modify the state of the model concurrently: the thread receiving file changes from the server, the main thread with interactions from the user, and various background tasks like uploading pictures. Furthermore, we can’t read all of a user’s metadata off of disk immediately on app startup because it would block the user from seeing their photos for far too long. For example, on a modern Android smartphone (Nexus 5) with about 300 bytes of metadata per photo, it takes a full second to read metadata for a mere 5,000 photos out of SQLite.Before we look at Carousel’s in-memory model, let us take a moment to consider common practices of loading and caching data to display. On Android, this is typically done with Loaders and Cursors. For the photos tab on our Dropbox mobile app, we store photo metadata in SQLite and then retrieve it using a SQLiteCursor which wraps a query to SQLite. This has a few problems because the Cursor interface doesn’t mesh well with the interface SQLite exposes. SQLite is a single-threaded library, so to read the results of a SQLite query in C, one needs to execute a query and step through the result rows all while holding a lock on the database connection. But the Cursor interface allows for later access to the query result, so what SQLiteCursor does is that the Java object runs the SQL query lazily, caching a fixed amount of query results at a time (2MB by default). There are quite a few drawbacks here, especially if you have to deal with more than 2MB of data. First, the Cursor grabs the next page of data by rerunning the query with an OFFSET clause. If the data set changed between the first time the query was run and the second, it might miss returning some rows. Second, it’s easy to miss the fact that the query is run the first time the Cursor needs data (generally during the first call to moveToFirst or getCount), which might be on the main thread. Even if you work around this while preparing the query by forcing the query to run on a background thread, a second run of the query will be whenever you advance past the first 2MB, which is also likely to be on the main thread. A disk read of 2MB on the main thread can cause a several-second hang. On iOS, although we get to interface directly with SQLite, we have similar problems because the naive implementation is to load data with a single SQL query, generally with a limit to the number of rows returned. When we built our original iOS and Android apps, we chose to work around these issues by paginating the photos tab, choosing a page size of approximately 5,000 photos where the latency to do a blocking disk read of a page of metadata is “tolerable”.In Carousel, we wanted to fix the experience for users with more than a single page of photos by using a different model for loading photo metadata off of disk. When it comes time to render the view to the user, it doesn’t matter how we store the user’s data on disk as long as we can quickly bind data to the view. We prepare an in-memory “view model”, a data structure that provides exactly this: an index on our metadata that is fast enough to query on the UI thread at render time. We use the view model to implement UI callbacks like iOS’s cellForRowAtIndexPath: and Android’s Adapter.getView(). Because the way we access this data structure is similar between iOS and Android, we can actually share this view model between the two platforms, implementing the logic only once in C++. Having a large data model in C++ rather than in Java also helps us avoid stuttering app behavior on Android due to large garbage collections. While we won’t go into depth on cross-platform mobile development with C++ here, we plan to write about this more in a future blog post.There were several performance characteristics we needed to be careful about in implementing this view model. First, it needed to be fast to update. Whenever a new photo gets added to the user’s Dropbox remotely on the server, perhaps because they added it from their computer, we need to be able to reflect that update quickly, without reloading the entire data set. Second, we also need to be able to load the initial view of Carousel without blocking on a full read of the metadata on disk. Both of these requirements drove us toward an interface that would allow for incremental updates, which is why we call our in-memory data model an “accumulator”. The accumulator prepares the view model to pass off to the UI whenever changes happen with the user’s data. The model that our timeline view uses has a fairly simple transactional interface:In one transaction, we add a batch of photos and their corresponding events, then finish by calling commit. At the end of commit, the accumulator might not have all the photos in a user’s collection because we’re still streaming metadata from the server or out of our on-disk SQLite cache, but we’re guaranteed to have a consistent data model where the photos that do appear are in the events we intend for them to be in. With this interface in place, we’ve been able to develop and optimize our database transactions without adding much complexity to the higher application layer. We make use of this interface by adding photos in a few places:These three code paths happen on separate threads, so we need to avoid having two threads with open EventsAccumulator transactions at the same time. Each thread locks the EventsAccumulator during a transaction, but it only has to spend that time pushing in-memory metadata into the accumulator. For example, the sync thread looks roughly like this, using calls to a server API like our /delta:This thread can perform the time-intensive tasks of making the network call and parsing the returned json before it tries to grab the accumulator lock, allowing the thread that’s reading metadata off of disk proceed unblocked. Of course, we still have to coordinate on accesses to disk between these two threads to give the app a consistent view of the data. The thread that’s reading data out of SQLite takes a lock on the database while it reads a batch of metadata that prevents us from executing write_delta_to_disk on the sync thread at the same time.With these threads running to populate the accumulator, the data that the UI needs to display can be changing very frequently. To give the UI code control over when the data can change during its calculations, we present the UI with an immutable snapshot of the data model every time we call commit.One reason why we care about using snapshots is so that the UI can perform an atomic swap of models to refresh its data. There are generally two major ways UI frameworks will support updating the data backing the UI:Android primarily uses the first pattern, while iOS generally uses the second. Incremental updates have an advantage in that they allow for animating changes to the data model (i.e. animating insertions and deletions). But they’re not strictly necessary, and one can derive them by taking diffs between snapshots. Our shared code prepares snapshots of the view model that support the first pattern. The snapshots allow looking up photos by absolute position in the ordering of all photos and by (event_index, index_within_event) pairs (iOS developers will recognize this pair as NSIndexPath in disguise). When we get a new snapshot, on iOS we call [UITableView reloadData:] to swap the new view model for the old. On Android, we wrap the snapshot with a thin layer that calculates the row index of each event and use it as the data source for our ListAdapter.An immutable data structure gives good logical properties to someone using it, but often worse performance than its mutable counterpart depending on how it’s implemented. In the simplest implementation, when we’d like to change an item, we would have to make a copy of the entire immutable data structure with that single item changed. Our first, naive implementation of a snapshot of all the photo metadata was a sorted array of all the photos in the timeline. Say a user with 50,000 photos hides a photo in their timeline; making a copy of the entire photo metadata array and re-sorting it (a guard for the more general case of photos changing events) takes us almost a second! That’s a terrible user experience. To remove this latency, we improved the performance of these immutable snapshots dramatically by changing up the structure so we could take fast, shallow copies.Taking advantage of the fact that we group our photos into events and that changes tend to happen to photos in a handful of events at once, we changed the structure of metadata snapshots from a single array of photos to an array of events with pointers to arrays of photos in each of them.To make a copy of this snapshot with a single event changed, we only have to make a deep copy of the single changed event and then copy pointers to other events that keep their own arrays of photos.This scheme gets a bit more complicated because our snapshots also cache information to make various lookups faster, but having the ability to make fast, shallow copies remains the core idea. We expose an interface for looking up a photo by absolute position in the entire snapshot; this is implemented by keeping the offsets of the beginnings of the events into the full list, then doing a binary search over these to find in which event a photo index lands. Here are the offsets for the earlier example:And here they are after a photo gets added:This accumulator and snapshot design works best because we can hold a user’s view model with metadata for all of their photos in memory. We can’t hold all of the user’s photos in memory at the same time, though, so we have a different solution designed to keep a window of the thumbnails in memory at any given time, fetching them around where the user is looking. There are also more parts of the UI layer that we’ve optimized between the snapshot model and the end result users see. For example, the layout for showing users’ metadata and photos in conversations is also significantly different from the events view. We’ll dive into more details on these topics in future blog posts.",https://blogs.dropbox.com/tech/2014/08/building-carousel-part-ii-speeding-up-the-data-model/,0,dropbox,,NULL,2014-08-06
Streaming File Synchronization,"Our users love Dropbox for many reasons, sync performance being chief among them. We’re going to look at a recent performance improvement called Streaming Sync which can improve sync latency by up to 2x.Prior to Streaming Sync, file synchronization was partitioned into two phases: upload and download. The entire file must be uploaded to our servers and committed to our databases before any other clients could learn of its existence. Streaming sync allows file contents to “stream” through our servers between your clients.First we’ll discuss the way Dropbox stores and syncs files. On your local machines, Dropbox attempts to conform to the host file system on your system. However, especially considering that Dropbox supports shared folders, the server side Dropbox file system has a different abstraction. Unlike a traditional file system, a relative path is insufficient.We define a namespace to be an abstraction for the root directory of a more traditional file system directory tree. Each user owns a root namespace. In addition, every shared folder is a namespace which can be mounted within one or many root namespaces. Note that users own namespaces and not vice versa. With this abstraction, every file and directory on the Dropbox servers can be uniquely identified by two values: a namespace and a relative path.Every file in Dropbox is partitioned into 4MB blocks, with the final block potentially being smaller. These blocks are hashed with SHA-256 and stored. A file’s contents can be uniquely identified by this list of SHA-256 hashes, which we refer to as a ‘blocklist’.Here, the blocklist for video.avi is ‘h1,h2,h3,h4’, where h1, h2, h3, and h4 represent hashes of the blocks b1, b2, b3, and b4.This is our big metadata database which represents our file system! Note that it doesn’t contain file contents, just blocklists. It is an append-only record where each row represents a particular version of a file. The key columns in the schema are:Dropbox server typesThere are two types of servers relevant to this discussion:The servers communicate via internal RPCs when necessary.First, we will discuss the protocol prior to streaming sync, to motivate this work.Each desktop client keeps a cursor (a JID) of its location in SFJ for each of its namespaces, which allows it to communicate how ‘up-to-date’ it is with the server.First, let’s discuss what happens on an uploading client when a file appears! The client first attempts to ‘commit’ the blocklist to the server under the (namespace, path). The metaserver checks to see if a) Those hashes are known. b) This user/namespace has access. If not, the commit call returns “need blocks” indicating which blocks are missing. For brand new files, this is often all of them.The uploading client must talk directly with the blockserver in order to add these blocks. We limit the number of bytes per request*, so this may take multiple requests.* In this diagram, there is a limit of 8MB, but we’ve experimented with other values as well.Finally, the client attempts the commit again. This time, it should definitely work. The metaserver will update SFJ with a new row. Congratulations, the file officially exists in Dropbox!Now let’s check out the downloading client. When the client finds out* that updates are available, it will make a “list” call to learn about the new SFJ rows. The call to list takes in the cursors for each namespace as input so that only new entries are returned.* Idle clients maintain a longpoll connection to a notification server.Awesome. There’s a new file. We need to reconstruct the file from the blocks. The downloading client first checks to see if the blocks exist locally (within existing files, or in our deleted file cache). For new files, these checks likely fail, and the client will download directly from the blockserver. The blockserver verifies that the user has access to the blocks and provides them. Similar to store_batch, this may take multiple requests.Now that the client has all the blocks, it can reconstruct the file and add it to the local file system. We’re done! We just demonstrated how a single new file is synced across clients. We have separate threads for sniffing the file system, hashing, commit, store_batch, list, retrieve_batch, and reconstruct, allowing us to pipeline parallelize this process across many files. We use compression and rsync to minimize the size of store_batch/retrieve_batch requests.To sum up the process, here’s the whole thing on one diagram:Typically, for large files, the sync time is dominated by the network time on calls to store and retrieve. The store_batch calls must occur before the SFJ commit. The list call is only meaningful after the SFJ commit. However, retrieve_batch’s dependency on commit is unnecessary. This pointed us toward an optimization which we call Streaming Sync. We want to overlap work on each of the clients. Ideally the downloading client could always be just one blockserver network call behind the uploading client.That looks better! What do we have to do to make this possible?We’re going to maintain metaserver state (separate from SFJ) from the initial failed commit from the UL Client. This will allow the DL Client to make progress prior to the SFJ commit. The DL Client will “prefetch” blocks that are part of these not-yet-committed files. Thus, when the SFJ commit occurs, the DL Client will be (nearly) done already. Lovely!Turns out the uploading client doesn’t need to change its behavior. However, the downloading client needs to hear about non-SFJ changes. We handled this by adding additional output to list. List now returns new SFJ rows as well as new Streaming Sync prefetchable blocklists.We decided to store the additional state in memcache rather than a persisted table. It is not vital that this data persists, and we chose not to incur the additional cost of writing to a table on a failed commit. The memcache entry looks very similar to an SFJ row, except it is versionless. Thus there is no need for a JID.Writes to memcache occur on failed calls to commit Reads from memcache occur on list Deletes occur on successful calls to commit (or memcache evictions)Clients must now maintain a “prefetch cache” of blocks which do not correspond to files in SFJ. Upon list, a client queues up prefetches, which go into this prefetch cache. On new versions of the client, you can find this in the “.dropbox.cache/prefetch_cache/” directory.Nope. Not quite. There’s a few more things left to discuss. We’ve only talked about the “normal” case where a file actually completes being uploaded. We need to make sure that even if the file never completes commit, we don’t break the process. Imagine starting an upload and changing your mind midway. We need to make sure that the server side memcache expires entries and does not thrash. Furthermore, we need the DL client’s prefetch cache to be purged periodically and intelligently so it does not grow unbounded.When accessing blocks, we check validity against the memcache table, but since memcache entry may be mutated, unavailable, expired, or even evicted during the prefetch, we need to arrange fallback behavior on the client when the server cannot verify that a block is eligible for streaming sync. This involved adding special return codes to the store protocol to indicate this condition.We found that streaming sync only affects files that are large enough to require multiple store/retrieve requests, so we limited the feature to large new files. Streaming sync provides an up-to-2x improvement on multi-client sync time. The improvement approaches 2x as the file’s size increases given equal UL/DL bandwidth, but in practice, the speedup is limited by the slower side of the connection. We did a test across two machines with the same network setup, (~1.2 mb/s UL, ~5 mb/s DL). There is an approximately 25% improvement on sync time.As we roll out this feature to the world, we’ll be tracking key metrics like number of prefetched blocks, size of the prefetch cache, and memcache hit/miss rates.You can find client side support for streaming sync in beta version 2.9 of the desktop client and stable version 2.10. We plan to roll out server side support over the course of the next couple of weeks. If you’re excited about sync performance work like this, we’re hiring!",https://blogs.dropbox.com/tech/2014/07/streaming-file-synchronization/,0,dropbox,"java,docker,python",NULL,2014-07-11
Open Sourcing Our Go Libraries,"Dropbox owes a large share of its success to Python, a language that enabled us to iterate and develop quickly. However, as our infrastructure matures to support our ever growing user base, we started exploring ways to scale our systems in a more efficient manner. About a year ago, we decided to migrate our performance-critical backends from Python to Go to leverage better concurrency support and faster execution speed. This was a massive effort–around 200,000 lines of Go code–undertaken by a small team of engineers. At this point, we have successfully moved major parts of our infrastructure to Go.One recurring theme that hindered our development progress was the lack of robust libraries needed for building large systems. This is not surprising since Go is still a very young language. To address this issue, our team started building various libraries to provide better abstractions, such as connection management and a memcache client. We are very excited to announce that we are open sourcing these libraries to help the broader community build large scale production systems.We have included several libraries to kickstart this effort. To highlight a few:We will continue to expand the set of libraries in the repository:https://github.com/dropbox/godropboxTo make sure that we continue to invest in this open source effort, we are committed to using the public version of this repository internally. We are migrating our internal systems to use the libraries directly from this repository. This ensures all fixes and improvements are applied publicly before they are pulled back internally.We hope you will join our community and help make these libraries better!",https://blogs.dropbox.com/tech/2014/07/open-sourcing-our-go-libraries/,0,dropbox,"backend,javascript,frontend,java,ruby,css,react,python,cloud,docker,animation,python3",NULL,2014-07-01
"Building Carousel, Part I: How we made our networked mobile app feel fast and local","When we began the journey of building a mobile app for Dropbox a few years ago, we started simple — our Android and iOS apps allowed our users to view their files on the go, and cache them for offline access. As smartphones became more popular, we realized we could provide another great service on mobile: automatic backup of all the photos and videos taken on these devices, so they’d be safe forever in Dropbox.Last Wednesday, on April 9, we took another giant leap forward with the introduction of Carousel. Carousel is a single home for all your photos and videos, independent of whether they’re local to the device you’re using, or already backed up to Dropbox.While Carousel seems pretty simple on the surface, there were a number of technical challenges we faced in building it. We needed to ship both an Android app and an iOS app on day one, which required us to think critically about how to share code between the two platforms. In order to support collections of over 100,000 photos, we needed to prioritize performance and find a way to beat the garbage collector on Android. In the coming weeks and months, we want to share the story of how we went about building Carousel and provide some insight into the hard engineering problems we solved along the way. Today, we’re going to focus on how we built Carousel to feel fast, responsive, and local, even though the data on which users operate is ultimately backed by the Dropbox servers.As we thought about what we wanted in the next iteration of a mobile photos product, we kept coming back to this guiding principle:A Dropbox-powered gallery app can and should be just as fast as a local gallery app and should never force the user to wait to complete an action. Users should be able to view, curate, and share their photos regardless of state; they should never have to wait or worry about which photos are local and which are not.As long as our app was slower than a local gallery app, we knew it would never become the central place where our users go to view and interact with their photos. With this guiding principle in mind, we took a critical look at the Dropbox app’s photos tab, and identified two key problems that made the app feel way too slow:1. The photos tab makes blocking HTTPS requests in order to sync user actions to the server. For instance, when the user tries to share a photo, this is what they see:The same is true when the user tries to delete a photo from the photos tab. In the event of no connectivity, these requests outright fail and require the user to try again later.2. There’s no way to view and interact with photos that are local only to the device (i.e. not yet uploaded to Dropbox).These two problems, when combined, made the app especially difficult to use in the context of sharing photos with others. In order to share using the Dropbox app, users first had to wait for their photos to back up, then wait on a blocking network request to complete the share. The app also couldn’t be used as a replacement for a traditional gallery, since photos captured offline can’t be viewed at all.To solve these problems, we need to build a unified data model, in which local photos and remote photos are treated as equivalent objects, with all the same capabilities and properties. Second, considering that humans can perceive application response delays at around the 100 ms mark, we simply can’t afford to make user-visible blocking network calls. Instead, we need to build an eventually consistent system, where the user can perform some action, immediately see the effect of that action locally, then eventually see the effect of that action globally on other devices. In the academic world, this is known as optimistic replication.To build a merged view of both local and server content, we first need the client to stay up to date with changes that are happening remotely on the server. To achieve that, we use HTTP long polling to get notified of changes, and use a variant of our delta API to pull those changes down. Delta returns the changes that have occurred to a user’s Dropbox since the last time the client called up to the server. That is, it provides the additions, deletions and modifications to photo metadata that have occurred since the prior cursor. When we fetch these changes, we write the most up-to-date server metadata into a server_photos table in SQLite. The server_photos table is purely a cache of the “truth,” which lives on the server.Meanwhile, our client-side camera roll scanner computes a fast hash of each photo to determine which photos have not yet been backed up to Dropbox. We turn a photo that needs to be uploaded into a photo_upload_operation, and likewise serialize it into SQLite.Finally, before we can render the view, we have a third input source in the form of client-side user actions. Whenever the user hides or deletes a photo in Carousel, we want the action to take effect instantly. We can then asynchronously write that change back to the server. To do so, we construct a HideOperation, or DeleteOperation, which also gets persisted to SQLite.Every user action in Carousel thus becomes an operation, which will eventually be synced to the server. These operations are placed into in-memory operation queues and persisted to SQLite for conservation across app launches. For each queue, there’s a dedicated operation sync thread, which waits until an operation is ready to execute, then makes the HTTPS call necessary to submit the change to the server. Whenever we need to render a view to the user, we consult these pending operations to make sure we’re reflecting the user’s latest actions. It’s only safe to remove these operations once we’re certain we’ve seen their effect come down in the delta. We thus end up with an architecture that looks like this:Let’s walk through an example of rendering the primary grid view to the user.Inside the implementation of list_photos, we:1. Read all server photo metadata out of the server_photos table. 2. Add in all the local photos pending upload. 3. Remove photos which have been hidden or deleted.For example, suppose our server_photos table contains the following data:Our photo_upload_operations table contains the following data:And our photo_modification_operations table contains the following data:Our call to list_photos() will produce as final output the result of unioning local and server content, then applying the pending hide:Note that in practice, forcing the UI to call list_photos() to perform a read from SQLite every time there’s a change to the photo model would be prohibitively expensive. Instead, we keep the photo model loaded in memory, and modify it as changes come in (either via user actions in the app, or remote changes on the server). This is not all that different than the delta API we use to sync down changes from the server. To keep things fast, we essentially introduce another level of delta between disk and memory. In the next blog post, we’ll take a look at exactly how this works, and how it enabled us to build an app that can handle over 100,000 photos.The key idea in the example we walked through above is that applying a client-side photo addition and hide on top of cached server state should provide the same result as eventually uploading the photo and applying the hide on the server. Whenever we render data in Carousel, we first consult the cached server state, then “re-play” pending operations on top of it. In the case of hide & delete, we then rely on last-write-wins semantics on the server to resolve any conflicts.This works really well for photos that are already in Dropbox, since the photos already have server IDs. Each pending operation can store the server ID(s) on which it should be applied. But what happens when we want to allow modifications to photos that haven’t finished uploading yet? As an additional constraint, keep in mind that due to the multi-platform nature of Dropbox, the photo might be uploaded from a source other than the Carousel client. Even when that happens, we still need to resolve any pending actions that were taken on that photo.There are a few different ways to ensure an action taken on a local-only photo gets synced to the server properly. We wanted something simple and relatively stateless to keep the client-side logic easy to reason about. To achieve this, we introduced the concept of a device-specific ID, henceforth referred to as a LUID (locally unique ID), as the canonical way to refer to each photo. A LUID is a stable identifier, meaning it can be used to refer to a photo both before and after it has been uploaded. A LUID is simply an autoincrement integer, and it works like this:When we scan the device for new photos and find a photo that needs to be uploaded to Dropbox, we create a LUID for that local photo. We then add an entry in the local_photo_luids table, which maps the LUID to its native camera roll ID.When a new server photo S comes down in delta, we check if S.hash matches any local photo hashes. If not, we create a new LUID, and add an entry to the server_photo_luids table, which maps the LUID to its server ID.In the event the hash does match some local photo L, it means L has finished uploading and we now have its server metadata available. We assign S.photo_luid = L.photo_luid. At the same time, we also mark the relevant photo_upload_operation as completed. To prevent conflicts (for instance if the same photo gets added to the user’s Dropbox multiple times), the first server photo with the same hash is the one that will “complete” the upload operation and claim the LUID.You’ll notice by using this logic, we always have a stable way to refer to a particular photo without worrying about whether it is on the server or not. This reduces a lot of complexity in the app, since the UI can simply treat LUIDs as the basis of equality between photo objects. When a local photo finishes uploading, we don’t need to worry about tracking down each reference to it and “upgrading” the reference to use the new server ID. The LUID abstracts that away.With LUIDs in hand, let’s take a look at what happens when a user shares in Carousel.Suppose the user selects a batch of photos, some of which are still local only to the device, and some of which are already in Dropbox. Even if one of these photos finishes uploading while the user is still selecting photos, their selection will be preserved, since the selection set is based on LUIDs.After the user selects the recipients with whom they’d like to share, we can construct the corresponding share operation.When we render the resulting conversation view, we read the cached server state for the conversation uniquely identified by the recipients. We then re-play this pending share on top of it, just like all the operations we’ve seen before. We could spend a whole blog post going into more depth here, but for now we’ll defer that discussion.If any of the LUIDs within the share operation are still local only (i.e. they do not have entries in the server_photo_luids table), then we know the share is not yet ready to be submitted to the server. The share operation queue can therefore sleep, and wait until the local-only photos in question are uploaded. We consider this a dependency on the share operation, which must be resolved before the operation is ready for remote execution. As part of constructing the share operation, we also mark the relevant photo_upload_operations as “blocking a share”, so that they become re-prioritized to the front of the upload queue.When the dependent photo uploads complete, the share operation is ready to execute on the server. We look up the server IDs (via the server_photo_luids lookup table) and send a request to the server to perform the share.The best part is that all of this happens asynchronously, so the user is free to continue using the app, or go about their day. No spinners, no waiting.The big lesson we learned from building the Dropbox app photos tab was: don’t block the user! Instead of requiring changes to be propagated to the server synchronously, we built Carousel from day one as an eventually consistent system. With mobile networks still slow and unreliable, we knew this would be the only way to deliver a Dropbox-backed gallery that felt fast and local.The asynchronous, delta-based design to our mobile library empowered us to build an app that was much faster than the Dropbox photos tab. This design enabled us to hide the latency between client and server from the user. In the next installation of this series, we’ll go into more depth on the latency between disk and memory, and how optimizing that was also critical to making the app feel fast.In the meantime, go download Carousel, and let us know your thoughts! ",https://blogs.dropbox.com/tech/2014/04/building-carousel-part-i-how-we-made-our-networked-mobile-app-feel-fast-and-local/,0,dropbox,,NULL,2014-04-14
"Introducing Pyston: an upcoming, JIT-based Python implementation","Hello everyone, I’m very excited to announce Pyston, a new open-source implementation of Python, currently under development at Dropbox.  The goal of the project is to produce a high-performance Python implementation that can push Python into domains dominated by traditional systems languages like C++.Here at Dropbox, we love Python and try to use it for as much as we can.  As we scale and the problems we tackle grow, though, we’re starting to find that hitting our performance targets can sometimes become prohibitively difficult when staying on Python.  Sometimes, it can be less work to do a rewrite in another language.  I personally love Python, and it pains me every time we decide to rewrite something, so I wanted to do something about it.  After some abandoned experiments with static compilation, we looked around and saw how successfully JIT techniques are being applied in the JavaScript space: Chrome’s V8 engine, in particular, has greatly pushed the status quo of JavaScript performance.  Our hope is that by using similar techniques, we can achieve similar performance improvements for Python.Pyston is still in the earliest stages and is not ready for use, but we’re hopeful that by announcing it early in its lifecycle and open-sourcing the code, we can collaborate with the Python and JIT communities throughout its development.  There’s only room for so much detail in this blog post, but we wanted to talk about why we think we need a new Python implementation, and go into a little bit of how Pyston works.There are already a number of Python implementations using JIT techniques, often in sophisticated ways.  PyPy has achieved impressive performance with its tracing JIT; Jython and IronPython are both built on top of mature VMs with extensive JIT support.  So why do we think it’s worth starting a new implementation?In short, it’s because we think the most promising techniques are incompatible with existing implementations.  For instance, the JavaScript world has switched from tracing JITs to method-at-a-time JITs, due to the compelling performance benefits.  Whether or not the same performance advantage holds for Python is an open question, but since the two approaches are fundamentally incompatible, the only way to start answering the question is to build a new method-at-a-time JIT.Another point of differentiation is the planned use of a conservative garbage collector to support extension modules efficiently.  Again, we won’t know until later whether this is a better approach or not, but it’s a decision that’s integral enough to a JIT that it is difficult to test in an existing implementation.The downside of starting from scratch is, unsurprisingly, that creating a new language implementation is an enormous task.  Luckily, tools are starting to come out that can help with this process; in particular, Pyston is built on top of LLVM, which lets us achieve top-tier code generation quality without having to deal with the details ourselves. Nonetheless, a new Python implementation is a huge undertaking, and Pyston will not be ready for use soon.At a high level, Pyston takes parsed Python code and transforms it to the LLVM intermediate representation (IR).  The IR is then run through the LLVM optimizer and passed off to the LLVM JIT engine, resulting in executable machine code.  LLVM contains a large number of optimization passes and mechanisms for easily adding more, which can lead to very fast code.The problem, though, is that LLVM can’t reason about Python code, because all the low-level behavior is hidden behind the type dispatching you have to do in any dynamic language.  To handle this, Pyston employs type speculation: it is typically impossible to prove that a variable will have a specific type, but Pyston can often predict with some certainty what the type of an object can be.  Once a prediction is made, Pyston will verify the prediction at runtime, branching between a fast path where the prediction holds, and a slow path where it doesn’t.Pyston also includes other modern techniques such as hidden classes for fast attribute lookups and inline caches for fast method calls.  You can find more technical details on the Github page, along with a separate blog post that goes into more technical detail.Pyston is still in its infancy and right now only supports a minimal subset of the Python language.  It’s not quite fair to state benchmark numbers, since 1) Pyston doesn’t support a large enough set of benchmarks to be representative, and 2) Pyston doesn’t support all runtime features (including ones that might introduce slowdowns), so it’s not a true apples-to-apples comparison.  With those caveats, Pyston generally is able to beat CPython’s performance, but still lags behind PyPy.The code has been released on Github under the Apache 2.0 license, along with a growing amount of technical documentation.  There’s a lot of work to be done, and we’re looking to grow the team: if this kind of thing interests you, please apply!Stay tuned for more updates as the project progresses.  If you’d like to subscribe to our announcement mailing list, you can do so here.",https://blogs.dropbox.com/tech/2014/04/introducing-pyston-an-upcoming-jit-based-python-implementation/,0,dropbox,"frontend,animation,docker,python,python3",NULL,2014-04-03
Video Processing at Dropbox,"Every day millions of people upload videos to Dropbox. Besides wanting their memories safe forever, they also want to be able to watch them at any time and on any device. The playout experience should feel instant, despite the fact that the content is actually stored remotely. Low latency playback of content poses interesting technical challenges because of the three main factors below.1. Codec diversityMost end users are familiar with extensions like .mp4, .avi, .flv, but not everybody is familiar with the fact that the file extension does not necessarily match the internal encoding of the content. People assume that an .mp4 file will certainly play on a Mac laptop, but that’s not always a safe assumption because the content might be encoded with some Microsoft/Google/Adobe/RealMedia specific codec (e.g. VC1/VP8). The video codec landscape has been very fragmented for at least 30 years now, and despite the efforts of MPEG to create open standards, the situation is still quite messy. The good news is that modern phones tend to produce mostly coherent content using H.264/AVC as codec and MPEG-4 container format, which indeed corresponds to the majority of the content we see in Dropbox.2. Limited end-user network bandwidthUsers access their Dropbox content either via their home/office connection or via a mobile connection. Leaving mobile aside, even home connections are not as fast/reliable as most network providers advertise (see the ISP speed report from Netflix for some fun numbers), so bandwidth adaptation is a must to guarantee a fluid video playout.3. Client capabilitiesDifferent client devices impose different constraints, mostly due to the underlying hardware chipsets, both in terms of memory bandwidth and CPU power. For instance, the iPhone 3GS only supports baseline profile H.264/AVC.The solution to these problems is to transcode (decode and re-encode) the source video to a target resolution/bit rate and codec that is suitable for a given client. At the beginning of the development of this feature, we entertained the idea to simply pre-transcode all the videos in Dropbox to all possible target devices. Soon enough we realized that this simple approach would be too expensive at our scale, so we decided to build a system that allows us to trigger a transcoding process only upon user request and cache the results for subsequent fetches. This on-demand approach:We managed to achieve our first 2 goals above by using HTTP Live Streaming (HLS). The basic idea of HLS is to structure the data in several playlists describing quality layers and data segments that are transmitted over HTTP. The standard was born as an Apple specific solution but is now an open RFC and is also (partially) supported on Android devices. The protocol effectively works in 3 steps that the player has to follow sequentially. The very first URL a player hits returns the main HLS playlist file that looks in our case something like:Each #EXT-X-STREAM-INF tag provides a URL to a playlist for content at a different target bit rate. In the example above, we have 3 layers at increasing qualities, so the player can pick the best one for a given connection speed. The second step consists of fetching the layer playlists, potentially parallelizing the operation for all layers to save roundtrip time. Each layer playlist looks something like:Unlike the first response, each URL in the layer playlist now points to a segment of actual data. Apple has very good technical notes providing generic recommendations on how to encode and segment content for HLS streaming.To begin streaming, a client application first issues a request to our web servers to obtain a temporary token (in the form of a URL) for the main HLS playlist. Since video playout typically happens on dedicated players that are not necessarily part of a client application, the token includes a one time password and expiration information that enables our servers to authenticate the external player before returning the content to it. The handler of this first request verifies if the content is already cached and, if that’s not the case, kicks off a transcoding job with different parameters based on client capabilities and network connection. Since H.264/AVC video transcoding is an extremely intensive operation and each transcoder machine can only perform a limited number of transcodes in parallel, it’s important to pick the best worker at every request. The URL we return to the client also embeds information that allows us to route the request back to the transcoding worker, which is important to be able to serve the content while it’s being transcoded and before it’s cached in our backend.Our worker clusters are implemented on Amazon AWS and consist of the following components:We use ffmpeg for the actual transcoding because it supports most formats. Our pipeline implements the following three steps.1) prepare the stream for cookingSince we want to stream data as we transcode it, we need to rearrange the input stream in a way that is suitable for piping it into ffmpeg. Many people refer to this process as “fast-starting” the video, and there are a few tools available on the internet that can help you get started. Ultimately, we wrote our own solution in python to allow us to debug issues and profile performance. In practice fast-starting for mp4 consists of extracting the “moov atom,” which contains most of the video’s metadata, rearranging it to the beginning of the file, and then adjusting the internal offsets to the data accordingly. This allows ffmpeg to immediately find the information about resolution, duration and location of data atoms and start the transcoding as the data is fed into it.2) re-encode the streamThe command line for ffmpeg looks something like the following:We use H.264/AVC baseline profile level 3.0 to guarantee compatibility with all devices including iPhone 3GS (we are planning to improve on that in the near future). Some of the parameters are the result of us trading off a bit of quality to minimize the startup time for live transcoding. Specifically, we found that reducing the value of muxdelay, having only one reference frame and disabling scenecut detection all contributed in reducing the latency introduced by ffmpeg. The output container format is MPEG transport as required by HLS.3) Segment the transcoded outputThe output of ffmpeg is segmented with a C++ tool we developed internally on top of libavcodec. Apple provides a segmenter tool but we decided to not use it because it runs only on Mac (we are linux friends like most readers here) and does not natively support pipelining. Also, recent versions of ffmpeg (we use 2.0) come with a segmenter tool, but we found it introduces significant latency to our pipeline. In summary, the reasons why we ended up writing our own tool were because it allows us to 1) optimize end-to-end latency, 2) guarantee the presence and the positioning of IDR (Instantaneous Decoder Refresh) frames in every segment and 3) customize the length of the segments we generate.The last point is particularly important because the length of the video segment is directly proportional to the transmission time from the server to the client on a bandwidth constrained channel. On one hand, we want very short segments to lower the transmission time of each of them but on the other hand we’d like to minimize the number of in-flight requests and the overhead per request due to the roundtrip latency between the client and the server. Since we are optimizing for startup latency, we begin with smaller segments and then ramp up to longer ones to diminish request overhead, up to the target segment length of 5 seconds per segment. Specifically, the length for the very first segments looks something like 2s, 2s, 3s, 3s, 4s, 4s, 5s, …. We picked these values because a) the standard poses some restrictions on how fast we can increase the length of consecutive segments (this is to avoid possible underflows) and Android does not allow for fractional segment lengths (those were introduced in version 3 of the HLS standard).During the development and tuning process of the pipeline we saw the startup time reducing dramatically from ~15/20 seconds to ~5 seconds, as you can see from the rocky graph below.Still, that is not sufficient to provide the “feel instant” experience we wanted for our users so we revisited the idea of pre-transcoding some of the material and we decided to process only the first few seconds of every video. The pre-transcoding cluster is independent of the live transcoding one to not affect the performance of live traffic and is hooked up with a pipeline that is triggered on every file upload. The first few seconds of every video are processed and stored in our cache, and the remaining part is generated on demand whenever the user requests. This approach allows us to transmit to the client the first segments very quickly while the transcoder starts up and seeks to the desired offset. We retain references to all processed material so we can easily implement different retention policies as needed.The combination of pre-transcoding, shorter segments at the beginning and lowered buffering time in the video processing pipeline allowed to reach our goal of 2-3 seconds startup time on a client on a good connection, providing the desired instant experience. We learned a few things when building the system at scale:",https://blogs.dropbox.com/tech/2014/02/video-processing-at-dropbox/,0,dropbox,,NULL,2014-02-18
Improving Dropbox Performance: Retrieving Thumbnails,"Dropbox brings your photos, videos, documents, and other files to any platform: mobile, web, desktop, or API. Over time, through automatic camera uploads on iOS and Android, you might save thousands of photos, and this presents a performance challenge: photo thumbnails need to be accessible on all devices, instantly.We pre-generate thumbnails at various resolutions for the different devices at upload time, to reduce the cost of scaling photos at rendering time. But when users are quickly scrolling through many photos, we need to request a large number of thumbnails. Since most platforms have limitations on the number of concurrent requests, the requests might get queued and cause slow render times. We present a solution that allows us to reduce the number HTTP requests and improve performance on all platforms, without major changes to our serving infrastructure.Let’s look at this problem in more detail on the web, specifically the Photos tab at www.dropbox.com/photos. Here’s what the Network view in Chrome’s Developer Tools looks like if we were to load every photo thumbnail on the page individually:You can see that a limited set of images is loaded in parallel, blocking the next set of thumbnails from being loaded. If the latency of fetching each image is high—e.g. for users far away from our datacenters—loading the images can drastically increase the page load time. This waterfall effect is common for web pages loading lots of subresources, since most browsers have a limit of 6 concurrent connections per host name.A common workaround for web pages is to use domain sharding, spreading resources over multiple domains (in this case photos1.dropbox.com, photos2.dropbox.com, etc.) and thus increasing the number of concurrent requests. However, domain sharding has its downsides—each new domain requires a DNS resolution, a new TCP connection, and SSL handshake—and is also not practical when loading thousands of images and requiring many domains. We saw similar issues on our mobile apps: both iOS and Android have per-host or global limits on the number of concurrent connections.To solve the problem, we need to reduce the number of HTTP requests. This way we avoid problems with request queueing, make full use of the available connections, and speed up photo rendering.Before embarking on any performance improvement, we need to make sure we have all of the instrumentation and measurements in place. This allows us to quantify any improvements, run A/B experiments to evaluate different approaches, and make sure we’re not introducing performance regressions in the future.For our web application, we use the Navigation Timing API to report back performance metrics. The API allows us to collect detailed metrics using JavaScript, for example DNS resolution time, SSL handshake time, page render time, and page load time:Similarly, we log detailed timing data from the desktop and mobile clients.All metrics are reported back to our frontends, stored in log files and imported into Apache Hive for analysis. We log every request with metadata (e.g. the originating country of the request), which allows us to break down the metrics. Hive’s percentile() function is useful to look at the page load time distribution – it’s important to track tail latency in addition to mean. More importantly, the data is fed into dashboards that the development teams use to track how we’re doing over time.We instrumented our clients to measure how long it takes to load thumbnails. This included both page-level metrics (e.g. page render time) and more targeted metrics measured on the client (e.g. time from sending thumbnail requests to rendering all the thumbnails in the current viewport).With the instrumentation in place, we set off on improving the thumbnail loading times. The first solution we had in mind was SPDY. SPDY improves on HTTP by allowing multiple multiplexed requests over a single connection. This solves the issue with request queueing and saves on round-trips (a single TCP connection and SSL handshake needs to be established for all the requests). However, we hit a few roadblocks on the way:Instead of SPDY, we resorted to plain old HTTPS. We used a scheme where clients would send HTTP requests with multiple image urls (batch requests):The server sends back a batch response:The response is:Since the scheme is relatively simple and uses plain HTTPS instead of SPDY, it allowed us to deploy it on all platforms and we saw significant performance improvements: 40% page load time improvement on web.However, we don’t see this as a long-term strategy – we’re planning on adding SPDY support to all of our clients and take care of pipelining at the protocol level. This will simplify the code, give us similar performance improvements and better cacheability (see note about consistent batches above).The Dropbox performance team is a small team of engineers focused on instrumentation, metrics and improving performance across Dropbox’s many platforms. If you obsess over making things faster and get excited when graphs point down and to the right, join us!",https://blogs.dropbox.com/tech/2014/01/retrieving-thumbnails/,0,dropbox,"php,frontend,css",NULL,2014-01-27
Outage post-mortem,"On Friday evening our service went down during scheduled maintenance. The service was back up and running about three hours later, with core service fully restored by 4:40 PM PT on Sunday.For the past couple of days, we’ve been working around the clock to restore full access as soon as possible. Though we’ve shared some brief updates along the way, we owe you a detailed explanation of what happened and what we’ve learned. What happened?We use thousands of databases to run Dropbox. Each database has one master and two replica machines for redundancy. In addition, we perform full and incremental data backups and store them in a separate environment.On Friday at 5:30 PM PT, we had a planned maintenance scheduled to upgrade the OS on some of our machines. During this process, the upgrade script checks to make sure there is no active data on the machine before installing the new OS.A subtle bug in the script caused the command to reinstall a small number of active machines. Unfortunately, some master-replica pairs were impacted which resulted in the site going down.Your files were never at risk during the outage. These databases do not contain file data. We use them to provide some of our features (for example, photo album sharing, camera uploads, and some API features).To restore service as fast as possible, we performed the recovery from our backups. We were able to restore most functionality within 3 hours, but the large size of some of our databases slowed recovery, and it took until 4:40 PM PT today for core service to fully return. What did we learn?Distributed state verificationOver the past few years our infrastructure has grown rapidly to support hundreds of millions of users. We routinely upgrade and repurpose our machines. When doing so, we run scripts that remotely verify the production state of each machine. In this case, a bug in the script caused the upgrade to run on a handful of machines serving production traffic.We’ve since added an additional layer of checks that require machines to locally verify their state before executing incoming commands. This enables machines that self-identify as running critical processes to refuse potentially destructive operations.Faster disaster recoveryWhen running infrastructure at large scale, the standard practice of running multiple replicas provides redundancy. However, should those replicas fail, the only option is to restore from backup. The standard tool used to recover MySQL data from backups is slow when dealing with large data sets.To speed up our recovery, we developed a tool that parallelizes the replay of binary logs. This enables much faster recovery from large MySQL backups. We plan to open source this tool so others can benefit from what we’ve learned.We know you rely on Dropbox to get things done, and we’re very sorry for the disruption. We wanted to share these technical details to shed some light on what we’re doing in response. Thanks for your patience and support.Akhil Head of Infrastructure",https://blogs.dropbox.com/tech/2014/01/outage-post-mortem/,0,dropbox,,NULL,2014-01-12
Dropbox Status Update,"UPDATE 1/12 at 7:23pm PT: Dropbox should now be up and running for all of you, but we’re working through a few last issues with the Dropbox photos tab.  More info on our main blog and the latest post here.UPDATE 1/12 at 1:59pm PT: Hi everyone, we wanted to give an update on where things stand.As of this morning at 4:10am PT, nearly all users (over 99%) can access their files on dropbox.com. The Photos tab is still turned off, but you can access your photos via the Files tab on dropbox.com or the desktop client. We’re continuing to make a lot of progress restoring full service to all users, and are doing so in careful steps.About 5% of our users are still experiencing problems syncing from the desktop client, and about 20% of users are having issues accessing Dropbox through our mobile apps. Within a few hours, we’ll be rolling out a change that will further improve things for those users. We’ll give an update after that.Your files have been safe this entire time. Thanks again for your patience.UPDATE 1/12 at 8:48am PT: We’re still seeing service issues for a small number of users. We’ve been working through the night to restore full service as soon as possible and we’ll continue until this is complete.UPDATE 1/11 at 11:16pm PT: We’re continuing to make progress on reducing the number of users experiencing service issues. We’ll keep providing updates here.UPDATE 1/11 at 6:35pm PT: Dropbox is still experiencing lingering issues from last night’s outage. We’re working hard to get everything back up, and want to give you an update.No files were lost in the outage, but some users continue to run into problems using various parts of dropbox.com and our mobile apps. We’re rapidly reducing the number of users experiencing these problems, and are making good progress.We’re also working through some issues specific to photos. In the meantime, we’ve temporarily disabled photo sharing and turned off the Photos tab on dropbox.com for all users. Your photos are safely backed up and accessible from the desktop client and the Files tab on dropbox.com.We know how much you all rely on Dropbox, and we’re sorry for the trouble. Thanks for your patience — we’ll keep you up to date.UPDATE 1/11 at 10:24am PT: We’re still experiencing service issues related to the outage last night. We apologize and are working to get the service fully restored as soon as possible.UPDATE 1/10 at 8:36pm PT: Dropbox site is back up! Claims of leaked user information are a hoax. The outage was caused during internal maintenance. Thanks for your patience!1/10 at 6:40pm PT: We are aware that the Dropbox site is currently down. This was caused during routine internal maintenance, and was not caused by external factors. We are working to fix this as soon as possible. We apologize for the inconvenience.",https://blogs.dropbox.com/tech/2014/01/dropbox-status-update/,0,dropbox,,NULL,2014-01-11
Scaling MongoDB at Mailbox,"Mailbox has grown unbelievably quickly. During that growth, one performance issue that impacted us was MongoDB’s database-level write lock. The amount of time Mailbox’s backends were waiting for the write lock was resulting in user-perceived latency.While MongoDB allows you to add shards to a MongoDB cluster easily, we wanted to spare ourselves potential long-term pain by moving one of the most frequently updated MongoDB collections, which stores email-related data, to its own cluster. We theorized that this would, at a minimum, cut the amount of write lock contention in half. While we could have chosen to scale by adding more shards, we wanted to be able to independently optimize and administer the different types of data separately.I started by poring through the MongoDB documentation. I quickly found the cloneCollection command. However, to quote the MongoDB 2.2 documentation: “cloneCollection cannot clone a collection through a mongos: you must connect directly to the mongod instance.” In other words, you can’t use this command with a sharded collection. You can’t use renameCollection on sharded collections either, closing off other possibilities. There were other possible solutions, but they all would’ve impacted performance for Mailbox users or would have simply failed to work at Mailbox’s scale.So, I wrote a quick Python script to copy the data, and another to compare the original versus the copy to ensure data integrity. Along the way, I encountered many surprises. For example, a single Python process using gevent and pymongo can copy a large MongoDB collection in half the time that mongodump (written in C++) takes, even when the MongoDB client and server are on the same machine.Our experiences have culminated in Hydra, our newly open-sourced set of tools we’ve developed for MongoDB collection migration.To copy all documents in a collection, I started with an intentionally naive implementation that didn’t have much more code than this:It was obvious that such a naive approach wouldn’t perform well for larger amounts of data, so I quickly experimented with different means of achieving faster copy performance. I implemented various micro-optimizations, like adjusting how many documents the MongoDB driver fetched at once. However, those only yielded only marginal performance improvements. My goal was to finish the data migration in about a day, I was still far from that goal.An early experiment I did was to measure the “speed of light” for MongoDB API operations – the speed of a simple C++ implementation using the MongoDB C++ SDK. Being rusty at C++ and wanting my mostly Python-proficient colleagues to easily be able to use/adapt the code for other uses, I didn’t pursue the C++ implementation too far but found that for simple cases, a naive C++ implementation was typically 5–10 times as fast as a naive Python implementation for the same task.So, I returned to Python, which is the default language of choice for Dropbox. Moreover, when performing a series of remote network requests, such as queries to mongod, the client often spends much of its time waiting for the server to respond; there didn’t seem to be very many CPU-intensive parts for copy_collection.py (my MongoDB collection copying tool). This was corroborated by the very low CPU usage of the initial copy_collection.py.I then experimented with adding concurrent MongoDB requests to copy_collection.py. Initial experiments with worker threads resulted in disappointment. Next, I tried using worker processes communicating through a Python Queue object. The performance still wasn’t much better, because the overhead of the IPCs was overwhelming any potential concurrency benefits. Using Pipes and other IPC mechanisms didn’t help much either.Next, I decided to see how much performance I could squeeze out of a single Python process using asynchronous MongoDB queries. One of the more popular libraries for this is gevent, so I decided to give it a try. gevent patches standard Python modules, such as socket, to execute asynchronously. The beauty of gevent is that you can write asynchronous code that reads simply, like synchronous code.Traditionally, asynchronous code to copy documents between two collections might have looked like this:With gevent, the code uses no callbacks and reads sequentially:This simple code will copy documents from a source MongoDB collection to a destination, based on their _id fields, which are the unique identifiers for each MongoDB document. copy_documents delegates the work of copying documents to greenlets (which are like threads but are cooperatively scheduled) that run copy_document(). When a greenlet performs a blocking operation, such as any request to MongoDB, it yields control to any other greenlet that is ready to execute. Since greenlets all execute in the same thread and process, you generally don’t need any kind of inter-greenlet locking.With gevent, I was able to achieve much faster performance than either the thread worker pool or process worker pool approaches. Here’s a summary of the performance of each approach:Combining gevent with worker processes – one for each shard – yielded a linear increase in performance. The key to using worker processes efficiently was to eliminate as much IPC as possible.Somewhat surprisingly, using gevent in just a single process could produce a full copy of a collection in just under half the time as the mongodump tool, which is written in C++ but queries synchronously and is single-process/thread.Because MongoDB is not transactional, when you try to read a large MongoDB collection while updates are being performed to it, you will receive a result set that reflects MongoDB’s state at different points in time. For example, suppose you start reading a whole collection using a MongoDB find() query. Your result set could look like this:Moreover, to minimize the downtime required to point the Mailbox backend to the new copy of the collection, it was necessary to figure out a way to stream changes from the source MongoDB cluster to the new MongoDB cluster with as little latency as possible.Like most asynchronously replicating data stores, MongoDB uses a log of operations – its oplog – to record and distribute a record of the insert/update/remove operations executed on a mongod instance to other mongod replicas. Given a snapshot of the data, the oplog can be used to apply all changes performed since the snapshot was taken.So, I decided to stream oplog entries from the source cluster and apply those changes at the destination cluster. Thanks to an informative post on Kristina Chodorow’s blog, I was quickly able to grasp the basics of the oplog format. Replicating inserts and removes was trivial, because their serialization format is straightforward. On the other hand, updates took more work.The structure of update oplog entries was not immediately obvious, and in MongoDB 2.2.x, it uses duplicate keys that can’t be displayed by the Mongo shell, let alone most MongoDB drivers. After some thought, I devised a workaround that simply used the _id embedded in the update to trigger another copy of the document from the source. While this doesn’t have identical semantics as applying just the specified update, this guarantees that the copied data is at least as recent as the op we’ve received. Here is a diagram showing how intermediate versions of documents (in this case, v2) are not necessarily copied, but the source and destination are still eventually consistent:I also ran into a performance issue replaying ops on the destination cluster. Though I had a separate process to replay ops for each shard, applying ops serially (my initial approach for prototyping and ensuring correctness) was far too slow to keep up with the onslaught of Mailbox queries.Applying ops concurrently seemed to be the way to go, but the question was how to preserve correctness. Specifically, two operations affecting the same _id cannot execute out of order. A simple workaround I devised was to maintain, in a Python set, the set of _ids being modified by in-progress operations. When copy_collection.py encounters another update to an _id that is currently being updated, we block the later update and any other ops that come after it from being applied. We start applying new ops only when the older operation on the _id has finished. Here’s a diagram to illustrate op blocking:Comparing the copied data to the original is normally a straightforward operation. Doing it efficiently also isn’t particularly challenging when you use multiple processes and gevent.However, doing it when the source and the copy are both being updated requires some thought. At first, I tried just logging warnings whenever compare_collections.py (the tool I wrote to compare two collections) found a data inconsistency in a document that had been recently updated. Later, I could repeat verification for those documents. However, that doesn’t work for deleted documents, for which there remains no last modified timestamp.I started thinking about the term “eventual consistency,” which is often used when talking about asychronously replicating systems such as MongoDB’s replica sets and MySQL’s master/slave replication. Given enough time (i.e. after some amount of retries) and barring catastrophe, the source and the copy will eventually become consistent. So, I added retry comparisons with an increasing backoff between successive retries. There are potential issues with certain cases, such as data that oscillates between two values. However, the data being migrated didn’t have any problematic update patterns.Before performing the final cutover from the original MongoDB cluster to the new MongoDB cluster, I wanted the ability to verify that the most recent ops had been applied. So, I added a command-line option to compare_collections.py to compare the documents modified by the most recent N ops. Running this for a sufficiently large set of ops during downtime would provide additional confidence that there weren’t undetected data inconsistencies. Running it for even hundreds of thousands of ops per shard only takes a few minutes. This also mitigates concerns regarding undetected data inconsistencies resulting from the compare/retry approach.Despite taking various precautions to handle errors (retries, catching possible exceptions, logging), there were still an uncomfortable number of issues arising during my final test runs leading up to the production migration. There were sporadic network issues, a specific set of documents that was consistently causing mongos to sever its connection from copy_collection.py, and occasional connection resets from mongod.Soon, I realized that I couln’t identify all the relevant failure scenarios, so I shifted my focus to quickly recovering from failures. I added logging of _ids of documents for which compare_collections.py had detected inconstencies. Then, I created another tool whose sole job was to re-copy the documents with those _ids.During the production migration, copy_collection.py created an initial snapsphot of hundreds of millions of emails and replayed more than a hundred million MongoDB operations. Performing the initial snapshot, building indices, and catching up on replication took about 9 hours – well within the 24 hour goal I had set. I continued to let copy_collection.py replay ops from the source cluster’s oplogs for another day while I used compare_collections.py to verify all copied data three times (for additional safety).The actual cutover to the new MongoDB cluster happened recently. The MongoDB-related work was very short (a few minutes). During a brief maintence window, I ran compare_collections.py to compare documents modified by the last 500,000 operations in each shard. After detecting no inconsistencies in the most recently updated data, we ran some smoke tests, pointed the Mailbox backend code to the new cluster, and brought the Mailbox service back up to the public. Our users haven’t reported any issues caused by the cutover. This was a success in my mind, as the best backend migrations are invisible to our users.In contrast, our backend monitoring showed us the true benefits of the migration:The decrease in the percentage of time the write lock was held was far better than the linear (50%) improvement we had expected based on our MongoDB profiling. Great success!We’re open-sourcing Hydra, the suite of tools we developed to perform the aforementioned MongoDB collection migration. We hope this code will be useful for anyone who needs to perform a live re-partitioning of their MongoDB data.",https://blogs.dropbox.com/tech/2013/09/scaling-mongodb-at-mailbox/,0,dropbox,"angular,nodejs,frontend,javascript",NULL,2013-09-12
Welcome Guido!,"  Today we’re excited to welcome a new member of the Dropbox family under unusual circumstances. Though he’s joining us now, his contributions to Dropbox date back to day one, all the way to the very first lines of code.Some people only need to be introduced by their first name, and the BDFL is one of them. Dropbox is thrilled to welcome Guido, the creator of the Python programming language and a long-time friend of ours.From the beginning, it was clear that Dropbox had to support every major operating system. Historically, doing so presented a serious challenge for developers: because each platform required different development tools and programming languages, developers had to write the same code multiple times.We didn’t have time for that, and fortunately Python came to the rescue. Several years earlier, Python became my favorite programming language because it had a balance of simplicity, flexibility, and elegance. These qualities of Python, and the community’s work to support every major platform, let us write the code just once before running it everywhere. They have also influenced our greater design philosophy at Dropbox as we set out to build a simple product that brings your life together.It’s been five years since our first prototype was saved as dropbox.py, and Guido and the Python community have been crucial in helping us solve interesting challenges for more than 100 million people.So we welcome Guido to Dropbox with admiration and gratitude. Guido inspires all of us and has played a critical part in how Dropbox ties together the products, devices and services in your life. We’re delighted to have him as part of the team.",https://blogs.dropbox.com/tech/2012/12/welcome-guido/,0,dropbox,"frontend,python,javascript,css,react,python3",NULL,2012-12-07
Caching in theory and practice,"Hello, my name is Pavel Panchekha. I was an intern at Dropbox back in ’11, and one thing I’ve investigated are various caching algorithms. The Dropbox mobile client caches frequently-accessed files, so that viewing them doesn’t require a network call. Both our Android and iOS clients use the LRU caching algorithm, which often selects good files to cache. But while this is the usual algorithm for caching, I wondered: are there better algorithms, and if not, why is LRU the best option?Let’s formalize the problem. We have a large set of files, and we’d like an algorithm to determine which (k) files to keep at any point. We’re assuming all files have the same size. In fact, Dropbox stores files in 4MB blocks, so this simplification isn’t too far off (we’ll see later how to avoid it). To determine which files to keep, every time we want to use a file, we tell the cache to fetch it for us. If the cache has a file, it will give us its copy; if not, the cache has to fetch the new file, and it might also want to remove a file from its cache to make room for the new file.Note also that we’re stuck with an on-line algorithm: we can’t predict what files a user will want in the future.The cache needs to be fast, along two metrics. First, The cache should ensure that as many of the requests for files go to it (cache hit), not over the network (cache miss). Second, the overhead of using a cache should be small: testing membership and deciding when to replace a file should be as fast as possible. Maximizing cache hits is the goal of the first part of this post; quickly implementing the cache will be the topic of the second part.How do we measure the worst case number of cache misses? Unlike a normal algorithm, our runtime is driven by the user’s actions. So our worst-case performance corresponds to our worst-case user: one who maximally breaks our cache at every step.But a pure adversarial analysis won’t work, since a user can always make our cache perform badly by just requesting lots of files –eventually, some of them won’t be cached.The key is to compare how well our algorithm performs with how well our algorithm could possibly perform. We need some benchmark. Zero cache misses is a lower bound but is usually impossible. So instead, let’s compare our algorithm with one that can “plan ahead” perfectly: let’s compare our algorithm – which at any point only has the requests from the past – with some sort of optimal magic “future-seeing” algorithm.More specifically, we’re going to find the ratio of cache misses from our algorithm to the number of cache misses for the optimal algorithm. And then we’re going to try to minimize this ratio across all possible sequences of file requests from the user. Generally, we’ll argue that the algorithm we’re analyzing will have at most (A) misses during any particular sequence of instructions, during which the optimal algorithm must have at least (O) misses; thus the “competitive ratio” is at most (A / O). This type of analysis is called competitive analysis.In general, our method will be to pick a sequence that a chosen algorithm performs very poorly on. We find how many cache misses, (A), that algorithm sees for that sequence of requests. Usually, we’ll be able to calculate (A) precisely. Then, we’ll try to think up the cleverest possible way to cache files for that specific sequence; the number of cache misses we see we’ll call (O). We’ll usually find some possible way of caching files and calculate the number of cache misses for that, so we’ll get an upper bound on (O). The competitive ratio is (A / O), and since we had an upper bound on (O), we get a lower bound on the competitive ratio. Furthermore, our algorithm could perform even worse on a different sequence, so (A / O) is definitely a lower bound. This lets us say that some algorithm is really bad, but doesn’t let us say that some algorithm is really good. We’ll also prove some upper bounds on the competitive ratio, which will let us claim that some algorithms are optimal. Together, these will give us a way to compare caching algorithms.Before we go ahead to analyze a bunch of caching algorithms, we need caching algorithms to analyze. So let’s quickly list a bunch of popular ones:This is a nice batch of the more common and simple caching algorithms, so let’s look at how we perform using the competitive analysis above.We can easily construct sequences to stump the Most Recently Used algorithm. For example, consider the sequence of file accesses (1, 2, dots, k, k+1, k, k+1, k, dots). Most Recently Used will kick out (k) to make room for (k+1), then kick out (k+1) to make room for (k), and so on. It will have a cache miss on every file lookup for this sequence of files. And it’s so easy to do better: an optimal algorithm might, for example, kick out (1) to make room for (k+1), and never have a cache miss after that (since both (k) and (k+1) are in the cache after that). The optimal algorithm sees at most (k+1) cache misses, while Most Recently Used sees (N) cache misses, making it ((N / (k+1)))-competitive. Since we can make (N) as large as we want, this can be arbitrarily large – we might call the Most Recently Used algorithm (infty)-competitive. So, really bad.The Least Recently Used algorithm is better. For example, on that input sequence, it does precisely what the optimal algorithm might do. But it still doesn’t do that well. Imagine if our sequence of requests is for files (1, 2, dots, k, k+1, 1, 2, dots, k, k+1, dots). The Least Recently Used algorithm will miss every time, since for every request, the file requested was just kicked out. And the optimal algorithm can always just swap for the most-recently-requested file. So first, it would fail to find (k + 1) in the cache and replace (k) with it. Then it would fail to find (k) and replace (k – 1) with it. Then (k – 1) with (k – 2), and so on. This yields one cache miss every (k) requests; so if there are (N) requests total, the optimal algorithm would face (k + frac{N}{k}) failures (the “k” for populating the cache with (1, dots, k)), while the Least Recently Used algorithm would face (N) failures. Thus the Least Recently Used algorithm is at best (N / (k + (N / k)))-competitive, which for large (N) works out to be (k)-competitive.This doesn’t show that the Least Recently Used algorithm is (k)-competitive; it tells us that the Least Recently Used algorithm isn’t better than (k)-competitive. But with a bit more effort, we can prove that the Least Recently Used algorithm is precisely (k)-competitive.To do that, we’ll have to make an argument about all possible input sequences. The core of the proof is to look at what must happen for LRU to fail. If the Least Recently Used algorithm has (k+1) cache misses, it must be because (k+1) new files were requested. But if this happens, at least one of those files wasn’t in the cache that the optimal algorithm had (since it, too, can only cache (k) files).To capture this property precisely, let’s divide the sequence of files into phases – during each phase, only some (k) specific files are requested. LRU may fail on at most each of these new files before they are all in the cache – at most (k) times. Meanwhile, the optimal algorithm fails at least once, since at least one of those files isn’t yet in the cache (if they all are, then we never ended the previous phase). So LRU is precisely (k)-competitive.This doesn’t sound that good. In a way, the larger our cache, the less impressively LRU performs. But in fact, our argument that Least Recently Used is (k)-competitive is applicable to any algorithm for which we can predict what files it will cache. So while (k) times worse than perfect seems pretty poor, it is in fact the best we can do (unless we use randomized algorithms; I’ll discuss why not to do that in a bit).LRU only made use of very basic timing information. A smarter algorithm, you might imagine, might actually maintain some popularity information: which files you use often, and which more rarely. Does it do any better?It seems that Least Frequently Used should do much better than LRU, since it incorporates actual information about how popular various files are, instead of just timing information.But let’s do the analysis proper, just in case. To make LFU perform poorly, we’d need to make it keep switching between two files, each time kicking one out to make room for the other. This might happen if we use, say, files (1, 2, dots, k-1) very frequently, and files (k) and (k+1) infrequently, but equally so. If we just request (1, dots, k-1) once or twice and then alternate between (k) and (k+1), this isn’t too much of a problem, since eventually both (k) and (k+1) will be more frequently used than (1, dots, k-1) and will both be cached. But if we first use (1, dots, k-1) a bunch, so that neither (k) nor (k+1) are ever more popular than them, we can create a lot of cache misses. What we are setting up is a case where usage patterns change. First, we used (1) through (k-1) a lot, and then we changed to using (k) and (k+1) a lot.An example such sequence requests (1, dots, k-1) (N) times, and then alternates between (k) and (k+1) (N) times. Both (k) and (k+1) are less frequently used than any of (1, dots, k-1), so each is kicked out to make room for the other. This leads to (k-1) cache misses to load (1) through (k-1) into the cache, and then (2N) cache misses for the requests to (k) and (k+1). On the other hand, the optimal algorithm could do so much better. For example, it could kick out (1) to make room for (k+1) when it stops being used, leading to (k+1) total cache misses. So the LFU algorithm had (2N + k – 1) misses, and the optimal algorithm had (k + 1). The quotient of these can be made arbitrarily large by increasing (N), so LFU can be arbitrarily bad.This result is curious. LFU semi-intelligently made use of popularity data, but fared so much worse than LRU, which just made use of basic timing data. But, the cases that make LFU perform poorly are relatively real-world. For example, suppose you have a large project that you’re working on, and then you finish said project and no longer access those files. Your cache would be storing those old files instead of the new ones you’re using. So our analysis told us something surprising: that LFU, which looked so promising, could actually be absurdly bad, in perhaps real-world situations.In fact, if you think about it, LRU does make some use of popularity information. If a file is popular enough to be used more often than once every (k) times, it will always be in an LRU cache. But by forgetting any information more than (k) files ago, the LRU algorithm prevents really old files from taking precedence over new ones.You’d think it’d be possible to combine the best of LRU and LFU to make an algorithm that performs better than either. Turns out, yes and no.When we proved LRU no better than (k)-competitive, we choose a sequence where the next file requested was always the file not in the cache. But we can do this for any deterministic algorithm! This means that the worst-case behavior of any deterministic algorithm is guaranteed to be no better than (k)-competitive.But in a practical sense, better algorithms do exist. For reference, the ARC1 and CAR2 algorithms tend to outperform Least Recently Used caches. Of course, each has the same worst-case behavior that the Least Recently Used algorithm has, but they manage to trade off between frequent and recent items in a way that often leads to better performance in practice. Of course, both are more complex than the Least Recently Used algorithm.We can get around the theoretical deficiencies of deterministic algorithms – that the user can predict which files aren’t in the cache and thus keep requesting those – by having our algorithm make partially-random choices. This will make it harder for users to hit the worst case, but it often makes the algorithm perform worse in practice. The best a randomized algorithm can do is (O(log k)) (in fact, approximately the natural log of (k)); see Fiat et al.3. Randomized caching algorithms have the downside of behaving in unexpected ways for the user – “Why is that file taking so long to open, I just looked at it!”. So in practice, they’re rarely used.Tangent: while randomized algorithms cannot be used directly in practice, they do tell us something about the expected performance of deterministic algorithms. This comes from a beautiful theorem by John von Neumann, called the Minimax Theorem. Imagine that the algorithm designer and his adversary play a game: the designer chooses a caching algorithm, the adversary a sequence of files, and then the winnings are decided based on how many cache misses the cache had. Phrased this way, algorithm design falls under the purview of game theory. We can represent a randomized algorithm as a strategy that involves choosing an algorithm at random from some set, and we can represent a randomized sequence of files as a random choice from a set of possible sequences.Continuing the tangent, let’s consider what the Minimax Theorem tells us about this game. The Minimax Theorem tells us that there exists an equilibrium strategy, where the worst-case winnings for each player is maximized. Since they’re the worst-case winnings for each player, they’re minimum winnings, so we have a minimum maximized – hence the theorem’s name. Such an equilibrium strategy might be a randomized strategy. In fact, since randomized algorithms can deliver guaranteed (O(log k)) performance, better than any deterministic algorithm, we might suppose that the maximum worst-case winnings for the adversary are at most (O(log k)). Similarly, the adversary will likely want to play some manner of randomized input sequence, since otherwise there would be added structure for a cache to possibly extract.Still on tangent, note that if the algorithm designer is committed to a randomized algorithm, there may be no reason to play a randomized input sequence. This is a consequence of the second part of the Minimax Theorem (which, sadly, is not as well-known): if one player is committed to a strategy, there is an optimal, deterministic response, which attains results at least as good as those from the equilibrium strategy. In particular, if the randomized algorithm being used is well-known, there must be a sequence of inputs that has the most expected cache misses; but this can’t take longer than with a randomized input sequence (otherwise, we would have chosen this deterministic sequence as our “randomized” one). But we can turn this around: if the input sequence is pre-chosen, there is an optimal deterministic response. This option better describes the usual human user, who will not actively try to thwart the Dropbox caching algorithm, but simply accesses files in a normal fashion. In this case, the sequence of files is random and pre-determined, so there is an optimal deterministic response. And the expected number of cache misses from such is at most (O(log k)). So a good deterministic algorithm, while it has a worst-case competitiveness of (O(k)), may have an expected competitiveness of at most (O(log k)). And, in fact, LRU is one of these good deterministic algorithms.Another way to convince yourself that the (k)-competitiveness of LRU is not that bad is compare an LRU cache not with an optimal cache of the same size, but with an optimal but smaller cache. In this case, you can prove a better result. For example, an LRU cache is at most twice as bad as an optimal cache half its size. Compared to an optimal cache of 100 files, an LRU cache for 200 files is at most twice as bad.Overall, the caching algorithm you want to use is usually LRU, since it is theoretically very good and in practice both simple and efficient. For example, the Dropbox iOS and Android clients both use LRU caches. The Linux kernel uses a variant called segmented LRU.On to some code.Our LRU implementation needs to do two things quickly. It needs to access each cached page quickly, and it needs to know which files are most and least recent. The lookup suggests a hash table, maintaining recency suggests a linked list; then each step can be done in constant time. A hash table can point to its file’s node in the list, which we can then go ahead and move around. Here goes.To look up an item that's already in the cache, we just need to move its node in the list to the front of the list.To kick an item, we need only take the node at the end of the list (the one that's least recently used) and remove it.Finally, to add an item, we can just link it to the front of the list and add it to the hash table.There it is, a working, (k)-competitive, LRU cache.You'll note that we've been assuming so far that all files are the same size. But in practice, this is of course untrue. How do we deal with bigger and smaller files? Well, it turns out, Dropbox naturally subdivides files into blocks (4MB big files, in fact). So instead of caching particular files, we can cache particular blocks, which are close enough in size that the Least Recently Used algorithm above works. Equivalently, we just kick out files until there is enough room for whatever file we want to load.Another problem that a real-world cache needs to solve is the issue of cache invalidation – that is, since the files we are caching can change on the server, how do we tell that our cache is out of date? A simple way is to always download an index, which tells you the file's revision number, but not the file data itself. You can do this on a per-directory basis, so that it's not too much data by itself. Then every time you find a file in the cache, you simply check when your copy was last modified and when the server's copy was last modified. This lets you know whether to renew your copy. Going even further, you can cache these indices for each directory, and use the same logic to determine whether they need to be downloaded again. This is what the Android and iOS clients do.Caches can be used in front of any slow part of your application -- communication over a network, reads from disk, or time-intensive computation. Caching is especially important in mobile programs, where network communication is often both necessary and costly, so it's good to know the theory and do it right. Luckily, the best solution for caching problems is usually the Least Recently Used algorithm, which is both efficient and simple to implement.Thanks to Dan Wheeler, Tido the Great, Aston Motes, Albert Ni, Jon Ying, and Rian Hunter for proofreading.1 N. Megiddo & D. Modha (2003), ""ARC: A Self-Tuning, Low Overhead Replacement Cache""2 S. Bansal & D. Modha (2004), ""CAR: Clock with Adaptive Replacement"".3 A. Fiat, R. Karp, M. Luby, M. McGeoch, D. Sleator & N. Young (1991), ""Competitive paging algorithms"".",https://blogs.dropbox.com/tech/2012/10/caching-in-theory-and-practice/,0,dropbox,"java,python",NULL,2012-10-16
Comtypes: How Dropbox learned to stop worrying and love the COM,"Here at Dropbox, we often use Python in uncommon ways. Today, I’ll be writing about a module that few Python users have even heard of before—comtypes. Comtypes is built on top of ctypes and allows access to low level Windows APIs that use COM. This module allows you to write COM-compatible code using only Python. For example, the Dropbox desktop client feature that allows you to upload photos from a camera uses comtypes to access Windows Autoplay. But before we talk about comtypes, we have to talk about COM.The Component Object Model is a standard introduced by Microsoft back in 1993. It allows two software components to interact without either one having knowledge of how the other is implemented, even if the components are written in different languages, running in different processes, or running on different machines and different platforms. Many Windows APIs still rely on COM, and occasionally, we have to work with one of them. The camera upload feature we released this year runs on Windows XP, an OS from 10 years ago, as well as Windows 8, an OS that hasn’t been released yet.  And it does all this using a standard that was created almost 20 years ago.On Windows, COM is both a standard and a service. It provides all the systems and utilities necessary to make inter-component compatibility possible. The standard requires interfaces to be compiled into a binary format that is language agnostic. For this purpose, it includes a specification for its own interface language—the Microsoft Interface Definition Language, aka MIDL—which is compiled into a binary called a type library that is then included inside a runnable such as a .dll or .exe. COM also allows run-time querying of supported interfaces, so that two objects can agree on an interface much like strangers meeting in a foreign country—”Do you speak IHardwareEventHandler version 2? No? Well, parlez-vous version 1?” On top of that, it also provides for object reference counting, inter-process marshalling, thread handling, and much more. Without this functionality, a component implementer would have to make sure objects used by a different process are cleaned up eventually, but not while they’re still being referenced. She’d have to serialize arguments to pass between components, and figure out how to control access from multi-threaded components into objects that may or may not be thread-safe.COM handles these things for you, but this functionality comes at a cost. It involves a fair amount of complexity, which is unfortunately necessary, and a lot of syntactic convolution,  which is just plain unfortunate. Writing an object that uses a COM component, aka a COM client, is difficult. Writing a COM object that other components can use, aka a COM server, can be downright devilish.If you think COM seems like magic, you’re right—it is definitely some sort of black magic. COM requires incantations such asand the use of strange ritual equipment like MIDL compilers. To ensure unambiguity in class and interface identification, everything is referenced by GUIDs, which are undescriptive and unwieldy at best. And when creating a COM server, the sheer number of configuration options at every step of the way can be paralyzing. You have to answer questions such as “Are my threads running in Single Threaded Apartments or Multithreaded Apartments?” and “What does it mean to set my ThreadingModel to Both instead of Free?” Understanding these questions requires a lot of COM-specific background knowledge, and most articles about these choices are pages long, and often involve charts, diagrams, and sample code.I came to Dropbox with enough knowledge of COM to squeak by, and still consider myself no more than an advanced novice. If one were to attempt to write pure Python code that used or, heaven forbid, implemented a COM object, one would need to generate and parse the binary type library files that specify COM interfaces, perform all the complex Windows registry rituals, track all the reference counts to COM objects, as well as correctly write the endless syntactical mumbo jumbo. Fortunately for us, the comtypes module exists to abstract (almost) all of this horribleness away from us.If COM is black magic, then comtypes is the mysterious witch doctor service that you contract to perform the black magic for you. For simple tasks, everything likely works fine. Unfortunately, if you need to do anything very complex, you run the risk of being left in the dark as to what sort of invocations were performed, only to find the demon knocking on your door.Still, comtypes makes life much easier. When it works well, you can simply feed comtypes the path to the dll or exe of the object you’re trying to use, and then write pretty straightforward code likeThis (slightly simplified) sample code allows access to the contents of a camera attached to the computer.  The deviceobj is a Python wrapper around a COM object that is actually implemented elsewhere on the system, one that represents a camera we can interface with. Underneath, comtypes will be busy CoCreateInstancing, QueryInterfacing, and wrapping ctypes objects with Python objects for your ease of use. There’s usually no need for you to worry about the million things that are going on underneath. But unfortunately, things don’t work smoothly all the time, so what kind of hackers would we be if we didn’t open it up to see how it works?The magic begins in the comtypes.client module. The handy helper function GetModule will take a binary file like a .tlb or .exe, extract the binary data, and automatically generate Python code that, like a header file, specifies all the interfaces, methods, and structs that you need to use a particular COM object. Anyone’s who’s worked with the Windows API might be familiar with the rabbit-hole of struct and type declarations. A FORMATETC struct, for example, is one that is used in drag and drop APIs. It is declared as two mystery types followed by three 32 bit ints. Further digging will reveal that one of the unknown types is an enum, but the other is another struct involving more unknown types. For it to be usable in Python, you have to break things down into known types without the benefit of importing hundreds of Windows headers. GetModule will do all of these things for you, but the generated code hides the interesting part, the wrapper classes that actually proxy to the real COM objects underneath. So it’s time to dig a little deeper.This brings us to the comtypes class IUnknown, the root of all evil (no joke—check __init__.py:979). In COM, IUnknown is the grandfather of all interfaces, the interface which all other interfaces inherit from. It contains only three functions:In comtypes land, IUnknown is actually a base class. For any COM interface that you intend to call into—IPortableDevice for example—you must create a class that inherits from IUnknown. All the methods in the interface are declared in the variable _methods_ as tuples, specifying function name, types and names of args and return values.The class IUnknown itself is actually pretty simple. The magic happens in its metaclass _cominterface_meta, which turns these tuples into bound methods.  For the uninitiated, all Python classes are actually objects, and metaclasses are things that make classes.  When you declare a class like IPortableDevice that inherits from IUnknown, the metaclass of IUnknown takes COMMETHODs declared above and creates two bound methods: IPortableDevice.Open, which takes in two parameters and returns nothing; and IPortableDevice.Content, which takes no parameters and returns one. These wrapper methods check that calls are made with the appropriate number and type of inputs, a necessity when communicating between untyped, flying-by-the-seat-of-your-pants Python and statically typed, compiled languages like C++.  The wrappers then proxy the method calls into an actual COM object that was instantiated under the covers, wrap the return values in Python types, and return them to you, transforming returned error codes into Python exceptions along the way. It’s wonderful, except when it doesn’t work exactly as intended.The most painful such incident brought development to a dead halt for two days. The only symptom was that the program would occasionally crash after reading a bunch of images from a camera. The bug was non-deterministic and no exception was generated. After endless hours of printing and prodding, I finally found the root cause of the problem. In COM, the implementer of an interface typically does AddRef on the object when he creates it so that it is “born” with a reference which is passed to the caller of CreateObject, while the user of the interface is responsible for calling Release when he is done with it. Additional calls to AddRef and Release are only necessary if the user makes copies of the reference to the object. So in comtypes, the __init__ method of a comtypes object does not call AddRef on the COM interface, but deletion does call Release. This in itself is only passingly strange, because it usually works.However, the clever wrapping of COM objects sometimes results in comtypes objects being created unexpectedly, and then also deleted unexpectedly. For example, in the following codeyou would expect device_item to be a pointer to an IDeviceItem. Normally, you would have to “dereference” the pointer to get to the item itself, as inHowever, when you index the array, comtypes helpfully transforms idevice_item_array[0] from type pointer(IDeviceItem) to type IDeviceItem, so instead we haveIn the process, it unexpectedly creates an instance of IDeviceItem. More importantly, it unexpectedly destroys an instance of IDeviceItem. So, the following codeactually crashes Python because it creates and destroys a hundred IDeviceItem objects, resulting in a hundred calls to Release on the real COM object. After the first Release call, the COM object is considered deleted. Whenever the garbage collector for that COM object is triggered, everything explodes.The workaround? Save your reference into a Python object and keep it around. Don’t index your COM object arrays more than once.This results in exactly one call to Release, which occurs when the Python object deviceitem is destroyed.All of this happened within my first few months at Dropbox, and I barely spoke Python at the time. I learned what a metaclass was before I had fully mastered list slicing syntax. Meanwhile, my counterpart on the Mac camera uploads side was not having the easiest time either. Without the benefit of a compatibility enforcer like COM, he was trying to ferret out why an OS X library was trying to execute Dropbox code as PowerPC assembly on an X86 machine (it’s complicated—the explanatory comment is about fifty lines long). It made me feel a little bit better about dealing with incorrect vtable pointers and bad reference counting.Discovering comtypes was an integral part of the development of the photo feature, and it certainly presented enough excitement to be considered an adventure. In the end, for all the problems I encountered, having comtypes made it much easier to access COM APIs. Reference counting bugs are the price you pay when you work with low-level code, but it certainly would have been much more work to write our own Python wrappers around COM. COM may be difficult to use, and comtypes occasionally frustrating, but with a working knowledge of how to use the first and how to work around unexpected pitfalls in the second, we can plow ahead with future Windows features. In fact, not long after we released the camera feature, we found ourselves again needing to interact with a COM component.  With all this knowledge under my hat, I made myself a COM client in no time. It was a glorious victory.",https://blogs.dropbox.com/tech/2012/10/adventures-with-comtypes/,0,dropbox,"css,numpy,frontend,python,python3",NULL,2012-10-04
Dropbox dives into CoffeeScript,"During July’s Hackweek, the three of us rewrote Dropbox’s full browser-side codebase to use CoffeeScript instead of JavaScript, and we’ve been really happy with how it’s been going so far. This is a controversial subject, so we thought we’d start by explaining why.CoffeeScript: JavaScript: By the way, the JavaScript has a scoping bug, did you catch it??We’ve heard many arguments against CoffeeScript. Before diving in, we were most concerned about these two:Probably the most misleading argument we hear against CoffeeScript goes something like this: If you like Python or Ruby, go for CoffeeScript — it’s really just a matter of syntactic preference. This argument frustrates us, because it doesn’t consider history. Stick with us for a minute:Especially considering the strange, difficult and rushed circumstances of its origin, JavaScript did many things well: first class functions and objects, prototypes, dynamic typing, object literal syntax, closures, and more. But is it any surprise that it got a bunch of things wrong too? Just considering syntax, things like: obscuring prototypical OOP through confusingly classical syntax, the var keyword (forgot var? congrats, you’ve got a global!), automatic type coercion and == vs ===, automatic semicolon insertion woes, the arguments object (which acts like an array except when it doesn’t), and so on. Before any of these problems could be changed, JavaScript was already built into competing browsers and solidified by an international standards committee. The really bad news is, because browsers evolve slowly, browser-interpreted languages evolve slowly. Introducing new iteration constructs, adding default arguments, slices, splats, multiline strings, and so on is really difficult. Such efforts take years, and require cooperation among large corporations and standards bodies.Our point is to forget CoffeeScript’s influences for a minute, because it fixes so many of these syntactic problems and at least partially breaks free of JavaScript’s slow evolution; even if you don’t care for significant whitespace, we recommend CoffeeScript for so many other reasons. Disclaimer: we love Python, and it’s Dropbox’s primary language, so we’re probably biased.An interesting argument against CoffeeScript from Ryan Florence, that seemed plausible to us on first impression but didn’t hold up after we thought more about it, is the idea that (a) human beings process images and symbols faster than words, so (b) verbally readable code isn’t necessarily quicker to comprehend. Florence uses this to argue that (c) while CoffeeScript may be faster to read, JavaScript is probably faster to comprehend. We’d expect cognitive science provides plenty of evidence in support of (a), including the excellent circle example cited by Florence. (b) is easily proven by counterexample. Making the leap to (c) is where we ended up disagreeing:On to some code samples.JavaScriptCoffeeScript JavaScriptCoffeeScript JavaScriptCoffeeScriptWe’ll let this comparison speak for itself. We consider it our strongest argument in favor of CoffeeScript. In the process of converting, we shaved off more than 5000 lines of code, a 21% reduction. Granted, many of those lines looked like this:Regardless, fewer lines is beneficial for simple reasons — being able to fit more code into a single editor screen, for example.Measuring reduction in code complexity is of course much harder, but we think the stats above, especially token count, are a good first-order approximation. Much more to say on that subject.In production, we compile and concatenate all of our CoffeeScript source into a single JavaScript file, minify it, and serve it to browsers with gzip compression. The size of the compressed bundle didn’t change significantly pre- and post-coffee transformation, so our users shouldn’t notice anything different. The site performs and behaves as before.Rewriting over 23,000 lines of code in one (hack)week was a big undertaking. To significantly hasten the process and avoid bugs, we used js2coffee, a JavaScript to CoffeeScript compiler, to do all of the repetitive conversion tasks for us (things like converting JS blocks to CS blocks, or JS functions to CS functions). We’d start converting a new JS file by first compiling it individually to CS, then manually editing each line as we saw fit, improving style along the way, and making it more idiomatic. One example: the compiler isn’t smart enough to convert a JS three-clause for into a CS for/in. Instead it outputs a CS while with i++ at the end. We switched each of those to simpler loops. Another example: using string interpolation instead of concatenation in places where it made sense.To make sure we didn’t break the site, we used a few different approaches to test:Dropbox now writes all new browser-side code in CoffeeScript, and we’ve been loving it. We’ve already written several thousand new lines of coffee since launching in July. Some of the things we’re looking to improve in the future:To Brendan Eich and Jeremy Ashkenas for creating two fantastic languages.",https://blogs.dropbox.com/tech/2012/09/dropbox-dives-into-coffeescript/,0,dropbox,"webpack,html,frontend,css,react",NULL,2012-09-13
Some love for JavaScript applications,"During our last hack week, Aakanksha Sarda and I set out to build a library that helps JavaScript developers use the Dropbox API. My main goal was to take “static” Web applications to the next level. However, the JavaScript library can be useful for any application that runs a significant part of its logic in the browser, as well as for node.js server-side code.If you prefer getting your hands dirty right away, go ahead and register for a Dropbox API key, set up an application in your Dropbox, borrow sample code, and read the library documentation.Thanks to recent improvements in browser support and VM performance, I often find myself writing small and medium applications completely in JavaScript, whenever I can get away with it. JavaScript runs on users’ browsers, so all the application’s files are static, and can be served by any plain old file server such as nginx, pretty much any Web hosting service, and your humble Dropbox.My interest in static Web apps is greatly influenced by how easy they are to deploy. For example, the deployment process for my Dropbox-hosted applications is the cp command (copy if you’re on Windows). Dropbox syncs the files to its servers, and even lets me revert a bad deployment. I’ve been using this beautiful, simple paradigm for all my applications that don’t need to store per-user data.However, up until now, having to handle per-user data has been a different story. I needed a database to store the data and an application server to talk to the database server. My applications then needed accounts and authentication to ensure that users wouldn’t overwrite each others’ data. Finally, deploying a new version of the application involved pulling the updated code from a git repository, migrating the database schema, and restarting the application server. I was afraid I’d make a mistake, so I ended up writing complex scripts to handle the deployment.To my despair, the code for having small applications store per-user data was much longer than the actual application code. Deploying the apps was a problem on its own, and left me yearning for the minimalistic “copy to Dropbox” approach. Fortunately, today’s announcement fixes everything!My hack week project, dropbox.js, makes it easy to use Dropbox for storing per-user data. For example, let’s consider Checkbox, a To Do manager hosted entirely in this Dropbox folder. While Checkbox won’t be winning design or usability awards any time soon, it is fully functional! It can store your To Do list right in your Dropbox, in less than 70 lines of HTML and less than 300 lines of commented CoffeeScript (which compiles into less than 350 lines of JavaScript).Let’s skim the Checkbox source code. The action happens in the app’s CoffeeScript code. The Checkbox class is the application’s view and controller, so it renders the UI and handles DOM events. The Tasks and Task classes implement the data model and the Dropbox integration.Checkbox uses the “App folder” Dropbox access level, so Dropbox automatically creates a directory for my app data in my users’ Dropboxes. The data model design favors ease of development and debugging. Each task is stored as a file whose name is the task’s description. Tasks are grouped under two folders, active and done. Operations on tasks cleanly map to Dropbox file operations in Dropbox. For example:Most importantly, each Dropbox operation takes up one line of code in the Tasks implementation. Thanks to dropbox.js, the Checkbox source code is all about the application’s logic, and does not get distracted with infrastructure issues.In the end, dropbox.js makes it easy for me to store my users’ data in a system whose features and reliability go beyond those of the storage backing many enterprise-grade applications. Checkbox-managed To Do lists are transmitted securely, stored redundantly, and backed up. My own To Do lists are in my Dropbox, so I debugged my application with my file manager, and deployed it using cp. I have never SSHed into a server, and I haven’t issued a single SQL query.Want in on the action? This little JavaScript application can help you run small Web applications out of your Dropbox!Hopefully, I’ve convinced you to try out dropbox.js for your next prototype or hobby project. In fact, I won’t be offended if you stop reading now and start using it! However, I think you’ll also find it interesting to read more about the goals, tough decisions, and story behind the library’s design. Let’s start with the goals:I think that infrastructure libraries should silently support your development efforts, and stay out of your way. Every bit of brainpower lost on figuring out low-level issues is not spent on making your application awesome. So the overarching goal for dropbox.js is to have you spend as little time as possible thinking about it.To that end, the libraries aims to follow the principle of least surprise, and to make the common use cases especially easy to implement. For example, the interfaces of the methods for reading and writing Dropbox files heavily borrow from fs.readFile and fs.writeFile in node.js, as you can see in the code below.Dropbox has more features than your average filesystem, such as revision history, and these advanced features can be accessed by passing an options object. For example, the same readFile method can be used to retrieve an old revision of a file.However, passing an options object is not mandatory, and the default options reflect the common use case, such as reading the most recent revision of a file. Compare and contrast with the Win32 CreateFile function, which takes 7 parameters.Last but not least, the library’s interface acknowledges both experienced JavaScript programmers and experienced Dropbox API users. For example, listing a folder’s contents can be done by calling a readdir method, whose interface matches fs.readdir, or by calling a metadata method, whose interface is closer to the /metadata REST API. I really hope that dropbox.js works out of the box for you, and helps you integrate with Dropbox quickly and painlessly. At the same time, I realize that a small library can’t possibly cover all the use cases, and some of you will have to extend or modify the library.The library exposes some of its internals on purpose, so you can break the abstractions when you need to. For example, methods that make AJAX calls, such as readFile and writeFile, return the XmlHttpRequest object used for the AJAX call. If you want to implement progress bars for Dropbox operations, you can set up listeners for the relevant XmlHttpRequest events. To further encourage extension, the library’s internal methods are fully documented using JSDoc, just like the public APIs.Asides from source code, dropbox.js includes an automated build script, and a mostly-automated test suite with very solid coverage. If you need to modify the library, the README file will help you use the test suite to verify the correctness of your changes and incorporate your changes in a minified library build or an npm package. The entire library is hosted on GitHub, so you can easily submit your patches and not get stuck maintaining a fork.Perhaps the most delicate aspect of integrating with a Web service is the user authentication piece. Dropbox authentication uses a four-step process that bounces the user between the application’s Web page and the Dropbox servers, so applications will most likely need to customize the process. dropbox.js defines an authentication driver interface for the custom code, and also includes three implementations that will get you going through the prototype stage of your application. For example, the code below covers both library initialization and authentication.While writing the JavaScript library, I struggled a lot figuring out how much I can diverge from the Dropbox REST API, in the name of offering a more intuitive API. While I didn’t want to create a RequestProcessorFactoryFactory-like monster, I also wanted to hide some aspects of the REST API that I found confusing.  For example, the JavaScript library wraps the /metadata output, and uses the stat name for the concept of metadata, to match existing filesystem APIs. The rev parameter that shows up in many API calls is renamed to revisionTag in dropbox.js, to avoid the misconception that its value would be a sequential integers, like Subversion revisions. I look forward to seeing how this decision plays out, and to learning from it.When I started working on the JavaScript library, I envisioned that the Dropbox object would implement a high-level API, and developers would only call into Dropbox.Client if they would need a low-level API. I had high hopes for the high-level API. It was supposed to provide File and Directory classes, automatically cache Dropbox data in IndexedDb, and automatically kick off the authentication process on token expiration. Unfortunately, by the end of the hack week, the low-level API was barely completed, so the current dropbox.js release ships an empty Dropbox object, and the code samples use Dropbox.Client. Ship early, ship often 🙂dropbox.js was designed and developed during the most recent Dropbox hack week, a hackathon where we put our regular work on hold for 5 days and get to work on our crazy ideas and pet projects.I got the idea to write a JavaScript client for the Dropbox API when I brainstormed for hack week projects, and I realized that all my ideas would be best prototyped as JavaScript applications. At first, I had many doubts about the idea. Would other people use this? Would I be able to make a decent API? Fortunately, I shared my idea with a few colleagues, who encouraged me to commit to the idea and post it to the company’s internal list of hack week projects.After posting the project, I received an amazing (and humbling) amount of support from fellow Dropboxers. Before hack week, Chris Varenhorst added CORS headers to the Dropbox API responses, which allow dropbox.js to work in the browser.During hack week, Aakanksha Sarda fleshed out the code for all the file operations, figured out how to deal with binary files (e.g., images), got the automated test suite running in the browser, and wrote Dropstagram, a Dropbox-powered photo-editing application, using WebGL shaders. A few other Dropboxers used dropbox.js and gave us great feedback and a sense of urgency. Rich Chan built a true Internet Terminal running on JavaScript and Dropbox and a stunning visualizer for the revision history of any text file in your Dropbox. Franklin Ta prototyped a Google Chrome extension that lets you download and upload files straight into / from your Dropbox. David Goldstein worked on a secret project that will make Dropbox apps even more awesome, and used dropbox.js to write a browser-based .zip unpacker for your Dropbox files.Hack week provided an amazing backdrop for the development of dropbox.js. Graham Abbott set up a pod for everyone working with dropbox.js, so it was very easy for us to collaborate and exchange ideas and feedback. Jon Ying sprinkled some design magic on the applications that we developed. The kitchen staff stayed up and prepared hot food for us every night of the week. Dropboxers (including Drew and Arash) came by our pod, looked at our demos, and cheered us on.After hack week, Chris Varenhorst, Dima Ryazanov, and Brian Smith helped me work through some last-minute technical difficulties. Jon Ying gave a makeover to the Checkbox sample app. Albert Ni, Alex Allain, Glara Ahn and Jon Ying helped make this blog post happen.I hope you enjoyed reading about the dropbox.js. Above all, I really hope that you will go download the library, and build something amazing with it!I expect that “powered by Dropbox” Web applications will become a great tool for learning Web programming and building class projects. I’ll walk the walk myself this coming winter, when I’ll teach Web Programming at MIT.I look forward to learning about the projects that come out of the next Dropbox hack week, and I’ll be counting how many of them use dropbox.js 🙂",https://blogs.dropbox.com/tech/2012/08/some-love-for-javascript-applications-2/,0,dropbox,"html,frontend,php,python,react,css,webpack",NULL,2012-08-31
Plop: Low-overhead profiling for Python,"It’s almost time for another Hack Week at Dropbox, and with that in mind I’d like to present one of the projects from our last Hack Week.A profiler is an indispensable tool for optimizing programs.  Without a profiler, it’s hard to tell which parts of the code are consuming enough time to be worth looking at.  Python comes with a profiler called cProfile, but enabling it slows things down so much that it’s usually only used in development or simulated scenarios, which may differ from real-world usage.At our last hack week, I set out to build a profiler that would be usable on live servers without impacting our users.  The result, Plop (Python Low Overhead Profiler) is now available on Github.Plop is a sampling profiler, similar to Google’s gperftools.  Every 10 milliseconds, a timer is fired which causes the program to record its current stack trace.  After 30 seconds, the collected samples are aggregated and saved.  There is a web-based viewer for the resulting call graphs (using d3.js)Here’s a sample profile, from a simple Tornado-based web server. Click on the image for a full-size interactive view (it’s big, so you’ll need to either scroll around a lot or use your browser’s zoom controls).Each bubble is a function; they’re color-coded by filename and you can mouse over them for more details. The size of the bubble represents the amount of time spent in that function.  The thickness of the lines connecting the bubbles represents how frequently that function call appears on the stack.  The disconnected bubbles around the edge are actually common utility functions that are called from many places throughout the code – they’re so common that drawing the connections to every function that calls them would make the graph unreadable.Since hack week, we’ve integrated Plop into our servers and run it regularly.  Every time we push new code to the site, a script collects a profile for the new version.  This has proven not to be disruptive, with less than 2% CPU overhead while the profile is being collected. It’s still a work in progress (especially the viewer), but has already proven useful in identifying performance regressions.",https://blogs.dropbox.com/tech/2012/07/plop-low-overhead-profiling-for-python/,0,dropbox,"html,frontend,php,python,css,python3",NULL,2012-07-10
zxcvbn: realistic password strength estimation,"Over the last few months, I’ve seen a password strength meter on almost every signup form I’ve encountered. Password strength meters are on fire.Here’s a question: does a meter actually help people secure their accounts? It’s less important than other areas of web security, a short sample of which include:With that disclaimer — yes. I’m convinced these meters have the potential to help. According to Mark Burnett’s 2006 book, Perfect Passwords: Selection, Protection, Authentication, which counted frequencies from a few million passwords over a variety of leaks, one in nine people had a password in this top 500 list. These passwords include some real stumpers: password1, compaq, 7777777, merlin, rosebud. Burnett ran a more recent study last year, looking at 6 million passwords, and found an insane 99.8% occur in the top 10,000 list, with 91% in the top 1,000. The methodology and bias is an important qualifier — for example, since these passwords mostly come from cracked hashes, the list is biased towards crackable passwords to begin with.These are only the really easy-to-guess passwords. For the rest, I’d wager a large percentage are still predictable enough to be susceptible to a modest online attack. So I do think these meters could help, by encouraging stronger password decisions through direct feedback. But right now, with a few closed-source exceptions, I believe they mostly hurt. Here’s why.Strength is best measured as entropy, in bits: it’s the number of times a space of possible passwords can be cut in half. A naive strength estimation goes like this:This brute-force analysis is accurate for people who choose random sequences of letters, numbers and symbols. But with few exceptions (shoutout to 1Password / KeePass), people of course choose patterns — dictionary words, spatial patterns like qwerty, asdf or zxcvbn, repeats like aaaaaaa, sequences like abcdef or 654321, or some combination of the above. For passwords with uppercase letters, odds are it’s the first letter that’s uppercase. Numbers and symbols are often predictable as well: l33t speak (3 for e, 0 for o, @ or 4 for a), years, dates, zip codes, and so on.As a result, simplistic strength estimation gives bad advice. Without checking for common patterns, the practice of encouraging numbers and symbols means encouraging passwords that might only be slightly harder for a computer to crack, and yet frustratingly harder for a human to remember. xkcd nailed it:As an independent Dropbox hackweek project, I thought it’d be fun to build an open source estimator that catches common patterns, and as a corollary, doesn’t penalize sufficiently complex passphrases like correcthorsebatterystaple. It’s now live on dropbox.com/register and available for use on github. Try the demo to experiment and see several example estimations.The table below compares zxcvbn to other meters. The point isn’t to dismiss the others — password policy is highly subjective — rather, it’s to give a better picture of how zxcvbn is different.A few notes:zxcvbn has no dependencies and works on ie7+/opera/ff/safari/chrome. The best way to add it to your registration page is:zxcvbn-async.js is a measly 350 bytes. On window.load, after your page loads and renders, it’ll load zxcvbn.js, a fat 680k (320k gzipped), most of which is a dictionary. I haven’t found the script size to be an issue; since a password is usually not the first thing a user enters on a signup form, there’s plenty of time to load. Here’s a comprehensive rundown of crossbrowser asynchronous script loading.zxcvbn adds a single function to the global namespace:It takes one required argument, a password, and returns a result object. The result includes a few properties:The optional user_inputs argument is an array of strings that zxcvbn will add to its internal dictionary. This can be whatever list of strings you like, but it’s meant for user inputs from other fields of the form, like name and email. That way a password that includes the user’s personal info can be heavily penalized. This list is also good for site-specific vocabulary. For example, ours includes dropbox.zxcvbn is written in CoffeeScript. zxcvbn.js and zxcvbn-async.js are unreadably closure-compiled, but if you’d like to extend zxcvbn and send me a pull request, the README has development setup info.The rest of this post details zxcvbn’s design.zxcvbn consists of three stages: match, score, then search.Search is the crux of the model. I’ll start there and work backwards.zxcvbn calculates a password’s entropy to be the sum of its constituent patterns. Any gaps between matched patterns are treated as brute-force “patterns” that also contribute to the total entropy. For example:That a password’s entropy is the sum of its parts is a big assumption. However, it’s a conservative assumption. By disregarding the “configuration entropy” — the entropy from the number and arrangement of the pieces — zxcvbn is purposely underestimating, by giving a password’s structure away for free: It assumes attackers already know the structure (for example, surname-bruteforce-keypad), and from there, it calculates how many guesses they’d need to iterate through. This is a significant underestimation for complex structures. Considering correcthorsebatterystaple, word-word-word-word, an attacker running a program like L0phtCrack or John the Ripper would typically try many simpler structures first, such as word, word-number, or word-word, before reaching word-word-word-word. I’m OK with this for three reasons:With this assumption out of the way, here’s an efficient dynamic programming algorithm in CoffeeScript for finding the minimum non-overlapping match sequence. It runs in O(n·m) time for a length-n password with m (possibly overlapping) candidate matches.backpointers[j] holds the match in this sequence that ends at password position j, or null if the sequence doesn’t include such a match. Typical of dynamic programming, constructing the optimal sequence requires starting at the end and working backwards.Especially because this is running browser-side as the user types, efficiency does matter. To get something up and running I started with the simpler O(2m) approach of calculating the sum for every possible non-overlapping subset, and it slowed down quickly. Currently all together, zxcvbn takes no more than a few milliseconds for most passwords. To give a rough ballpark: running Chrome on a 2.4 GHz Intel Xeon, correcthorsebatterystaple took about 3ms on average. coRrecth0rseba++ery9/23/2007staple$ took about 12ms on average.Entropy isn’t intuitive: How do I know if 28 bits is strong or weak? In other words, how should I go from entropy to actual estimated crack time? This requires more assumptions in the form of a threat model. Let’s assume:Here’s some back-of-the-envelope numbers:I added a .5 term because we’re measuring the average crack time, not the time to try the full space.This math is perhaps overly safe. Large-scale hash theft is a rare catastrophe, and unless you’re being specifically targeted, it’s unlikely an attacker would dedicate 100 cores to your single password. Normally an attacker has to guess online and deal with network latency, throttling, and CAPTCHAs.Up next is how zxcvbn calculates the entropy of each constituent pattern. calc_entropy() is the entry point. It’s a simple dispatch:I gave an outline earlier for how repeat_entropy works. You can see the full scoring code on github, but I’ll describe two other scoring functions here to give a taste: spatial_entropy and dictionary_entropy.Consider the spatial pattern qwertyhnm. It starts at q, its length is 9, and it has 3 turns: the initial turn moving right, then down-right, then right. To parameterize:The space of total possibilities is then all possible spatial patterns of length L or less with t turns or less:(i – 1) choose (j – 1) counts the possible configurations of turn points for a length-i spatial pattern with j turns. The -1 is added to both terms because the first turn always occurs on the first letter. At each of j turns, there’s d possible directions to go, for a total of dj possibilities per configuration. An attacker would need to try each starting character too, hence the s. This math is only a rough approximation. For example, many of the alternatives counted in the equation aren’t actually possible on a keyboard: for a length-5 pattern with 1 turn, “start at q moving left” gets counted, but isn’t actually possible.CoffeeScript allows natural expression of the above:On to dictionary entropy:The first line is the most important: The match has an associated frequency rank, where words like the and good have low rank, and words like photojournalist and maelstrom have high rank. This lets zxcvbn scale the calculation to an appropriate dictionary size on the fly, because if a password contains only common words, a cracker can succeed with a smaller dictionary. This is one reason why xkcd and zxcvbn slightly disagree on entropy for correcthorsebatterystaple (45.2 bits vs 44). The xkcd example used a fixed dictionary size of 211 (about 2k words), whereas zxcvbn is adaptive. Adaptive sizing is also the reason zxcvbn.js includes entire dictionaries instead of a space-efficient Bloom filter — rank is needed in addition to a membership test.I’ll explain how frequency ranks are derived in the data section at the end. Uppercasing entropy looks like this:So, 1 extra bit for first-letter-uppercase and other common capitalizations. If the uppercasing doesn’t fit these common molds, it adds:The math for l33t substitution is similar, but with variables that count substituted and unsubstituted characters instead of uppers and lowers.So far I covered pattern entropy, but not how zxcvbn finds patterns in the first place. Dictionary match is straightforward: check every substring of the password to see if it’s in the dictionary:ranked_dict maps from a word to its frequency rank. It’s like an array of words, ordered by high-frequency-first, but with index and value flipped. l33t substitutions are detected in a separate matcher that uses dictionary_match as a primitive. Spatial patterns like bvcxz are matched with an adjacency graph approach that counts turns and shifts along the way. Dates and years are matched with regexes. Hit matching.coffee on github to read more.As mentioned earlier, the 10k password list is from Burnett, released in 2011.Frequency-ranked names and surnames come from the freely available 2000 US Census. To help zxcvbn not crash ie7, I cut off the surname dictionary, which has a long tail, at the 80th percentile (meaning 80% of Americans have one of the surnames in the list). Common first names include the 90th percentile.The 40k frequency list of English words comes from a project on Wiktionary, which counted about 29M words across US television and movies. My hunch is that of all the lists I could find online, television and movie scripts will capture popular usage (and hence likely words used in passwords) better than other sources of English, but this is an untested hypothesis. The list is a bit dated; for example, Frasier is the 824th most common word.At first glance, building a good estimator looks about as hard as building a good cracker. This is true in a tautological sort of way if the goal is accuracy, because “ideal entropy” — entropy according to a perfect model — would measure exactly how many guesses a given cracker (with a smart operator behind it) would need to take. The goal isn’t accuracy, though. The goal is to give sound password advice. And this actually makes the job a bit easier: I can take the liberty of underestimating entropy, for example, with the only downside of encouraging passwords that are stronger than they need to be, which is frustrating but not dangerous.Good estimation is still difficult, and the main reason is there’s so many different patterns a person might use. zxcvbn doesn’t catch words without their first letter, words without vowels, misspelled words, n-grams, zipcodes from populous areas, disconnected spatial patterns like qzwxec, and many more. Obscure patterns (like Catalan numbers) aren’t important to catch, but for each common pattern that zxcvbn misses and a cracker might know about, zxcvbn overestimates entropy, and that’s the worst kind of bug. Possible improvements:Even with these shortcomings, I believe zxcvbn succeeds in giving better password advice in a world where bad password decisions are widespread. I hope you find it useful. Please fork on github and have fun!Big thanks to Chris Varenhorst, Gautam Jayaraman, Ben Darnell, Alicia Chen, Todd Eisenberger, Kannan Goundan, Chris Beckmann, Rian Hunter, Brian Smith, Martin Baker, Ivan Kirigin, Julie Tung, Tido the Great, Ramsey Homsany, Bart Volkmer and Sarah Niyogi for helping review this post. ",https://blogs.dropbox.com/tech/2012/04/zxcvbn-realistic-password-strength-estimation/,0,dropbox,,NULL,2012-04-10
Hilary Mason Speaks at Dropbox,"We host a monthly tech talk series we call “Droptalks“. In the past, we’ve hosted Steve Souders, Guido van Rossum, Greg Papadopoulos, and Amit Singh.A couple weeks ago, we were lucky to have Hilary Mason in town. Hilary is the Chief Scientist of bit.ly, the world-famous URL shortener. Bit.ly may seem like a simple service, however, when done at such a large scale there is much more behind the scenes. There’s also a lot of neat data to play with.Hilary spoke about some of the challenges and lessons from her work trying to derive meaningful uses from the mass of data that flows through bit.ly. She spoke about the history of bit.ly, some of the philosophy of analyzing time-series data, and some cool engineering tricks. She even gave demos of three internal tools at bit.ly that will be released as products in the next few months (really cool stuff!).The slides are available.For those interested in learning more about Analytics and Data Science, Hilary suggested a few introductory books:",https://blogs.dropbox.com/tech/2012/02/hilary-mason-speaks-at-dropbox/,0,dropbox,"frontend,sass,python,css,docker,animation",NULL,2012-02-23
Using the Dropbox API from Haskell,"I love Haskell. My first encounter with Haskell started out about eight years ago. Like many people in those days, when I was in high school I spent a lot of time playing around with code on my computer. Reading and understanding open source projects was a main source of knowledge and inspiration for me when I was learning how to program. When I came upon the bzip2 homepage and consequently Julian Seward’s homepage I found a short note about Haskell and how it was a super fun and interesting language to write a compiler for. Haskell? What’s that?After reading more about Haskell, functional programming, lazy evaluation, and type inference and seeing the elegance of the various code samples, I was hooked. I spent the next couple of weeks going through “Yet Another Haskell Tutorial” and I remember it being incredibly difficult yet incredibly rewarding. After I wrote my first fold over a recursive algebraic datatype, I felt like I was finally starting to speak Haskell. I felt like I had massively improved as a programmer.While I’ve been at Dropbox, Python has been my main language of computational expression. Even though it was a bit rocky at first, I’ve grown to really love Python and the culture of fast iteration and duck typing. Like a good Pythonista, I’m of the opinion that types are training wheels, but that’s really only until you use a language with a real type system. In C# or Java, types can get in your way and even force you to write overly verbose code or follow silly “design patterns.” In Haskell, types help you soar to higher computational ground. They encourage you to model your data in coherent, concise, and elegant ways that feel right. They aren’t annoying.More people should use Haskell. The steep learning curve forces you to understand what you are doing at a deeper level and you will be a better programmer because of it. To help that happen, this post will be in a semi-literate programming style and I’ll be describing a Dropbox API app written in Haskell. This won’t be like a normal tutorial so you’ll probably have to do a bit of supplemental reading and practice afterward. The goal is to give you a flavor of what a real program in Haskell looks like.This post assumes no previous knowledge with Haskell but it does assume moderate programming ability in another language, e.g. C, C++, Ruby, Python, Java, or Lisp. Since this post does not assume previous Haskell experience the beginning will be more of a Haskell tutorial and core concepts will be sectioned off to facilitate the learning process. This post is a published version of a Literate Haskell file. Code lines prefixed with the “>” character are actually part of the final program. This makes it so that it’s possible to simply copy & paste the text here into your favorite editor and run it, just make sure you save the file with a “.lhs” extension. If you ever get tired of reading you can get the real source at the GitHub repo.The Haskell implementation we’ll be using is The Haskell Platform, it’s a full stack of Haskell tools prepackaged to work out of the box on all three major desktop operating systems. It’s based on GHC, The Glasgow Haskell Compiler. GHC is an advanced optimizing compiler focused on producing efficient code.Years ago Robert Love of Linux kernel hacker fame wrote a FUSE file system that made it so that user-created folders were populated with Beagle search results using the folder name as the search query. It was called beaglefs. The point was to demonstrate the power of user-space file systems, notably the power of having so much more library code available to you than in kernel-space.We can do a similar thing with the Dropbox API. We’re going to write a hypothetical Dropbox API app that populates user-created folders with Creative Commons licensed images found by performing a web image search using the folder name as the search term. Using Dropbox, all the user has to do to perform an image search is simply create a folder.Let’s get started!Despite being more than two decades old, Haskell is still evolving. You can tell your Haskell compiler to allow newer language features using this syntax, this is called the LANGUAGE Pragma. Please don’t worry about what these exact language extensions do just yet, you can read more in the GHC docs.This is an import declaration. Declarations like these inform the Haskell module system that I am going to use definitions from this other module. A module in Haskell is a collection of values (functions are values in Haskell too!), datatypes, type synonyms, type classes, etc.Here I am telling the module system to bring in all definitions from the module Yesod in the namespace of this module. If I wanted to I could also access those definitions prefixed with “Yesod.” similar to Java.Yesod is a fully-featured modern web-framework for Haskell. We’ll be using it to create the web interface of our API app.This is a another import declaration. This is just like the Yesod import except we use the “hiding” syntax to tell the Haskell module system to not import the “when” definition into this module’s namespace. The module we are importing is the main module from HXT, the XML processing library that we use to parse out the search results from an image search result page. More details on this much later.These import declarations are slightly different. Instead of bringing in all names from the modules I am only bringing in specific names. This is very similar to the “from module import name” statement in Python.Remember how I said that I could also access names by prefixing them with “Yesod.” earlier? Adding qualified to the import declaration makes it so you must refer to names in other modules using the module prefix. The “as C” part in the first line makes it so that I can do C.try instead of Control.Exception.Lifted.try.This is an algebraic datatype declaration. Nevermind what algebraic means for the moment, this is the basic way to define new types in Haskell. Even though it’s very short this little code actually does a couple of things:Constructors play a big role in Haskell. Wherever a name can be bound to a value in Haskell, you can also use contructor pattern matching, or deconstruction, to extract out the data contained within that value. Here’s a function to get out the channel component of our ImageSeach type:imageSearchChan takes in an ImageSearch argument and return the channel wrapped inside of it. You’ll see deconstruction a lot more later.So far we’ve defined a type called ImageSearch and we’ve also defined a function called ImageSearch. This is okay because in Haskell type names and value names live in different namespaces.The Maybe type is one algebraic datatype that you’ll see a lot in Haskell code. It’s often used to denote an error value from a function or an optional argument to a function.Unlike ImageSearch, the Maybe type has not one but two constructors: Nothing and Just. You can use either constructor to create a value in the Maybe type. A value of Nothing usually denotes an error in the Maybe type. A Just value indicates success.Another difference from our ImageSearch type is that Maybe isn’t a concrete type on its own; it has to wrap some other type. In Haskell, a “higher-order” type like this is called a type constructor; it takes a concrete type and returns a new concrete type. For example, Maybe Int or Maybe String are two concrete types created by the Maybe type constructor. This is similar to generics, like List<T>, in Java or C#. We use the type variable “a” in the declaration to show that the Maybe type constructor can be applied to any concrete type.It’s important to note that type constructors, like Maybe, are different from data constructors, like Nothing or Just.This is a function definition in Haskell. I’ll explain the definition in the next section but first I wanted to talk about the use of “::” since it keeps coming up. This is how we explicitly tell the Haskell compiler what type we expect a value to be. Even though a Haskell compiler is good enough to infer the types of all values in most cases, it’s considered good practice to at least explicitly specify the types of top-level definitions.The arrow notation in the type signature denotes a function. toStrict is a function that takes an L.ByteString value and returns a B.ByteString value.One cool thing about Haskell is that functions can only take one argument. I know it doesn’t sound cool but it’s actually really cool. How do you specify a function that takes two arguments you ask? Well that’s a function that takes a single argument and returns another function that takes another argument. This reformulation of multiple-argument functions is called currying. Currying is cool because it allows us to easily do partial application with functions, you’ll see examples of this later.Here’s the type signature for a function that takes two arguments of any two types and returns a value in the second type:We use the type variables “a” and “b” to indicate that twoArgFunc is polymorphic, i.e. we can apply twoArgFunc to any two values of any two respective types.With currying in mind, it shouldn’t be too hard to figure out that the arrow is right-associative, i.e. “a -> b -> b” actually means “a -> (b -> b)”. That would make sense, twoArgFunc is a function that takes an argument and returns another function that takes an argument and returns the final value. Say that over and over again until you understand it.What if we grouped the arrow the other way?In this case weirdFunc is a function that takes another function as a its sole argument and returns a value. This is much different from twoArgFunc which instead returns a second function after it accepts its first argument. Passing functions to functions like this is a common idiom in Haskell and it’s one of the strengths of a language where functions are ordinary values just like integers and strings.The definition of toStrict makes use of the function composition operator, “.”, but Haskell functions don’t have to be defined this way. Here’s a function defined using a variable name:What about two arguments?We can define a function that adds five to its argument by making using of currying:We curried in the 5 argument to the plus function and just as we said, it returns a new function that takes an argument and adds a five to it:Another way to define functions is to use a lambda abstraction. You can write a function as an expression by using the backslash and arrow symbols. Lambda abstractions are also known as anonymous functions. Here’s another way to define plus:All functions in Haskell are pure. This means that functions in Haskell cannot do anything that causes side-effects like changing a global variable or performing IO. More on this later.Okay now that you’re cool with functions let’s get back to toStrict. Here’s another way to define it using a lambda:Instead, we define toStrict using the function composition operator, “.”. The function composition operator takes two functions and returns a new function that passes its argument to the right function and the result of that is passed to the left function and the result of that is returned. Here’s one possible definition:Yes this is a real way to define an operator in Haskell! The function composition operator comes standard in Haskell but even if it didn’t you’d still be able to define it yourself. In Haskell, operators and functions are actually two different syntaxes for the same thing. The form above is the infix form but you can also use an operator in the prefix form, like a normal function. Here’s another way to define “.”:In this definition of the function composition operator we use prefix notation. It is only necessary to surround an operator with parentheses to transform it into its prefix form. Switching between infix and prefix notation works for any operator, e.g. you can add two numbers with (+) 4 5 in Haskell.The ability to switch between prefix and infix notation isn’t limited to operators, you can do it with functions too by surrounding the function name with backticks, “`”:This is useful for code like: ""Dropbox"" `isInfixOf` ""The Dropbox API is sick!""One last thing about operators; Haskell has special syntax to curry in arguments to operators in infix form. For example, (+5) is a function that takes in a number and adds five to that number. These are called sections. To further illustrate, all of the following functions do the same thing:With operator currying it’s important to recognize that the side you curry the argument in matters. For example, (.g) and (g.) behave differently.Next to the “.” operator there is another function-oriented operator that you’ll see often in Haskell code. This is the function application operator and it’s defined like this:Weird right? Why does Haskell have this?In Haskell, normal function application has a higher precedence than any other operator and it’s left-associative:Conversely, “$” has the lowest precedence of all operators and it’s right-associative:Using “$” can make Haskell code more readable as an alternative to using parentheses. It has other uses too, more on that later.listToPair is a little function I use to convert a two-element list to a two-element tuple, usually called a pair.A list in Haskell is a higher order type that represents an ordered collection of same-typed values. A list type is denoted using brackets, e.g. “[a]” is the polymorphic list of any inner type and “[Int]” is a concrete type that denotes a list of Int values. Unlike vectors or arrays in other languages, you can’t index into a Haskell list in constant time. It is more akin to the traditional Linked List data structure.You can construct a list in a number of ways:The “:” operator in Haskell constructs a new list that starts with the left argument and continues with the right argument:One last thing about the list type in Haskell, it’s not that special. We can define our own list type very simply:Yep, a list is just a recursive algebraic datatype.In the Lisp tradition, lists are a fundamental data structure in Haskell. They provide an elegant model for solving problems that deal with multiple values at once. While lists are still fresh in your mind let’s go over two functions that are essential to know when manipulating lists.The first function is foldr. foldr means “fold from the right” and it’s used to build a aggregate value by visiting all the elements in a list. It’s arguments are an aggregating function, a starting value, and a list of values. The aggregating function accepts two argu—Screw it, let’s just define it using recursion:Notice how we used the “:” operator to deconstruct the input list into its head and tail components. Like I said, you can use foldr to aggregate things in a list, e.g. adding all the numbers in a list:“[1..5]” is syntactic sugar for all integers from 1 to 5, inclusive.Another less useful thing you can do with foldr is copy a list:foldr’s brother is foldl; it means “fold from the left.”foldl collects values in the list from the left while foldr starts from the right. Notice how the type signature of the aggregating function is reversed, this should help as a sort of mnemonic when using foldr and foldl. Though it may not seem like it, the direction of the fold matters a lot. As an exercise try copying a list by using foldl instead of foldr.map is another common list operation. map is awesome! It takes a function and a list and returns a new list with the user-supplied function applied to each value in the original list. Recursion is kind of clunky so let’s define it using foldr:Even though foldr is more primitive than map I find myself using map much more often. Here’s how you would map a list of strings to a list of their lengths:Remember “$”, the function application operator? We can also use map to apply the same argument to a list of functions:We curried in the 5 value into right side the “$” operator. That creates a function that takes a function and then returns the application of that function to the value 5. Using map we then apply that to every function in the list. This is why having an “$” operator in a functional language is a good idea but it’s also why map is awesome!I talked about lists but I kind of ignored tuples. Like lists, tuples are a way to group values together in Haskell. Unlike lists, with tuples you can store values of different types in a single tuple.A more subtle difference from lists is that tuples of different lengths are of different types. For instance, writing a function that returns the first element of a three-element tuple is easy:Unfortunately, there is no general way to define a “first” function for tuples of any length without writing a function for each tuple type. Haskell does at least provide a fst function for two-element tuples.Final note, see how we ignored the second and third elements of the tuple deconstruction by using “_”? This is a common way to avoid assigning names in Haskell.Yesod makes heavy use of Template Haskell. Template Haskell allows you to do compile-time metaprogramming in Haskell, essentially writing code that writes code at compile-time. It’s similar to C++ templates but it’s a lot more like Lisp macros. Template Haskell is a pretty exotic feature that is rarely used in Haskell but Yesod makes use of it to minimize the amount of code you have to write to get a website up and running.The mkYesod function here generates all the boilerplate code necessary for connecting the HTTP routes to user-defined handler functions. In our app we have two HTTP routes:The first route connects to a resource called HomeR. The second route, located at /dbredirect, connects to a resource called DbRedirectR. We’ll define these resources later.This part of the app brings us to one of the most powerful parts of Haskell’s type system, type classes.Type classes specify a collection of functions that can be applied to multiple types. This is similar to interfaces in C# and Java or duck typing in dynamically-typed languages like Python and Ruby. An instance declaration actually defines the functions of a type class for specific type. Formally, type classes are an extension to the Hindley-Milner type system that Haskell implements to allow for ad-hoc polymorphism, i.e. an advanced form of function overloading.Note that the word instance used in this context is very different from the meaning in object-oriented languages. In an object-oriented language instances are more akin to Haskell values.Section 6.3 of the Haskell 2010 Report has a graphic of the standard Haskell type classes. Many core functions in the standard library are actually part of some type class, e.g. (==), the equals operator, is part of the Eq type class. For fun, let’s make a new type class called Binary. It defines functions that convert data between the instance type and a byte format:You can imagine I might use this class when serializing Haskell data over a byte-oriented transmission medium, for example a file or a BSD socket. Here’s an example instance for the String type:For this instance toBinary is defined as a composition of E.encodeUtf8 and T.pack. Notice the use of T.pack, you’ll see it a lot. T.pack converts a String value into a Text value. E.encodeUtf8 converts the resulting Text value into a ByteString value. fromBinary does the inverse conversion, it converts a ByteString value into a String value.Let’s define another instance:fromBinary in this instance may look kind of gnarly but you should know what it does; it converts a ByteString value to an Int32 value. Understanding it is left as an exercise for the reader.Now getting Haskell data into a byte format is as easy as calling toBinary. An important distinction between type classes and the interfaces of C# and Java is that it’s very easy to add new functionality to existing types. In this example, the creators of the both the String and Int32 types didn’t need any foreknowledge of the Binary type class. With interfaces, it would have been necessary to specify the implementation of toBinary and fromBinary at the time those types were defined.It’s also possible to define functions that depend on their arguments or return values being part of a certain type class:Here packWithLengthHeader requires that its input type “a” be a part of the Binary type class, this is specified using the “Binary a =>” context in the type signature. A subtle point in the definition of this function is that it requires the Int type to be a part of the Binary type class as well (the return value of B.length).Yesod requires you to declare a couple of instances for your app type. Most of the definitions in the Yesod type class are optional and have reasonable defaults but it does require you to define approot, the root URL location of your app. This is necessary for Yesod to be able generate URLs.Here’s another instance declaration. The RenderMessage type class in this case is actually based on two types, ImageSearch and FormMessage. It defines a function called renderMessage which takes two arguments and returns defaultFormMessage.There are multiple ways to specify errors in Haskell. In purely functional contexts it’s not uncommon to see the use of either the Maybe or Either types. In monads based on the IO monad, I usually like to use Haskell exceptions. What’s a monad you say? It’s complicated. Just kidding 🙂 I’ll get to them later.For now, we’re defining a new exception type. It’s the same algebraic datatype declaration you saw earlier for our ImageSearch type except now there’s this “deriving” thing. A Haskell compiler can automatically derive instance declarations for some of the standard type classes. For EitherException we automatically derive instances for the type classes Show and Typeable. As a note, the ability to automatically derive instances for the Typeable type class was enabled by the LANGUAGE pragma DeriveDataTypeable above.The last line declares EitherException to be an instance of C.Exception. It might be weird that we didn’t define any functions for this instance. This is because type classes sometimes provide default implementations for the functions in the class. The C.Exception type class actually provides default implementations for types that are part of the Typeable type class.exceptOnFailure is a function that takes a monad that wraps an Either type and returns that same monad except now wrapping the right side of the Either type. This makes sense to me very clearly but I know, o patient reader, that this must look like gibberish to you.First let’s talk about monads. Monads are types within the Monad type class. The Monad type class specifies two functions (or an operator and a function):The actual Monad type class definition is slightly more complicated but for simplicity’s sake this will do.The first operator “>>=” is called bind. What does it do? Well it depends on your monad instance! What we can say for sure is that it takes takes a monad of type “a”, a function that maps from “a” to a monad of type “b”, and returns a monad of type “b”.The second function, return, takes a value and wraps it in the monad. What it means to be wrapped in the monad, again, depends on the instance. Please note, the return function isn’t like the return statement in other languages, i.e. it doesn’t short-circuit execution of a monad bind sequence.If that sounds abstract that’s because it is! Monads are a sort of computational framework, many different types of computation are monadic, i.e. they fit the form imposed by bind. Why are monads important? Perhaps the most important reason they exist in Haskell is that they provide a purely functional way to perform (or specify how to perform) IO. Monads are much more than just a way to do IO, however, their general applicability extends to many things.I won’t dwell on monads too much in this post but for the purposes of your immediate understanding, it suffices to explain the IO monad and do notation. Just as I’m not dwelling on monads, you shouldn’t either. It takes a long time to really understand and master what’s going on. The more you program in Haskell, the more it’ll make sense.So why can’t we do IO in Haskell without the IO monad? Haskell is a purely functional language, that means that functions in Haskell mimic their mathematical counterparts. A pure function is a mathematical entity that consistently maps values from one domain to another. You expect cos(0) to always evaluate to 1, if it ever evaluated to something else something would be very wrong.In a purely functional language how would you define getChar?It takes no arguments so how can it deterministically return what the user is submitting? The answer is it can’t. You can’t do this in a purely functional language.So what are our options? The answer is to generate a set of actions to take as IO is occurring, in a purely functional manner. This is what the IO monad is and this is why values in the IO monad are called IO Actions. It’s a form of metaprogramming. Here’s an IO action that prints “yes” if a user types “y” and “no” otherwise:Why does this work? Notice how the “output” of getChar isn’t tied to its own value, instead the bind operation gets the output value for us. We’re using monads here to build and model sequential and stateful computation, in a purely functional way!You can imagine that writing real programs in the IO monad could get ugly if you used “>>=” and lambdas everywhere so that’s why Haskell has some syntactic sugar for writing out complex monadic values. This is called do notation. Here’s the same IO action from above written in do notation.In do notation each monad is bound using bind in order and values are pulled out using the left-pointing arrow “<-”.We’re coming to the close of yet another Haskell monad explanation but before we finish I really want to emphasize that monads and the IO monad in particular aren’t that special. Here’s my very own implementation of the IO monad:Of course in the real IO monad, getChar isn’t hard-coded to return the same thing each time and print actually prints something on your terminal. IO actions are run by your Haskell runtime which is usually written in a language where you can actually call a non-pure getChar function, like C.Now, back to exceptOnFailure. Let’s look at it again:Is it still confusing? 🙂Remember how I said earlier that the Either datatype was used to denote errors in Haskell? The definition of Either looks like this:You can use the either function to return different values depending on the Either datatype passed in. By convention the Left constructor is used to denote an error value.For exceptOnFailure, first we create a function that takes an Either value and if it’s a failure we throw an exception using C.throwIO otherwise we call return to rewrap the success value. Then we curry in that function to the right side of the “>>=” operator.Here I’ve defined a couple of convenience functions for using the Dropbox SDK. This is just so I don’t have to use DB.withManager every time I call these functions. Also all of the vanilla Dropbox SDK functions return an Either value in the IO monad so we make use of exceptOnFailure to automatically throw an exception for us if something goes wrong.C.try is the normal way to catch exceptions in Haskell. Unfortunately it uses ad-hoc polymorphism within the Exception type class to determine which exception it catches. Since tryAll has an explicit type signature, it’s bound to the instance of C.try that catches C.SomeException.This is a constant I use for the name of my session key that stores the OAuth request token when authenticating the user to my API app but more on that later.The Dropbox API uses OAuth to grant apps access to Dropbox user accounts. To access any of the HTTP endpoints of the Dropbox API an app must provide an access token as part of the HTTP request. Access tokens are revokable long-lived per-user per-app tokens that are granted to an app at the request of a user.Acquiring an access token is a three step process:A request token actually consists of two components, a key and a secret. Only the key component should be exposed in plaintext. To ensure proper security, the secret should only ever be known to the Dropbox servers and the API app attempting authentication. To exchange an authenticated request token for an access token, the app must also provide the original secret of the request token. This prevents third-parties from hijacking authenticated request tokens.An access token is long-lived but at any point in time can become invalid. When an access token becomes invalid it is the responsibility of the API app to go through the authentication process again. This allows Dropbox and its users to revoke access tokens at will.Users can enable our app for their Dropbox account using the web. Yesod is the web framework we are using to implement web pages in Haskell. In Yesod all HTTP routes are values in the Handler monad. The convention is that the handler name is the combination of the HTTP method (GET in this case) and the name of the resource. getHomeR is the handler for the GET method on the “HomeR” resource which is located at root HTTP path, “/”. Handlers are connected to HTTP routes served by the web server via the use of Template Haskell above.The Handler monad is essentially a decorated IO monad so don’t worry about what it is that much. You should use it just like you would use the IO monad.getYesod retrieves the app’s value. In our app this value has the ImageSearch type that we defined at the very beginning. Here we’re deconstructing the ImageSearch value and extracting only the config component (while ignoring the channel component). The config value stores some information about our app, like app key and locale, that is used by the Dropbox SDK.getUrlRender gets the URL render function for your app. It turns a resource value for your app into a Text URL.Here we call the DB.authStart function (by way of myAuthStart). This function performs the first step of the Dropbox API authentication process. We pass in the URL, DbRedirectR, that we want the user to be redirected to after they authenticate and we get back our new unauthenticated request token and the Dropbox URL where the user can authenticate it. Note that T.unpack converts a Text value into a String value.The myAuthStart function is in the IO monad so we make use of liftIO to execute myAuthStart in the Handler monad. Since Handler is a wrapper around the IO monad, the liftIO function “lifts” the IO action into the higher monad.Do notation allows you to bind names using “let” in a do block. This is for when you need to assign a name to a value that isn’t coming from a monad. Here we’re deconstructing the request token from myAuthStart to get the key and the secret.A session in Yesod is a set of key-value pairs that is preserved per browsing session with our web site. It’s implemented using encryption on top of HTTP cookies. We store the key and the secret of the request token in the session using setSession. We’ll need the secret to finish the authentication process after the user authenticates our app.setSession expects two Text values so we use T.pack to turn the second argument from a String type into a Text type.Finally we redirect the user to the URL given to us from myAuthStart, authUrl. Dropbox will ask the user if they want to allow our app to have access to their account. After they respond, Dropbox will authenticate the request token and then redirect the user to the URL passed to the call to myAuthStart above.When Dropbox redirects the user back to our site it passes along some query args in the GET request: “oauth_token” and “uid”. Yesod provides a convenient way, using runInputGet, to extract those in the handler.The (,) operator is a special prefix-only operator that creates pairs for us:You might be wondering what “<$>” and “<*>” are. Relax, these are regular operators. They are used for applicative functors. Applicative functors are kind of like monads except not as powerful. I’m going to do something horrible here and define “<$>” and “<*>” in monad terms:If you pretend that bind, “>>=”, doesn’t exist and you only have “<*>” and return defined for your type, then your type isn’t a monad, it’s an applicative functor. The only exception is that return is instead called pure in the applicative functor type class:The actual Applicative type class definition is slightly more complicated but for simplicity’s sake this is good enough.Now let’s put it all together, in our definition of getDropboxQueryArgs we apply (,) in the applicative functor, then pass the resulting value to runInputGet. runInputGet then runs the applicative functor in the Handler monad.I know it sounds crazy, I know it does, but luckily you don’t have to fully understand what’s going on behind the scenes to understand how it’s supposed to behave. Keep writing and reading Haskell and eventually it’ll make a lot of sense. Trust me, if I can understand this stuff you can too.This is the handler for the location of the redirect in the app authentication process. After the user has given our app access to their account they are redirected here.Remember how before we redirected the user to the Dropbox authentication URL we first set a key-value pair in the Yesod session using setSession? After the user is redirected, we use the lookupSession function to get the token back out. lookupSession returns a Maybe value so that if a key-value pair does not exist in the current session it can return Nothing, otherwise it will return the value wrapped in the Just constructor.when is a nice function courtesy of Control.Monad that runs the second argument, a value in some monad, only if the first argument is true. It’s defined like this:If mtoken is bound to a Nothing value we’ll return an error message to the user asking them to enable cookies and discontinue normal execution by using sendResponse. After the isNothing check we are guaranteed that mtoken is bound to a Just value.We extract the token from mtoken using fromJust and pass that along to T.splitOn. T.splitOn will split a Text value into a list of Text values using the input argument (“|” in this case) as the delimiter. Then we use the “@” syntax to simultaneously bind the result to the rt name and deconstruct the first element of the result into the sessionTokenKey name.To get the request token key that the user authenticated at the Dropbox website we use getDropboxQueryArgs. Checking this key against the request token key that we stored in the session helps prevent request forgery. If the keys don’t match we stop execution of this handler by calling invalidArgs. “/=” is the not equals operator, like “!=” in other languages.We do this verification because we want to prevent other sites from successfully coercing a user into invoking this handler. We only want the Dropbox website to invoke this handler.Here’s listToPair in action! Since it returns a tuple we use this nifty built-in function called uncurry:Using uncurry and listToPair, we pass the DB.RequestToken constructor the request token key and secret that we stored in the session. Since rt contains two Text values we use “map T.unpack” to convert them into two String values.Now we can finish up the authentication process and get our access token. We pass in the authenticated request token to DB.authFinish (by way of myAuthFinish) and if everything is successful we obtain the access token.In this app we make use of Concurrent Haskell. This allows us to create multiple independent threads of control in the IO monad. There are logically two main concurrently running threads in this app, the web server thread where all of our handler code is run, and the thread that is updating the Dropbox accounts of the users of our app with the relevant search results. We’ll talk about our use of threads a lot more later.Channels are a mechanism for typed inter-thread communication in Haskell. We use the channel component of our app value to send over the access token so that we can begin the updating process for this user’s Dropbox account.Finally, we send a success message to the user’s browser indicating they have linked their account to our app.We’ve gone over monads and we’ve gone over their weaker counterparts, applicative functors. Arrows are another pattern you’ll likely see used in Haskell code. They commonly serve as a general framework for representing and combining operations that convert input to output, kind of like filters. Arrows, like monads and applicative functors, are actually types and arrow types are instances of the Arrow type class:The actual Arrow type class definition is slightly more complicated but for simplicity’s sake this will do just fine 🙂Given my analogy to filters you might think that regular Haskell functions resemble arrows and you’d be right. Arrows are generalizations of regular Haskell functions and functions are, in fact, defined as instances of the Arrow type class:To understand this instance declaration it’s important to note that the arrow symbol, “->”, is an operator in the Haskell type language. Just like normal operators, operators in the type language also have prefix forms, e.g. “(->) a a” is the same type as “a -> a.”In “instance Arrow (->)”, we turned the “->” operator into its prefix form and used that to define the Arrow instance for functions. In Haskell, the “->” operator in the type language is actually a type constructor, i.e. when you apply it to two types it creates a new type, a function of those two types.It’s common to use the Arrow type class as a basis for combinator libraries; libraries that allow you to combine domain-specific functions in intuitive ways to create new functions. HXT is one such combinator library, it’s a library for manipulating XML documents. In our app we define an arrow, selectImageUrls, that takes an XML document, denoted by the XmlTree type, and extracts all of the links that contain the string “imgurl=”. These are links in the image search result page that contain the URLs to the found image files.Above I wrote that there were two main threads of execution in this app: the thread that served out HTTP requests for our web front-end and the thread that implemented the image search functionality in our user’s Dropboxes. For the latter part, there are actually many threads. There is one thread that is listening on the Haskell channel for new users linking our app to their accounts from the web server thread. There is a thread per user account that polls the user’s Dropbox account every 30 seconds and waits for new folders for it to populate with images. There is also a thread per new folder per user account that is responsible for populating a specific folder with the images found during the image search.In other languages like C, C++, Java, or Python this unbounded use of threads wouldn’t be very efficient since threads in those languages often map 1:1 to kernel-level threads. Normally you can’t depend on kernel-level threads scaling into the tens of thousands. In Haskell (or at least in modern versions of GHC) threads are relatively cheap and the runtime does a good job of distributing many Haskell threads across a bounded number of kernel-level threads, usually one per CPU.handleFolder is the thread that is responsible for populating a specific folder in a user’s Dropbox with the image search results.We use the file name portion of the folder path as the search term.src is the generated full image search URL. We use the URLEncoded library to generate a properly escaped URL query string.Here we fetch the image search result page using simpleHttp. simpleHttp returns a lazy ByteString type but our XML library requires a String type so we have to convert between the two using a combination of T.unpack, decodeUtf8With, and toStrict.Here we make use of the HXT XML processing library to parse out all the relevant image search URLs from the HTML document returned from the search query. Notice the use of the selectImageUrls arrow defined earlier. images is of type [String].forM_ executes a monad for each element in its list argument. The second argument is a function that takes an element from the input list and returns the corresponding monadic value.Using forM_ we’re performing an IO action for each image URL we parsed out of the result page to ultimately upload that image into the user’s Dropbox. We wrap the monad expression in a tryAll to prevent an exception in the processing of any single element from stopping the entire process.Each of the URLs that were parsed out of the HTML contain the source URLs of the images in an embedded query arg, “imgurl”. In this code snippet we extract the actual source URL of the image, imgUrl, from the “imgurl” query arg and we generate the path into the user’s Dropbox where we want to place the image, dropboxImgPath.simpleHttp performs an HTTP request to the location of the source image and returns the response body in a lazy ByteString.This is the call to the Dropbox SDK that allows us to upload a file. We upload the file data, image, to the path we generated earlier, dropboxImgPath. If a file already exists at that path, DB.addFile won’t overwrite it.handleUser is the thread that runs for each user that is linked to our API app. It monitors the user’s Dropbox for new folders that we should populate with search results. It polls the user’s Dropbox every 30 seconds and loops forever.While this thread is running it’s possible for the handleNewUsers thread to send us a new access token to use through the channel given by the chan argument.I use the “let ... in ...” syntax to privately define the getCurrentAccessToken function. This function repeatedly polls the access token channel using isEmptyChan until it’s empty at which point it returns the last access token that was pulled off the channel.We wrap all the IO actions in this run of handleUser just in case a transient exception occurs.session is the name bound to the DB.Session value that the Dropbox SDK interface needs to upload file data into a user’s Dropbox.We make use of myMetadata to get a collection of all the children inside the root of our API app sandbox, “/”.This snippet of code extracts out the list of new Dropbox paths that we should be populate with image search results.mapMaybe is a combination of map and filter. Any element that the input function returns Nothing for is filtered out of the returned list. Elements that the function returns a Just value for are included in the output list without the Just wrapper. We use it here to return all the paths in the “/” folder that are folders and we exclude children that are plain files.The “DL.” operator returns all the elements in the first list operand that aren’t included in the second list operand, it’s like a set difference operation.Here we use the forM_ function again. This time we spawn off a new thread using forkIO for each new folder we found in the app’s sandbox folder.We need to give our parent IO action access to the new list of paths in the sandbox folder so it can keep track of what paths are new.threadDelay is like sleep() in other languages; It pauses execution for 30 seconds.If an error occurred while polling the user’s account for new folders we bind an empty list to curFolders, otherwise we bind the current list of folders to curFolders.After sleeping for 30 seconds we loop by recursing. This is the common way in Haskell to loop in a monad. Before we recurse here we update the total lists of folders we’ve ever seen so that we don’t attempt to update them again.handleNewUsers is the thread that is listening for newly linked users to our API app via the channel and spawns off a handleUser thread for each new user.We make use of the “@” syntax again to simultaneously bind the ImageSearch argument to the app_ name and deconstruct it into its chan and dbConfig components. The map_ argument keeps a mapping from user ID to the channel of the thread that is handling that user ID. We need that so we can update the access token a thread is using if it is revoked.readChan gets a value off the channel shared between this thread and the web server thread. Each value is a tuple that contains a user ID and an access token for that user ID.We look up the user ID we were given in our map of thread channels. If we have a thread handling the account of user ID we got, we send it the new access token by writing to its channel. If we don’t have a thread handling this specific user account then we create a new channel, spawn off a new handleUser thread, and update our channel map.Finally we loop with the new map.So it’s been a long and arduous path but finally we arrive at that main IO action. The main IO action kicks off execution for every Haskell program just like in C/C++ and Java.Here are the default credentials for the app. We use the Haskell value undefined, otherwise known as _|_. This is a polymorphic constant that you can use anywhere in Haskell, it can be of any type. An exception will be thrown if an undefined value is ever evaluated in your Haskell program. To get this app to work you will need to supply your own values for these constants.In theory these should be parsed out of the command line or a configuration file but for the purposes of this demo app we define them inline here.Create the inter-thread communication channel using newChan.Create our application specific ImageSearch value. It contains both the channel and a DB.Config value.Kick off the handleNewUsers thread that accepts new users to our app.And finally, call warpDebug which kicks off our Yesod web interface.That’s it. That’s our Dropbox API app in Haskell. If you were a newcomer to Haskell this would be a healthy time to have tons of questions. Actually if I’ve done my job right you should be very curious to know more about Haskell 🙂 Head on over to HaskellWiki and start your journey. If you want a nice friendly book to help you get more formally acquainted I can recommend both “Real World Haskell” and “Learn You a Haskell for Great Good”. One piece of advice for your new Haskell journey: don’t sweat the monads.As for our API app, it’s actually not finished yet. One huge thing missing is that it doesn’t remember which users linked to our app and what folders we’ve populated across restarts, we’d need to store that data in some kind of persistent database to fix that.Another pain point is the user has to wait 30 seconds in the worst case for their folders to be populated with images. This is because each of our handleUser threads poll the Dropbox API every 30 seconds. While this is bad from a user experience perspective it’s also bad from an engineering perspective. This will cause the load we induce on the Dropbox API to increase linearly with the number of users using our app, we’d instead like it to increase linearly with the number of active users using our app. Currently there’s no way to get around this issue but we’re working on it!Other minor improvements include picking a better algorithm to decide which folders we populate with photos, better HTML for our web interface, and streaming uploads to the Dropbox API. I’m sure there are more.As an exercise, consider fixing some of these problems, remember you can fork this project on GitHub. By the way our Haskell SDK is also on GitHub, you may need to fork that too.If you have any questions, feel free to reach out. My email is rian+dropbox+com. Have fun!Many thanks to Kannan Goundan, Brian Smith, Dan Wheeler, ChenLi Wang, Martin Baker, Tony Grue, Ramsey Homsany, Bart Volkmer, Jie Tang, Chris Varenhorst, and Scott Loganbill for their help in reviewing this post. Also special thanks to Michael Snoyman for creating Yesod and accepting my patches. Lastly, a huge thanks to all those who have researched and pushed Haskell forward throughout the years.",https://blogs.dropbox.com/tech/2012/01/using-the-dropbox-api-from-haskell/,0,dropbox,,NULL,2012-01-02
A Python Optimization Anecdote,"Hi! I’m Pavel and I interned at Dropbox over the past summer. One of my biggest projects during this internship was optimizing Python for dynamic page generation on the website. By the end of the summer, I optimized many of dropbox.com’s pages to render 5 times faster. This came with a fair share of challenges though, which I’d like to write about today:The Problem Dropbox is a large website with lots of dynamically generated pages. The more pages that are dynamically generated from user input, the bigger the risk becomes for Cross-site scripting attacks. To prevent this, we must ensure to escape all dynamically generated text before it gets sent to the browser. Note that we also need to escape things differently in different string contexts such JavaScript code, HTML text, and HTML attributes. This is because an abusive user-generated string placed in JavaScript may not be abusive if placed in HTML and vice-versa.All user-generated strings on the Dropbox website are passed through a special escaping function we’ve written that takes context into account. The problem is that having defined our own escaping function we need to ensure that it’s as fast as possible since basically all user input is passing through this function. In this article we’ll focus on one particular escaping context, HTML text.A quick disclaimer, before you accuse us of reinventing the wheel and ignoring both the xml.sax.saxutils.escape and cgi.escape functions, it doesn’t escape quite enough characters. This is especially true if you consider the possibility for confusing the browser into believing your page is UTF-7, at which point the equal sign has to be escaped. So we have to write our own.First steps The original function looked like this:Applying it to some test templates gives (times in seconds; the digits (obviously) aren’t all significant):: html1 14.1678471565Not very fast. (I blame the intern who wrote it…) Of course, the code wasn’t optimized for speed, but for readability. But like I said, given that this is actually a non-negligible part of our render time, it could use some optimization.Inlining The first fact of Python optimization is that function calls are slow. html1 is awful in this regard: it calls a function per character! Worse yet, the common case is calling a function that does nothing at all! So the first step is to inline. This also lets us join the on_ascii and on_unicode cases (originally separated because we also use the above for escaping JavaScript, where we do make a distinction between Unicode and ASCII literals).This has a pretty good 15% or so improvement:: html2 12.8864150047Implicit Loops Now, the preceding code sample maybe isn’t too pretty, but the improvement is nothing worth sneering at. There’s more we can do though. The second fact of Python optimization is that the loop overhead can also be pretty significant. This leads to our second attempt, which gets rid of the loop in favor of a generator expression. Fortunately, switching to generator expressions makes the resulting function as readable as the original.What are the timings with this new version?: html3 13.4748219418Hmm. The readability improvements are nice, but the speed dropped. Maybe we can get both speed and reability with some tweaks?More OptimizationsWe already picked out a neat 10% improvement and still have a wonderfully readable function. But we can go faster…Generators?? Really? The problem with the generator expressions used above is that Python actually constructs a generator. Let’s look at the bytecode:Luckily, we can avoid making the generator by simply using a list comprehension instead of a generator expression:This brings us back to faster than html2.: html4 11.2531888485Looks like the generator expression is actually slower than the list comprehension in this case, a good explanation is probably that our sample set of strings are probably too small to reap the benefits of using a generator. As expected, our disassembly now looks a lot more friendly:Sets?? Really? But this is still slower than it should be. One low-hanging fruit still stands out: why are we doing a linear search on the whitelist?The timings bear out the guess that sets are faster than strings:: html5 8.6205868721The question remains, Can we go faster?Deeper Python Optimization If we’re going to optimize this function, we might as well extract all the performance we possibly can. One peculiarity of modern Python is that it has two LOAD instructions: LOAD_GLOBAL and LOAD_FAST. Due to a implementation detail loading a local variable is faster than loading a global variable. Consequently, you want to avoid global variables in tight loops. The above disassembly pointed to two such globals: ord and WHITELIST.We don’t expect this to buy us much, but why not?: html6 7.87281298637In absolute measurement a second is not much of an improvment but as a percentage it’s still a significant of time.String InterpolationString interpolation is another thing Python isn’t very fast at. Let’s see if removing that helps.: html7 5.58323383331That’s a huge boost from saving the string interpolation!Every Problem Can be Solved With a Big Enough Hash Table Any web programmer knows that performance problems can be universally solved with caching. (Heh heh, if only…) In any case, if the gain from removing string interpolation was so great, maybe we can just cache the results of that characters to escaped form: function, and not do string concatenation either. We’ll set up a cache of characters to HTML escapes:Then we read and write our cache as necessary:Since our web servers are long-running processes, the cache should eventually capture all of the characters people are using; Python’s dicts are then fast enough to give us a speed boost over string concatenation and =str(ord(c))=.: html8 4.5724029541Another big boost from avoid what seems like a trivial computation.Premature Optimization… If we’re going down the setdefault branch so rarely, and since that’s the only place we’re using str and ord, maybe it’s not worth making those local variables?We don’t expect much of a benefit here, but maybe a percent change or so…: html9 4.565928936Wait, Why a Python Loop At All? Wait, why are we using a Python loop at all? Python loops are slow… Instead, maybe we can use the C code in Python’s re module? We can use regexes to find anything that isn’t in our whitelist, and do the replacement. That way, the “do-nothing” operation on whitelisted characters becomes much cheaper.Unfortunately, there’s no way to get the entire matched range from a Python match object. So we’re forced to call the group method– and function calls are slow!What are our results?: htmlA 4.54020690918Hmm, this isn’t that great, actually. We did save a percentage point, but this is a lot less readable (at least, in my opinion) than html9, and the gains aren’t worth it.Until, that is, we try it on whitelisted-only text:: html9 3.84376811981 : htmlA 0.796116113663Whoosh! html9 got a bit faster, by virtue of avoiding hitting the cache at all, but the regex-based solution *rocked*, since it could do all of the skipping of characters in C, not Python.In fact, the regex-based solution is slower for punctuation, since it has to do multiple function calls instead of just one function call and dictionary lookup. And for short strings, the overhead to initiating the regex search is worse. Let’s try a test on punctuation-heavy, short text snippets:: html9 1.59476995468 : htmlA 3.44844794273Measure Twice, Cut Once So we have one function that’s faster for English alphanumeric text and one that’s faster for everything else. But we can’t just shaft our non-US users, and we don’t want to settle for a function that’s five times slower for the common case! So we have a few options.The first is simply to expand our whitelist — most file names have a dot in them, and spaces, dashes, and similar are popular. At signs appear in emails. And so on. Of course, one has to be careful not to permit any XSS vector while doing so; but a conservative expansion by adding -|!,. _ to our whitelist should be safe. Of course, this helps both versions, so it’s not really a fix.Another fix presents itself, though: can we figure out which version of html to use quickly?Why .6, you ask? Well, the exact constant isn’t too important (the function *works* whichever way the test goes, it’s just an optimization), but some testing showed it to be the approximate break-even point of the two methods.With hope and trepidation, we run the tester…: htmlB 5.16241598129 : html9 3.97228693962 : htmlA 3.95208191872Awww…As we hoped, the result is fast on whitelisted-only characters…: htmlB 1.24477005005 : html9 3.41327309608 : htmlA 0.696345090866And is passable on punctuation:: htmlB 5.97420597076 : html9 3.61161899567 : htmlA 8.88924694061But the overhead is pretty saddening…Sampling We can improve things a bit by testing just a few characters for punctuation/unicode.We use =6= instead of =.6 * min(10, len(s))= because if the string is shorter than 10 characters, either alternative is going to be fast.This leads to a marked improvement. For alphanumeric strings:: htmlC 0.836707115173 : html9 3.34154415131 : htmlA 0.701889276505For unicode-heavy strings:: htmlC 3.70150613785 : html9 2.82831597328 : htmlA 6.77485609055This is now really looking like an option for production use. But checking is still quite a bit of overhead still– in the case where the user has just about the break-even balance of English and international characters, we’re looking at a 20% premium. Can we do better?The User is Always Right Well, so checking which to use has very high overhead. What to do? Well, we’ve got to think about when each case will come up: how might we predict that lots of non-English-alphanumerics are going to come up? What users are likely to store files with international names, or use an international name? (Hopefully, people who use lots of punctuation in both are few…)Well luckily, Dropbox added translations a bit back, so we already have all of the infrastructure in place to detect what locale a user is from. So a quick optimization is to switch to the html9 version (the list-comprehension-based one) for international users, and use the fast htmlA version (regexes) for US-based users, and also users from countries with mostly-Latin alphabets. The code here is mostly tied to Dropbox internals, so I’m not going to show it, but I’m sure you get the point.This final optimization removes the check overhead while giving us most of the benefit. Success!What We Didn’t Do Now there are some ways to optimize the above even further. The obvious approach is to rewrite the entire thing in C. This lets us squeeze the last bits of performance out of the above, but it would also make things much harder to maintain.Similarly, using PyPy or a similar JIT would probably help on the inner loop of the pure-Python version, likely making it as fast as the regex-based approach.Conclusions The first, most basic conclusion is that the basic facts of Python optimization inline functions, use implicit loops, move computation into C if possible are perfectly valid. Another fact: Python dictionaries are *fast*. The WHITELIST set and the CACHE_HTML_ESCAPES dict both rely on the superb hash table implementation for their performance gains.Other “common wisdom”, like using locals instead of globals, yields relatively little gain.Optimizing inner loops in a high-level loops requires lots of trial and error. It was absolutely amazing that moving to string concatenation from string interpolation gave such a huge boost to performance.Finally, measurement is key. It was measurement that told us that HTML escaping was slow to begin with; and without measuring performance, we would never have guessed that string interpolation was so slow.Thanks for reading!",https://blogs.dropbox.com/tech/2011/10/a-python-optimization-anecdote/,0,dropbox,"python,python3",NULL,2011-10-24
Dennis Ritchie,"Many people spend their lives endlessly searching for the perfect answer to “why?” It stops them early in their tracks and holds them back from reaching their true potential.Then there are those who are confronted with the question “why?” and laugh directly in its face. They interest themselves solely with the “how?” For these people any “why?” will do as long as there is sufficient “how?” associated with it. Dennis Ritchie was one of those people. A fiercely gifted programmer, he relished in the “how?”If you were to ask any common person in these modern times “Why Unix?” Well it would be obvious. Unix because of file systems, because of the internet, because of Windows, because of Mac OS X, because of Linux and software freedom. What about “Why C?” Well that would be obvious too. C because of C++, because of Java, because of Perl, because of Python, the list goes on.But if you were to ask someone “Why Unix?” or “Why C?” in 1972 you’d have a hard time finding an answer, even from Dr. Ritchie himself. The only true answer you’d be able to find would be “For Space Travel” or maybe “For Fun” and little else.People like Dennis Ritchie made it respectful for those who were more interested in the “how?” rather than the “why?” to follow their own path. A programmer could program for the sake of the craft and the art without feeling like she was wasting her time or potential. Sometimes you just want to make something and that should be okay. Programs became works of art rather than slaves to science or utility.And because they were works of art it takes an artist to understand the elegance, the universality, and the eternality of his accomplishments. Unix will be here forever and as much as people would like to say otherwise, the file system will be here forever. I’m sorry to say it but those who don’t yet understand why the file system is the ultimate abstraction still have much to learn.As time has progressed from then to now the works of Dennis Ritchie have taught us that the “why?” doesn’t always have to dictate the “how?” and sometimes the “how?” can dictate the “why?”",https://blogs.dropbox.com/tech/2011/10/dennis-richie/,0,dropbox,,NULL,2011-10-14
Translating Dropbox,"¡Hola, mundo! Welcome to our new engineering blog. We’d like to start with a post on i18n, because aside from being exceedingly fresh in our minds right now, hearing our take on it might be useful to other startups. Going global can be an intimidating prospect, especially if you’re like me and illiterate in all the new languages. I’ll focus today on assembling a translation team because it’s one of the trickier challenges.Back in March 2008, Facebook’s community famously translated the site into French in just 24 hours. Community translation, done right, often offers the highest quality because users know your product inside-out and naturally use the right tone and formality level. Community translation also makes it possible to go from a handful of languages to over 100.While intriguing, we’re holding off on community-led translation for now and going with a hybrid approach instead. The main reason is the substantial engineering investment. We’d need to:So it requires a big initial effort. Facebook had a staff of about 500 in March 2008, for example, whereas we have 50. By contrast, professional translators are incredibly convenient and barrier-free. We can send them the latest version of 12 files in 12 different formats, they’ll determine the difference from the last version, and send us back the translations, ready to go.Here’s how our approach works: we hire a firm to translate everything beforehand, then ask our generous userbase to review and send us their corrections, iteratively. Our firm reads through every suggestion per English string, one string at a time, and makes the most popular correction. Before sending a feedback round to translators, we quickly skim it on our own first through a special admin page, to do three things:Here it is in action. Say I’d like to report a dubious French translation. First I click the green tab to the left of the screen:Then I type or paste a few letters of the translation I’d like to improve. It pops up a list of matches:After selecting the translation, the original English text shows up underneath. I type in my improved translation and the reason I like it better. Done.This feature also exists on experimental builds of our desktop app, released on the forums.Grouping suggestions by English string is key. It offers good usability: people can type just a few letters, autocomplete the translation in question, and view it alongside the original English string. Further, it organizes everything for easy translator review. One issue complicates things: strings with placeholder variables. For example:Hello, %(first_name)s!We want users to be able to autocomplete text as it appears on the page: “¡Hola, Dan!”, not “¡Hola, %(first_name)s”, and at the same time, internally group all instances of this placeholder string together. You might think it would be as easy as wrapping the gettext _() function, but consider typical gettext code:greeting = _('Hello, %(first_name)s') % {'first_name' : user.first_name}That is, placeholder substitution happens outside the _() translation call. Our solution: on the server, we subclass python’s unicode builtin type, overriding __mod__ to remember its filled-in placeholders. We then wrap the gettext _() function to do two things:At the very end of the response, we take the list of translated strings and serialize into JSON, ready to go for browser-side javascript autocompleter code. For each string, this list includes the placeholder form “¡Hola, %(fname)s” (for bookkeeping), and filled-in form “¡Hola, Dan” (for autocompletion). The reason we keep autocompletion entirely browser-side is that it makes the experience feel instant. Another nice result is the autocompletion menu is always narrowed down to text that appears on the page — less choices to wade through.One last complicating detail: AJAX. Some of our AJAX requests, for example in the events feed, contain translated display text. We want users to be able to select text no matter where it came from. To solve, we pack a list of new translations within each AJAX request, similar to the list from the initial request.For anyone about to do similar work, I thought I’d give a quick summary of some of the other challenges we needed to cover for today’s launch. Let us know in the comments if you’d like to hear more about any of these topics; we’d be happy to expand in future posts.We encountered some typical i18n problems:And a few not-so-typical:This is a totally new world for us, so if you think of any improvements we can make or areas we missed, we’d love to hear about it!",https://blogs.dropbox.com/tech/2011/04/hello-world/,0,dropbox,"java,python",NULL,2011-04-18
