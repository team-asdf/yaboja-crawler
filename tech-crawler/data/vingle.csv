title,content,url,source,keyword,image,createdAt
GIF 사용을 멈춰주세요!,"요즘에는 통신망의 발달과 디바이스 성능의 개선으로 인해 어딜 가나 다양한 형식의 미디어를 즐길 수 있게 되었습니다.그 중 GIF는 우리가 흔히 볼 수 있는 이미지 포맷 중 하나인데, 보통 “움직이는 짤방"" 혹은 줄여서 “짤방"" 으로 표현하기도 합니다. 요즘에는 어딜 가나 GIF를 볼 수 있고, 아예 모바일 디바이스의 키보드에서 GIF를 바로 첨부할 수 있도록 그 인기는 날이 갈수록 치솟아가고 있죠.하지만, 우리는 이제 움직이는 GIF (Animated GIF)를 쓰지 말아야 합니다. 움직이는 GIF는 올해로 30살이 된 오래된 규격이며, 비효율적입니다.도대체 얼마나 비효율적이냐구요? 한번 알아봅시다.여기 각각 다른 포맷에서 동일한 프레임을 캡쳐한 두 이미지가 있습니다. 여러분들은 시각적으로 “두 포맷간 품질의 차이""를 느끼실 수 있으신가요?글쎄요, 저는 잘 모르겠네요.놀랍게도 두 포맷의 품질 차이는 거의 느낄 수 없지만, 파일 크기는 무려 10배 이상 차이가 납니다.이렇게 큰 파일 크기 차이가 발생하는걸 보면, GIF는 정말 “거품” 입니다. 서비스를 운영하는 쪽 뿐만 아니라, 서비스를 사용하는 유저에게도 손해입니다.GIF는 불필요하게 대역폭을 낭비해 엄청나게 많은 전송량을 발생시킵니다.특히 대역폭이나 사용량이 제한되어 있는 모바일 네트워크에서는 그 손해가 더 크게 느껴질 수 밖에 없습니다.자, 아래 동영상은 대역폭을 제한한 상태에서 각 두 포맷을 재생한 것입니다. 한번 보시죠. (두 비디오 모두 MP4 Video -> GIF 순입니다)H.264 MP4의 경우 대부분 재생이 원활하고 데이터를 적게 소모한 반면, GIF는 재생이 원할하지도 않고 데이터 소모도 엄청납니다! 특히 4개의 Animated Image를 보여주는 페이지에서는 총 GIF 파일의 크기가 100MByte를 넘어서는것을 확인할 수 있습니다. 엄청 크죠?그 이유는, 각 포맷의 특성에 있습니다.GIF의 경우 256색을 표현할 수 있는 컬러 팔레트와, 각 프레임의 모든 픽셀에 대한 정보를 무손실 압축 데이터로 담고 있으며 , H.264를 포함한 비디오 포맷들은 기본적으로 손실 압축이고, GIF와는 비교가 안될 정도로 다양한 최적화 기술들이 들어가있습니다.예를 들면 아래와 같은 기술들이 있습니다:그래서 발전된 기술들이 많이 쌓인 비디오 포맷들이 GIF보다 더 나은 압축 효율을 보여줄 수 있는 것입니다.자, 우리는 이제 GIF가 효율이 정말 좋지 않다는걸 직접 확인했습니다. 근데 그것말고도 GIF를 쓰지 말아야 할 이유가 아직 더 남아있습니다.이번에 이야기 할 주제는, 디코딩 성능입니다.2000년 초까지만 해도 압축된 비디오 데이터를 해석하는 것 (디코딩)은 온전히 CPU의 몫이었습니다. 2005년 즈음부터 MPEG-TS를 시작으로 GPU를 통해 하드웨어 디코딩이 가능하기 시작했죠. 물론 요즘 우리가 쓰는 스마트폰에 탑재된 프로세서도 하드웨어 디코딩을 지원합니다.하드웨어 디코딩이 중요한 이유는, GPU를 통해 디코딩을 처리하는게 훨씬 전력 효율이 좋기 때문입니다. 아무리 CPU의 연산 성능이 좋아졌다곤 하지만, GPU는 아예 디코딩을 위한 블럭들이 칩에 박혀있기 때문에 그 효율을 따라갈 수 없습니다. 만약 GPU를 통한 하드웨어 디코딩이 가능하지 않았더라면, 스마트폰에서 Youtube 비디오를 2시간 넘게 즐기는건 불가능했었을겁니다.제가 여기서 “하드웨어 디코딩"" 을 이야기 하는 이유는, 바로 GIF 이미지를 디코딩하는것은 GPU에서 처리하지 못하기 때문입니다. H.264/AVC 비디오의 경우 대부분의 디바이스에서 하드웨어 디코딩이 가능합니다. 라즈베리파이에서도 되거든요.이번에도 비디오를 가져왔습니다. 먼저, 데스크탑 브라우저에서의 디코딩 성능 차이를 보시죠: (MP4 Video -> GIF 순입니다)처음이 H.264 MP4 비디오 재생, 두번째가 GIF 이미지 재생입니다. Instrument App의 CPU Activity Graph와 Live Processes Table를 보시면 확실히 GIF 이미지를 재생할 때 더 많은 CPU 자원을 소모하는 것을 확인할 수 있습니다.(MP4 Video -> GIF 순입니다)각 포맷을 로드할때의 CPU Usage 차이가 보이시나요? 게다가 비디오가 훨씬 더 균일한 FPS를 보여줍니다.이번에는 iPhone X의 iOS Safari에서 테스트 한 영상입니다. (MP4 Video -> GIF 순입니다)여기서는 몇가지 지표를 더 확인할 수 있는데, H.264 MP4 비디오를 재생할 때 Graphics Activity가 더 높은걸 확인할 수 있습니다. 그리고 역시 GIF 이미지를 재생할때 더 높은 CPU 사용량을 보입니다.더 높은 CPU 사용량을 보인다는 것은, 더 많은 배터리를 소모한다는 이야기이기도 합니다. 또한, 저사양 디바이스에서 원할한 재생이 어려울 수도 있겠죠.그 이외에도 GIF를 피해야 하는 이유는 많습니다.컬러 팔레트에서 최대 사용가능한 색상의 수가 256색으로 제한되어 있어 색 표현에 제약이 있으며,일부 브라우저에서 재생 가능한 FPS (Frame rate speed)가 제한 되어 있기도 합니다.마지막으로, GIF89a 규격을 보면 “GIF는 애니메이션을 위해 디자인 되지 않음”이 명시되어 있습니다.이정도면 이제 GIF보다 비디오를 쓰는게 더 낫다는건 충분히 이해하실겁니다. 그런데 “호환성""이 걱정되신다고요? 그러실 줄 알고 미리 준비했습니다.H.264 비디오는 대부분의 플랫폼에서 호환이 되므로, 걱정할 필요가 없습니다. 그냥 쓰세요!제가 개인적으로 권장하고, Vingle에서 사용하고 있는 Animated Image (GIF)용 비디오 규격은 다음과 같습니다:위 규격은 Vingle이 지원하는 모든플랫폼 (IE 10+, Android 4.1.4+, iOS 9+) 에서 재생이 가능합니다.비디오를 클라이언트에서 재생하기 위해서는, 몇가지 변경사항이 필요합니다. 플랫폼별로 필요한 변경사항들을 나열해보면 다음과 같습니다:저게 전부입니다! 웹은 정말 간단하죠?Android의 경우 보통 이미지 뷰어로 Facebook에서 만든 Presco를 사용하는데, 비디오를 재생하려면 Presco 대신 별도의 비디오 플레이어 라이브러리를 사용해야 합니다. 저는 Vingle Android App에서 사용중인 ExoPlayer를 추천합니다.iOS의 경우 iOS팀의 현수님이 쓰신 글로 대신합니다 :)비디오를 재생하려면 GIF로 비디오로 변환하는 작업 또한 필요합니다.여기서는, 크게 두가지 경우로 나누어 설명하겠습니다.위와 같이 클리앙과 같은 온라인 커뮤니티에 업로드 할 목적이거나, 일부 컨텐츠에 대해서만 GIF 대신 비디오로 보여줄 목적이라면, 별도 프로그램 없이도 변환할 방법이 있습니다. Giphy 같은 GIF 호스팅 서비스를 사용하는 방법입니다.Giphy에 변환하고자 하는 GIF를 업로드하면, Giphy가 GIF를 알아서 비디오로 변환해줍니다! 우리는 그 비디오를 그냥 다시 다운받기만 하면 되는거죠.플랫폼 내에 GIF 업로드 기능이 이미 있는 상태에서 비디오 변환을 추가하려면, 별도로 서버에서 비디오 변환을 처리해야합니다.비디오 변환을 서버에서 직접 처리하는 경우, 저는 개인적으로 FFmpeg를 추천합니다. 위에서 언급한 권장 규격으로 출력하는 경우 ffmpeg 명령은 다음과 같습니다:GIF를 직접 비디오로 변환하는 경우, 주의해야 할 점이 몇가지 있습니다:출력 비디오의 해상도가 반드시 2의 배수여야 합니다.4:2:0 Chroma subsampling의 경우 해상도가 2의 배수여야 합니다.만약 GIF 이미지의 가로 또는 세로 크기가 홀수면, 아래와 같은 에러를 출력하고 비디오 변환에 실패합니다.이 경우, scale이나 crop 같은 video filter를 사용해 출력 해상도를 제어하시면 됩니다.아래는 scale filter를 사용하는 예 입니다:MP4 container의 moov atom을 앞쪽으로 옮겨야 합니다(다소 주제가 어려울 수 있겠지만, 이 내용을 최대한 쉽게 설명드리겠습니다.)MP4 컨테이너의 경우 여러개의 청크로 구성되어 있습니다. 그리고 청크를 atom 이라고 부릅니다.atom 은 비디오 데이터, 오디오 데이터, 챕터 정보와 같은 메타데이터 등 여러 타입이 있는데, 그 중 moov atom이 있습니다. moov atom은 일종의 목차 같은 역할을 합니다. “재생을 시작할 비디오/오디오 데이터의 위치"" 를 담고있는거죠.여러분들이 MP4 비디오를 재생하면, 플레이어들은 비디오를 재생하기 위해 내부적으로 이 moov atom을 찾게됩니다. 하지만 정말 불행하게도, atom들의 순서는 정해져있지 않습니다. 즉, 만약 moov atom이 맨 끝의 atom 이라면, 파일을 다 읽어들이기 전까지는 재생을 시작하지 못합니다. 비디오 데이터가 파일의 어디에 있는지 moovatom을 읽기 전까지는 특정할 수 없으니까요.MP4 비디오가 로컬에 있는 경우에는 디스크에서 탐색을 수행하기 때문에 그래도 괜찮지만, 문제는 네트워크를 통해 MP4 비디오를 스트리밍 하는 경우입니다. moov atom을 찾기 전까지는 비디오 재생이 아예 불가능하기 때문입니다.요즘 대부분의 모던 브라우저들은 Range Request Header를 사용해 MP4 파일을 탐색 (seek) 하도록 구현이 되어 있지만, 여러 요청을 주고 받기 때문에 네트워크 지연으로 인해 재생을 시작하기까지 시간이 다소 소요될 수 있습니다.최악의 상황은, 만약 웹서버가 Range 헤더를 읽어 Partial Content Response를 지원하지 않는 경우입니다. 이 경우 비디오 파일을 모두 다운로드 받기 전까지는 재생을 시작할 수 없습니다.아래 비디오를 통해 Partial Content Response 지원 유무에 따른 그 차이를 확인해보세요:moov atom을 맨 앞으로 옮긴 경우, 이러한 seeking이 필요가 없기 때문에 단일 요청을 보내고 응답을 받자마자 즉시 재생을 시작할 수 있습니다. 또한, moov atom을 맨 앞으로 옮겨주었기 때문에 웹서버가 Partial Content response를 지원하지 않아도 걱정할 필요가 없습니다.다음 이미지는 moov atom을 맨 앞으로 옮김으로서 단일 요청만으로 재생을 시작하는 것을 보여줍니다:그 이외에도, 다양한 오픈소스 프로젝트들이 있으니 참고해보세요:Vingle 에서 Video Pipeline을 구축한 관련 경험에 대해 궁금하시다면, 아래 슬라이드를 참고해주세요:또한, Google의 Addy Osmani가 작성한 Image Optimization Guide도 한번 읽어보시는걸 강력하게 추천합니다:만약 컨버팅 프로세스를 직접 만들고 싶지 않다면, SaaS 제품을 사용하시는것도 괜찮은 방법입니다. 개인적으로 추천하는 플랫폼은 다음과 같습니다:아마 이런 생각을 하시는 분도 계실겁니다.네. WebP나 WebM을 사용할 수도 있습니다. 하지만 저희는 다음과 같은 이유로 WebP와 WebM을 선택하지 않았습니다:또한, HEIC/HEIF나 HEVC와 같은 차세대 포맷을 생각한다면, 더욱 WebP나 WebM을 선택할 이유가 없었습니다. 그 이유는 다음과 같습니다:HEIC/HEIF는 이미지를 위한 새로운 규격으로, MP4와 비슷한 일종의 컨테이너입니다. 내부적으로는 HEIC나 AVC로 이미지를 인코딩해서 쓸 수 있으며, 다양한 사용 사례들 (e.g. 스틸 이미지, 이미지 컬렉션, 이미지 시퀀스 — iOS의 Live Photo- 등)을 염두하고 디자인 된 규격이기 때문에 상당이 유연하고,HEVC는 H.264/AVC 를 이어갈 코덱 (그래서 H.265/HEVC 라고 부르기도 합니다) 으로 H.264/AVC를 ‘뛰어넘는' 압축 효율을 가지고 있습니다.게다가 두 포맷 모두 메이저 플랫폼 뿐만 아니라(iOS 11 / macOS High Sierra / Windows 10), GPU Vendor들(Intel, NVIDIA, AMD, Qualcomm) 이 지원을 시작했습니다.HEIC/HEIF의 경우 아래 링크를 통해 다양한 사용 사례들을 확인하실 수 있으니 한번 참고해보세요!이야기 했던 내용들을 간단히 요약하면 다음과 같습니다.빙글에서도 유저들이 올리는 말도안되는 사이즈의 GIF 때문에 아주 오랫동안고통을 받았는데요. (100MByte를 넘어가는 GIF 이미지들, 눈물 없이는 볼 수 없는 CDN 요금, 빙글 앱만 켜면 휴대폰이 손난로가 된다는 피드백… 😭)다음 글에서는 저희가 어떻게 GIF -> Video Conversion process를 완벽히 serverless 구조로 만들었는지 이야기하고, Lambda와 S3를 사용해 직접 변환 프로세스를 구현하는 방법을 다루어보겠습니다.다음에 또 만나요!빙글은 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다. 같이 일해요!",https://medium.com/vingle-tech-blog/stop-using-gif-as-animation-3c6d223fd35a?source=---------0---------------------,medium,gif,NULL,2018-08-23
"Serverless와 기술도입, Backend Application의 미래","빙글의 Backend Application을 Monolithic EC2 + Ruby On Rails에서 Microservice Architecture 기반의 AWS Lambda + Node.JS 로 옮겼다는 이야기를 몇 번인가 발표하고 난 뒤로, 외부 사람들을 만나서 System Migration에 대한 이야기를 하다보면 언제나 비슷한 질문을 듣게됩니다.(그 외에 Microservice Architecture 라는게 도대체 뭐냐? 같은 단순 개념 질문이야 한국에서는 사례가 드물기에 당연한 것 같고요. 구글 검색 해보시면 더 정확한 걸 왜 굳이 저에게 물어보시는 지는 모르겠지만..)사실 개인적으로, 이런 질문을 들을때마다 좀 당황스럽습니다. 사실 새로운 기술을 선택하고, 팀원들을 설득하고, 그 기술에 맞는 도구를 (없다면) 직접 만들고 하는 긴 과정을 할지 말지 선택할 때, 당연히 저것보다 훨씬 중요한 질문들이 많았기 때문입니다.오늘은 저희가 Serverless에 베팅을 걸었을때 제가 고민했던게 뭐였는지, 그리고 지나고 나서 돌아봤을때 놓친게 뭐였는지, 그리고 이런 기술선택을 할때 저희가 답을 고민해야하는 질문이 도대체 뭔지 (답이야 상황에 따라 다르겠지만) 에 대해 Serverless로의 migration 경험을 토대로 이야기해보려고 합니다.(질문들은 굳이 답변하자면 “더 싸요” / “아니요"" / “원래 이런건 허락 안 받아요” 긴 합니다)우선, 많은 팀들이 “기술 선택” / “기술 도입” 이라는걸 매우 추상적으로, 그리고 거대 트렌드 차원에서만 생각하고 실행한다는 생각이 듭니다.Michael Burry 라는 투자가가 있습니다. 이사람은 2008년 금융위기에서 주택시장의 붕괴 (정확히는 주택담보부증권의 붕괴) 를 몇년 앞서 예측하고, 이를 기반으로 주택담보부증권을 공매도하여 +489% 라는 기록적 수익률을 남긴걸로 유명한데요. 이 사람은 이 공매도를 유지하는 기간 내내, 자신이 관리하는 기금의 투자자 들로부터 줄소송 (“이렇게 잘나가는 주택시장을 공매도라니 미쳤냐” / “내 돈 돌려내라”) 을 당했습니다.그렇다고 이사람이 묻지마 투자를 한건 당연히 아니였습니다. 영화에도 나오지만, 이 사람은 아주 단순하게도, 주택담보부증권의 raw level data 몇만건을 직접 하나씩 읽고 확인하여 믿기 어렵지만 아주 논리적인 결론, “주택담보부증권은 전부 지나치게 높게 평가되어 있으며, 곧 default에 빠질 것이다” 에 다다랐죠. (그외에도 주택 대출 사기가 급증했다 등의 시장 data도 있었고요)(주택담보부증권은 집을 담보로 돈을 빌린 대출 몇천-몇만건을 묶어서 하나의 증권으로 만드는거라, 그 증권을 만든 변호사나 그 대출들을 전부 확인하지, 투자자들은 무디스나 S&P같은 증권평가회사들에서 정한 증권의 평가등급만 봤던 겁니다. 증권 안에 있는 몇천건의 대출 하나하나의 상환 상태나 default 가능성을 확인 해보지는 않았던거죠)당시는 미국 정부, 은행, 금융평가원, 연방준비위원회 모두 주택시장은 안정적이며 리스크는 낮다고 하던 시절 이였으니, 투자자들이 이랬던걸 이해 못할건 아닙니다.이 사람은 결국에는 공매도 포지션을 팔아 자신에게 소송을 건 투자자들에게도 기록적인 수익률을 남겨준뒤, 자신이 운영하던 펀드를 닫아버립니다.(참고로 이사람은 몇년뒤 다시 펀드를 열었고, 요즘은 비트코인에 투자한다고 하더군요)제가 이 이야기를 감명깊게 봤던건, 여기서 “자산"" 을 “Framework” / “Library” “Architecture” 로 바꾸면 많은 개발자들에게 그대로 적용되는 내용이기 때문입니다.예를들어 Docker.물론 Docker가 근본적으로 나쁘다 이런게 아니라, (빙글에서도 여전히 Docker 사용하고 있어요;;) “Cloud에 올릴꺼면 당연히 instance가 한대여도 Container 써야지!!!!!!!!!” 가 아니면 설명될수 없는 도입이 너무 많다는 겁니다.개인적으로는 ELK를 이 예시로 참 자주 느끼는데요. ELK 그 자체가 악하다거나 나쁘다거나 이런게 아니라, 도대체 왜 Log 분석이나 search는 무조건 ELK 로 하는걸까요?제가 실제로 겪었던 사례 하나는,물론, ELK가 압도적인 우위를 가지는 기능도 분명히 있고, 그런것들을 고려해서 선택하는건 당연히 이상할게 없죠. 다만 다른 대안이 없는것도 아닌데, “Log 분석은 다들 ELK로 하니까” ELK를 선택하는건 뭔가 뒤틀려 있다는 겁니다;기술은 언제나 상황과 맥락에 따라 그 가치가 달라지고, 회사나 팀마다 상황이 분명히 다 다를텐데 말이죠. “100% 나쁜 기술” “100% 좋은 기술"" 이런건 단언코 없습니다. 하다못해 공인인증서도, 한때는 정말 진보된 보안 기술로 취급되던 시절과 상황이 있었고, Web application의 standard model이 Apache / PHP / Mysql 이던 시절도 있었듯이요.물론 빙글에서도 이런 실수, 참 많이도 했습니다;그럼 도대체 기술을 선택할때, 어떤 질문을 던져야 하는걸까요?당연한 것들이긴 한데, 제가 생각하는 질문은 이런것들입니다.기술적으로 아무리 우월하고 (일단 그게 “우리 상황에서도” 우월한건지도 체크하는게 먼저지만) 아무리 멋있고 아무리 문서가 이쁘고 폰트가 이쁘고 스크린샷이 예뻐도, 그것들은 우리가 새로운 기술을 배우고 도입해야 하는 결정적인 이유는 당연히 아닌 겁니다.사실 그래서 저는 언제나 세미나 같은데서 저희 경험을 이야기 하고나서, 이런 질문을 기대합니다. “팀원들은 어떻게 설득했나요?” / “그게 비즈니스적으로도 도움이 되나요?” / “생산성이 정말 더 좋아지나요?” 같은거요정말 고맙게도 가끔은 듣긴 하지만, 앞에도 언급했듯이, serverless도 이런 측면에서 고려되기 보다는 기술 그 자체의 물적 특성 (infra 관리가 줄어든다, 돈을 아낀다.. 등등) 으로만 접근되는 경우가 많은것 같습니다. 이 부분을 좀더 파보죠.AWS와 Serverless를 적극적으로 도입하면서 생긴 재밌는 변화중 하나는, 저희가 기존에도 “쉽지만 귀찮다” 는 이유로 유저에게 도움이 되는데도 안하고 있던 일들을 하게 됬다는 겁니다.예를들어, 빙글은 아주 초창기 부터 vin.gl 이라는 도메인을 가지고 있었습니다. 개발자중 한명이 저거 goo.gl 처럼 쇼트너로 쓸수 있겠는데? 라고 했거든요.하지만 실제로 vin.gl을 지원하는 일은 아주 오랫동안 미뤄져 왔습니다. 사실 만드는건 엄청 쉬운 일처럼 보였음에도 불구하고 말이죠. (vin.gl/p/1 -> vingle.net/posts/1 redirect. 너무 간단하잖아요?)가장 큰 이유는 그걸 “유지보수” 해야한다는 점 이였습니다. 만드는건 좋은데, 저게 만약 트래픽이 몰려서 스케일링을 해야하는 상황이 생기면?참고로, 당시에 나와있던 가장 Fully Managed 된 PaaS는 Heroku였고, Heroku는 트래픽이 몰리면 스케일링이 최소 10~15분정도 걸렸습니다. 그 동안은 서버 다운 인거죠즉, 개발자로서 우리가 스스로에게 뭉뚱그려서 “쉽지만 귀찮다"" 라는 기표를 사용하는 메시지가, 사실 그 기의를 좀더 자세히 들여다보면였던거죠. 물론 아무도 이걸 이렇게 명확하게 설명하진 않았습니다. 그냥 직관적으로 “귀찮다 / “손이 많이간다” 고 표현한거죠. 그러니, 사실 “쉽지만” 이라는 말 자체가 기만적 이였던 겁니다. Application Logic만 간단한거지, 유저에게 실제로 가치를 제공할수 있는 단위 (Scaling / Monitoring / continuous deployment 되는 Service) 로서는 여전히 엄청 어려운 일이였던거죠.돌이켜보면 제 스스로도 이랬던 것 같은데, 개발자들은 이런경우 “어렵다” 고 표현하기 보다, “귀찮다"" 고 표현하곤 합니다; 그러니까 시스템을 유지보수하는것도 개발자의 일임에도, 그걸 인정하기는 정말 싫으니 (사실, 만드는게 재밌지 하루종일 그래프보면서 모니터링하고 스케일링 하는게 뭐가 재밌습니까),“내가 재밌어 하는 부분은 쉬운데 내가 싫어하는 부분은 어려워” 를 “쉬운데 귀찮아” 정도로 번역했던것 같네요. 비트겐슈타인 말마따나, “언어는 규칙적이지만 맥락을 포함한 언어의 사용은 언어가 아니라 언어게임이며 이것은 불규칙적 입니다”3년뒤, AWS와 Serverless / Microservice 로 옮기고 난 시점에서 vin.gl shortener를 만들어보자는 이야기가 나왔을때는 답변이 많이 달라져 있었습니다.그리고 결과적으로, vin.gl shortener는 사용자들에게 나갈수 있는 수준으로 완벽하게 만드는데까지, 채 24시간이 걸리지 않았습니다.이유는 간단했는데, 실제로 제가 해야하는 일 자체가 줄었기 때문입니다.vin.gl을 위해, 저는 Cloudfront / Lambda@Edge / S3 / Athena를 사용했습니다.Scaling- 시스템 구성요소 전부 auto scaling 됩니다.Monitoring- CloudFront / Lambda는 Cloudwatch로 error rate등이 제공되고, raw level log는 CloudFront S3 Log를 Athena로 쿼리해서 봅니다.Deployment- 코드를 zip으로 압축하여 s3에 올리기만 하면, 거의 즉시 배포됩니다.이러고 나니, 개발자로서 해야하는 일은 정말 Business Logic을 작성하는게 끝이였습니다.또, 개발자들이 이런 업무를 무의식적으로 “귀찮다"" 라고 표현하는데는 나름의 귀납적이며 경험적인 근거가 있습니다.Netflix 사례인데, 주말일수록 outage가 적게 나고, 아침 9시에 outage가 가장 많이 납니다. 이유는 알만하죠? 퇴근할때 배포하긴 싫으니까, 작업은 다 해놓고 다음날 아침에 와서 Deploy => Service Down 주말엔 일을 안하니까 (이 데이터로 봐서는 Netflix는 “안한"" 다기 보다는 평일보다 “적게"" 하는 듯 하는군요..) 당연히 배포가 적고, 그러니 Outage가 안나고.정리해보면, 내가 새로운 기능을 만들고 배포할때이 두가지가 문제 였던 겁니다.과학적인 근거는 찾아봐야겠지만, 저는 기본적으로 인간의 뇌는 새롭거나 낯선 사고를 피하도록 설계 되어있다고 생각합니다. 그리고 나중에는 내가 사고를 피하는 부분이 있다는 사실 자체를 잊어버리도록 만드는것 같아요.다들 그런게 하나씩은 있잖아요? 이불킥 기억 이라던가, 인정하기 싫은 내 자신의 결점 이라던가. 물리적인 예시로는, 지름길이 있다는걸 아는데도 생각없이 걷다보면 무의식적으로 익숙하지만 먼 길로 간다거나진화적으로 생각해봐도 어떤 사고회로가 계속 작동할수 있다는 사실 자체가, 그 사고를 하는 뇌가 죽지 않고 살아있는데 도움이 된다는 의미가 되니, 이미 잘작동하는 회로일수록 계속해서 살아남아 강화되어 작동하도록 진화하지 않았을까요? 물론 과학적 근거는 없습니다;개발자들을 예로 들자면, “운영"" 업무가 있다는걸 초기에는 무척 낯설게 받아들입니다. 혼자서 Toy project 만 하던 사람이 회사에 처음 입사해서 서비스 개발을 하기 시작하면, 그때는 “아 Bussiness Logic 만드는거 말고 이런 운영 업무도 있구나. 흠.. 내 본업은 개발을 하는거 아닌가” 라고 생각하죠. 그런데, 그게 엄청 익숙해지면 그게 존재한다는 사실 자체를 까먹게 됩니다. “쉬운데 손이 많이간다” / “쉬운데 귀찮다"" 같이요. 그리고 나중에야, 자신의 사고를 Loop를 돌면서 계속해서 파보다보면 문득 제 사고가 의식적으로 피하는 부분이 있었다는걸 깨닫게 됩니다.어떤 기술을 선택하는지가 정말 막중한 작업으로 느껴지는건 바로 이것 때문입니다.저희는 기존에 Opsworks / ECS등을 사용해서 서비스 운영을 했었는데, (그 전에는 Heroku를 사용했고요) 이러다보니 당연히 독립적인 서비스를 만들자 (Microservice)로 가자는 말이 쉽게 안나옵니다. 설정할게 너무 많고, 그 서비스의 특성에 맞게 scaling / monitoring 다 구축해야하고..그러다보니 자연스럽게 잘 돌아가고 있는 Ruby On Rails 에 기능을 붙이는걸로 해결하게 되고, 그러다보니 시스템이 무거워지고, 그러다보니 배포 주기를 길게 가져가게 되고, 그러니 기능 변경사항이 있을때 몰아서 배포하게 되고, 그러니 배포마다 리스크가 커지고, 그러니 배포 자체를 꺼리게 되고..Lambda / Cloudwatch / Kinesis / API Gateway 같은 fully-managed serverless 서비스들이 팀 차원에서 우리에게 준 가장 큰 영향은, 개발을 시작하기도 전에 운영때문에 사고를 차단하는 일을 줄여 줬다는 겁니다.즉 아무도 “이제는 운영업무가 줄었으니까 좀더 다양한 Application을 만들어보자” 라고 하지 않았음에도, 팀원들 각자가 Serverless나 Microservice 같은 환경변화에 적응하면서, 자연스럽게 다양한 Application을 만들고 더 빠르게 배포하고 유연하게 대처하게 된 겁니다. 사고의 흐름 자체가 지속적으로 바뀐거죠.마지막으로, Serverless가 과연 긴 미래가 보장된 기술인지를 짚어보려고 합니다.기술 선택에 있어, “미래” 가 있는 기술 인가는 두말할것 없이 중요합니다. 저는 Apple이 Flash를 지원하지 않았던게 좋은 예시라고 생각하는데요,(참고로 저는 고등학교를 300MB짜리 Flash파일 하나로 만든 web page를 만들어서 특별전형으로 들어갔습니다… 아이폰이 한국에 들어오기 전이라서 다행 이였어요;)통시적으로 생각해보면, Backend Application을 만드는 방법은 몇가지 단계를 거치며 변화해 왔습니다.여기서 읽어낼수 있는 트렌드는 물론, “개발자들이 좀더 편하게 Business Logic 작성에 집중할수 있도록” 일겁니다. 최대한 Scailing, Failover, monitoring 같은 infra 관리와 관련된 것들을 자동화 하는 트렌드인거죠.자연스럽게도, 저는 Serverless가 그런 트렌드의 미래라고 생각합니다. 그게 AWS같은 Cloud Vendor들이 막대한 투자를 하고 있는 이유겠고, 제가 외부에서 Serverless의 현재 한계를 지적하시는 분들을 만날때마다 (AWS Lambda에서 5분이 넘어가는 작업은 어떻게 돌리죠? / 메모리를 많이 쓰는 작업은 어떻게 돌리죠?) 자신있게 “시간이 지나면 다 해결 될 겁니다” 라고 말할수 있는 이유기도 합니다.(실제로, Lambda는 최근 몇년간 최대 실행시간은 5배, 최대 메모리 크기도 5배 늘어났습니다;)물론 모든 Application이 다 무조건 Serverless에서 돌아가게 되는 미래를 상상하는건 아닙니다. 아직도 3G폰을 쓰는 사람이 있듯이 분명 특정 상황에서는 기존의 Container나 bare metal이 더 우위를 가지는 상황이 있겠지만, 대부분의 범용적인 Application들은 Serverless에서 돌아가는게 너무 당연한 미래. 저희 팀이 시스템을 Serverless로 옮기자고 했을때 거기에 베팅한거겠죠.원칙만 지키면 간단한것처럼 쓰긴 했지만, 기술 선택과 도입, 당연히 어렵습니다; 공식 외운다고 선형대수 문제 다 풀수 있는게 아니듯이, 각자의 상황과 문맥에 맞춰 각자의 답을 찾는 노력을 들이는게 중요한 것 같습니다. 그렇게 노력을 들이고 나면 심지어 잘못된 선택을 하더라도, 우리가 뭘 놓쳤었는지 나중에 돌이켜 볼 수는 있잖아요?사실 제가 정말 아쉬운건, 그 “노력” 을 들이는걸 귀찮아 하는 사람들이 많다는 겁니다.인간은 생각보다 환경에 매우 취약합니다. 시간이 지나서 환경에 동화되면, 자연스럽게 그 환경을 비판할수 있는 사고 자체를 피하게 됩니다. 환경에 동화되는 과정은 곧 우리의 뇌가 엄청난 가소성(plasticity) 을 발휘해 우리의 사고 방식 자체를 바꾸는 과정이기 때문입니다.잘못된 기술을 대충 선택하는건, 마치 “앞으로 우리는 미래도 없고 생산성도 프로젝트가 커지면 커질수록 낮아지는 방식으로 일할꺼야” 라고 우리 팀원들의 뇌의 작동방식을 바꾸는것과 마찬가지인거죠. (물론 그렇게 말하려는게 목적이라면 얼마든지 대충해도 됩니다^^)여튼, 기술 선택하고 도입하는거, 팀이나 회사에서는 정말 중요한 문제입니다. 좀더 많은 사람들이 이걸 이해하고 고민하는 분위기가 됐으면 좋겠네요.빙글에서는 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다.— revise,",https://medium.com/vingle-tech-blog/serverless%EC%99%80-%EA%B8%B0%EC%88%A0%EB%8F%84%EC%9E%85-backend-application%EC%9D%98-%EB%AF%B8%EB%9E%98-8f114a8b00d5?source=---------1---------------------,medium,serverless,NULL,2018-07-18
Do you know DynamoDB Stream,"안녕하세요. 빙글에서 백엔드 개발을 맡고 있는 권세중입니다.요즘 많은 분들이 Serverless에 입문함과 동시에 DynamoDB를 접하게 되는데요. DynamoDB의 가장 신기하고 멋진 기능이지만, 잘 다뤄지지 않는 기능이 하나 있으니, 바로 DynamoDB Stream입니다. 아마 많은 분들이 이런 기능이 있다는 것도 모르고 스쳐 지나가지 않았을까 싶네요. 빙글에서는 초기부터 DynamoDB Stream을 통해 많은 기능을 구현해 왔는데요. 오늘은 그 경험을 기반으로, DynamoDB Stream에 대한 전반적인 소개와, 왜 DynamoDB Stream이 멋진 기능인지 이야기 해보려고 합니다.서비스를 만들다 보면, “data record의 변화를 안정적이고, 스케일 가능하게 Capture” 하는게 꼭 필요해집니다. 쉽게는 Like를 찍을때마다 likes_count += 1 부터, 어렵게는 wiki에서 문서의 변경사항을 정리해서 이메일로 보내달라던가요.이런 부분을 우리는 흔히 Application Layer에서 처리하곤 합니다. 아마 이런 코드 익숙하시겠죠.이런 코드를 큰 스케일에서 돌려보신분들은 아시겠지만, 이런 로직은 매우 쉽게 고장납니다. 이 예시만 해도,이런 문제들에 대한 까다롭지만 가장 확실한 대답중 하나가 바로 DynamoDB Stream 이라고 할수 있겠습니다. 빙글에서는 이런류의 business logic에 대해 적극적으로 DynamoDB Stream을 적용했고, 결과적으로 매우 만족하고 있습니다 :)DynamoDB Stream 이란 DynamoDB Table 의 변경사항 (INSERT, DELETE, MODIFY) 에 대한 정보들을 Stream 형태로 처리할 수 있도록 해주는 기능입니다. Mysql같은 RDS에 익숙하신 분들이라면, Replication에 자주 쓰이는 binlog 기능을 생각하시면 됩니다. 그것보다 데이터가 훨씬 이해하기 쉽다는 것만 빼면요.해당 기능을 enable 하게 되면 그 시점부터 해당 Table의 변경사항이 Queue 에 쌓이기 시작합니다. 따로 dequeue해서 처리해주지 않으면, 이벤트가 발생한지 24시간 뒤에 해당 이벤트는 삭제되게 됩니다. random access로 stream 데이터를 지운다거나 변경하는 행위는 불가능하며, 반드시 dequeue를 통해 처리해야 합니다.재밌는 예외상황이 하나 있는데, 아주 짧은 시간 내에 서로를 상쇄하는 event를 발생 시키는 경우입니다. 예를들자면 record를 insert하자마자 바로 다시 delete하는 경우 말입니다. (혹은 같은 record를 아주 짧은 시간내에 두번 modify한다던지요)이 경우, DynamoDB Stream은 자동적으로 서로 상쇄 가능한 이벤트를 삭제해 줍니다. Insert Delete의 경우 두 이벤트를 모두 보내지 않고, 연속으로 Modify 하는경우는 마지막 Modify Event만 오게 되죠.좀 더 자세한 작동원리와 주의사항은 DynamoDB Stream를 참고하시면 되겠습니다.그럼 Queue에 쌓인 data는 어떻게 dequeue할까요? 빙글에서 주로 사용하는 방법은 Lambda Trigger입니다. DynamoDB Stream을 Lambda에 연결하면, 주기적으로 DynamoDB Stream에서 Event를 Dequeue하여 Lambda에 넘겨주는 식이죠.(이때, Lambda가 Error를 내거나 하여 처리가 실패하면, DynamoDB Stream은 Dequeue된 event를 다시 queue에 넣고 다음 Lambda로 재시도 해줍니다).EC2등으로 처리하는 infra부터 직접 구축하는것도 가능합니다. DynamoDB Stream은 내부적으로 Kinesis Stream과 매우 유사한데, (사실 실제로는 똑같다고 봐도 무방합니다.) 그러니 Kinesis Client Library 를 사용하여 직접 Stream에서 dequeue 하는걸 만드시면 되겠죠.오늘은 이 둘 중에서도, 저희가 많이 경험한 Lambda를 사용하여 처리하는 방법에 대해 알아보겠습니다.DynamoDB Stream 이 뭔지 알아봤으니 DynamoDB Stream 을 사용하는 방법에 대해 간략하게 알아보도록 하겠습니다. 예제에서는 AWS Lambda, Node.js, Typescript, Serverless framework 을 사용했습니다.일단 Stream data 를 쌓기 위해서 Stream 기능을 활성화시키는게 우선이겠죠? DynamoDB Table Overview 탭에서 Manage Stream 버튼을 누르시면 아래와 같은 팝업이 나오게 됩니다.쌓을 Stream data 종류를 선택하는 건데요. 각 Type 별로 쌓이는 데이터의 타입이 조금씩 달라지게 됩니다. 문서 참고는 RecordsType 을 참고해주시면 됩니다. 문서에서 Response Syntax 에 정의 된 Object 구조를 보시면 Keys, NewImage, OldImage 가 있는데요. View type 에 따라 해당 attributes 들이 내려오는지 아닌지가 결정됩니다.이때 Keys는 DynamoDB의 “키값"" 만 주는것을 의미합니다. HashKey / SortKey만 Event로 주는거죠. 반대로, Image는 record 전체를 주는것을 의미합니다.차이는 Record Size입니다. DynamoDB는 record 한개에 최대 400kb 까지 넣을수 있는 document storage이기 때문에, image를 전부 보내면 Stream에서 데이터 처리를 너무 많이 해야하는 경우가 생길수 있습니다. 이를 피하기 위해, Key만 주고 image에 접근해야하는 경우에만 직접 DynamoDB에 다시 Query해서 사용할수 있도록 해주는거죠.여기서 적절한 type 을 설정한 다음에 Enable 해주시면 아래와 같이 ARN 주소가 나오게 됩니다. 이 ARN 주소는 Stream data 를 받는 쪽에서 사용하게 됩니다. 저는 NewImage and OldImage 로 예제를 진행해보도록 하겠습니다.AWS Console 에서 설정해주는 건 이게 끝입니다. 이제 코드로 넘어가도록 하겠습니다.Serverless framework 는 간단하게 말하면 Lambda function 생성, 설정, 기타등등의 것들을 CLI 에서 처리하게 해주는 npm library 입니다. 자세한 내용은 serverless 를 참고해주세요.여기서는 serverless.yml의 내용중 DynamoDB Stream Callback 에 붙을 Lambda function 을 생성하는 방법에 대한 설정만 정리해보겠습니다.이런식으로 작성하시면 DynamoDB Stream Callback 용 Lambda function 이 생성되게 됩니다. 실제로 DynamoDB 에 연결됐는지 확인하려면 table trigger 탭을 확인해보시면 됩니다.Lambda function 배포설정까지 끝났으니 function을 구현해보도록 하겠습니다.아래는 Lambda Function에 event parameter로 들어오게 되는 Object입니다.이때, Record는 Event type 에 따라 Image type 이 다르게 내려옵니다. INSERT 일 경우는 NewImage, REMOVE 일 경우엔 OldImage, MODIFY일 경우 OldImage, NewImage 둘 다 내려오게 됩니다. 데이터를 처리하는 간단한 예제를 보겠습니다.이런식으로 구현해봤는데요. 로직은 간단합니다. 판매도서 기록을 남기는 table 이 있다고 가정할 때 데이터가 들어올 경우 해당 도서의 sell count 를 올려주고 데이터가 지워질 경우 해당 도서의 sell count 를 내려주는 코드입니다.Stream 기능은 훨씬 다양한 용도로 사용할 수 있습니다. 예를 들면,이렇게 다양하게 활용 가능한 것들을 여러분들이 상황에 알맞게 사용하시면 더 좋은 효율을 낼 수 있지 않을까 싶습니다.허나 어떤 기술이 그렇듯 DynamoDB Stream 이 항상 올바른 선택은 아닙니다. 예를 들어보죠. user, user_token, user_stat 테이블이 있습니다. user 테이블에서 record 가 삭제되면 나머지 두 테이블의 해당 user 에 대한 record 도 동시에 삭제가 되어야 겠죠. 이때, DynamoDB Stream 은 적합한 툴이 아닙니다. Lambda 가 Stream records 를 1초 마다 polling 해줘서 실시간으로 돌아가는 것처럼 보이지만 Lambda 내부에서 records 가 처리되는 시간까지 감안하면 1초 이상이 걸릴 수도 있습니다. 이런 경우 API 가조금 느려지는 것을 감수하더라도 API Level에서 처리해주는게 맞습니다. 물론, 빙글에서도 이런 실수를 꽤 했었습니다;; 기술 선택은 언제나 어렵죠.이 글을 통해, 여러분들은 사용하기 전에 발생할 수 있는 문제와 얻게 되는 이득을 잘 고려해서 올바른 기술 선택을 하실 수 있으면 좋겠네요.DynamoDB Stream 동작 방식이 Kinesis stream 과 유사하기 때문에 DynamoDB Stream 을 좀더 다양하게 사 용해보시고 싶으신 분들은 Kinesis stream 문서도 같이 찾아보시는 것을 추천드립니다. 이번 아티클이 여러분들께 도움이 되었으면 좋겠네요. 그럼 다음 아티클로 찾아 뵙겠습니다. 감사합니다.빙글에는 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다.",https://medium.com/vingle-tech-blog/do-you-know-dynamodb-stream-14b284bf38d5?source=---------2---------------------,medium,stream,NULL,2018-07-16
Texture Best Practice,"— Texture —안녕하세요!현재 Vingle에서 iOS 개발자로 재직중인 하현수라고 합니다.Github나 그 외 개발자 커뮤니티에선 Geektree0101 닉네임으로 활동하고 있습니다.지난 2018년 2월에 Vingle Tech-Talk에서 Texture를 주제로 발표를 했었습니다. 해당발표자료는 아래의 링크에서 확인하실 수 있습니다.발표를 계기로 “Texture를 어떻게 활용해야 잘 썼다고 할 수 있을까?” 를 고민하던 찰나에 Texture Best Practice를 포스팅하게 되었습니다. 아래와 같이 총 5부작으로 구성되어있습니다.Texture자체가 Learning Curve가 높은 편은 아니지만 1 ~ 5편까지 난이도를 단계적으로 포스팅 하였습니다.현재 빙글에선NewsFeed, User List, Profile, Talk(Chatting), Q&A, Interest Show, Card, Notification List 등 서비스 약 70% 이상을 Texture로 제작하였고 새로 개발되는 모든 UI 구성요소들이 Texture를 이용하여 개발되고 있습니다.포스팅을 하면서, Texture를 사용하는 기간동안 있었던 여러 문제점들과 좋았던 점 그리고 각종 노하우들을 총정리해보는 시간이 였던것 같습니다.Texture는 과거에 AsyncDisplayKit로 알려져있으며, Fork 약 2,600 Star 약15,000 을 보유하고 있고 Showcase는 약 30개의 플랫폼들이 등록되어 있으며, 많은 회사들이 사용하고 있습니다. 나름 튼튼한 UI 라이브러리 입니다.또한, 슬렉 커뮤니티도 있으며, 현재 약 1670명의 회원들과 Texture 라이브러리를 관리 및 개발하시는 핀터레스트 개발자분들이 활동하고 계시며, Texture를 사용하시는 다른 플랫폼의 메인 iOS개발자 분들과 직접적으로 커뮤니케이션을 하실 수가 있습니다. 필자 또한 자주 활동하는 편입니다.Texture Best Practice라고 주제를 잡았는데, 다소 부담스럽기는 합니다. 하지만, 개발방법이라는게 사람의 사고가 담겨져있고 기술이라는게 항상 변하는것이기에 이 포스팅이 100% 완벽하거나 10년 후까지 보장 할 수는 없습니다.단지, UI개발을 xib나 storyboard를 쓰지않고도 이렇게 접근하여 이런 식으로 개발할 수도 있구나 라는 것을 얻어 가시면 충분하다고 생각합니다. 이러한점 구독자 여러분들께 양해부탁드립니다.마지막으로, 읽어주시는 모든 구독자 여러분들께 진심으로 감사의 말씀을 드리며, 이러한 환경과 도움을 주신 Vingle 개발자 여러분들과 Vingle iOS 팀여러분들께 감사의 말씀을 드립니다.감사합니다.[채용 관련 안내]빙글의 커뮤니티와 컨텐츠에 elegant한 UI를 더해서 사용자들에게 더 좋은 경험과 빙글에게 성공을 가져다 줄 iOS 엔지니어를 찾습니다.빙글 iOS 엔지니어인 당신은 기획 팀, 디자인 팀과 함께 사용자들에게 즐거운 놀라움을 줄 수 있는 앱을 만들어갑니다. 컨텐츠는 물론이고 커머스와 광고까지 장인 정신으로 아름다운 UI와 코드를 동시에 구현합니다.",https://medium.com/vingle-tech-blog/texture-best-practice-1f0ba1a9d903?source=---------3---------------------,medium,,NULL,2018-05-19
CloudFront Log를 Athena로 쿼리하기,"CloudFront에는 S3에 law-level access-log를 쌓아주는 멋진 기능이 있습니다. 사실 아직까지도 상당수의 CDN Vendor들은 이 기능을 제대로 지원하고 있지 않고, 흥미롭게도(사실 어느정도는 예측 가능하게도) 심지어 지원하는 vendor들중 일부는 AWS S3를 스토리지로 이용하고, 유저들에게 S3 bucket을 주는 식으로 구현합니다. Akamai 가 대표적이죠.S3야 워낙 멋진 제품이니까, 저장은 그렇다 치고, 그럼 이 데이터를 어떻게 쿼리하지? 가 문제가 되는데요. 다양한 방법이 있습니다. Local로 모든 csv.gz 파일을 다운받아서 grep을 해도 되고, EMR에 Spark나 Hive를 올려서 쿼리할수도 있겠고요. (물론 저희정도 되면 이렇게 하기엔 데이터 량이 택도 없이 많습니다..)물론, 가장 좋은 방법은 이꼴저꼴 보지않고 관리할것도 없고, 쓰는만큼만 돈 내면 되는 Athena를 사용하는 것입니다.만약 이미 Cloudfront와 Athena에 대한 지식이 있으신 분들이라면, 사실 이부분은 상당히 쉽습니다. 아마존 에서도 공식 가이드가 이미 나와있고요.이런 기존 방법들의 유일한 문제 하나는, Athena의 Table Partition을 사용하지 못한다는 점입니다.Athena에 익숙하지 않은 분들을 위해 설명하자면, Athena에서 Table partition은 해당 테이블의 데이터가 어디 (s3 location)에 있는지를 partition parameter들과 함께 정의하는 일종의 data indicator입니다.이게 필요한 이유는, Athena의 경우 partition을 지정함으로서, 해당 테이블의 가능한 모든 데이터를 접근하는게 아니라, 특정 데이터만 접근할수 있게 할수 있기 때문입니다. 또한, Athena는 데이터를 “읽은"" 만큼 만 과금됩니다. 즉 실제 쿼리에서 필요하지도 않은 데이터를 스캔하면 스캔 할수록 요금이 비싸지는 구조죠.예를들어,“지난 7일동안 500 에러가 난 url들을 에러 횟수와 함께 보고싶다”고 할때, 쿼리를 어떻게 짜더라도 partition을 사용하지 않으면 athena는 “모든 데이터를” 스캔합니다. 하지만 만약 테이블의 partition을 연/월/일/시 로 지정하고 대략 아래와 같이 partition을 미리 만든다면,아래와 같이 쿼리를 날릴수 있고,이러면 Athena (Presto)는 쿼리를 실행하면서 Partition에 관련된 조건문을 먼저 읽어서 이 쿼리를 실행할때 읽어야 하는 파티션을 좁히는 작업을 자동으로 먼저 하게 됩니다. 결과적으로, 이 경우 지정된 파티션 (2018/02/01/00) 이후의 데이터만 읽게 되죠.음 그럼 뭔가 이상합니다. 파티션은 이렇게 그냥 하면 무조건 좋은건데, 아마존 공식 가이드에는 이게 왜 쏙 빠져 있는걸까요.이유는 CloudFront의 Log file 모양 때문입니다.음. 좀 이상하긴한데.. 그래도 어차피 S3니까, 대충 prefix 맞춰서이렇게 만들면 되지 않을까? 네 안됩니다. 정확히는 에러는 전혀 안나지만, 해당 파티션에 데이터가 읽히지 않습니다.이유는 간단합니다. Presto (Athena는 facebook의 Presto engine을 fully-managed 형태로 만든 것입니다) 는 기본적으로 HDFS같은 “파일시스템"" 을 위해 설계되었습니다. 아마존에서 이걸 S3를 대상으로 쓸수 있도록 만들긴 했지만, 애초에 Presto에 있던 제약, 즉 “Partition은 반드시 “폴더""”를 대상으로 지정해야 한다” 는 해결하지 못한거죠.심지어 S3에서는 “폴더"" 라는게 존재 하지 않습니다. “/” 로 표시되는 것들은 “prefix”에 불과합니다. 그걸 S3 Console에서는 보기 좋으라고 폴더 모양으로 묶어서 보여주긴 합니다만.. 그건 UI에 불과하고요. 여하튼, 다른 방법이 없습니다. 아마존에 문의도 해봤습니다만, S3에서 폴더가 아닌건 알지만 현재 구현상으로는 무조건 “/”로 끝나는 prefix 안에 들어가야 한답니다그래서 이걸 Athena / CloudFront Logs와 마찬가지로, serverless형태로 infra 관리를 전혀 안해도 되는 형태로 구성해봅시다.빙글에서는 새로만든 거의 모든 infra를 CloudFormation (serverless.yml 안의) 로 관리하고 있기 때문에, 대부분의 configuration을 cloudformation을 통해 진행했습니다하지만 물론 상황에 따라, CloudFormation을 안쓰고 계시다면 위 작업을 AWS Console이나 API를 이용해 직접 해주셔야 합니다.마지막으로, Lambda Code는 상당히 간단합니다.이로써 알아서 파티션까지 되는 CloudFront Log용 Athena Table이 생겼습니다. 구성 요소들은 다 알아서 scaling / manage 되는것들이니 딱히 관리할 건 없습니다. 아마도 유일한 모니터링이 필요한 요소는 Lambda이겠습니다. Lambda는 어쨌든 Error가 날수 있으니까요왼쪽은 Data Scanned가 1GB, 오른쪽은 Data Scanned가 1.5GB로 쿼리에 따라 정확히 필요한 부분만 읽었음을 알수 있습니다.빙글에서는 이 CloudFront Log가 정말 엄청난 사이즈로 쌓이기 때문에, 이런식의 “필요한 날짜 범위의 데이터만” 스캔할수 있는게 많은 도움이 되고 있습니다.빙글에는 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다.",https://medium.com/vingle-tech-blog/cloudfront-log%EB%A5%BC-athena%EB%A1%9C-%EC%BF%BC%EB%A6%AC%ED%95%98%EA%B8%B0-d38a5ea8c3e?source=---------4---------------------,medium,,NULL,2018-02-28
Improvement feed performance with Texture(AsyncDisplayKit),"…지난 2017년 8월, 약 일년이 넘는 긴 시간동안 진행 해왔던 리뉴얼 작업이 끝이 나고 4.0.0 버젼을 업데이트 하고 난 이후 몇 달간 유저로 부터 받은 피드백입니다.유저들이 경험한 발열과 피드 퍼포먼스 저하 현상의 원인은 다음과 같습니다.위와 같은 문제 해결과 동시에 앞으로 해야할 일도 많고 시간은 부족한 상황에서도 저희는 큰 결심을 하고 Texture를 도입하게 되었습니다.사실 2017년 상반기에 피드를 리뉴얼하고 내부적으로 테스트를 했을 때 당시, 피드 퍼포먼스에 대한 문제점을 인지는 하고 있었고 동시에 Texture 도입에 대해서 생각은 했었으나 도입을 하더라도 팀원들이 새롭게 학습을 해야한다는 점과 시간제한 때문에 도입을 하지 않았습니다.지금 생각하면 왜 망설였는지, 왜 진작에 Texture를 도입하지 않았는지 후회는 되지만, 유저들의 쓴소리가 빙글 iOS 팀의 앱개발에 있어서 중요한 터닝포인트가 되었기 때문에 오히려 매우 감사 할 따름입니다.제가 이번 포스팅에서 소개 하고자 하는 건 피드 성능 개선에 있어서 Texture 라이브러리가 우리 제품에 어떠한 기여를 했으며, 개선 과정과 도입하면서 습득한 노하우를 공유하고자 합니다.Texture에 대해서 간단히 소개 먼저 하자면Texture는 UIKit위에 구축된 iOS프레임워크 라이브러리이며 아무리 복잡한 UI라도 유저의 반응성에 대한 응답이 매우 뛰어며, 내부적으로 재설계된 Runloop와 효율적인 Layer 사전 조합 기능 덕분에 Thread의 안정성과 UI 랜더링에 있어서 매우 뛰어난 퍼포먼스를 보여줍니다.더 자세한건 여기 클릭 해 주세요.서론에서 말했듯이, 우리가 해당 라이브러리를 사용하는데 망설였던 이유에 대해서 다시 짚어 보자면 우선 새롭게 학습을 해야한다는 점과 시간 부족이라는 것이다. 이건 읽는 프로젝트에 새로운 라이브러리를 도입하려고 하는 독자 여러분들도 공감하실꺼라 생각합니다.하지만, 직접 사용해보고 해당 라이브러리의 내부 코드를 분석하면서 생각이 달라졌으며, 다음 도입과정에 대해서 이에 대한 경험을 더 자세하게 설명하겠습니다.따라서 우리는 해당라이브러리를 도입하게 되었고, 도입전에 앞서 우리는 3가지 사항에 대해서 점검을 했습니다.[1] UI에 대한 코드리뷰 및 생산성우선 (1) UI에 대한 코드리뷰 및 생산성에 대해서 설명하자면, 우리가 어떤 UI를 개발하고 PR을 올렸을 때, 해당 내역에 대해서 코드리뷰를 진행 한다. 하지만 읽고 있는 독자 여러분들도 아시다 싶이 Xib로 된 코드를 리뷰한다는 것은 사실 쉽지 않았습니다.위에 사진과 같이 솔직히 xib script는 수 많은 컴포넌트와 Constraints 들을 봤을 때 가독성이 그렇게 뛰어나지 않을 뿐더러, 굳이 해당 xib를 리뷰하기 위해선 PR의 해당 branch를 pull받아서 xcode를 통해서 보는 방법 밖에 없었습니다.심지어 열어본 xib도 시간대비 리뷰하기가 비효율적이라고 생각합니다. (각종 attributes에 대해서도 체크해야 하니깐 말입니다.)반대로 Programatic UI(코드로만 설계된 UI)의 경우는 어떨까? iOS 분야에 좀 계신 분들은 아실껍니다. SnapKit(objc: Masonry)라는 Constraint를 쉽게 다루는 라이브러리 있습니다.그나마 코드로 Constraint 잡는 것과 가독성에 대한 문제들에 대한 부담은 덜여주지만, 저는 이것 또한 효율적이고 생산성에 좋은 가에 대해서 다시한번 생각을 해보았습니다.간단해 보여도 Constraint는 Constraint일 뿐 xib상에서 잡은 Constraint나 코드로 잡은 Constraint는 UI가 복잡해지면 복잡해질수록 오히려 가독성도 떨어지고 버그를 만들기 쉬우며, 자칫 잘못하면 퍼포먼스 저하로 이어지기가 쉬웠습니다.하지만 Texutre의 경우 CSS FlexBox를 이용한 Layout 설계와 이를 응용한 LayoutSpec 기능을 제공함으로써, 가독성이 뛰어나고 디자이너의 요구 사항에 대해서 완벽히 맞출 수 있는 UI개발방법을 제공합니다.LayoutSpec에 대한 자세한 설명은 여기를 클릭해주세요.위의 스펙에 따라서 UI를 설계함으로써 우리는 다음과 같은 가독성이 뛰어나고 정확한 UI를 개발할 수가 있었습니다.위의 사진과 같이 LayoutSpec을 이용하여 글처럼 읽기 좋은 UI를 설계 할 수가 있습니다. 위의 코드를 분석하자면따라서, 개발자입장에서는 가독성이나 UI개발하는데 있어서 시간이 절약되며, 디자이너입장에선 요구사항을 바꾸더라도 숫자나 특정 값만 바꾸면 되니 유지보수 측면에도 효율적이라고 할 수 있습니다.이렇게 LayoutSpec과 FlexBox덕분에 Constraints지옥으로 부터 탈출 할 수가 있었습니다.flexBox연습하기 좋은 사이트예제: https://github.com/GeekTree0101/TextureAVAssetVideoFeed[2] RxSwift(RxCocoa) 사용가능성우리는 2017년 초부터 Objective-c에서 Swift로 넘어감과 동시에 RxSwift를 도입하기 시작했다. 짧지않은 긴시간 동안 RxSwift를 사용해왔기 때문에 이에 대한 편의성을 버릴 수는 없었습니다. 하지만 Texture의 경우 Node들에 대해서 RxCocoa가 제공해주는 인터페이스를 직접적으로 제공해주지는 않지만 View Property를 통해서 사용이 가능합니다.주의할 점은 view property를 접근 하는데 있어서 MainThread상에서 접근하는지 체크하는 것이 중요합니다.왜냐하면 Texture내부코드를 보면 view를 접근할 때 Thread에 대해서 매우 엄격하기 때문입니다.우리는 이러한 실수로 인한 크래시를 미연에 방지하고자 ASControlNode에 대해서 RxCocoa를 이용한 Wrapper를 만들었습니다.왜 ASControlNode냐 하면 ASControlNode의 Subclass들에서 알 수가 있습니다.ASTextNode, ASImageNode, ASVideoNode, ASMapNode 들이 ASControlNode의 subclass 이기 때문에 우리는 아래와 같이 터치이벤트를 subscribe할 수가 있었습니다.예제: https://github.com/GeekTree0101/RxASControlEvent굳이 저걸 쓸 필요가 없다면, Texture에서 기본으로 제공해주는 didLoad override method 또는 onDidLoad Block 내에서 처리해주는 것도 방법입니다.[3] 기존 UI 재사용성(Xib 또는 Programatic UI)위에 사진은 현재 피드에 필요한 UI들이다. 현재 빨간 사각형 영역에 있는 UI는 모두 Texture로 개발된 상태이지만, 일부 UI에 대해서는 재사용이 필요한 경우가 있었습니다. 대표적으로페이스북 Audience에서 제공해주는 전용 UI 컴포넌트 들이다. 이들은 UIKit로 만든 순수한 UI인데, 이것을 어떻게 Texture로 만든 피드상에 보여줄 것인지가 가장 큰 문제였습니다.저는 이것을 해결함과 동시에 Texture의 우수성에 다시 한번 더 놀라게 되는데 그 것은 바로 ASDisplayNode에서 viewBlock을 제공한다는 점 입니다.위의 API Spec을 용용하면 다음 코드와 같이 쓸 수가 있습니다.뿐만 아니라 UIKit에 새로 추가된 UI나 현재 Texture에서 제공하지 않는 UI들도 viewBlock을 이용해서 적용할 수 있다는 것입니다.주의 사항은 퍼포먼스 보장은 확실치 않지만, 기존 UI보다 나쁜영향을 주지는 않았습니다.그 외 여러 좋은 정보들을 공유드리자 하면, 우선 가장 강력하게 추천드리고 싶은 것 Texture Slack를 말하고 싶습니다.다른 라이브러리에 비해 활동하는 사람들은 많지는 않으나 대부분 활동하시는 분들이 서로 공유를 많이 하고 도와주는 편이라. 마치 한국에 있는 두레마을의 정과 같은것을 느끼실 수가 있습니다.현재 저도 활동하는 편이고 도움 많이 받고 있습니다.그리고 Texture기반으로 한 RxSwift MVVM Best practices 를 설계해서 아래 사진과 같이 간단한 앱을 만들었으며,Instrument로 time profiling 해본결과는 위의 사진과 같이 우수한 퍼포먼스를 볼 수가 있었습니다.예제: https://github.com/GeekTree0101/RxMVVM-Texture마지막으로, 2018년 2월 7일 Vingle Tech-talk 4차에서 해당 내용에 대해서 발표를 하였고 아래의 링크를 통해서 보다 더 구체적인 내용을 시청 하실 수 있습니다.준비중입니다. ㅠㅠ그리고",https://medium.com/vingle-tech-blog/improvement-feed-performance-with-texture-asyncdisplaykit-2ef2ee11f06e?source=---------5---------------------,medium,,NULL,2018-02-25
Serverless CloudFront Log Querying with Athena and Table Partition,"CloudFront has this nice raw-level access log. Not like other CDNs, this gives you truly interactive and customizable insights about your CDN. kind of obviously, most of otherCDN vendors that supports this kind of logging also uses S3 to store their raw-level access log also. Such as AkamaiOne problem is how can you query this massive data. Well, we can always just download all those csv.gz files to your local machine and query it. Or use EMR to run Spark or Hive or Presto to query this. Or, Even better, you can use AWS Athena to query the logs in S3 in truly serverless manner.For anyone who is already familiar with Athena and CloudFront, this is fairly easy. There are several good articles about this including this or even AWS official guideOnly one problem is that we can’t use Athena table partition for this type of logs.For those whom not familiar with Athena table partition, Partition is literally indicating where the data is stored, with some flag parameters that can be used for identifying given partition.The reason why you need this is simple. To let Athena to only scan the data that you need to scan.“500 error count grouped by url, happened within last 7 days”However you write this query, Athena will “read” all the data if you don’t use the partition. and you have to pay for that reads.But if there is partition such as year / month / day / hour and you’ve already registered those partitions, such asThen you can simply do,And Presto (Athena) will read conditions for partition from where first, and will only access the data in given partitions only. As a result, This will only cost you for sum of size of accessed partitions.Then you might ask, why AWS official guide doesn’t even mention about partition if it’s that crucial?Well, it’s because CloudFront creates log file like thisYou might think, “Well i can just set partition location with prefix!”Unfortunately, this doesn’t work. Even though in S3 “folder” (represented as “/”) is just “prefix” not actual “folder”,Presto (which is what the Athena is under the hood) doesn’t support those kinds of prefix or regex or wildcard based location since it’s originally built for HDFS(Hadoop). I’ve even got confirmed from AWS support about this, and there isn’t any clear timeline about when this will be supported by Athena.But like i said earlier, using partition is really crucial especially if you’re dealing with massive traffic. so I’ve come up with serverless solution for this.I’m already managing CloudFront with serverless framework, So whole thing written based on serverless and CloudFormation.Depends on your situation, like if you already made cloudfront, you might need to some of this on AWS console directly.And Lambda code is fairly straight forwardThat’s it! now you have this perfectly partitioned Athena table that you can query, and you don’t have to manage anything what so ever. (Well you might want to monitor lambda errors, but that’s it)You can see left query scanned 1.49GB, right scanned 1006MB depends on partition query! nice.",https://medium.com/vingle-tech-blog/serverless-cloudfront-log-querying-with-athena-and-table-partition-c18b6e6f9eb4?source=---------6---------------------,medium,"serverless,partition",NULL,2018-02-23
Search Service in Serverless Architecture,"Search Service 는 제가 빙글에 입사하고 나서 처음으로 맡은 독립적인 서비스입니다. 그래서 유난히 애착이 가는 서비스인데요. 현재는 제가 만들었을 때보다 많은 것이 추가되었습니다. 오늘 포스팅에서는 제가 만들었을 당시의 Search Service 에 대해 적어보도록 하겠습니다.Search Service 를 설명하려면 빙글이 어떤 서비스인지를 미리 설명 드려야할 것 같은데요. 간단하게 말하면 빙글은 사람이 주인 기존의 SNS 와 다르게 관심사가 주가 되는 SNS 서비스 입니다. 자기가 관심 있는 것의 Contents 만 받아 볼 수 있는 것이죠. 아래 사진이 관심사 페이지를 들어가면 나오는 화면입니다. 현재는 카드밖에 없지만 나중엔 여러가지 추가되겠죠? ㅎㅎ빙글에 대해서 알아보았으니 이제 빙글의 주가 되는 Interest Service 가 어떻게 돌아가는지 알아보겠습니다. 저희 서비스는 8월에 진행한 리뉴얼의 영향으로 인해 기존의 Monolithic Architecture 나 Microservice Architecture 의 구조랑 조금 다른 구조를 하고 있는데요.(자세한 내용은 https://medium.com/vingle-tech-blog/serverless-microservice-architecture%EC%97%90%EC%84%9C%EC%9D%98-inter-communication-caching-80a43c979121 참고) 아래처럼 client 에서는 기존 Monolithic Service 인 Rails 서버로 요청을 보내고 Rails 서버에서는 해당 요청을 SDK 를 사용해서 lambda 로 보내는 역할을 해줍니다. Rails 는 일종의 Gateway 역할입니다. 적어도 Interest Service 에서는 말이죠.위에서 Interest Service 의 구조에 대해 설명드렸는데요. 아마 AWS 에 대해 잘 모르시는 사람들은 lambda 가 뭔지, dynamoDB 가 뭔지 잘 모르실거라고 생각됩니다. 부가설명이 본 포스팅 목적보다 더 길어지는 느낌이 없잖아 있지만 .. search service 에 대해 적기 전에 간단하게 search service 에서 사용하는 AWS service 들에 대해 알아보도록 하겠습니다.Lambda 입니다. 장점은 기존의 EC2와 달리 서버가 항상 띄워져 있지 않습니다. 그리고 사용한 만큼만 과금을 합니다. 또 사용량이 많아지면 자동으로 Auto Scaling 을 해줍니다. 그래서 서버가 터질 일이 거의 없죠. 허나 하나의 이벤트가 처리되는게 5분내로 처리되어야 합니다. 그 이상 걸리게 되면 Timeout이 나면서 Lambda가 죽게됩니다. 빙글의 모든 Microservice 들은 이 Lambda 를 이용하여 구성되어 있습니다.2. DynamoDBAWS 의 NoSQL 서비스인 DynamoDB 입니다. 가장 큰 특징이라고 한다면 AutoScaling 을 제공을 합니다. DynamoDB 는 capacity 라는 것으로 DB 의 스펙을 관리하는데 DB의 read, write 량이 많아지거나 줄어들게 되면 알아서 조정을 해줍니다. 그리고 Streaming 기능을 지원해주는데요. DynamoDB 의 Record 가 생성, 변경, 삭제됨에 따라서 해당 이벤트를 Lambda 에서 처리할 수 있도록 해줍니다. 오늘 설명드릴 Search Service 에서도 이 Streaming 기능을 사용합니다.그러면 실제로 Search service 는 어떻게 구현할까요? RDB 를 사용했더라면 LIKE 문을 이용해서 구현할 수 있을겁니다. 물론 RDB 의 LIKE 같은걸 DynamoDB 에서 제공을 해주긴 합니다. 허나 전체 Record 를 scan 하기 때문에 resource 낭비도 심하고 performance 또한 좋지 못합니다. 이런 점을 해결하기 위해 도입한게 CloudSearch 라는 AWS Service 입니다.3. CloudSearch마찬가지로 사용량에 따라 자동으로 AutoScaling 해줍니다. 그리고 Serverless service 라서 ElasticSearch 와는 다르게 따로 관리가 필요 없습니다. 또한 Lambda 와 마찬가지로 사용(upload, read, storage)한 양에 따라서만 비용을 지불하면 됩니다.자 이제 드디어 빙글의 관심사 검색 서비스에 대해 알아보도록 하겠습니다.예전 구조를 보니까 뭔가 되게 초라해 보이네요. 구조는 이게 전부입니다. DynamoDB Streaming Callback 을 Lambda 로 연결하고 Lambda 에서는 해당 이벤트를 적절하게 처리해서 CloudSearch 에 적용시키는 역할을 합니다. 좀 더 자세하게 설명드리면 사용자가 새로운 관심사를 만들었습니다. 그럼 Rails 에서 앞쪽의 Interest Service Lambda 로 새로운 관심사를 생성하라는 요청을 보내겠죠? 그럼 Lambda 에서는 DynamoDB 에 Record 를 생성합니다. DynamoDB 에서는 생성되었다는 Event 를 뒤쪽의 Callback Lambda 로 넘기고 Callback Lambda 에서는 해당 Event 를 알아서 처리하는 거죠. Event 의 종류는 Streaming 에서 자동으로 분별해주지 않기 때문에 Lambda 에서 알아서 분별해야 합니다. 아래와 같이 말이죠.애초에 포스팅에서 설명드리고 싶었던 내용들은 전부 설명을 드렸는데요. 그 외에 부가적으로 몇 가지 주의사항(?)을 적어보겠습니다.당연한 소리일수도 있겠지만 Record 가 변경되고 해당 변경에 대해 뭔가를 처리해야된다고 해서 항상 Callback 을 사용하는 것이 맞지는 않습니다. 그 이유는 Callback 은 실시간으로 처리되지 않기 때문입니다. Streaming 은 Queue 에 처리되어야 하는 Event 정보를 담는데요. 해당 Queue 는 일정 시간이 지나거나 지정해둔 Queue 의 크기만큼 Event 가 쌓이지 않을 경우 Callback 으로 넘어가지 않습니다. 그래서 실시간성이 중요시 되는 기능이나 서비스에서는 DynamoDB Callback 사용은 피하는 것이 좋습니다.2. Model Share In Different Service하나의 Model 을 여러 서비스에서 사용하는 경우인데요. 저희 서비스로 예를 들면 User Service 에서 Interest Service 의 Model 을 필요로 하는 경우를 예로 들 수 있겠네요.예전에는 전자와 같이 그냥 Model 에 바로 접근했습니다. 그래서 생긴 문제가 하나의 Model을 여러 곳에서 접근하게 돼서 Model 관리가 복잡해졌습니다. 이를 해결하기 위해 구조를 뒤와 같이 변경했습니다. Model 에 접근하는 Service 는 하나고 다른 Service 에서 해당 Model 을 사용할 일이 있을 경우에는 접근 가능한 Service 에 필요한 요청을 수행하긴 위한 기능을 구현하고 다른 Service 에서는 SDK 를 이용하여 그 기능을 요청합니다. 조금 번거롭긴 하지만 이렇게 함으로써 Model 을 여러 곳에서 관리하면서 생기는 Dependency 를 줄일 수 있었습니다.3. Data MigrationSearch Service 는 보통 기존의 Service 위에 붙이는 경우가 많습니다. 따라서 기존의 데이터들을 Migration 해줘야 하는데요. Migration 하는 과정에서 Service 에 문제가 생긴 경우가 있었습니다. 기존 데이터를 Migration 하기 위해서 전체 Record 를 scan 한 다음 데이터를 집어넣었는데요. Auto Scaling 을 믿고 따로 capacity 관리를 안한 상태에서 진행했습니다. 허나 scan 과정에서 Read Capacity 가 순간적으로 많은 양이 들어온 탓인지 장애가 나게 되었습니다. 물론 Auto Scaling 으로 알아서 Scaling 이 되면서 문제를 해결할 수 있었습니다만 Real time 으로 Scaling 이 되지 않는 만큼 순간적으로 들어오는 이벤트를 예측할 수 있다면 capacity 를 강제적으로 올려서 장애를 막는 것이 좋아보입니다.이상으로 이번 포스팅을 모두 마치도록 하겠습니다~ 다음엔 더 좋은 내용으로 찾아 뵙겠습니다.빙글에는 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다.",https://medium.com/vingle-tech-blog/search-service-in-serverless-architecture-3cf74732037e?source=---------7---------------------,medium,"serverless,architecture",NULL,2018-01-28
Serverless microservice architecture에서의 inter-communication caching,"빙글은 8월에 진행된 서비스 리뉴얼과 함께, 기존의 Ruby on rails 기반의 monolithic 앱에서 구현되어 있던 서비스 로직을 상당부분 Serverless 기반의 microservice architecture로 옮겼다.그걸 하면서도 정말 많은걸 배웠고, 그 과정에서 필요했던 다양한 도구들 (Lambda용 http api framework라던가, DynamoDB ORM이라던가..) 오픈소스로 만들었다. 그부분도 정말 자랑스럽지만, 정작 그 과정에서는 너무 정신없이 바빠서 블로깅은 엄두도 못냈고, 매일 매일 새로운 기능을 추가하면서 우리가 큰 그림에서 좋은 architecture에서 벗어나고 있는건 아닌지 걱정하기 바빴다;그래서 약간은 여유가 생긴 요즘에야, 앞으로 하려고 하는것들에 대해 좀더 쓰고 정리해보려고 한다.우선 현재 빙글에서 구축한 시스템 구조는 대략 다음과 같다.이것보다 훨씬 서비스가 많고, 서비스 각각의 구성이 다른것들이 있으며, 서로간의 요청이 훨씬 다양하다는 것만 빼면.여기서 왼쪽의 큰 부분이 기존의 Rails Application이고, 우리가 서비스 리뉴얼 과정에서 했던 것은 여기서 business logic을 분리하여 AWS Lambda / API Gateway 기반의 service들로 옮기는 것이였다. 이 service들은 모두 Typescript로 작성되었고, Ruby On Rails Application과 각각의 Service들은 HTTP API 로 통신했다.우선 이부분은 매우 성공적으로 끝났고, 만족스러웠다. 이부분만 별도의 포스팅을 해야될 정도인데, 단언컨데 이렇게 바꾸고 나서 우리 backend team의 생산성이 정말 두배는 올라간 것 같다.다만 오늘 집중해서 다루려는 부분은 <서비스간의 통신에서의 cache> 이다. 구체적인 예시를 생각해보자.Card의 Interest tag목록은 어차피 유저가 Card내용 수정을 통해 수정하지 않는 이상 바뀌지 않기 때문에 지금도 대부분의 경우 Interest Service가 하는 일은 memcached에서 데이터를 읽어 return하는게 끝이다.그렇다면, Feed Service에서 애초에 Interest Service가 사용하는 memcached를 직접 hit하게 하면 훨씬 빠르지 않나?이 부분은 Netflix의 EVCache와 서비스 구조에서 많은 영감을 얻었다.좀더 상세한 설명은 이 영상과 이 영상을 추천한다.즉, Service A가 Service B에 C Operation을 요청한다고 했을때,Vingle의 모든 microservice들은 Swagger API 에 따라 자신이 제공하는 API들을 정리한 XML을 가지고 있다. 그래서 aws-sdk가 aws가 제공하는 다양한 service들의 API 를 모아놓듯이, 우리도 Swagger를 이용해 API Client를 자동으로 generate해서 아래와 같이 사용하고 있다.그래서, 처음엔 정말SDK를 사용하는 모든 Client들이 하나, 혹은 몇개의 memcached 서버를 공유해야한다는게 좀 꺼름직했지만 이렇게 하면 되겠다 싶었는데..라는 문제가 남아있었다. 예를들어 카드의 관심사 목록은 거의 바뀌지 않으니 cache TTL을 5분으로 하든 3주로 하던 1년으로 하던 상관 없지만, 일단 사용자가 카드를 수정하여 “바뀌면”, 반드시 바로 cache에 반영되어야 했다. 안그러면 사용자가 카드를 수정했는데 안바뀐 것 처럼 보일테니까또 하나 문제는,사실 우리가 SDK를 수동으로 Service가 업데이트 되는것에 맞춰서 업데이트 하고 있었다면, 그냥 cache key를 수동으로 맞추자! 하고 넘어 갔을지도 모르겠다.근데 일단 우리는 이미 SDK를 자동으로 swagger doc을 읽어서 업데이트 하게 하고 있었고, 나는 이게 “그냥 개발자가 알아서 맞추자!” 라고 하면 너무너무 실수하기 쉬운 환경이라고 생각했다. 하다못해 같은 const string을 사용하자고 하는것도 서로다른 application에 부탁하면 가끔씩 띄어쓰기나 대소문자 실수하는 마당에..;그래서 몇가지 해결책을 생각했다.SDK에서도 해당 operation 의 url이야 당연히 아니까, 그걸 key로 쓰면 되지 않나? 아이디어는 괜찮아 보였는데, 이건 service 쪽에서 문제가 있었다. 우리는 Cascading routing을 사용하기 때문에, 하나의 Route안에서 자신의 주소를 정확히 알수가 없다; (자신이 어떤 부모 아래에 있냐에 따라 상위 주소가 달라지니까) 물론 실제로 API가 실행되는 단계에서는 알수 있지만, 그건 “다른 API의 cache를 날려야할때"" 가 문제였다.대략 아래와 같이 작성해야 하는데,딱봐도 L70이 상당히 애매해보인다.. 여기서 L70이 L58의 Route의 Cache를 날리라는 명령이라는게 너무 비직관적이고, L53이 바뀌거나 해서 애초에 url이 상위에서 바뀌면?라는 CS가 들어올게 뻔하다..operationId란 Swagger에서 모든 route에 대해 가지고 있기를 요구하는 uniqe한 string이다. Swagger spec에 있으니 SDK에서도 알수 있고, Service에서도 반드시 알아야 한다. 최초의 card의 interests tag 목록 예제로 보자면,결과적으로, 많은 경우 10~20ms정도 걸리는 HTTP 요청이 아니라 Memcached의 ~3ms 정도의 응답을 사용할수 있게 됬다.Microservice로 넘어오면서, “Service 각각이 어떤 정보를 알고 있나 / 알아야 하나” 가 매우 중요한 질문이 됬다. 사실 우리가 서비스마다 담당하는 팀이 다르고 사람이 다르다면, 오히려 별 문제가 아니였을것이다. 그 경우엔 어차피 서로 내부 정보는 모르는거고, 그럼 뭘 공유해야 하는지만 정리하면 끝나니까. 그런데 서비스 숫자는 많고 사람은 4명이다 보니.. 서비스간에 통신을 설계할때는 특히라고 사고의 흐름을 스위칭 하는게 필요하다.현재 proof of concept는 끝났고, 전체 적용을 준비중이다.빙글에는 이런 문제를 함께 풀어갈 사람을 언제나 기다립니다.",https://medium.com/vingle-tech-blog/serverless-microservice-architecture%EC%97%90%EC%84%9C%EC%9D%98-inter-communication-caching-80a43c979121?source=---------8---------------------,medium,"serverless,microservice,architecture",NULL,2017-10-22
코드 밖에서 디버깅하기,"보통 개발자들이 가장 두려워하는 상황은 문제도, 에러도 없이 예상하지 못 한 버그나 결과가 나올 때라고 생각합니다.이번에 겪은 일도 위와 같았습니다. 분명히 문제는 계속해서 발생하는데, 어디를 찾아봐도 에러나 문제가 될만한 부분을 찾기가 어려웠습니다.이를 디버깅하면서얻은 교훈은 코드 밖에서 발생하는 버그도 존재한다는 것이었습니다. (엄밀히 말하면 잘못된 코드가 맞긴 맞지만.)재밌는 일화를 본다 생각하고 읽어보면 흥미로울 것이라 생각합니다.버그는 HTTP 요청을 보내는 곳에서 발생했다. 사용자의 post에 tag를 기록하는 API 요청이었는데, 분명 한 번만 보내도록 코딩되었음에도 간헐적으로 중복된 tag가 서버에 기록되는 경우가 발생했다.이는 View단에서 문제뿐 아니라, 여러 기능들을 많이 마비시켰다. 더 의아했던 건, 서버 측에서 분명히 tagId에 대해 unique 옵션이 활성화되어 있었음에도 이런 일이 발생했다는 점이었다.이에 대해 다음과 같이 디버깅 과정을 거쳤는데, 별 성과가 없었다.이 문제는 2주가 넘게 이어졌다. 이로 인해 발생한 부가적인 기능 문제들 때문에 계속해서 CS 문의가 들어왔고, Admin들도 고통받고 있었다.원인을 알 수 없다는 이유로 계속해서 이 문제를 방치해 둘 수가 없어서, 하루 날을 잡고 원인을 파악하려고 애썼다. 이 프로세스는 다음과 같이 진행되었다.그리고 문제라고 의심되는 점을 2를 수행하면서 발견할 수 있었다. 계속해서 아무런 문제도 발생하지 않았는데, 우연히 네트워크 연결이 안좋아지면서 겪은 경험에 ‘혹시…?’하고 의심되는 점이 생겼다.우리 서비스의 post 작성 flow는로 이루어진다.위 사진은 Tag를 선택하는 Modal의 모습이다. 의심가는 부분은 ‘post를 서버에 저장’하는 곳이었다. 이 비동기처리 부분이 완료되지 않았음에도, 유저가 발행을 클릭하는 순간 저 Modal이 닫혀버렸기 때문이다. 이 경우, 유저의 경험은 다음과 같이 된다.여기서 2~3초간 대기하는 부분이 의심갔다. 이 때, 유저는 에러가 난 건지, 아닌지, 로딩 중인지, 아닌지, 현재 상태에 대한 어떤 UI/UX적인 힌트도 받을 수 없었다. 따라서 유저 입장에서는 무언가 잘못되었음을 느끼고 우측 상단에 게시를 다시 누르고, 또 다시 Tag를 선택해서 발행을 한 번 더 누를 수도 있다는 생각이 들었다. (애초에 Save 중일 때 닫혀버린 Modal의 버튼만 Loading spinner로 바뀌고 우측 상단에 게시 부분은 spinner로 바뀌지 않는 것도 문제라고 생각한다.)의심이 든 부분을 당장 고쳐보기로 했다. Modal close하는 로직을 post save가 완료된 시점에 하도록 바꾸면, 유저 입장에서는 spinner를 통해 로딩 중이라는 현재 스테이트에 대한 힌트도 얻을 수 있고, 다시 우측 상단에 게시를 클릭하는 액션도 막을 수 있었기 때문이다.고쳐진 Flow는 다음과 같다.이와 같이 Flow가 바뀌자 우선 post가 저장되는 state에 대한 UX가 훨씬 좋아졌다. 또한 애초에 발생했던 중복 Tag에 대한 문제도 더 이상 발생하지 않게 되었다.이상입니다.긴 글 읽어주셔서 감사합니다.",https://medium.com/vingle-tech-blog/%EC%BD%94%EB%93%9C-%EB%B0%96%EC%97%90%EC%84%9C-%EB%94%94%EB%B2%84%EA%B9%85%ED%95%98%EA%B8%B0-c05f034193e?source=---------9---------------------,medium,,NULL,2017-03-30
