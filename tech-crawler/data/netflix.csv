title,content,url,cnt,source,keyword,image,createdAt
Delivering Meaning with Previews on Web,"As the Netflix catalog of films and series continues to grow, it becomes more challenging to present members with enough information to decide what to watch. How can a member tell if a movie is both a horror and a comedy? The synopsis and artwork help provide some context, but how can we leverage video previews (trailers) to help members find something great to watch?Our goal was to create a rich and enjoyable video preview experience to give our users a deeper understanding of the content we have to offer. This deeper understanding about the characters, mood, and other elements of a title is what we consider as meaning.Giving members meaning via video previews brings new technical and experiential challenges. It would need to have fast playback, smooth transitions, and minimal friction. The tasks ahead of us were:The previous version of the home page rendered all rows of titles as its highest priority. This included fetching data from the server, creating all DOM nodes, and loading images. Our goal was to enable fast vertical scrolling through 30+ rows of titles.For the new experience, we needed our page to load faster, minimize memory overhead, and allow for smooth playback. We knew these performance optimizations would come with a tradeoff against existing member behavior. To understand these tradeoffs, we began by measuring the existing home page load.When the home page loads, we first render the billboard image and the top three rows on the server. Once this page has been delivered to the client, we make a call for the rest of the homepage, render the rest of the rows, and load all the images.Here’s what the page load looks like from a data perspective.That’s a lot of DOM nodes! In addition to the CPU load of generating those nodes, the sheer number of images we load saturates the member’s bandwidth.We conducted experiments to determine when to load images and how many to load. We found the best performance by simply rendering only the first few rows of DOM and lazy loading the rest as the member scrolled. This resulted in a decreased load time for members who don’t scroll as far, with a tradeoff of slightly increased rendering time for those who do scroll. The overall result was faster start times for our video previews and full-screen playback.To get an optimal video preview playback experience, weOur concern with introducing video previews was the impact it would have on network throughput. We did not want to saturate a member’s connection to the point where network contention would slow down non-video requests. We tackled this problem by matching the video preview stream resolution to be roughly the same size as the video preview canvas, thus lowering the amount of video data requested.To improve video preview start times, we developed a new business logic layer on the client that specifically focused on the creation and interaction of video player instances. This allowed us to decouple video player creation and state management from being tied only to the UI render cycle. With this system, we can efficiently create a video player instance during the UI animation when a focused title expands. While the 400ms animation is taking place, we are already pre-buffering the preview video. When the expanded title canvas renders, it is passed the previously created player instance.After we optimized the start time of video playback, we tackled audio state management. Members expect that clicking any mute button should globally toggle sound. Previously, we had separate components and places in our application state that represented whether audio should be muted. We converged onto a single component and backing representation for the mute state. We also added a serialization routine inside of the business logic layer to capture mute state in a cookie, so when members refreshed the page, their preference would be preserved.Our main goal with the user experience was to allow our members to focus on each movie/series, understand what it is about, and avoid any friction from the interaction.Our first point of friction was due to a large amount of information already shown on the expanded title canvas. We know this information is valuable to many members but would distract from the new video preview experience.After trying different variations, we arrived at an expanded title canvas that slowly fades out this extra information over a 10 second period, gradually allowing the member to fully focus on the video preview. This fade out begins as soon as the video starts playing and reverts to full opacity on any interaction. This worked well for members who wanted to be immersed in the video, while still giving control to members who wanted to read the summary or add the title to their list.Another key part of reducing friction was to transfer the playing video from the expanded title canvas to the title details canvas, as shown below. The UI canvases were distant relatives in regards to the UI structure making it difficult to pass information between them.We solved this by utilizing unique session identifiers for each canvas type that could be used to query for video playback state. This allowed us to get the current video position from the expanded title canvas when creating the title details canvas and pass in the previous timestamp as the initial position for the new video player instance.Code ExampleAlthough we feel this is a great step forward in delivering more meaning to our members, we still see opportunities to make the website more effortless for content discovery and playback. In addition, there are more potential wins for members with low bandwidth and CPU restrictions by reducing payload and streamlining components.We’ve just scratched the surface of meaning we can bring to our members. Stay tuned for more great things to come.",https://medium.com/netflix-techblog/delivering-meaning-with-previews-on-web-3cedc0341b9e?source=collection_home---4------0---------------------,0,netflix,,NULL,2018-11-12
Scaling Time Series Data Storage — Part II,"In January 2016 Netflix expanded worldwide, opening service to 130 additional countries and supporting 20 total languages. Later in 2016 the TV experience evolved to include video previews during the browsing experience. More members, more languages, and more video playbacks stretched the times series data storage architecture from part 1 close to its breaking point. In part 2 here, we will explore the limitations of that architecture and describe how we’re re-architecting for this next phase in our evolution.Part 1’s architecture treated all viewing data the same, regardless of type (full title plays vs video previews) or age (how long ago a title was viewed). The ratio of previews to full views was growing rapidly as that feature rolled out to more devices. By the end of 2016 we were seeing 30% growth in one quarter for that data store; video preview roll-outs were being delayed because of their potential impact to this data store. The naive solution would be to scale the underlying viewing data Cassandra (C*) cluster to accommodate that growth, but it was already the biggest cluster in use and nearing cluster size limits that few C* users have gone past successfully. Something had to be done, and that too soon.We challenged ourselves to rethink our approach and design one that would scale for at least 5x growth. We had patterns that we could reuse from part 1’s architecture, but by themselves those weren’t sufficient. New patterns and techniques were needed.We started by analyzing our data set’s access patterns. What emerged was three distinct categories of data:For each category, we discovered another pattern — the majority of access was to recent data. As the age of the data increased, the level of detail needed decreased. Combining these insights with conversations with our data consumers, we negotiated which data was needed at what detail and for how long.For the fastest growing data sets, video previews and language information, our partners needed only recent data. Very short duration views of video previews were being filtered out by our partners as they weren’t a positive or negative signal of member’s intent for the content. Additionally, we found most members choose the same subs/dubs languages for the majority of the titles that they watched. Storing the same language preference with each viewing record resulted in a lot of data duplication.Another limiting factor we looked into was how our viewing data service’s client library satisfied a caller’s particular need for specific data from a specific time duration. Callers could retrieve viewing data by specifying:For the majority of use cases, these filters were applied on the client side after fetching the complete data from the back-end service. As you might imagine, this led to a lot of unnecessary data transfer. Additionally, for larger viewing data sets the performance degraded rapidly, leading to huge variations in the 99th percentile read latencies.Our goal was to design a solution that would scale to 5x growth, with reasonable cost efficiencies and improved as well as more predictable latencies. Informed by the analysis and understanding of the problems discussed above, we undertook this significant redesign. Here are our design guidelines:Data CategoryData AgePerformancePreviously, we had all the data combined together into one cluster, with a client library that filtered the data based on type/age/level of detail. We inverted that approach and now have clusters sharded by type/age/level of detail. This decouples each data set’s different growth rates from one another, simplifies the client, and improves the read latencies.For the fastest growing data sets, video previews and language information, we were able to align with our partners on only keeping recent data. We do not store very short duration preview plays since they are not a good signal of member’s interest in the content. Also, we now store the initial language preference and then store only the deltas for subsequent plays. For vast majority of members, this means storing only a single record for language preference resulting in huge storage saving. We also have a lower TTL for preview plays and for language preference data thereby expiring it more aggressively than data for full title plays.Where needed, we apply the live and compressed technique from part I, where a configurable number of recent records are stored in uncompressed form and the rest of the records are stored in compressed form in a separate table. For clusters storing older data, we store the data entirely in compressed form, trading off lower storage costs for higher compute costs at the time of access.Finally, instead of storing all the details for historical full title plays, we store summarized view with fewer columns in a separate table. This summary view is also compressed to further optimize for storage costs.Overall, our new architecture looks like this:As shown above, Viewing data storage is sharded by type — there are separate clusters for full title plays, preview title plays and language preferences. Within full title plays, storage is sharded by age. There are separate clusters for recent viewing data (last few days), past viewing data (few days to few years) and historical viewing data. Finally, there is only a summary view rather than detailed records for historical viewing data.Data writes go to into the most recent clusters. Filters are applied before entry, like not storing very short video previews plays or comparing the subs/dubs played to the previous preferences, and only storing when there is a change from previous behavior.Requests for the most recent data go directly to the most recent clusters. When more data is requested, parallel reads enable efficient retrieval.Last few days of viewing data: For the large majority of use cases that need few days of full title plays, information is read only from the “Recent” cluster. Parallel reads to LIVE and COMPRESSED tables in the cluster are performed. Continuing on the pattern of Live and Compressed data sets that is detailed in part 1 of this blog post series, during reads from LIVE if the number of records is beyond a configurable threshold, then the records are rolled up, compressed and written to COMPRESSED table as a new version with the same row key.Additionally, if language preference information is needed, then a parallel read to the “Language Preference” cluster is made. Similarly if preview plays information is needed then parallel reads are made to the LIVE and COMPRESSED tables in the “Preview Titles” cluster. Similar to full title viewing data, if number of records in the LIVE table exceed a configurable threshold then the records are rolled up, compressed and written to COMPRESSED table as a new version with the same row key.Last few months of full title plays are enabled via parallel reads to the “Recent” and “Past” clusters.Summarized viewing data is returned via parallel reads to the “Recent”, “Past” and “Historical” clusters. The data is then stitched together to get the complete summarized view. To reduce storage size and cost, the summarized view in “Historical” cluster does not contain updates from the last few years of member viewing and hence needs to be augmented by summarizing viewing data from the “Recent” and “Past” clusters.For full title plays, movement of records between the different age clusters happens asynchronously. On reading viewing data for a member from the “Recent” cluster, if it is determined that there are records older than configured number of days, then a task is queued to move relevant records for that member from “Recent” to “Past” cluster. On task execution, the relevant records are combined with the existing records from COMPRESSED table in the “Past” cluster. The combined recordset is then compressed and stored in the COMPRESSED table with a new version. Once the new version write is successful, the previous version record is deleted.If the size of the compressed new version recordset is greater than a configurable threshold then the recordset is chunked and the multiple chunks are written in parallel. These background transfers of records from one cluster to other are batched so that they are not triggered on every read. All of this is similar to the data movement in the Live to Compressed storage approach that is detailed in part 1.Similar movement of records to “Historical” cluster is accomplished while reading from “Past” cluster. The relevant records are re-processed with the existing summary records to create new summary records. They are then compressed and written to the COMPRESSED table in the “Historical” cluster with a new version. Once the new version is written successfully, the previous version record is deleted.Like in the previous architecture, LIVE and COMPRESSED records are stored in different tables and are tuned differently to achieve better performance. Since LIVE tables have frequent updates and small number of viewing records, compactions are run frequently and gc_grace_seconds is small to reduce number of SSTables and data size. Read repair and full column family repair are run frequently to improve data consistency. Since updates to COMPRESSED tables are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.Since we do a lot of parallel reads of large data chunks from Cassandra, there is a huge benefit to having a caching layer. The EVCache caching layer architecture is also changed to mimic the backend storage architecture and is illustrated in the following diagram. All of the caches have close to 99% hit rate and are very effective in minimizing the number of read requests to the Cassandra layer.One difference between the caching and storage architecture is that the “Summary” cache cluster stores the compressed summary of the entire viewing data for full title plays. With approximately 99% cache hit rate only a small fraction of total requests goes to the Cassandra layer where parallel reads to 3 tables and stitching together of records is needed to create a summary across the entire viewing data.The team is more than halfway through these changes. Use cases taking advantage of sharding by data type have already been migrated. So while we don’t have complete results to share, here are the preliminary results and lessons learned:Viewing data storage architecture has come a long way over the last few years. We evolved to using a pattern of live and compressed data with parallel reads for viewing data storage and have re-used that pattern for other time-series data storage needs within the team. Recently, we sharded our storage clusters to satisfy the unique needs of different use cases and have used the live and compressed data pattern for some of the clusters. We extended the live and compressed data movement pattern to move data between the age-sharded clusters.Designing these extensible building blocks scales our storage tier in a simple and efficient way. While we redesigned for 5x growth of today’s use cases, we know Netflix’s product experience continues to change and improve. We’re keeping our eyes open for shifts that might require further evolution.If similar problems excite you, we are always on the lookout for talented engineers to join our team and help us solve the next set of challenges around scaling data storage and data processing.",https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-ii-d67939655586?source=collection_home---4------1---------------------,0,netflix,"css,react,frontend,javascript,python",NULL,2018-11-05
Netflix Media Database — the Media Timeline Data Model,"In the previous post in this series, we described some important Netflix business needs as well as traits of the media data system — called “Netflix Media DataBase” (NMDB) that is used to address them. The curious reader might have noticed that a majority of these characteristics relate to properties of the data managed by NMDB. Specifically, structured data that is modeled around the notion of a media timeline, with additional spatial properties. This blog post details the structure of the media timeline data model used by NMDB called a “Media Document”.The Media Document model is intended to be a flexible framework that can be used to represent static as well as dynamic (varying with time and space) metadata for various media modalities. For example, we would like to be able to represent (1) per-frame color and luminosity information for a video file with 29.97 fps NTSC frame rate, (2) subtitle styling and layout information in a timed text file that is using units of the “media timebase”, as well as (3) spatial attributes of time varying 3D models generated by VFX artists, all with full precision in temporal as well as spatial dimensions.The Media Document model is designed to be versatile, to allow representing a wide number of document types, ranging from documents resulting from the analysis of an encoded video stream and containing VMAF scores, to documents providing information about events happening simultaneously within multiple timed text streams, to documents providing structured information about a series of DPX images forming a movie clip. To satisfy all these use cases, the Media Document is built around a few core principles that are detailed in the following.We use the Media Document model to represent timed metadata for our media assets. Hence, we designed it primarily around the notion of timed events. Timed events can be used to represent both intrinsically “periodic” as well as “event-based” timelines. Figure 1 visualizes a periodic sequence of successive video frames. The event of interest, in this case, is a shot change event that occurs after the third frame.A Media Document instance snippet corresponding to Figure 1 could be as follows.Timed events are similar to TTML (Timed Text Markup Language) subtitle events but with the main difference that in the case of the Media Document, these events are not meant to be displayed to end-users. Rather, they represent metadata about the media asset, that corresponds to the specified time interval. In our model, we made the choice that all events in a given Media Document instance correspond to a single timeline, matching the timeline of the media asset (we would like to point out that the Media Document timing model is equivalent to the timing model associated with the par element from the SMIL specification.). One goal behind this choice is to facilitate timed queries, within a document instance (“get all events happening between 56 seconds and 80 seconds in the movie”) but also across document instances (“is there any active subtitle in all languages of a movie between 132 seconds and 149 seconds in the movie?”).In our model, each event occupies a time interval on the timeline. We do not make any assumption on how events are related. For example, in ISO Base Media File Format (BMFF) files, samples may not overlap and are contiguous within a track. In the Media Document model however, events may overlap. There may also be gaps in the timeline, i.e., intervals without events. Figure 2 shows an event-based subtitle timeline where some of the intervals do not have events. A Media Document instance snippet corresponding to Figure 2 could be as follows.Just like the timing model, a Media Document is associated with a single spatial coordinate space, and events may be further qualified by spatial attributes, providing details on where the event occurs in this coordinate space. This enables us to offer spatial queries (“get all events appearing in this region of the media asset across the entire movie”) or spatio-temporal queries (“get all events happening during given time interval(s) in given region(s)”).Figure 3 shows the visualization of a video timeline composed of two temporal events that are separated by a shot change. Within each temporal event, a different spatial region (corresponding to a human face and illustrated with a colored rectangle) forms the region of interest. A complete Media Document instance corresponding to this media timeline is depicted at the end of this section.Inspired by industry leading media container formats, such as the SMPTE Interoperable Master Format (IMF) or ISO BMFF, the Media Document model groups events that have similar properties. Two nested levels of grouping are available: tracks and components. Our model is flexible: two events spanning a common interval of a timeline may be placed in the same component, or in two different components of the same track, or even in components of different tracks.The semantics of components and tracks may be defined freely by the author of the Media Document instance. In a typical instantiation for multiplexed media assets, a Media Document instance would contain a track element per media modality in the media asset, i.e., a Media Document instance for an audio-video asset would have two tracks. Such a scenario is illustrated in Figure 4 for an asset containing the audio, video as well as the text modality.As was alluded to above, a Media Document instance snippet corresponding to Figure 4 could be as follows.Alternatively, for a multi-channel audio asset, the Media Document instance would have one track, but within the track, a separate component element would provide the metadata and events for each channel, as shown in Figure 5.A Media Document snippet corresponding to Figure 5 could be as follows.The overall nested structure of a Media Document is shown in Figure 6. Each level requires authors to specify information that is common (mandatory) for all Media Document instantiations (an id at each level, temporal and spatial resolution units at the component level, time interval information at the event level, spatial information at the region level). Further, each level allows authors to provide metadata that is specific to each Media Document type at each level (e.g., VMAF score for each frame at the event level or average at the document level, or loudness information for audio at a component or track level).While a Media Document instance could be represented in any of the popular serialization formats such as JSON, Google Protocol Buffers, or XML, we use JSON as the preferred format. This partly stems from the common use of JSON as the payload format between different web-based systems. More importantly, many of the popular distributed document indexing databases such as Elasticsearch and MongoDB work with JSON documents. Choosing JSON as our serialization format opens up the possibility to use any of these scalable document databases to index Media Document instances. Note that indexing of the event level time interval information as well as the region level spatial information provides spatio-temporal query-ability out of the box.The following example shows a complete Media Document instance that represents face detection metadata through the timeline of the video sequence shown in Figure 3. The video sequence in question is a high-definition video sequence (1920x1080 spatial resolution) with a frame rate of 23.976 frames per second. It comprises two distinct temporal events. Each of these events contains a single spatial region of interest that corresponds to the bounding box rectangle for a detected human face.The previous section presented the basic principles of the Media Document model. Media Document objects are widely used within various Netflix media processing workflows. Following is a typical life cycle:In order to sustain this life cycle at the Netflix scale, we realized that it was necessary to adopt a “schema-on-write” approach. In this approach, every Media Document type is associated with a schema. All Media Document instances of a specific type that are submitted to NMDB first undergo validation against the schema defined for that type. A Media Document instance is rejected if it does not comply with validation rules. More specifically, we decided to express our validations rules using a subset of the JSON Schema syntax. Hence a producer of Media Document instances is first asked to provide the JSON schema that describes the structure of the associated Media Document type. This approach leads to several benefits:Furthermore, because developers require flexibility in their Media Document definitions and often need to evolve their domain-specific metadata or more generally their domain-specific Media document types over time, we allow updating the Media Document schema. However, to retain the benefits indicated above, we have limited updates on schema to only addition of optional fields. This ensures both forward and backward compatibility between Media Document instances and Media Document readers while maintaining the stability of Media Document instance indexing and querying. In a nutshell, this design choice has helped ease adoption of the NMDB system while enabling us to operate NMDB at scale. Finally, when a non-compatible change of schema is necessary, one could create a new Media Document type.In the next blog post, we will dig deeper into our implementation of the NMDB system. We will discuss our design choices for realizing the service availability and service scale requirements that arise from the Netflix business needs.— by Subbu Venkatrav, Arsen Kostenko, Shinjan Tiwary, Sreeram Chakrovorthy, Cyril Concolato, Rohit Puri and Yi Guo",https://medium.com/netflix-techblog/netflix-mediadatabase-media-timeline-data-model-4e657e6ffe93?source=collection_home---4------0---------------------,0,netflix,"python,sass,animation,frontend",NULL,2018-10-31
The New Netflix Stethoscope Native App,"We are happy to announce the next big release in user focused security, the Stethoscope native app. The new native app includes basic device health recommendations with inline clickable instructions on how to update settings. It can also communicate with a web app (such as a Single Sign On provider) in order to make device health suggestions at pivotal moments.Last year we introduced Stethoscope along with the concept of user focused security. TL;DR: Basic device hygiene is a fundamental security practice. People want to securely configure their devices, but they may not know what the best practices are, or how to comply with them. Empowering users to see the state of their devices and how to get them into an ideal state improves the overall security posture of an organization.The first release of Stethoscope gave users visibility into data that is sourced from various Device Management platforms: JAMF, LANDESK, and Google MDM. One issue with this approach was that the web view users only saw updates as frequently as the device management software reports data, which is typically once a day. Additionally, as new patches or recommendations came out users had to go back to Stethoscope web or rely on out of band communications like emails to see if they had fallen into a bad state. Finally, it required an organization to implement one or more of these platforms and manage their users’ devices.We’ve chosen to address these issues by providing our users with a native app that gives real time feedback to users.The Stethoscope native app can make the same basic recommendations about security practices that Stethoscope web can:Practices are evaluated real time in the app to give immediate feedback to users. If a user makes a settings change, they can rescan to see updated results. In addition to health scans being run on demand by users, the app provides a local server for outside processes to run device health scans. This provides a mechanism for applications to grant access conditionally based on the state of a connecting device.Unique scenarios may require unique device health recommendations, so the app can be configured to be appropriate to the situation. The default policy and instructions can be configured at build time. Additionally when other apps are running scans they can specify which security practices they care about, and what state they should be in.The Stethoscope app was built with not just device health in mind, but also with security in mind. The app does not run as root, and has no elevated privileges. The app does not change settings for users automatically. This respects the user’s ownership of their device settings, but also has the benefit of not adding risk of settings being changed maliciously via the app. Device information can be sensitive, so we limited who is able to run scans. This is enforced via a CORS policy, which is configured at build time. The local server only listens on loopback so that device scans cannot be run outside of the local machine.Currently Mac OS and Windows 10 devices are supported.Operating systems frequently release security patches, and people change device settings, so we think it’s important to remind people at a regular cadence about the state their devices are in.While users are ultimately in charge of their devices’ settings, we think it’s appropriate to nudge people when they access sensitive data.It was a natural fit to integrate health checks into our single sign on provider. When someone accesses a sensitive application the SSO provider makes a request to the app for data. If the check passes, they are logged in automatically. Otherwise they see a reminder to update their settings.Checks at authentication time may not be the best fit for everyone. We’ve spoken to some groups who do not have the ability to modify their SSO providers, and some who have no centralized SSO or desire to collect user’s device information. An option we’ve seen used by one early Stethoscope adopter is to simply provide a signed build along with a recommendation for people to run it, and no ties to outside systems.The Stethoscope native app is built with open source software. The native app is built using Electron. Most of the device information is collected via osquery. The app bundles in osqueryd, and when the app starts it launches osqueryd as a child process. When the app runs a scan it runs osquery queries via a Thrift socket, and creates the user interface with React. In order for web applications to query device information, the app starts an Express server that listens only on loopback. GraphQL is used to create a standard schema and query language for applications to get device data. Example device queries can be found here.For SSO we use a customized version of PingFederate, which allows us to add steps into the authentication process. We inserted a step that queries the native app, and displays appropriate messaging based on the results of the scan. This part of the process is fairly specific to Netflix, so we have chosen not to open source it for now.The world and the workforce are shifting to mobile. It’s important to ensure mobile devices are securely configured in addition to traditional corporate devices. We are working on a native mobile app, written with React Native. It makes basic recommendations about security practices to users for their mobile devices the same way the Electron app makes recommendations for Mac OS, and Windows devices.We hope that other organizations find the Stethoscope app to be a useful tool, and we welcome contributions and opportunities for collaboration. We have open sourced the app, which can be found in the Netflix Skunkworks organization: https://github.com/Netflix-Skunkworks/stethoscope-app. While we’re excited for the opportunity to collaborate, we chose to open source the app in Skunkworks for now, as we’re early in this journey, and are still making frequent changes.Our Identity and Access Engineering and Enterprise Security teams are hiring managers at our Los Gatos office: Identity Engineering Manager and Corporate Information Security Manager. If you’d like to help us work on Stethoscope and related tools, please apply!By: Nicole GrinsteadThe Stethoscope app was written by and is maintained by Rob McVey, Jesse Kriss, Andrew White and Nicole Grinstead",https://medium.com/netflix-techblog/the-new-netflix-stethoscope-native-app-f4e1d38aafcd?source=collection_home---4------1---------------------,0,netflix,,NULL,2018-10-30
VMAF: The Journey Continues,"by Zhi Li, Christos Bampis, Julie Novak, Anne Aaron, Kyle Swanson, Anush Moorthy and Jan De CockHow will Netflix members rate the quality of this video — poor, average or excellent? Which video clip looks better — encoded with Codec A or Codec B?For this episode, at 1000 kbps, is it better to encode with HD resolution, with some blockiness, or will SD look better?These were example questions we asked ourselves as we worked towards delivering the best quality of experience for Netflix members. A few years ago, we realized that we were unable to answer these questions effectively by simply relying on “golden eyes.” Expert viewing was not scalable across content, encoding recipes, and the overall output of our encoding pipeline. It was possible to deploy existing video quality metrics, such as PSNR and SSIM at scale, but they fell short of accurately capturing human perception. Thus, we embarked on a journey to develop an automated way to answer the question, “How will a Netflix member rate the quality of this encode?” This was the birth of VMAF.Video Multi-method Assessment Fusion, or VMAF for short, is a video quality metric that combines human vision modeling with machine learning. The project started as research collaboration between our team and Prof. C.-C. Jay Kuo from University of Southern California. His research group had previously worked on perceptual metrics for images, and together, we worked on extending the ideas to video. Over time, we have collaborated with other research partners such as Prof. Alan Bovik from the University of Texas at Austin and Prof. Patrick Le Callet from Université de Nantes with the goal of improving VMAF accuracy related to human subjective perception, and broaden its scope to cover more use cases. In June 2016, we open-sourced VMAF on Github, and also published the first VMAF techblog. In this new techblog, we want to share our journey.Outside of Netflix, the video community is finding VMAF a valuable tool for quality assessment. Because of industry adoption, the project is benefitting from broader contribution from researchers, video-related companies and the open-source community.We are pleased to see that other research groups have cross-verified the perceptual accuracy of VMAF. Rassool (RealNetworks) reports high correlation between VMAF and DMOS scores for 4K content. Barman et al. (Kingston University) tested several quality assessment metrics on gaming content and concluded that VMAF was the best in predicting the subjective scores. Lee et al. (Yonsei University) applied quality metrics for multi-resolution adaptive streaming and showed that VMAF and EPSNR demonstrated the highest correlation with perceptual quality. VMAF and VQM were the best performing quality metrics in the study of Gutiérrez et al. (Université de Nantes) where MOS scores were generated for HD and UHD content. We have also read studies where it is claimed that VMAF does not perform as expected. We invite industry and researchers to evaluate the latest VMAF models and encourage them to share with us counterexamples and corner cases that can potentially improve the next VMAF version. We also give best practices of using VMAF at a later section to address some of the concerns.VMAF can be used as an optimization criterion for better encoding decisions, and we have seen reports of other companies applying VMAF for this purpose.Traditionally, codec comparisons share the same methodology: PSNR values are calculated for a number of video sequences, each encoded at predefined resolutions and fixed quantization settings according to a set of test conditions. Subsequently, rate-quality curves are constructed, and average differences between those curves (BD-rate) are calculated. Such settings work well for small differences in codecs, or for evaluating tools within the same codec. For our use case — video streaming — the use of PSNR is ill-suited, since it correlates poorly with perceptual quality.VMAF fills the gap, and can capture larger differences between codecs, as well as scaling artifacts, in a way that’s better correlated with perceptual quality. It enables us to compare codecs in the regions which are truly relevant, i.e. on the convex hull of attainable rate-quality points. Comparing the convex hulls between different codecs and/or different configurations gives a comparison of the Pareto front of both options, in the rate-quality regions that practically matter. Some of our team’s recent work on codec comparisons was published in a tech blog on shot-based encodes, and in academic papers at the Picture Coding Symposium 2018 and SPIE Applications of Digital Image Processing XLI.VMAF is used throughout our production pipeline, not only to measure the outcome of our encoding process, but also to guide our encodes towards the best possible quality. An important example of how VMAF is used within encoding is in our Dynamic Optimizer, where encoding decisions for each individual shot are guided by bitrate and quality measurements for each encoder option. VMAF scores are essential in this optimization process to get accurate quality measurements, and to select the final resolution/bitrate points on the convex hull.Researchers in different business areas — TV UI teams and streaming client teams, for example — are constantly innovating to improve streaming quality. With VMAF, we have a tool that allows us to run system-wide A/B tests and quantify the impact on members’ video quality. For example, a researcher changes the adaptive streaming algorithm or deploys new encodes, runs an experiment, and compares VMAF between the old and new algorithms or encodes. This metric is well-suited for assessing quality in experiments because of its consistency across content and accuracy in reflecting human perception of quality. For example, a VMAF score of 85 will mean “good” quality for all titles, as opposed to using bitrate, where the same bitrate may indicate different quality between titles.When we first released VMAF on Github back in June 2016, it had its core feature extraction library written in C and the control code in Python, with the main goal of supporting algorithm experimentation and fast prototyping. Upon user’s request, we soon added a stand-alone C++ executable, which can be deployed more easily in the production environment. In December 2016, we added AVX optimization to VMAF’s convolution function, which is the most computationally heavy operation in VMAF. This led to around 3x speedup of VMAF’s execution time. Most recently in June 2018, we added frame-level multithreading, a long-due feature (special shout out to DonTequila). We also introduced the feature of frame skipping, allowing VMAF to be computed on every one of N frames. This is the first time that VMAF can be computed in real time, even in 4K, albeit with a slight accuracy loss.With help from the FFmpeg community, we packaged VMAF into a C library called libvmaf. The library offers an interface to incorporate VMAF measurements into your C/C++ code. VMAF is now included as a filter in FFmpeg. The FFmpeg libvmaf filter is now a convenient paved path for running VMAF on compressed video bitstreams as input.Since we open-sourced VMAF, we have been continuously improving its prediction accuracy. Over time, we have fixed a number of undesirable cases found in the elementary metrics and the machine learning model, yielding more accurate prediction overall. For example, the elementary metrics are modified to yield improved consistency with luminance masking; motion scores at the scene boundaries are updated to avoid overshoot due to scene changes; the QP-VMAF monotonicity is now maintained when extrapolating into high QP regions. Clearly, a VMAF model’s accuracy also heavily depends on the coverage and accuracy of the subjective scores that it is trained on. We have collected a subjective dataset with a broadened scope compared to our previous dataset, including more diverse content and source artifacts such as film grain and camera noise, and more comprehensive coverage of encoding resolutions and compression parameters. We have also developed a new data cleaning technique to remove human bias and inconsistency from the raw data, and open-sourced it on Github. The new approach uses maximum likelihood estimation to jointly optimize its parameters based on the available information and eliminates the need for explicit subject rejection.The VMAF framework allows training of prediction models tailored to specific viewing conditions, no matter whether it is on a mobile phone or on a UHD TV. The original model released when we open-sourced VMAF was based on the assumption that the viewers sit in front of a 1080p display in a living room-like environment with the viewing distance of 3x the screen height (3H). This is a setup that is generally useful for many scenarios. In applying this model to the mobile phone viewing, however, we found that it did not accurately reflect how a viewer perceives quality on a phone. In particular, due to smaller screen size and longer viewing distance relative to the screen height (>3H), viewers perceive high-quality videos with smaller noticeable differences. For example, on a mobile phone, there is less distinction between 720p and 1080p videos compared to other devices. With this in mind, we trained and released a VMAF phone model.An example VMAF-bitrate relationship for the default model and the phone model is shown above. It can be interpreted that the same distorted video would be perceived as having a higher quality when viewed on a phone screen than on a HD TV, and that the VMAF score differences between the 720p and 1080p videos are smaller using the phone model.Most recently, we added a new 4K VMAF model which predicts the subjective quality of video displayed on a 4K TV and viewed from a distance of 1.5H. A viewing distance of 1.5H is the maximum distance for the average viewer to appreciate the sharpness of 4K content. The 4K model is similar to the default model in the sense that both models capture quality at the critical angular frequency of 1/60 degree/pixel. However, the 4K model assumes a wider viewing angle, which affects the foveal vs peripheral vision that the subject uses.VMAF is trained on a set of representative video genres and distortions. Due to limitations in the size of lab-based subjective experiments, the selection of video sequences does not cover the entire space of perceptual video qualities. Therefore, VMAF predictions need to be associated with a confidence interval (CI) that expresses the inherent uncertainty of the training process. Towards this end, we recently introduced a method to accompany VMAF prediction scores with a 95% CI, which quantifies the level of confidence that the prediction lies within the interval. The CI is established through bootstrapping on the prediction residuals using the full training data. Essentially, it trains multiple models, using “resampling with replacement”, on the residuals of prediction. Each of the models will introduce a slightly different prediction. The variability of these predictions quantifies the level of confidence — the closer these predictions are, the more reliable the prediction using the full data will be.The example plot above based on the NFLX Public Dataset showcases a 95% CI associated with each data point. It is interesting to note that points on the higher-score end tend to have a tighter CI than points on the lower-score end. This can be explained by the fact that in the dataset to train the VMAF model, there are more dense data points on the higher end than the lower. Notably, the bootstrapping technique will not necessarily improve the accuracy of the trained model, but will lend a statistical meaning to its predictions.Oftentimes we have been asked whether a particular way of calculating VMAF score is appropriate, or how to interpret the scores obtained. This section highlights some of the best practices of using VMAF.VMAF scores range from 0 to 100, with 0 indicating the lowest quality, and 100 the highest. A good way to think about a VMAF score is to linearly map it to the human opinion scale under which condition the training scores are obtained. As an example, the default model v0.6.1 is trained using scores collected by the Absolute Category Rating (ACR) methodology using a 1080p display with viewing distance of 3H. Viewers voted the video quality on the scale of “bad”, “poor”, “fair”, “good” and “excellent”, and roughly speaking, “bad” is mapped to the VMAF scale 20 and “excellent” to 100. Thus, a VMAF score of 70 can be interpreted as a vote between “good” and “fair” by an average viewer under the 1080p and 3H condition. Another factor to consider is that the best and the worst votes a viewer can give is calibrated by the highest- and lowest-quality videos in the entire set (during training, and before the actual test starts, subjects are typically accustomed to the experiment’s scale). In the case of the default model v0.6.1, the best and the worst videos are the ones compressed at 1080p with a low quantization parameter (QP) and the ones at 240p with a high QP, respectively.A typical encoding pipeline for adaptive streaming introduces two types of artifacts — compression artifacts (due to lossy compression) and scaling artifacts (for low bitrates, source video is downsampled before compression, and later upsampled on the display device). When using VMAF to evaluate perceptual quality, both types of artifacts must be taken into account. For example, when a source is 1080p but the encode is 480p, the correct way of calculating VMAF on the pair is to upsample the encode to 1080p to match the source’s resolution. If, instead, the source is downsampled to 480p to match the encode, the obtained VMAF score will not capture the scaling artifacts. This is especially important when using VMAF as the quality criterion for per-title encoding, where the construction of the convex hull is crucial for selecting the optimal encoding parameters.The above example illustrates the convex hull forming when VMAF is calculated correctly (left) and incorrectly (right). When VMAF is calculated with encode upsampled to match the source resolution, one can easily identify intersections among curves from different resolutions. On the other hand, if the source is downsampled to match the encode resolution, the low-resolution encodes will yield undeservingly high scores, and no intersection pattern among the curves can be spotted.When upsampling an encode to match the source resolution, many upsampling algorithms are available, including bilinear, bicubic, lanczos, or even the more advanced neural net-based methods. It is untrue that the higher quality the upsampling algorithm is, the better. In principle, one should pick an algorithm that can best match the display device’s. In many cases, the precise circuit for upsampling the video in a display is unknown. In this case, we recommend using bicubic upsampling as the approximation in general.A frequently asked question is: if both the source and the encoded video have a resolution lower than 1080p, can a 1080p model (e.g. the default model v0.6.1) still apply? Note that the default model measures quality at the critical angular frequency of 1/60 degree/pixel. One way to think about the default model is that it is a “1/60 degree/pixel” model, which means that it assumes that 60 pixels are packed into one degree of viewing angle. If applying the geometry, one can find that it equally applies to 1080p video with 3H, or 720p video with 4.5H, or 480p video with 6.75H. In other words, if applying the 1080p model to 720p / 480p videos, the resulting scores can be interpreted as the ones obtained with viewing distance of 4.5H / 6.75H, respectively. At such long viewing distances, a lot of artifacts are hidden from the human eye, yielding much higher scores.Note that this interpretation is without the calibration of other factors such as the eye focal length, foveal vision, and viewer’s expectation on SD vs. HD videos.VMAF produces one score per video frame. It is often useful to generate a summary score per longer time duration, for example, for a video segment of a few seconds, or even for a two-hour movie. Although sophisticated techniques exist to temporally pool the per-frame scores, our empirical results suggest that simple arithmetic mean (AM) is the best way of averaging, in that it yields highest correlation with subjective scores.Another useful averaging technique is harmonic mean (HM). Oftentimes it produces a summary score very similar to AM, except that in the presence of outliers, HM emphasizes the impact of small values.The examples above demonstrate the differences between AM / HM in the absence / presence of small-value outliers. Temporal pooling based on HM is useful when one wants to optimize quality based on VMAF while avoiding the occasional low-quality video encodes.To understand the effects of different treatments in A/B tests, we need metrics to summarize per-frame VMAF scores into per session metrics (i.e. one number per session). To add to the challenge, since in adaptive streaming the best bitrates to stream are selected based on a variety of factors (such as throughput), each session will have a different “combination” of per-frame VMAF values based on how long each stream was played and during which time period of the session. To reach a single VMAF summary number per session, we must 1) determine an appropriate level of aggregation and 2) build upon these aggregates with metrics that reflect different aspects of perceptual quality.If we aggregate the per-frame VMAF scores to one average number per stream, we will miss drops in quality that happen during the session. If we do not aggregate at all and use per-frame values, the computational complexity for creating final summary metrics based on per-frame values for every session will be too high. To this end, we recommend going with intervals in the granularity of seconds to strike a balance between analytic accuracy and ease of implementation.In order to understand the effects of different treatments in A/B tests, we recommend metrics which capture three key aspects of quality: aggregate quality, startplay quality, and variability. These could be the average VMAF over the entire session, average VMAF over the first N seconds, and the number of times the VMAF value drops below a certain threshold relative to the previous values.From an internal project to help deliver the best video quality to Netflix’s members, to an open-source project that starts attracting a community of users and contributors, VMAF has been constantly evolving and continuously finding new areas of applications. We are pleased to see that inside and outside Netflix, VMAF has been applied to codec comparison, encoding optimization, A/B experimentation, video analysis, post-processing optimization, and many more areas. Independent researchers have helped cross-verify the perceptual accuracy of VMAF. The open-source community has helped speed up the tool, create easy-to-use interfaces, and moderate the Github repo.But we are just getting started.Through conversations with researchers and VMAF adopters, we have realized that there are many areas that the current VMAF can improve upon. To give a few examples:In the coming years, we strive to continue improving VMAF, and we invite industry, academia and the open-source community to collaborate with us.We would like to acknowledge the following individuals for their help with the VMAF project: Prof. C.-C Jay Kuo (University of Southern California), Joe Yuchieh Lin, Eddy Chi-Hao Wu, Haiqiang Wang, Prof. Patrick Le Callet (Université de Nantes), Jing Li, Yoann Baveye, Lukas Krasula, Prof. Alan Bovik (University of Texas at Austin), Todd Goodall, Ioannis Katsavounidis, members of the Video Algorithms team, Martin Tingley and Lavanya Sharan of Streaming S&A, and the open-source contributors to the VMAF project.",https://medium.com/netflix-techblog/vmaf-the-journey-continues-44b51ee9ed12?source=collection_home---4------2---------------------,0,netflix,"react,html,frontend,javascript",NULL,2018-10-25
"Lumen: Custom, Self-Service Dashboarding For Netflix","Netflix generates a lot of data. One of the ways that we gain useful insights is by visualizing that data in dashboards which allow us to comprehend large amounts of information quickly. This is particularly important when operational issues arise as our engineers need to be able to quickly diagnose problem areas and work to correct them.Operational issues, however, are just one potential use case for dashboards at Netflix. We also use dashboards to track and chart key business metrics, compare the results of experiments, monitor real-time data, and even find out if burgers are on the menu for lunch.In short, dashboards are important to Netflix, but not just any dashboarding platform would work well for us.In particular, any dashboarding platform for Netflix has the following constraints that must be met:Our platform needs to be highly flexible, dynamic, and performant while giving users a great amount of control. When we first invested in a dashboarding platform for our operational metrics over eight years ago, there was no reasonable solution that met the constraints above while continuing to be cost-effective for Netflix. So, we built our own. It is called Lumen.Lumen is a dashboarding platform that allows users to define JSON configuration files that are parsed at runtime in the browser to generate custom dashboards.At a high-level, Lumen’s architecture seems relatively simplistic. A config is loaded from the backend store and then parsed into an internal data structure. That data structure is then passed to a renderer, which generates the visualizations and UI controls for the dashboard. When a user then interacts with one of the rendered UI controls, the process repeats with new values applied to the same config.What this simple architecture hides is that Lumen configs can be quite complicated. Users can configure most aspects of their dashboards, including the visualizations shown, what data sources are used, and which UI controls to display, and they can even write configs which reconfigure themselves at runtime based on variable conditions.Being JSON-driven allows our users to easily create and edit their own dashboards while also integrating nicely with other tools that can produce JSON. While these configs can be complicated, as we have refined Lumen and ironed out the common patterns our users need, we have identified a few core concepts from which most other features and patterns can be implemented through composition.So now, each dashboard in Lumen is mainly comprised of the following concepts:Composing these four concepts gives our users the flexibility to build dynamic and reactive dashboards on their own using whatever data they want. These concepts also come together nicely for each visualization “cell” that gets rendered to make them reusable across multiple dashboards when desired.Our users control exactly which fields and values they want to be dynamic and controllable from the user interface. This allows some users to build dashboards targeted at very specific use cases while others can build dashboards that serve very broad and exploratory use cases.While the architecture of Lumen provides a flexible and dynamic platform for our users, it doesn’t necessarily lend itself to a snappy and responsive user experience by default. For instance, Lumen has little-to-no knowledge of how the data sources for a given dashboard will behave. Will they send large payloads? Will they require a lot of client-side data processing? Will they have reasonable latencies?In order to meet the needs of our operational use cases and make for a pleasant user experience, we quickly realized that we couldn’t do data fetching and parsing the way we normally would in a web application.Most web applications fetch and parse data on the main JavaScript thread in the browser. When you deal with large amounts of data or complex user logic, this can lead to “jank” in the browser which is most noticeable as your app begins to lag and even freeze while the main thread is busy.To circumvent this and related issues, the majority of data operations in Lumen are done in Web Workers. This allows Lumen to keep the main thread free for user interactions, such as scrolling and interacting with individual charts, as the dashboard loads all of its data.This design means we also load and execute mapper functions within workers. Since users are able to define custom transformations, this is beneficial as it allows us to minimize the blast radius of failures or bugs; even if a data transformation fails catastrophically, it won’t cause the entire application to crash. This happens in addition to keeping data transformation from blocking the main thread.Beyond workers, we have also invested in utilizing native Web Component technology to keep the application lightweight and its components portable. While not everything makes sense to share, aspects of Lumen such as our interactive Atlas graphs, URL management library, and more can be shared amongst other internal tools easily.Lumen has evolved a lot since it was first conceived to become a powerful, flexible, and dynamic platform to serve Netflix’s varied dashboarding needs. It receives more than 150,000 views each week across roughly 5,000 unique dashboards from around 1,500 unique users. Those views generate more than 450,000 charts each day from more than a dozen different backend data sources.The flexibility of the platform has been a huge boon to Lumen’s adoption internally and we are continually confronted with our users finding new and interesting use cases for the platform, including fun “hacks”, like building dashboards for lunch food.As Netflix continues to grow, we expect the need for a robust dashboarding platform to continue to increase and are excited to continue improving Lumen. There are currently three major areas we see for improvement in Lumen.The first is to reduce legacy features that no longer fit in with the core concepts Lumen is now based on. As with many long-lived projects, some features added in the past no longer make sense; some were too specific to a limited use-case and others have been replaced by more general solutions that give users more control.The second is to improve the usability of Lumen by making it easier for users to create and manage their dashboards. Currently, this means improving the config editing tools we provide users with. In the future, this will mean giving users a robust, WYSIWYG-style interface that reduces cognitive load while making it easier to discover the powerful features Lumen provides.Finally, we want to continue investing in the extensibility of Lumen. While the current architecture makes it easy to bring your own data source, creating custom visualizations or adding first-class support for integrations with other tools requires changes to Lumen’s source code. We plan to make the system much more pluggable such that users are able to extend the platform without needing direct support from maintainers.Our ultimate goal with Lumen is to meet all the challenges for our myriad users while providing them an experience that allows them to focus on the critical information and value in their dashboards instead of the tool they’re using. The future for Lumen is bright!Lumen is developed and maintained by Trent Willis, John Tregoning, and Matthew Johnson at Netflix. We are always looking for new ideas and ways to improve. So, if you’re interested in contributing or just chatting about ideas in this space, please reach out on LinkedIn or Twitter!",https://medium.com/netflix-techblog/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c?source=collection_home---4------0---------------------,0,netflix,"react,webpack,frontend,angular,python",NULL,2018-10-17
The Netflix Media Database,"Let us imagine we are working on the next generation adaptive video streaming algorithm. Our goal is to minimize the playback startup time for the millions of Netflix members all over the world. To do that we need to gather aggregate statistics (minimum, maximum, median, mean, any number of percentiles) for the header sizes of our ISO BMFF (Base Media File Format) formatted bitstreams. The Netflix trans-coding pipeline services a huge catalog of content and produces a large ladder of bitstreams (with varying codec + quality combinations) for every title. In the past, we would need to write one-off scripts that would crawl the bitstream header information from our bitstreams in an arduous fashion before we could analyze the data. Such an approach is clearly not scalable — a software bug in our script would reset the entire effort. Further, a new “throwaway” script would be needed for analyzing another completely different dimension of our media data. Repeating this methodology over several times for problems from different domains made us realize that there was a pattern here, and set us on the path to build a system that would address this in a scalable way.This blog post introduces the Netflix Media DataBase (NMDB) — a highly queryable data system built on the Netflix micro-services platform. NMDB is used to persist deeply technical metadata about various media assets at Netflix and to serve queries in near real-time using a combination of lookups as well as runtime computation. NMDB enables developers (such as video streaming researchers) to focus their time on developing insights into media data and crank out awesome data-driven algorithms as opposed to worrying about the task of collecting and organizing data.An optimized user interface, meaningful personalized recommendations, efficient streaming and a vast catalog of content are the principal factors that define the end-user Netflix experience. A myriad of business workflows of varying complexities need to come together to realize this experience.Artwork imagery and title synopses (see above picture) that are pertinent to the story as well as insightful video previews go a long way in helping users find relevant shows and movies. The ever increasing scale of content ingestion at Netflix necessitates the development of systems that can assist our creative teams synthesize high quality digital merchandise assets in a timely fashion. This could be done for example, by providing them with meaningful raw imagery and video clips automatically (algorithmically) extracted from the source video assets. This could serve as a starting point for creating engaging digital media assets.As shown below, the content recommendations systems economically surfaces choices that are personalized to the content preferences and tastes of the end-user. Compact and effective feature representations of the content present in the Netflix catalog are critical to this function. Such representations could for example, be obtained by building machine learning models that use the media files (audio, timed text, video) as well as title metadata (genre tags, synopses) as their input.Efficient audio and video encoding recipes lead to creation of compact bitstreams — a precursor to efficient streaming of media over the Internet. Temporal and spatial video analyses such as detecting moments of shot and scene changes, as well as identifying salient parts and objects in the video frames help generate critical input for the video encoding system.Lastly, maintaining high standards on the quality of the source content ingested at Netflix is vital to a great end-user Netflix experience. The above image illustrates one such use case. This image corresponds to a video frame for a title from the Western Classical genre. In this case, a camera used for the production of the title is visible in the video. It is highly desirable to have an automated analysis system that would detect and localize (perhaps through a rectangular bounding box) the presence of the camera. Another such case is illustrated in the following picture. In this case, the subtitles text is placed on top of the text that is burnt in the video rendering both of them unreadable. A video text detection algorithm together with knowledge of timing and positioning of subtitles could be used to solve this problem automatically.We would like to note that the seemingly disparate use cases illustrated above actually overlap in their use of core component algorithms. For instance, shot change data serves a critical role for the video encoding use case. Different shots have different visual characteristics and merit different bit budgets. For the same reason, shot change data is also an essential ingredient for producing diverse raw imagery and video clips from source video assets. A collection of high quality raw artwork candidates could be obtained by selecting the top few candidates from each shot. Likewise, meaningful latent representations for video media can be constructed by composing per shot representations. As another example, while video text detection data serves an invaluable role in content quality control, it is also beneficial for the video encoding and the artwork automation use cases — video frames containing large amounts of text typically do not serve as good artwork image candidates.Further, many of these analyses tend to be very expensive computationally — it would be highly inefficient to repeat the same computation when addressing different business use cases. Together, these reasons together make an argument for a data system that could act as a universal storage for any analysis related to the timeline of a media. In other words, we need a “media database”.A media database houses media analysis data corresponding to media of varying modalities — these include audio, video, images, and text (e.g., subtitles). It is expected to service arbitrary queries on the media timeline. For example, what time intervals in the timeline of an audio track contain music, or the list of video frames in a video that contain text, or the set of time intervals in a subtitles file that correspond to dialog. Given the breadth of its scope, we believe that the following are the vital traits of a media database:The use cases outlined above inspired us to build NMDB — a universal store for any analysis related to the timeline of a media that can be used to answer spatio-temporal queries on the media timeline at scale. The Netflix catalog comprises a large number of media assets of varying modalities — examples of static assets includes images, and examples of playable assets include audio, text, and video. As was presented above, a myriad of business applications stand to benefit from access to in-depth semantic information pertaining to these assets. The primary goal of NMDB is to serve the requisite data needed by these applications — we think of NMDB as the data system that forms the backbone for various Netflix media processing systems.Efficient modeling of media timeline data is a core trait of NMDB. A canonical representation of the media timeline could support a large class of use cases while efficiently addressing user queries schema. This forms the subject for our next article in this series.— by Rohit Puri, Shinjan Tiwary, Sreeram Chakrovorthy, Subbu Venkatrav, Arsen Kostenko and Yi Guo",https://medium.com/netflix-techblog/the-netflix-media-database-nmdb-9bf8e6d0944d?source=collection_home---4------1---------------------,0,netflix,,NULL,2018-10-15
Rethinking Netflix’s Edge Load Balancing,"By Mike Smith, Netflix Cloud GatewayWe briefly touched on some of the load balancing improvements we’ve recently been making in our Open Sourcing Zuul 2 post. In this post, we’ll go into more detail on the whys, hows and results of that work.On the Netflix Cloud Gateway team we are always working to help systems reduce errors, gain higher availability, and improve Netflix’s resilience to failures. We do this because even a low rate of errors at our scale of over a million requests per second can degrade the experience for our members, so every little bit helps.So we set out to take our learnings from Zuul, plus those from other teams, and improve our load balancing implementation to further reduce errors caused by overloaded servers.In Zuul, we’ve historically used the Ribbon load balancer with a round-robin algorithm and some filtering mechanisms for blacklisting servers that have a high connection failure rate.Over the years we’ve gone through a couple of improvements and customizations designed to send less traffic to servers that have recently launched, to attempt to avoid overloading them. These have made significant improvements, but for some particularly troublesome origin clusters, we would still see much higher load-related error rates than desirable.If all servers in a cluster are overloaded, then there’s little improvement we can make in choosing one server over another, but we often see situations where only a subset of servers are overloaded. For example:It can be useful to have some principles in mind when starting on a project, to help guide us with the flood of small and large decisions we need to make daily while designing software. Here are some that were used on this project.We had coupled our previous load-balancing customizations to the Zuul codebase, which had precluded us from sharing these with other teams at Netflix. So this time we made a decision to accept the constraints and additional investment needed, and to design with reuse in mind from the get-go. This makes adoption in other systems more straightforward, and reduces the chances of reinventing-the-wheel.Try to build on top of the ideas and implementations of others. For example, the choice-of-2 and probation algorithms which were previously trialled at Netflix in other IPC stacks.Prefer local decision-making to avoid the resiliency concerns, complexities and lag of coordinating state across a cluster.Our operational experience over the years with Zuul has demonstrated that situating parts of a services’ configuration into a client service not owned by the same team … causes problems.One problem is that these client-side configurations tend to either get out of sync with the changing reality of the server-side, or introduce coupling of change-management between services owned by different teams.For example, the EC2 instance type used for Service X is upgraded, causing fewer nodes to be needed for that cluster. So now the “maximum number of connections per host” client-side configuration in Service Y should be increased to reflect the new increased capacity. Should the client-side change be made first, or the server-side change, or both at the same time? More likely than not, the setting gets forgotten about altogether and causes even more problems.When possible, instead of configuring static thresholds, use adaptive mechanisms that change based on the current traffic, performance and environment.When static thresholds are required, rather than have service teams coordinate threshold configurations out to each client, have the services communicate this at runtime to avoid the problems of pushing changes across team boundaries.An overarching idea was that while the best source of data for a servers’ latency is the client-side view, the best source of data on a servers’ utilization is from the server itself. And that combining these 2 sources of data should give us the most effective load-balancing.We used a combination of mechanisms that complemented each other, most of which have been developed and used by others previously, though possibly not combined in this manner before.We chose to buttress the Join-the-shortest-queue(JSQ) algorithm that is commonly used, with a 2nd algorithm based on the server-reported utilization to try and give us the best of both.Problems with JSQJoin-the-shortest-queue works very well for a single load-balancer, but has a significant problem if used in isolation across a cluster of load balancers. The problem is that the load balancers will tend to herd and all choose the same low-utilized servers at the same time, thus overloading them, and then moving on to the next least-utilized and overloading that, and onward …This can be resolved by using JSQ in combination with the choice-of-2 algorithm. This mostly removes the herding problem, and works well except for some deficiencies around each load balancer not having a complete picture of server utilization.JSQ is generally implemented by counting the number of in-use connections to a server from only the local load balancer, but when there are 10’s-100’s of load balancer nodes, the local view can be misleading.For example in this diagram, load balancer A has 1 inflight request to server X and 1 to server Z, but none to server Y. So when it receives a new request and has to choose which server is least utilized — from the data it has available to it — it will choose server Y. This is not the correct choice though — server Y is actually the most utilized, as two other load balancers currently have inflight requests to it, but load balancer A has no way of knowing that.This illustrates how a single load balancers’ viewpoint can be entirely different from the actual reality.Another problem we experience with relying only on the client-side view, is that for large clusters — particularly when combined with low traffic — a load balancer often has only a few in-use connections to a subset of the servers in a pool of hundreds. So when it’s choosing which server is least-loaded, it can often just be choosing between zero and zero — ie. it has no data on the utilization of either of the servers it’s choosing between, so has to just guess randomly.One solution to this problem could be to share the state of each load balancers’ inflight counts with all other load balancers … but then you have a distributed state problem to solve.We generally employ distributed mutable state only as a last resort, as the value gained needs to outweigh the substantial costs involved:An alternative simpler solution — and one that we’ve chosen — is to instead rely on the servers reporting to each load balancer how utilized they are …Server-Reported UtilizationUsing each server’s viewpoint on their utilization has the advantage that it provides the aggregate of all load balancers that are using that server, and therefore avoids the JSQ problem of an incomplete picture.There were 2 ways we could have implemented this — either:We chose the 2nd option, as it was simple, allowed for frequent updating of this data, and avoided the additional load placed on servers of having N load balancers poll M servers every few seconds.An impact of this passive strategy is that the more frequently a load balancer sends a request to one server, the more up-to-date it’s view of that servers’ utilization. So the higher the RPS, the higher the effectiveness of the load-balancing. But then conversely, the lower the RPS, the less effective the load-balancing.This hasn’t been an issue for us, but it is likely that for services that receive a low RPS through one particular load balancer (while receiving high RPS through another separate one), actively polling health-checks could be more effective. The tipping point would be where the load balancer is sending a lower RPS to each server than the polling frequency used for the health-checks.Server ImplementationWe implemented this on the server side by simply tracking the inflight request count, converting it to a percentage of the configured max for that server, and writing that out as a HTTP response header:The optional target-utilization can be specified by a server to indicate what percentage utilization they are intending to operate at under normal conditions. This is then used by the load balancer for some coarse-grained filtering done as described later.We experimented a little with using metrics other than the inflight count, such as operating system reported cpu utilization and load average, but found them to cause oscillations seemingly due to the lag induced by them being based on rolling averages. Thus, we decided to just stick with the relatively simple implementation of just counting inflight requests for now.As we wanted to be able to choose between servers by comparing their statistics, the existing simple round-robin implementation was going to have to go.An alternative within Ribbon that we tried was JSQ combined with a ServerListSubsetFilter to reduce the herding problem of distributed-JSQ. This gave reasonable results, but the resulting request distribution across target servers was still much too wide.So we instead applied some earlier learnings from another team at Netflix and implemented the Choice-of-2 algorithm. This has the advantage of being simple to implement, keeps cpu cost on the load balancer low, and gives good request distribution.To choose between servers, we compare them on 3 separate factors:These 3 factors are used to assign scores to each server, and then the aggregate scores are compared to choose the winner.Using multiple factors like this does make the implementation more complicated, but it hedges against edge-case problems that can occur from relying solely on one factor.For example, if one server starts failing and rejecting all requests, it’s reported utilization will be much lower — due to rejecting requests being faster than accepting them — and if that was the only factor used, then all the load balancers would start sending more requests to that bad server. The client-health factor mitigates against that scenario.When randomly choosing the 2 servers to compare, we filter out any servers that are above conservatively configured thresholds for utilization and health.This filtering is done on each request to avoid the staleness problems of filtering only periodically. To avoid causing high cpu load on the load balancer, we only do a best-effort by making N attempts to find a randomly chosen viable server, and then falling-back to non-filtered servers if necessary.Filtering like this helps significantly when a large proportion of the pool of servers have persistent problems. As in that scenario, randomly choosing 2 servers will frequently result in 2 bad servers being chosen for comparison, even though there were many good servers available.This does though have the disadvantage of depending on statically configured thresholds, which we’ve tried to avoid. The results of testing though, convinced us this was worthwhile adding, even with some general-purpose (ie. non service-specific) thresholds used.For any server that the load-balancer hasn’t yet received a response from, we only allow one inflight request at a time. We filter out these in-probation servers until a response is received from them.This helps to avoid overloading newly launched servers with a flood of requests, before giving them a chance to indicate how utilized they are.We use server age to progressively ramp up traffic to newly launched servers over the course of their first 90 seconds.This is another useful mechanism like probation that adds some caution about overloading servers in the sometimes delicate post-launch state.To ensure that servers don’t get permanently blacklisted, we apply a decay rate to all stats collected for use in the load-balancing (currently a linear decay over 30 secs). So for example, if a server’s error-rate goes up to 80% and we stop sending it traffic, then the value we use will decay down to zero over 30 seconds (ie. it will be 40% after 15 secs).A negative impact of moving away from using round-robin for load-balancing, is that where before we had a very tight distribution of requests across servers in a cluster, we now get a larger delta between servers.Using the choice-of-2 algorithm has helped to mitigate this a lot (compared to JSQ across all or a subset of servers in a cluster), but it’s not been possible to avoid it entirely.So this does need to be taken into account on the operational side, particularly for canary analysis where we typically compare absolute values of request counts, error rates, cpu, etc.Obviously this is the intended effect, but for teams used to round-robin where traffic is apportioned equally, this has some knock-on effects on the operational side.As the traffic distribution across origin servers is now dependent on their utilization, if some servers are running a different build that is more or less efficient, then they will receive more or less traffic. So:Moving from round-robin to this new load balancer has the useful effect of working very well in conjunction with staged rollouts of dynamic data and properties.Our best practice is to deploy data updates one region(datacenter) at a time to limit the blast radius of unanticipated problems.Even without any problems caused by the data update itself, the act of a server applying an update can cause a brief load spike (typically GC related). If this spike occurs simultaneously on all servers in a cluster, it can lead to a large spike in load-shedding and errors propagating upstream. In this scenario there’s little a load balancer can do to help, as all servers are experiencing the high load.A solution though — when used in combination with an adaptive load balancer like this — is to do rolling data updates across the servers in a cluster. If only a small a proportion of the servers are applying the update at once, then the load balancer can briefly reduce traffic to them, as long as there are enough other servers in the fleet to take the diverted traffic.We used synthetic load-testing scenarios extensively while developing, testing and tuning the different aspects of this load-balancer. These were very useful in verifying the effectiveness with real clusters and networks, as a reproducible step above unit-tests, but without yet using real user traffic.More details of this testing are listed later in the appendix, but summarizing the main points:We’ve found the new load balancer to be very effective at distributing as much traffic to each origin server as that server can handle. This has the valuable effect of routing around both intermittently and persistently degraded servers, without any manual intervention, which has avoided significant production problems from causing engineers to be woken in the middle of the night.It’s difficult to illustrate the impact of this during normal operation, but it can be seen during production incidents, and for some services even during their normal steady-state operation.A recent incident involved a bug in a service that was causing more and more server threads to block over time — ie. from the point a server launched, a few threads would block each hour until it eventually started to reach its maximum and shed load.In the below chart of RPS per server, you can see that before 3am there was a wide distribution across the servers. This was due to the servers with higher numbers of blocked threads being sent less traffic by the load balancers. Then after 3:25am autoscaling started launching more servers, each of which received roughly double the RPS of the existing servers — as they did not yet have any blocked threads and therefore could successfully handle more traffic.Now if we look at this chart of the rate of errors per server over the same time range, you can see that the distribution of errors across all the servers throughout the incident was fairly evenly distributed, even though we know that some servers had much less capacity than others. This indicates that the load balancers were working effectively, and that all servers were being pushed slightly past their effective capacity, due to too little overall available capacity in the cluster.Then when autoscaling launched new servers, they were sent as much traffic as they could handle, up to the point that they were erroring at the same low level as the rest of the cluster.So in summary, the load-balancing was very effective at distributing traffic to the servers, but in this case not enough new servers were launched to bring the overall error level all the way back down to zero.We have also seen a significant reduction in just the steady-state noise in some services of servers having blips of load-shedding due to GC events for a few seconds. That can be seen here where the errors reduced substantially after the new load balancer was enabled:An unanticipated impact has been to highlight some gaps in our automated alerts. Some existing alerts based on error rates from services, which would previously have fired when progressive problems were only affecting a small subset of a cluster, now fire much later or not at all, as error rates are kept lower. This has meant that teams have not been notified of sometimes large problems affecting their clusters. The solution has been to plug these gaps by adding additional alerts on deviations in utilization metrics rather than just error metrics.This post isn’t intended as a plug for Zuul — although it is a great system — but more to share and add another datapoint to the wealth of interesting approaches out there in the proxying/service-mesh/load-balancing community. Zuul is a great system to test, implement, and improve these kinds of load balancing schemes; running them with the demands and scale of Netflix gives us the capabilities to prove and improve these approaches.There are many different approaches that can be taken to improve load-balancing, and this one has worked well for us, producing significant reductions in load-related error rates and much improved balancing of real-world load on servers.As with any software system though — you should make decisions based on your own organizations’ constraints and goals, and try to avoid chasing the gnat of perfection.If this kind of work is interesting to you feel free to reach out to me or us here on the Cloud Gateway team at Netflix.This load test scenario reproduces the situation where a small origin cluster is going through a red-black deployment, and the newly deployed cluster is having cold startup problems or has some kind of performance regression. This was simulated by artificially injecting additional latency and cpu load for each request on the newly deployed servers.The test ramps-up to 4000 RPS sent to a large Zuul cluster (200 nodes) which in turn proxies to a small Origin cluster (20 instances), and after a few mins, enabling the 2nd slow origin cluster (another 20 instances).Here is a chart of the metrics for the new load balancer with all of it’s features enabled.And for reference, looking at how the traffic was split between the faster and slower server groups, you can see that the load-balancer was reducing the proportion to about 15% sent to the slower group (vs an expected 50%).This is again for the new load balancer, but with the server-utilization feature disabled — so therefore only client-side data was used for balancing.This was for the original round-robin load balancer with it’s server blacklisting feature.",https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c?source=collection_home---4------2---------------------,0,netflix,,NULL,2018-09-28
Netflix’s Production Technology = Voltron,"by Chris GossChange management is hard. In everyday production, there are numerous factors working against embracing change. Limited preparation time, whole new show = whole new crew, innumerable planning variables, and the challenge of driving an operational plan based on creative instincts. These are problems that technology is not yet built to solve. Time, training, and education can and will make a dent in our efforts, but creative planning is nuanced, and by nature, human.So, where do we start? What can production management technology do now to pave the way for future change? Having spent the past two years building Prodicle, our production suite of apps, we hit several pockets of success, while learning from numerous obstacles. Others have ventured down this path as well, and there are several start-ups and moderate size companies that reach out to us about their product offerings. It’s exciting to see others with the same level of passion and enthusiasm we have for improving the way entertainment gets made. After many meetings, memos, pitches and 30-day trials, there are a number of trends we often find ourselves discussing with all production SaaS providers and we think it’s worthwhile to share these key development pillars to start an ongoing conversation about how we’re thinking of pursuing these problems. Some have manifested themselves in how we’ve built Prodicle, others are ideas and pipedreams we hope to pursue in time. If you’re a SaaS provider seeking to dig your heels into the entertainment production space, here are some common themes we often discuss with our partners, our crews and our software development teams.We built Prodicle with the intention of being modular, which is critical based on the wide variety and volume of content we produce. We are not a domestic TV studio. We are a global streaming studio, which means we need to produce entertainment that satisfies the different tastes of our members all over the world. Live action films, multi-camera comedies, weekly talk shows, global competitions, international crime dramas, documentaries about the Rajneesh — all forms of content for all audiences. This means that one giant monolithic software solution will not work — build, buy or otherwise. We are often pitched the dreamstate of software: one end-to-end solution to rule them all. While all-in ecosystems may work for smaller studios/production companies, the variables across our content slate makes it very difficult to successfully operationalize. As such, our offering will always be a modular ecosystem of connected solutions, some things built by us, other things bought from vendors, and a whole bunch of stuff in between. The beauty is twofold — products that work great on their own, products that work better together. It’s like Voltron. Season 7 now streaming.It’s perfectly fine for a singular solution to solve one problem really well — in fact, this is often preferred. If your solution seeks to do seven things, chances are we’ve already solved for five of them — we just need the remaining two. Those two become the attractive secret sauce, and if they play well in a modular sandbox, we want to hear more! But when they don’t, and we have to go all-in on multiple ecosystems that overlap in functionality, the pain of deployment and support makes it a near non-starter. Passport is Prodicle’s menu-based portal that brings multiple solutions together in one place. It’s where we seek to grow our product offering for our crews. One modular location, many products.It’s important to have safeguards in place to protect our content. Filmmaking is collaborative and lots of people touch the final product. Safely getting in and out of our ecosystem is important to get right. If too complex, it becomes a barrier to change management and introduces less-than-ideal workarounds. If too liberal, then we face the possibility of content leaking early. It’s an exercise in balance. Where do we start? From the product/user perspective, we need one login. We cannot have a multitude of solutions all utilizing different usernames and passwords. Retail has got this figured out, the freelance work environment needs to solve it as well. If your product doesn’t offer SSO (single sign-on) it becomes an immediate point of resistance. SSO is crucial to making security as transparent to the user as possible. Second? Roles and permissions. Our Content Engineering team has built an internal solution to manage app-to-app roles and permissions in a single administrative portal. This means we can provision users across multiple apps in one location. We’d love the ability to call your APIs and set application roles individually and in-bulk from one place. It’s a user administrator’s dream. Working with tens of thousands of freelancers around the world mandates this as a requirement. SSO and APIs for roles/permissions is a biggie.Netflix adopted Google Cloud’s G Suite in 2013. This means our entire corporate file sharing platform is Google Drive. Thousands of users, millions of files within the cloud — accessible everywhere. When we started the studio, we were faced with a choice: a) deviate and use a different file sharing platform, perhaps different platforms for different types of content, or b) use G Suite on our productions so corporate, studio and production are all in one shared space. This was a tough call, as the variables within our content, as noted above, are extensive. However, we felt G Suite was and is flexible enough while subscribing to our core principles of modularity. Utilizing their service within our ecosystem was the fastest way to stand-up a global file sharing platform. We have hundreds of productions using the tools within G Suite, collaborating with our studio users. Instead of trying to build our own file platform, or buying a multitude of third party platforms, we started building production-centric features on top of G Suite. For dynamic watermarking, password protection and expiring links, we built Prodicle Distribution. It sits on top of Google Drive, taking advantage of the multitude of G Suite functions that would be silly for us to build ourselves, e.g. collaboration, commenting, file previews, etc. In doing this, utilizing consumer facing tech from companies whose full-time focus is workplace solutions (our main focus is great content), we’re able to leverage what works well and add what we need to that. We don’t expect consumer tech companies to build production functionality, but if we can leverage what they do well and we build what we need for us to do well (produce great content) it gives us a significant head start. We look for this in third-party production tools all the time. Again, we want production components that don’t exist anywhere else, not another folder-based file repository to store our scripts.There are groups, committees, and conferences where universal data IDs are endlessly discussed. While not impossible, the effort needed for unification is tremendous. Data reconciliation will be an ongoing challenge especially when data accessibility isn’t solved. As such, availability of production and studio generated data is a must-have. The production-to-studio relationship requires the real time transfer of assets and data. When data is locked behind a proprietary file format or within data architecture with no accessible APIs, the constraints significantly reduce the data’s usefulness. Studios are hampered by the time it takes for software companies to develop their UIs. This significantly limits the software’s functionality and reduces its value at the studio level. Investing in software is a dual focus: accessibility of the data and assets generated by the software and the operational efficiencies provided by the use of the software. Both needs have to be serviced as production and studio each have different data and operational needs. However, this raises a subsequent problem of…Most third-party production software offerings service singular needs. The products are either loved by productions and disliked by studios, or loved by studios and disliked by productions. This is a real challenge because the benefit is in the marriage of the two. But as a SaaS provider, how do you prioritize user needs? By listening to the producer, the studio exec, the production assistant? We seek to answer this question, but it’s not easy practice. Most of the solutions lie within the above — separating the data from the UX. A production UX has to be attractive to a production user — someone singularly focused on executing his/her job for one production. A studio UX is macro, servicing the needs of a multitude of productions each with their own unique variances. Does this equate to doubling the investment? Perhaps from a UX perspective, but less so from a data perspective — that is, if the data is easily accessible. If a studio can centralize pertinent micro-level and macro-level data, the UX can be agnostic — built by us or built elsewhere. This is where third-party offerings can be very attractive: solutions that offer unique operational advantages in a killer UX with data and assets that are easily accessible at the studio level for macro-level needs.Our entire industry is on the precipice of embracing new technologies all in pursuit of providing the best tools to our productions with the goal of empowering them to focus on what matters most — creating great stories. There are significant opportunities to partner on solving operational challenges, and there are a lot of them. We seek to utilize modern technology to solve these problems by building support for a Netflix platform that can be used on any Netflix production. That platform is modular — one to utilize an agnostic offering of SaaS products, built by us, built by others. Ideally, industry-wide discussions on normalizing the data models would be ideal. Perhaps, one stop at a time. We work to solve the gaps that no studio has yet to solve, giving our creators the advantage of cutting-edge production management software. Through this line of thinking, we want to leverage what exists to move fast in solving areas that do not yet exist. But it’s a paradigm shift — both in how the collective industry builds and how our users adopt. We are still in our infancy, having barely scratched the surface and there’s still so much to be done. It’s an exciting time to be one-part entertainment and one-part tech. For more information on this initiative, check out the Netflix media blog here.",https://medium.com/netflix-techblog/netflixs-production-technology-voltron-ab0e091d232d?source=collection_home---4------3---------------------,0,netflix,python,NULL,2018-09-26
Enhancing the Netflix UI Experience with HDR,"by Yuji Mano, Benbuck Nason, Joe DragoSome of you have probably heard about HDR, or High Dynamic Range, which can produce a higher level of brightness, a wider range of colors, and an increased level of detail and accuracy that empowers visual storytellers to deliver more impactful experiences for people to enjoy. Netflix has been delivering HDR video for several years now, so it made sense for us to start exploring ways to provide the benefits of HDR to the imagery we showcase in the browse experience where viewers initially discover and connect with stories.We’re excited to roll out experimental HDR images for the very first time to the Netflix app on the latest generation of game consoles. These are images that take advantage of a display’s HDR capabilities (just like HDR video) and not to be confused with HDR photos that your phone or camera might take by combining multiple exposures to generate a high-contrast scene.We’re starting with a small test on two shows: The Innocents and Marvel’s Iron Fist. This modest “proof of concept” test will allow us to begin learning how to create and deliver HDR images globally to millions of people. Members with HDR capable TVs using Netflix on an HDR enabled PS4 Pro, Xbox One S or X will have a chance to see these experimental images as these titles get promoted on the main billboard of the app. And while it’s a small set of titles, it’s a big first step down the path of an exciting technical innovation. Reaching this initial milestone was a culmination of many months of research and countless discoveries along the way.‘What’s the motivation behind HDR images?’ If you haven’t already noticed, the industry has been in the process of a technological transition from SDR (Standard Dynamic Range) to HDR over the past several years. The promise of HDR is not just in the superior picture quality it can deliver but in its ability to better preserve the creative intent of content authors.Netflix has been at the forefront of bringing HDR video to our service, but the reality is that the still images associated with these titles that viewers see prior to playback remain left behind in SDR format.Furthermore, consumers have been subjected to and have adjusted to a world where SDR is not shown the way it was meant to be shown. In order for HDR TVs to maximize consumer appeal, SDR content is algorithmically adjusted (or boosted) to fill up the range of colors and brightness levels closer to the capabilities of the display. While HDR video may benefit from HDR TVs as it tries to accurately represent colors and preserve the creative intent of the source content, SDR is unfortunately subject to the inverse effect of being altered to ‘appear’ more HDR.We see HDR becoming the default format in the future. We want to deliver our still images with the same standard of picture quality we have for video by using brighter and more vibrant pixels that HDR can produce. We want to preserve creative intent for all graphics associated with our content just like we do for our video. That’s The Motivation.You might be thinking: ‘Netflix has been streaming HDR for a couple of years now. What’s the big deal? Just throw some HDR images into the UI and you’re done, right?’ It turns out that compositing and displaying HDR graphics along with video is harder than you’d think — there are a variety of technical hurdles yet to clear, and it will take some time for the hardware that can do this compositing to find its way into reasonably priced TVs and streaming accessories. Also, it wasn’t even initially clear to us how to create or store HDR images.We started off by asking some of the basic high-level questions:While HDR capable TVs and PC monitors are becoming more and more standard, the truth of the matter is that the concept of an HDR image and how it exists in the realm of our HDR capable devices is an almost uncharted territory. Various standards for HDR video have emerged over the years such as Dolby Vision, HDR10, HDR10+, Hybrid Log-Gamma, etc. However, there is no such clear standard defined yet for HDR images.The Problem is in trying to answer the above questions when there is no clearly paved path for us to follow. It requires exploration and carving out new solutions based on existing technologies and at times making wrong turns in the process. Ultimately, we arrived at our intended destination, but by no means is this meant to be a map for others to follow. We are sharing part of our journey in hopes to find others in the industry with similar stories and to help pave a solid path forward for HDR graphics and images to exist in our ecosystem.In order to begin answering what an HDR image is we first need to define what HDR really means in this context. HDR in UI graphics is essentially no different from HDR in video. The underlying value of HDR is in its ability to specify a wider color gamut (more saturated colors) and wider luminance range (more shades of dark and bright) in the pixels delivered. So simply put, an HDR image is an image that contains any of these pixel characteristics.We would love at this point to be able to show you an actual HDR image on your screen to show you exactly what brighter, more saturated colors look like, but there isn’t any current technology that lets us do that (which is the basis of this whole problem).One way to imagine this is to picture a single frame of video from our app that contains pixels outside of what its SDR equivalent source can express. To visualize these differences in expressibility, our team first started off by implementing a debug feature within the Netflix app that is able to highlight HDR pixels in real-time on the screen.The above are screenshots with the HDR highlighter feature enabled during video playback of our HDR-packed Netflix Original — Altered Carbon. We grayscale the entire screen, then highlight each “HDR pixel” with various colors, typically with stronger colors for pixels that we consider “more HDR” (or farther away from what’s possible with SDR).We highlight pixels based on the following conditions:After visualizing where the HDR pixels actually are in real time within our production Netflix experience, we can now begin to understand where the benefit of HDR really shines (literally speaking).Next came the problem of how to store these HDR pixels into an image file format. The difficulty in choosing a format is that there is currently no commonly accepted standard in the industry. With the understanding of what an HDR pixel consists of, we were then able to define several requirements for what an HDR image file format needed:Ultimately, image files are nothing more than a bunch of numbers stored in a container. With that basic concept in mind, it’s not clear what makes an HDR pixel different from an SDR pixel. For example, imagine an image format containing 8 bits for each red, green, and blue channel of a pixel (this is very typical currently). To represent maximum white in this image we would store RGB(255, 255, 255). But the question is: ‘How bright of a white is that? For maximum red of RGB(255, 0, 0), how saturated of a red is that?’Currently, images that don’t have explicit color profiles are usually treated as using the limits of the current display, meaning that these maximum white and red values would just end up being the whitest and reddest that the screen can handle. This has served us reasonably well for a long time and for many use cases, but since every screen has different capabilities in terms of color gamut and brightness, it gives different results on every screen. What if we want to differentiate between a pixel showing a white piece of paper and one showing a bright halogen light on various screens?In order to know what those numbers are intended to mean in terms of an actual color value and brightness intensity, we need to know the color space (color primaries) and the brightness (absolute luminance) those values represent. Furthermore, there needs to be a transfer function (tone reproduction curve) that helps represent brightness levels in a way that more closely matches human visual capabilities. As mentioned earlier, SDR images mainly specifies sRGB that defines Rec.709 color primaries at 80 nits using a gamma curve. For HDR images there is no common and widespread standard yet, though HDR10 video uses Rec.2020 color primaries at 10,000 nits and a PQ curve.In order to specify a wider gamut of colors (Rec.2020) and higher luminance (0~10,000 nits) we really need more code points to represent that additional range of values nicely. While it’s possible to stuff a wider range of values into existing 8-bit per channel formats, that leaves less code points in between which can result in banding artifacts, typically seen in gradients (see image below for an exaggerated example of this). Just like HDR video requires at least 10-bits (per HDR10 standard), so do our images.Unlike HDR formats used for photography, it is critical to be able to compress our images as much as possible without losing too much quality, so that they can be delivered efficiently to millions of members under all kinds of network conditions. Also, we need to be able to decode them very quickly on the device so we can get them displayed on the screen as fast as possible. For some images in our UI, we want to have the option of an alpha channel for transparency. Although we found a variety of file format options that are ‘HDR-capable’, most did not meet these criteria, lacked enough adoption by the community, or their open source libraries were not license-friendly for us.Before we can even think about delivering HDR images to devices we need to first create them somehow. There obviously need to be tools that artists and designers can use to author the HDR images that get injected into that pipeline in the first place. Artists need to be able to import/export these files and understand how to manipulate these assets within the desired color space and brightness levels.After exploring all of the requirements above we ultimately came to the realization that many common image file formats that have existed for years can already technically support the characterization of HDR pixels, but there are trade-offs for each one. The key realization was that we could add an ICC Profile to each image to signal the HDR color profile of that image.ICC Profile — The information needed to define and convert between color profiles (color primaries, tone reproduction curve, absolute luminance) can be encoded in an ICC Profile, which is an ISO-standardized binary file format that can be embedded in many image file formats. It’s also natively supported by industry standard tools like Adobe Photoshop, and by operating systems like Windows and macOS.We chose to adopt the ICC Profile as our source of truth for defining color profiles and to differentiate between SDR and HDR images while allowing both to coexist even if they are stored in the same file format. As long as the image file format supports embedding of the ICC Profile, it satisfies our current needs. The only tricky thing is that as far as we are aware, no existing tools pay attention to the standard absolute luminance value specified in the ICC Profile. This required us to develop some custom tooling that understands absolute luminance and can generate custom ICC Profile definitions with this luminance value defined for our needs.Potential Options — We compared a lot of possible options for use as our initial HDR image file format. Here is a table showing some of the pros and cons for a few formats that have broad support and are decodable by our devices:Source and Delivery Formats — Right away we concluded that 8-bits were simply not enough to maintain a desirable picture quality for most HDR images. We ultimately decided on 16-bit PNG for our source image format and JPEG2000 for our compressed delivery image format.The source image format is what the artist might export out of Photoshop after authoring an HDR image. It needs to be lossless to preserve the pristine source and at this stage the encoded size is not too much of a concern. 16-bit PNG made a lot of sense here considering its ubiquity in tools and devices.The delivery image format is what we actually download for decoding and rendering on devices. Unlike the source format, here we need to carefully balance the desire for small file size and fast decompression with the desire for optimal image quality. JPEG2000 was a good choice among the higher-bit depth compressed formats above, primarily due to its acceptable performance and availability of an open source library to support it.Emerging Formats — Our decision to use 16-bit PNG and JPEG2000 in this very first phase of HDR image experimentation is by no means final. We are expecting that new emerging formats may become more standard in the future and are ready to adjust based on the broader industry adoption of some new HDR image file format.One such format is HEIF, which can store single frames (among other things) from HEVC encoded HDR video. However, due to its relative infancy there is still lack of support for it in terms of license-friendly open source decoders, device hardware decoder support, and authoring tools that can import/export HEIF.There is also work being done on a new AV1 image format that looks promising, and we’re hopeful this royalty-free format will continue to make progress and gain the adoption needed for widespread use.Once we had actual HDR images to work with, the next step was to add HDR image awareness to our device software implementation. Our Netflix SDK and rendering engine is responsible for powering the Netflix TV Experience to all CE devices including our HDR game consoles.Color Profile Support — We first needed to make our rendering engine color profile aware. This means associating color profiles with our image textures and perform color profile conversion on-the-fly (using GPU shaders) while rendering them. This simply means that we can now support compositing and rendering of any color profile whether it be an sRGB (SDR), Display P3 (WCG), Rec.2020 (HDR), or other arbitrary color profiles, so long as the source image file format can communicate the appropriate color profile. Currently we rely on the ICC Profile embedded into the image file format, but if/when other industry standard HDR image formats emerge, we should be able to support those as well assuming they will have some way to define their color profiles (whether this be explicit in the format itself or implicit through some standard).Higher Bit Depth Surfaces — In order to properly load image files with per-channel bit depths greater than 8, we must support surfaces (textures and render targets) whose bit depth is at least as large. For color channels (red, green, and blue), 10 bits is enough if you store these values with the PQ curve. However, current hardware requires these pixels are stored in 32 bits, leaving only two bits for the alpha channel, which isn’t sufficient for complex UI composition (feathered edges, cross fades, etc). Because of this, we must use even larger bit depth surfaces (64 bits per pixel) than actually required. The additional cost in memory and bandwidth is not cheap, as you can imagine.Blending Curves — The other problem presented by HDR is the introduction of the PQ curve in addition to the long understood gamma curve that is associated with SDR sRGB images. In order to properly blend images with different curves in a mathematically correct way, you must first de-curve (or linearize) the values. However, existing sampling and blending hardware does not compensate for tone curves which has resulted in ‘incorrect’ gamma blending of sRGB for all these years. To make matters even worse, this ‘incorrectness’ is widely understood and unfortunately now expected within the current state of SDR graphics pipelines. But now with the introduction of HDR and an even steeper curve such as PQ, these blending issues have far greater negative visual effect. In order to work around this, we only enable blending in hardware when compositing surfaces with a simple gamma curve, and convert that final surface into the PQ curve as a separate step using color conversion shaders with blending disabled. This maintains years of blending expectations for existing SDR images while blending HDR correctly, but at the cost of additional memory and GPU time.The latest Netflix update on game consoles already includes all the changes described above. With the end of the HDR processing pipeline ready to consume HDR images, the final piece of the puzzle was getting our UI to serve the first set of experimental HDR images from our image asset CDNs.The above is a screenshot of our HDR image billboard currently live in our production UI with the HDR highlighter feature enabled. It’s visible proof that the Marvel’s Iron Fist billboard image packs quite the punch with HDR pixels!Our exploration into bringing HDR images into the UI has definitely pushed us to various limits of the current state of technology that exists throughout this ecosystem. But we are still far from being done, with many more challenges anticipated ahead.HDR Workflows — While we have a solution for introducing HDR into parts of our image asset workflow pipeline, we have much more work to do internally within Netflix to introduce the concept of HDR into the complete end-to-end workflow. You can start to imagine what that workflow might look like when working with a variety of content production studios and design firms that generate a variety of image assets that then need to be imported into our own internal systems for further processing until they reach the user. We have product guidelines, art asset templates and various tools in place to support all of that, none of which currently have any concept of HDR images. Even the authoring of our experimental images required our engineers to explain to our creative designers how we might be able to generate HDR assets using their own tools like Photoshop. The concept and experience working with HDR images from the creative design perspective is all still very new and foreign.Ecosystem Support — Overcoming this creative learning curve is also hindered by the chicken-or-the-egg problem introduced by current limitations presented by devices. HDR support in terms of PCs and monitors is definitely starting to emerge now at an increased rate. However, while HDR displays are becoming more abundant, OS and application support that allows the artists to actually see HDR pixels on those displays is not there yet. Imagine asking an artist to create a beautiful image of a rainbow on a screen that could only show shades of gray. This is similar to what we are asking for with HDR images: we want them to create images with brighter and more saturated colors than they can possibly see on their screen.Device Hardware — Furthermore, the limitations in current generation devices go further down into CE hardware architecture. We are currently able to explore HDR images on the game consoles as they provide powerful but flexible hardware designed to push games to their graphical limits. They are built such that we can support larger bit-depth textures and composite our UI graphics and video with relative ease. The same luxury does not exist on other CE devices like TVs and mobile phones that have very specialized hardware built specifically for their graphics and video needs. The step up from 8-bits to 10-bits and above is also a huge one — especially when 8-bits have been the standard for so long. One of our focus areas is to help our partner device ecosystem to enable support for this feature in the coming years.We have been reaching out to other industry leaders to have discussions and to share our knowledge and learnings around this topic. Without industry-wide collaboration and momentum, we risk being stuck with SDR images in an HDR world. We hope we can work together to help push the industry forward so that viewers can enjoy an even richer experience on every device.Netflix works hard to be at the forefront of innovation, especially when it comes to delivering the best-in-class experience for our global members. While we are excited about reaching this milestone of delivering HDR images to select game consoles, we are eager to push the technology and capabilities further into many more devices and experiences.We are always open to new ideas on how we can improve. Interested in learning more or contributing? Please reach out!",https://medium.com/netflix-techblog/enhancing-the-netflix-ui-experience-with-hdr-1e7506ad3e8?source=collection_home---4------4---------------------,0,netflix,,NULL,2018-09-24
Quasi Experimentation at Netflix,"Colin McFarland, Michael Pow, Jeremy GlickExperimentation informs much of our decision making at Netflix. We design, analyze, and execute experiments with rigor so that we have confidence that the changes we’re making are the right ones for our members and our business. We have many years of experience running experiments in all aspects of the Netflix product, continually improving our UI, search, recommendations, video streaming, and more. Consequently, we’ve dramatically increased the maturity of the platform for A/B testing and the culture surrounding it. However, while A/B testing is a useful tool for many types of hypotheses, there are a few reasons that some hypotheses cannot be tested with an A/B test:For example: We may aim to better understand the interaction effects of promoting a specific title in our member in-product experience while also doing so via out-of-home marketing efforts. Are the effects additive or do they cannibalize from one another?How are we to measure the impact of the change? Usually, we’d like to run a classic individual-level randomized experiment but randomizing which individuals see a billboard ad, as an example, is not possible. However, while we cannot randomly assign individuals, we can randomly choose some cities within which to show billboards and other cities to leave without.Now we can look for changes in the test regions at-specific-times as compared to the control regions at-specific-times. Since random changes happen all the time, we need to look historically to figure out what kinds of changes are normal so we can identify the impact of our test. We call this a quasi-experiment, because groups of individuals are assigned based on location rather than assigning each individual at random, and without the individual randomization there is a much larger chance for imbalance due to skewness and heterogeneous differences. The results of a quasi-experiment won’t be as precise as an A/B, but we aspire towards a directional read on causality.How can we get more precise results from studies like the previous out-of-home marketing example? The majority of our development to date has been focused on improved test designs, not improved statistical models. Better models help most when there is rich data to support them, and for the marketing study, we know almost nothing about new members before they sign up. Instead, we can improve statistical power by increasing the number of comparisons we make. When the marketing materials are taken down, it is another opportunity to measure their impact, assuming a return to baseline signup rates.Then we can put up out-of-home marketing in the unexposed regions, measure its impact, and take it down again for yet another measurement. In even more complex designs, we might not have any pure “control” regions at all, but if we ensure that there are always regions that are not changing at the time of intervention, we can still get a measurement of the impact.In other situations, we might benefit more from more sophisticated models. For example, Netflix runs a content delivery network called Open Connect to stream content to our users. When we try to improve our delivery systems, we often need to make the change on an entire Open Connect server, without being able to randomize individual streams. Testing the impact of the improvements becomes another quasi-experiment, randomized at the server level. But we know much more about what is happening on the servers than we do about prospective members living in different cities. It is likely that we can make major improvements on our estimates through improved modeling as well as improved test design. For example, we could use pre-test information about which kinds of content are more or less likely to be served from each server; animation in SD is easier to stream successfully than action movies in UHD. Should we use blocking or matching designs? Or control for these differences using covariates?Over the past year, our marketing team has started to run a lot more quasi experiments to measure the business impact of marketing movies & TV shows across various canvases (in the member experience, in TV commercials, in out-of-home advertising, in online ads, etc.) towards the broader goal of maximizing the enjoyment of our content by our members.Our success with quasi experimentation in marketing at Netflix subsequently spurred interest across many other teams to scale their own quasi experiment efforts, i.e. we’ll be running a lot of quasi experiments. To scale further, we’re developing a new product, called “Quasimodo” within the wider Experimentation Platform, to automate some aspects of the scientists workflow so we can free up our scientists and run more quasi-experiments in parallel.We have 3 key ideas shaping our focus for Quasimodo:This is the first blog in a series around quasi-experiments here at Netflix. In the next blog we’ll look deeper at experimental design and analysis of quasi experiments and share some of the features we’ve developed already to help scale quasi experiments. If these are the sort of challenges you’d like to help us with, we’re hiring.",https://medium.com/netflix-techblog/quasi-experimentation-at-netflix-566b57d2e362?source=collection_home---4------5---------------------,0,netflix,,NULL,2018-09-19
Netflix at the Spinnaker Summit 2018,"Author: Andy GloverSince releasing Spinnaker as an open source project in late 2015, the Spinnaker user and contributor community has grown by leaps and bounds. Our strong partnership with Google and other companies has led to a plethora of innovations and features that enable rapid, safe, and reliable delivery to AWS, GCP, Azure, Pivotal Cloud Foundry, DC/OS, Kubernetes, Titus, and Oracle Cloud. Moreover, internally at Netflix, Spinnaker continues to increase in overall scope with the incorporation of globally safe data delivery, library management, and Open Connect appliance firmware delivery. Indeed, the many benefits of Continuous Delivery with Spinnaker have proven invaluable for Netflix and myriad other companies.Last year we hosted the inaugural Spinnaker Summit at Netflix and had a blast learning from various folks within the Spinnaker community about leveraging Spinnaker, adding new features, and customizing Spinnaker to fit diverse and unique needs. Consequently, we’re excited to sponsor this year’s Spinnaker Summit in Seattle, WA on Oct 8th and 9th.This year’s Summit promises to be an amazing two days, as there are over 45 presentations, panel discussions, labs, and breakouts. There are speakers from around the globe representing companies such as:There are talks covering the gamut of Spinnaker features including:What’s more, there’s a half-day Spinnaker bootcamp, a half-day Automated Canary Analysis workshop, office hours focusing on operating Spinnaker, Spinnaker best practices, and contributing to Spinnaker. The talks will be recorded and posted on the Spinnaker Youtube channel following the Summit.The Netflix Delivery Engineering team will be attending along with team members from our Edge, Titus, Open Connect, and Science and Algorithms teams. If you are currently using or even considering using Spinnaker then you should join us. You can register at spinnakersummit.com; we hope to see you there in October!",https://medium.com/netflix-techblog/netflix-at-the-spinnaker-summit-2018-ac694692d007?source=collection_home---4------6---------------------,0,netflix,,NULL,2018-09-18
Streaming Video Experimentation at Netflix: Visualizing Practical and Statistical Significance,"by Martin TingleyStreaming video experimentation at Netflix seeks to optimize the Quality of Experience (QoE for short) of the videos we stream to our 130 million members around the world. To measure QoE, we look at a wide variety of metrics for each playback session, including play delay; the rates of rebuffers (playback interruptions when the video buffer empties), playback errors, and user-initiated aborts; the average bitrate throughout playback; and Video Multimethod Assessment Fusion, a measure of perceptual video quality developed here at Netflix.Many of our experiments are “systems tests”: short-running (hours to a week) A/B experiments that seek to improve one QoE metric without harming others. For example, we may test the production configuration of the adaptive streaming algorithm, which selects video quality based on device capabilities, resolution limits based on the Netflix plan tier, and time-varying network conditions, against a new parameter configuration that aims to reduce play delay without degrading other metrics. Although each test that results in the rollout of a new production experience may only incrementally improve one or two QoE metrics, and only for some members, over time the cumulative impact is a steady improvement in our ability to efficiently deliver high quality streaming video at scale to all of our diverse members.The treatment effects in these streaming experiments tend to be heterogeneous with respect to network conditions and other factors. As an example, we might aim to reduce play delay via predictive precaching of the first few seconds of the video that our algorithms predict a member is most likely to play. Such an innovation is likely to have only a small impact on the short play delays that we observe for high quality networks — but it may result in a dramatic reduction of the lengthy play delays that are more common on low throughput or unstable networks.Because treatments in streaming experimentation may have much larger impacts on high (or low) values of a given metric, changes in the mean, the median, or other summary statistics are not generally sufficient to understand if and how the test treatment has changed the behaviour of that metric. In general, we are interested in how the distributions of relevant metrics differ between test experiences (called “cells” here at Netflix). Our goal is to arrive at an inference and visualization solution that simultaneously indicates which parts of the distribution have changed, and both the practical and statistical significance of those changes.We’ve had success summarizing distributions of metrics within test cells, and how those distributions differ between cells, using quantile functions and differences between quantile functions, with uncertainty derived from fast bootstrapping procedures. Our engineering colleagues have quickly adapted to test results being reported via quantile functions, as they can tap into pre-existing intuition from familiar concepts.The Quantile FunctionThe quantile function Q(𝜏) is the inverse of the cumulative distribution function for a given random variable. It accepts as argument a probability 𝜏 (between zero and one) and returns a threshold value such that draws of the random variable are less than this value with probability 𝜏. Formally,where F(x) is the cumulative distribution function for the random variable X. Q(0.50) returns the median, Q(0.95) returns the 95th percentile, and so forth.Quantile functions are a great way to summarize distributions, as statisticians and non-statisticians alike have a fair amount of intuition about them. Concepts like medians, deciles, and percentiles — all special cases of quantiles — are mainstays of popular press reporting on economic matters (“What Percent Are You?”) and are familiar from standardized test scoring.Here is a simulated example (with no relation to actual values, and with y-values suppressed) of data that might result from a streaming experiment aimed at reducing play delay for some subset of members:In this example, Cell 1 corresponds to the current production experience, and the other Cells correspond to three proposed parameter configurations. Note that the y-axis is in units of seconds, and that point estimates of the median and other familiar quantiles can easily be read from the plot. In this case, the quantile functions for the Cells 1 and 4 are nearly identical, whereas Cells 2 and 3 feature decreases and increases, respectively, in all quantiles of play delay as compared with Cell 1.Next, we need to determine if the differences between the treatment cells and the control cell are both practically and statistically significant.Practical and Statistical Significance in One ChartTo quantify how distributions of a given metric differ between the cells, we plot the difference between each treatment cell quantile function and the quantile function for the current production experience (Cell 1).As the Y-axes of these “delta quantile” plots are in the original units of the metric, all of the intuition that we have about that metric can be brought to bear to understand the practical significance of the differences between test cells. In this simulated example above, Cell 2 successfully reduces the long tail of play delay: the upper quantiles of play delay are about 5 seconds lower than in Cell 1, indicating that we’ve improved this metric for viewers with the worst experience. This is an easy to intuit summary of the test result.To quantify the statistical significance of differences between the quantile functions, we use two different bootstrapping procedures. First, to understand the variability within the production experience, we bootstrap that cell against itself by resampling (with replacement) twice, estimating the quantile function in each case, and then calculating the delta quantile function. Repeating this procedure, and then calculating confidence intervals as a function of 𝜏, yields the uncertainty envelope plotted above (some more technical details are below). Provided a balanced experimental design, these confidence intervals tell us about the distribution of the delta quantile function under the null assumption that the distribution of the metric is invariant across the test cells.In our simulated play delay example, the quantile function for Cell 4 is not statistically significant from that of Cell 1. In contrast, Cells 2 and 3 feature, respectively, statistically significant reductions and increases in most quantiles as compared with Cell 1, with the differences largest in magnitude for the upper quantiles. One advantage of this approach to uncertainty quantification is that we can quickly assess the significance of each test treatment with respect to the production experience. A downside is that the variability in the estimates of the treatment quantile functions are not taken to account.As a second uncertainty quantification, in this case for a specific delta quantile function, we bootstrap each treatment cell against the production cell by resampling (with replacement) from each; estimate the quantile functions; take differences; and then calculate confidence envelopes. This procedure, which remains valid even if the sample sizes differ between cells, takes into account the uncertainty in the estimated quantile functions of both the production experience and treatment experience, and will yield wider, more conservative confidence envelopes if the uncertainty in the treatment cell quantile function is larger than that of the control cell. The downside is that it becomes unruly to visualize more than one uncertainty envelope in a single plot.Here is the difference between the Cell 2 and Cell 1 quantile functions, along with the uncertainty envelope, for our simulated play delay example:This one plot provides an illustration of the statistical (uncertainty envelope) and practical (y-axis is units of seconds) significance of the test treatment, and how they vary across the quantiles. Simple questions like “how much has the 95th percentile changed, and is that change significant?” can be answered by inspection.Some technical detailsTwo interesting technical aspects of our bootstrap procedure are the significance adjustment to account for comparisons across many quantiles, and the speed gains achieved by operating on compressed data structures.Multiple Comparisons. The confidence envelopes on the delta quantile functions are initially calculated pointwise: for each value of 𝜏, we take the 0.025 and 0.975 percentiles of the bootstrap samples. Such intervals have, nominally, 95% probability of covering the true change at each value of 𝜏. What we want are simultaneous, or path-wise, intervals that feature nominal 95% probability of the uncertainty envelope covering the true function in its entirety. To produce simultaneous uncertainty intervals, we adjust the point-wise confidence level according to the Bonferroni Correction — using an estimate of the number of independent values of the delta-quantile function (see Solow and Polasky, 1994):Here, r(i,j) is the sample correlation coefficient between the delta-quantile function evaluated at the ith and jth values of 𝜏, calculated across the bootstrap samples, and N is the number of 𝜏 values at which the quantile functions are estimated. Note that as N increases, the correlations r(i,j) for nearby 𝜏 values will likewise increase, with the result that the value of Neq saturates for sufficiently large values of N, and the simultaneous uncertainty envelopes do not continue to grow wider as N increases.Fast Bootstrapping on Big Data. Streaming experiments at Netflix can involve tens of millions of data points, and our goal is to perform the statistical analysis on the fly, so reports can be interactive. We therefore require that the bootstrapping procedures described above be very fast, even on large data sets.Our approach is to approximate the data for each test cell using a compressed data object with a limited number of unique values. In particular, we approximate each empirical quantile function using several thousand evenly spaced points on the unit interval. Intuitively, this approximation assumes that changes in the metric smaller than dQ(𝜏) = Q(𝜏 + 1/N) — Q(𝜏) , where N is of order several thousand, are not of practical importance. Note that dQ(𝜏) varies with 𝜏; in the context of the play delay example, the distribution is right-skewed so that dQ(𝜏) increases with 𝜏. Intuitively, we assume that small changes in play delay may be important for those members who generally experience short play delays, whereas the smallest practically important change is a much larger for those members accustomed to lengthy play delays.The data for each test cell is then represented as a set of (value, count) pairs, and we can bootstrap on the counts using draws from a multinomial. Further speed gains are achieved by exploiting the Poisson approximation to the Multinomial, an established approach to bootstrapping that is also embarrassingly parallel.As a result of the approximation, the computational cost of the bootstrapping is independent of the size of the original data set, and is set instead via the number of unique values used to approximate the original quantile function. The only step that scales with the cardinality of the original data is the compression step, which, in our implementation, requires a global sort and linear approximation.Most any approach to data binning or compression, such as histograms or data sketches like the t-digest, can be used for fast bootstrapping on large data sets. In all cases, the resampling required by the bootstrap can be achieved via the Poisson approximation to the multinomial. As t-digests can readily be combined, the next step we are exploring is to pre-calculate t-digests for every possible combination of dimension filters that may be of interest for a given experiment, such as device type and Netflix subscription plan. When an analyst selects a particular slice of the data, the relevant t-digests can be combined and used as inputs to the fast bootstrapping algorithm.SummaryQuantile functions, and the difference in quantile functions between test experiences, have proven to be meaningful and intuitive tools for summarizing how the distributions of streaming Quality of Experience metrics, such as play delay or average bitrate, differ between test experiences. A key advantage of quantile and delta quantile functions is that the y-axes are in the meaningful units of the metric: plots readily provide a sense of practical significance to our engineering partners. Statistical significance is provided by confidence intervals derived from fast bootstrapping achieved by reducing the cardinality of the original data.This is just one way that we are improving streaming experimentation at Netflix, and the quantile function is a good summary of only some of our metrics. We are actively working on fast bootstrapping techniques for ratios, rates, zero-inflated observations, and other challenging metrics. Another line of our research involves fitting flexible conditional quantile functions in one or two dimensions, with the required regularization parameter estimated via cross validation and uncertainty estimated using the distributed “bag of little bootstraps.” More to follow in future posts.We are always open to new ideas on how we can improve. Interested in learning more or contributing? Please reach out!",https://medium.com/netflix-techblog/streaming-video-experimentation-at-netflix-visualizing-practical-and-statistical-significance-7117420f4e9a?source=collection_home---4------7---------------------,0,netflix,"machinelearning,tensorflow,python",NULL,2018-09-17
Announcing the Netflix Post Technology Alliance,"by Chris Fetner and Richard SmithYou’re working on the next great show or film that will deliver to Netflix. You know Netflix has a high quality bar for its content and you want the delivery to go smoothly. Do you know if the tools you have invested in can deliver to that quality? Will they connect to the sophisticated systems Netflix has in place? Until now, it hasn’t been easy to know that.As an artist, it’s important that your needs drive the technical innovation in the tools you use. The Netflix Post Technology Alliance will act as a conduit for Netflix to support creatives and technology partners on new feature sets for tools, so artists can spend their energy on what matters most — the storytelling.Today, we’re launching a new logo program to help with this. When you see the Netflix Post Technology Alliance logo, you’ll know the product meets Netflix technical and delivery specifications today, and will continue to do so in the future.Manufacturers of products bearing this logo are closely partnered with Netflix. They have early access to the Netflix technical roadmap and collaborate with Netflix on technical support, training, and updates. As Netflix technical requirements evolve, you can be assured products bearing this logo will evolve in step with us.Today, the Netflix Post Technology Alliance includes the industry leaders in four categories: cameras, creative editorial, color grading, and IMF packaging, with products from Adobe, Arri, Avid, Blackmagic Design, Canon, Colorfront, Fraunhofer IIS, Filmlight, Marquise Technologies, MTI Film, Ownzones, Panasonic, Red Digital Cinema, Rohde & Schwarz, and Sony. Visit pta.netflixstudios.com for a complete list of products.This is just a start, though. Over time, we’ll add more products in these categories, and we’ll expand the categories to include sound production, dubbing, and other disciplines in post-production. Any product that generates or manages any kind of sound data, image data, or metadata from production through post-production is a candidate for the Netflix Post Technology Alliance logo.There are two answers to this question:First, we don’t expect this program to cover every type of tool used throughout a production. As one example, we don’t expect lenses to be included. Lenses do generate metadata that’s useful in post, but since that metadata is incorporated into camera metadata, cameras themselves are the more suitable product for this logo.Second, we’re in the early stages. Many more of the other tools you use every day will be added to the program. If you are a tool & technology maker, or on an advisory board, recommend that the company consider submitting its product for inclusion.Finally, this is not a prescription for which products to use. As an artist, you should use the tools that make sense for your production, are best suited to your workflows, and serve your creative interests. With that in mind, this logo is an identifier to quickly tell you a product has been vetted for delivery to Netflix, and that the company who makes the product is committed to ongoing support and innovation.Empowering our creative partners is incredibly important to us at Netflix, and we look forward to building a more seamless experience from production through post-production. More information on the program, how to submit products for inclusion and FAQ, are available pta.netflixstudios.com.",https://medium.com/netflix-techblog/netflix-ptap-fcb0019b63fb?source=collection_home---4------8---------------------,0,netflix,"react,css,backend,frontend",NULL,2018-09-14
Keystone Real-time Stream Processing Platform,"Keystone Stream Processing Platform is Netflix’s data backbone and an essential piece of infrastructure that enables engineering data-driven culture. While Keystone focuses on data analytics, it is worth mentioning there is another Netflix homegrown reactive stream processing platform called Mantis that targets operational use cases. We’ll discuss Mantis and its important role in the Netflix ecosystem in a future post.Today, the Keystone platform offers two production services:In this post, we’ll go over some of the challenges, design principles, our platform mindset, high level architecture, and finally our visions and core values the platform offers to Netflix.Anatomy of a single streaming job:…and the platform manages these jobs:Netflix services 130 million subscribers from 190+ countries. The streaming platform processes trillions of events and petabytes worth of data per day to support day to day business needs. This platform is expected to scale out as subscribers continues to grow.Keystone Routing Service: this service is responsible for routing any events to managed sink per user configuration. Each delivery route is realized by an embarrassingly parallel stream processing job. Users may define optional filter and/or projection aggregations. Events are eventually delivered to a storage sink for further batch/stream processing with at-least-once delivery semantics. Users may choose different latency and duplicate tradeoffs.Stream Processing as a Service: SPaaS platform has only been in production for about a year, yet we have seen tremendous engineering interests, as well as a wide variety of requirements. Below is a summary of some common asks and tradeoffs.Keystone supports thousands of streaming jobs, targeting wide problem spaces ranging from data delivery, data analytics, all the way to enabling microservices architectural patterns. Due to the diverse nature of the the streaming jobs, in order to provide meaningful service level guarantees to each user, the infrastructure need to provide runtime & operational isolation, while at the same time, minimizing shared platform overhead.Although majority of the streams have fixed traffic pattern, we have to design the system to prepare for sudden changes (i.e. spikes due to a popular show coming online or unexpected failure scenarios), and be able to adapt and react to them in an automated fashion.Netflix operates its microservices fully in the cloud. Due to the elastic, constant changing, higher failure probability characteristics of the cloud. We need to design the system to be able to monitor, detect and tolerate failures all the way from network blips, instance failure, zone failure, cluster failure, inter-service congestion/backpressure, to regional disaster failures, etc.The platform currently services thousands of routing jobs and streaming applications. It’s cost prohibitive to rely on platform team to manually manage all of the streams. Instead, the user should be responsible to declare the lifecycle details of the jobs, and the infrastructure should automate as much as possible.We’d like to be able to develop and deploy changes quickly, multiple times a day. We’d also like to allow our users to confidently use the service with the same level of agility.One of the primary goals of the platform is to enable other teams to focus on business logic, making experimentation, implementation, operation of stream processing jobs easy. By having a platform to abstract the “hard stuff”, removing complexities away from users, this would unleash broader team agility and product innovations.On a high level, we strive to enable our user to:To enable user to focus on business logic without having to worry about the complexity involved in a distributed system or mundane details of some pre-existed solution, it is our goal to provide a rich set of composable operators that can be easily plugged into a streaming job DAG.Furthermore, streaming jobs themselves can become building blocks for other downstream services as well. We work with some of our partner teams to build “Managed Datasets” and other domain specific platforms.From our platform downwards, we also strive to integrate deeply with Netflix software ecosystem by leveraging other building blocks such as container runtime services, platform dynamic configuration, common injection framework, etc. This does not just help us to build a service based on other existing solutions, it also make development & operation environment familiar to our users.Any complex distributed system inherently comes with certain limitations, thus designings of such system should take considerations of various tradeoffs, i.e. latency vs duplicates, consistency vs availability, strict ordering vs random ordering etc. Certain use cases may require different combinations of these tradeoffs, so it’s essential that platform should expose the knobs and allow individual user to customize and declare the needs to the system.Failure is a norm in any large scale distributed system, especially in the cloud environment. Any properly designed cloud-native system should treat failures as a first class citizen.Here are some important aspects that impacted our design:Between users and platform: user should be able to declare the “goal state” via platform UI or API. The goal states are stored in a single source of truth store, the actual execution to move from “current state” towards the “goal state” is handled by the platform workflow without interaction with users.Between control plane and data plane: Control plane is responsible for workflow orchestration/coordination while data plane does the heavy lifting to make sure things happens and stay in desired state.Between different subcomponents: Each component is responsible for their own work and states. Each component lifecycle is independent.Runtime infrastructure: stream processing jobs are deployed on open sourced Netflix Titus Container runtime service, this service provides provisioning, scheduling, resource level isolations (CPU, Network, Memory), advanced networking etc.With considerations to aforementioned challenges and design principles, we closed on a declarative reconciliation architecture to drive a self-servable platform. On a high level, this architecture allows user to come to the UI to declare desired job attributes, the platform will orchestrate and coordinate subservices to ensure goal states are met as quickly as possible, even in face of failures.This following section covers the high level architecture and lightly touches various areas of the design. We’ll share more in depth technical details and use cases in future follow up posts.The declarative reconciliation protocol is used across the entire architectural stack, from control plane to data plane. The logical conclusion for taking advantage of this protocol is to store a single copy of user declared goal states as durable source of truth, where all other services will reconcile from. When state conflict arises, either due to transient failures or normal user trigger actions, the source of truth should always be treated as authoritative, all other versions of the states should be considered as the current view of the world. The entire system is expected to eventually reconcile towards the source of truth.Source of Truth Store is a durable, persistent storage that keeps all the desired state information. We currently use AWS RDS. It is the single source of truth for the entire system. For example, if a Kafka cluster blows away because of corrupted ZK states, we can always recreate the entire cluster solely based off the source of truth. Same principles apply to the stream processing layer, to correct any processing layer’s current states that deviates from its desired goal states. This makes continuous self healing, and automated operations possible.Another advantage we can take from this protocol design is that operations are encouraged to be idempotent. This means control instructions passed from user to control plane and then to the job cluster, inevitable failure conditions will not result in prolonged adversary effect. The services would just eventually reconcile on its own. This also in term brings operational agility.Control plane facilitates orchestration workflow through interactions with Netflix internal continuous deployment engine Spinnaker. Spinnaker internally abstracts integration with Titus container runtime, which would allow control plane to orchestrates deployment with different tradeoffs.A flink cluster is composed of job managers and task managers. Today, we enforce complete job instance level isolation by creating independent Flink cluster for each job. The only shared service is ZooKeeper for consensus coordination and S3 backend for storing checkpoint states.During redeployment, stateless application may choose between latency or duplicate trade-offs, corresponding deployment workflow will be used to satisfy the requirement. For stateful application user can choose to resume from a checkpoint/savepoint or start from fresh state.For routing jobs: through self service, a user can request a stream to produce events to, optionally declare filtering / projection and then route events to managed sink, such as Elasticsearch, Hive or made available for downstream real-time consuming. Self service UI is able to take these inputs from user and translate into concrete eventual desired system states. This allows us to build a decoupled orchestration layer that drives the goal states, it also allows us to abstract out certain information that user may not care, for example which Kafka cluster to produce to, or certain container configurations, and gives us the flexibility when it’s needed.For custom SPaaS jobs, we provide command line tooling to generate flink code template repository and CI integration etc.Once user customizes and checks in the code, the CI automation will be kicked off to build docker image, register the image and configurations with platform backend, and allow user to perform deployment and other administrative operations.We are currently focusing on leveraging Apache Flink and build an ecosystem around it for Keystone analytic use cases. Moving forward, we have plans to integrate and extend Mantis stream processing engine for operational use cases.To help our users to increase development agility and innovations, we offer a full range of abstractions that includes managed connectors, operators for users to plug in to the processing DAG, as well as integration with various platform services.We provide managed connectors to Kafka, Elasticsearch, Hive, etc. The connectors abstract away underlying complexity around custom wire format, serialization (so we can keep track of different format of payload to optimize on storage and transport), batching/throttling behaviors, and is easy to plug into processing DAG. We also provide dynamic source/sink operator that allows user to switch between different sources or sinks at runtime without having to rebuild.Other managed operators includes filter, projector, data hygiene with easy to understand custom DSL. We continue to work with our users to contribute proven operators to the collection and make them accessible to more teams.Multi-tenancy configuration management is challenging. We want to make configuration experience dynamic (so users do not have to rebuild/reship code), and at the same time easily manageable.Both default managed and user defined configurations are stored along with application properties files, we’ve done the plumbing to allow these configurations to be overriable by environment variable and can be further overridden through self-service UI. This approach fits with the reconciliation architecture, which allows user to come to our UI to declare the intended configs and deployment orchestration will ensure eventual consistency at runtime.Failures are inevitable in distributed systems. We fully expect it can happen at any time, and designed our system to self heal so we don’t have to be woken up in the middle of night for incident mitigations.Architecturally, platform component services are isolated to reduce blast radius when failure arises. The reconciliation architecture also ensures system level self-recovery by continuous reconciling away from drift behavior.On individual job level, the same isolation pattern is followed to reduce failure impact. However, to deal and recover from such failures, each managed streaming job comes with a health monitor. The health monitor is an internal component runs on in Flink cluster which is responsible for detecting failure scenarios and perform self-healing:Again, failures are inevitable, sometimes user may be required to backfill or rewind the processing job.For source data that is backed up into data warehouse, we have built functionality into the platform to allow dynamically switching source without having to modify and rebuild code. This approach comes with certain limitations and is only recommended for stateless jobs.Alternatively, user can choose to rewind processing to a previous automatically taken checkpoint.All individual streaming jobs comes with a personalized monitor and alert dashboard. This helps both platform/infrastructure team and application team to diagnose and monitor for issues.As platform and underlying infrastructure services innovate to provide new features and improvements, the pressure to quickly adopt the changes comes from bottom up (architecturally).As applications being developed and productionized, the pressure for reliability comes from top down.The pressure meets in the middle. In order for us to provide and gain trust, we need to enable both platform and users to efficiently test the entire stack.We are big believers in making unit tests, integration tests, operational canary and data parity canary accessible for all our users, and easy to adopt for the stream processing paradigm. We are making progress on this front, and still seeing lots of challenges to solve.In the past year and half, the Keystone stream processing platform has proven itself beyond the trillion events per day scale. Our partner teams have built and productionized various analytical streaming use cases. Furthermore, we are starting to see higher level platforms being built on top.However, our story does not end here. We still have a long journey ahead of us to fulfill our platform vision. Below are some of the interesting items we are looking into:This post presented a high level view of Keystone platform. In the future, we will follow up with more detailed drill downs into use cases, component features, and implementations. Please stay tuned.Authored by Zhenzhong Xu on behalf of Real-time Data Infrastructure and Mantis teams:Andrew Nguonly, Allen Wang, Cody Rioux, Gim Mahasintunan, Indrajit Roy Choudhury, Jaebin Yoon, Jeff Chao, Jigish Patel, Kunal Kundaje, Manas Alekar, Mark Cho, Monal Daxini, Neeraj Joshi, Nick Mahilani, Piyush Goyal, Prashanth Ramdas, Steven Wu, Zhenzhong Xu",https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a?source=collection_home---4------9---------------------,0,netflix,java,NULL,2018-09-10
Engineering to Improve Marketing Effectiveness (Part 2) — Scaling Ad Creation and Management,"by Ravi Srinivas Ranganathan, Gopal KrishnanOur world-class marketing team has the unique task of showcasing our growing slate of Original Movies and TV Shows, and the unique stories behind every one of them. Their job is not just about promoting awareness of the content we produce, but an even harder one — of tailoring the right content, with the right message to qualified non-members (acquisition marketing) and members — collectively, billions of users who are reached by our online advertising. These ads will have to reach users on the internet on a variety of websites and publishers, on Facebook, Youtube and other ad platforms.Imagine if you had to launch the digital marketing campaign for the next big blockbuster movie or must-watch TV show. You will need to create ads for a variety of creative concepts, A/B tests, ad formats and localizations, then QC (quality control) all of them for technical and content errors. Having taken those variations into consideration, you’ll need to traffic them to the respective platforms that those ads are going to be delivered from. Now, imagine launching multiples titles daily while still ensuring that every single one of these ads reaches the exact person that they are meant to speak to. Finally, you need to continue to manage your portfolio of ads after the campaign launches in order to ensure that they are kept up to date (for eg. music licensing rights and expirations) and continue to support phases that roll in post-launch.There are three broad areas that the problem can be broken down into :Overall, if you looked at the problem from a purely analytical perspective, we need to find a way to efficiently automate and manage the scale resulting from textbook combinatorial explosion.Total Ad Cardinality ≈Titles in Catalog x Ad Platforms x Concepts x Formats x A/B Tests x LocalizationsOur approach of handling the combinatorics to catch it at the head and to create marketing platforms where our ad operations, the primary users of our product, can concisely express the gamut of variations with the least amount of redundant information.Consider the ads below, which differ along a number of different dimensions that are highlighted.If you were to simply vary just the unique localizations for this ad for all the markets that we advertise in, that would result in ~30 variations. In a world with static ad creation, that means that 30 unique ad files will be produced by marketing and then trafficked. In addition to the higher effort, any change that needs to address all the units would then have to be introduced into each of them separately and then QC-ed all over again. Even a minor modification in just a single creative expression, such as an asset change, would involve making modifications within the ad unit. Each variation would then need to go through the rest of the flow involving, QC and a creative update / re-trafficking.Our solve for the above was to build a dynamic ad creation and configuration platform — our ad production partners build a single dynamic unit and then the associated data configuration is used to modify the behavior of the ad units contextually. Secondly, by providing tools where marketers have to express just the variations and automatically inherit what doesn’t change, we significantly reduce the surface area of data that needs to be defined and managed.If you look at the localized versions below, they reused the same fundamental building blocks but got expressed as different creatives based on nothing but configuration.This makes it possible to go from 1 => 30 localizations in a matter of minutes instead of hours or even days for every single ad unit!We are also able to make the process more seamless by building integrations with a number of useful services to speed up the ad assembly process. For example, we have integrated features like support for maturity ratings, transcoding and compressing video assets or pulling in artwork from our product catalog. Taken together, these conveniences dramatically decrease the level of time effort needed to run campaigns with extremely large footprints.One major aspect of quality control to ensure that the ad is going to render correctly and free from any technical or visual errors — we call this “functional QC”. Given the breadth of differences amongst various ad types and the kinds of possible issues, here are some of the top-line approaches that we have pursued to improve the state of creative QC.First, we have tools that plug in sensible values throughout the ad assembly process and reduce the likelihood of errors.Then, we minimize the total volume of QC issues encountered by adding validations and correctness checks throughout the ad assembly process. For eg. we surface a warning when character limits on Facebook video ads are exceeded.Secondly, we run suites of automated tests that help identify if there are any technical issues that are present in the ad unit that may negatively impact either the functionality or cause negative side-effects to the user-experience.Most recently, we’ve started leveraging machine vision to handle some QC tasks. For eg. depending on where an ad needs to be delivered, there might have to be the need to add specific rating images. To verify that the right rating image was applied in the video creation process, we now use an image detection algorithm developed by our Cloud Media Systems team. As the volume of AV centric creatives continues to scale and increase over time, we will be adding more such solutions to our overall workflow.In addition to the functional correctness, we also care a whole lot about semantic QC — i.e for our marketing users to determine if the ads are being true to their creative goals and representing the tone and voice of the content and of the Netflix brand accurately.One of the core tenets around which our ad platform is built is immediate updates with live renderings across the board. This, coupled with the fact that our users can identify and make pinpointed updates with broad implications easily, allows them to fix issues as fast as they can find them. Our users are also able to collaborate on creative feedback, reviews much more efficiently by sharing tearsheets as needed. A tearsheet is a preview of the final ad after it has been locked and is used to get final clearance ahead of launch.Given how important this process is to the overall health and success of our advertising campaigns, we’re investing heavily on QC automation infrastructure. We’re also actively working on enabling sophisticated task management, status tracking and notification workflows that help us scale to even higher orders of magnitude in a sustainable way.Once the ads are prepared, instead of directly trafficking them as such, we decouple the ad creation, assembly from ad trafficking with a “catalog” layer.A catalog picks the sets of ads to run with based on the intent of the campaign — Is it meant for building title awareness or for acquisition marketing? Are we running a campaign for a single movie or show or does it highlight multiple titles or is it a brand-centric asset? Is this a pre-launch campaign or a post-launch campaign?Once a definition is assigned by the user, an automated catalog handles the following concerns amongst other things :The catalog is hence a very powerful tool as it optimizes itself and hence the campaign it’s supporting — in effect, it turns our first party data into an “intelligence-layer”.All of this can add to a sum greater than its parts — for eg. using this technology, we can now run a Global Scale Vehicle — an always-on / evergreen, auto-optimizing campaigns powered by content performance data and ad performance data. Along with automatic budget allocation algorithms (we’ll discuss it in the next blog post in this series), this tames the operational complexity very effectively. As a result, our marketing users get to focus to building amazing creatives and formulating A/B tests and market plans on their end, and our automated catalogs help to deliver the right creative to the right place in a hands off fashion — automating the ad selection and personalization.In order to understand why this is a game changer, let’s reflect on the previous approach — every title that needed to be launched had to involve planning on budgeting, targeting, which regions to support any title in, how long to run and to what spend levels, etc.This was a phenomenally hard task in the face of our ever increasing content library, breadth and nuances of marketing to nearly all countries of the world and the number of platforms and formats needing support to reach our addressable audience. Secondly, it was challenging to react fast enough to unexpected variations in creative performance all while also focusing on upcoming campaigns and launches.In true, Netflix fashion, we arrived at this model through a series of A/B tests — originally, we ran several tests learning that an always-on ad catalog with personalized delivery outperformed our previous tentpole launch approach. We then ran many more follow-ups to determine how to do it well on different platforms. As one would imagine, this is fundamentally a process of continuous learning and we’re pleasantly surprised to find huge, successive improvements on our optimization metrics as we’ve continued to run growing number of marketing A/B tests around the world.We enable this technology using a number of Java and Groovy based microservices that tap into various NoSQL stores such as Cassandra and Elasticsearch and use Kafka, Hermes to glue the different parts by either transporting data or triggering events that result in dockerized micro-applications getting invoked on Titus.We use RxJava fairly heavily and the ad server which handles real-time requests for servicing display and VAST videos uses RxNetty as it’s application framework and it offers customizability while bringing minimal features and associated overheads. For the ads middle tier application server, we use a Tomcat / Jersey / Guice powered service as it offers way more features and easy integrations for it’s concerns such as easy authentication and authorization, out-of-the-box support for Netflix’s cloud ecosystem as we lack of strict latency and throughput constraints.Although we’ve had the opportunity to build a lot of technology in the last few years, the practical reality is that our work is far from done.We’ve had a high degree of progress on some ad platforms, we’re barely getting started on others and there’s some we aren’t even ready to think of, just yet. On some, we’ve hit the entirety of ad creation, assembly and management and QC, on others, we’ve not even scratched the full surface of just plain assembly.Automation and machine learning have gotten us pretty far — but our organizational appetite for doing more and doing better is far outpacing the speed with which can build these systems. With every A/B test having us think of more avenues of exploration and in using data to power analysis and prediction in various aspects of our ad workflows, we’ve got a lot of interesting challenges to look forward to.In summary, we’ve discussed how we build unique ad technology that helps us add both scale and add intelligence into advertising efforts. Some of the details themselves are worth follow-up posts on and we’ll be publishing them in the future.To further our marketing technology journey, we’ll have the next blog shortly that moves the story forward towards how we support marketing analytics from a variety of platforms and make it possible to compare proverbial apples and oranges and use it to optimize campaign spend.If you’re interested in joining us in working on some of these opportunities within Netflix’s Marketing Tech, we’re hiring! :)",https://medium.com/netflix-techblog/https-medium-com-netflixtechblog-engineering-to-improve-marketing-effectiveness-part-2-7dd933974f5e?source=collection_home---4------10---------------------,0,netflix,"java,python",NULL,2018-09-04
Part 2: Scheduling Notebooks at Netflix,"by Matthew Seal, Kyle Kelley, and Michelle UffordAt Netflix we’ve put substantial effort into adopting notebooks as an integrated development platform. The idea started as a discussion of what development and collaboration interfaces might look like in the future. It evolved into a strategic bet on notebooks, both as an interactive UI and as the unifying foundation of our workflow scheduler. We’ve made significant strides towards this over the past year, and we’re currently in the process of migrating all 10,000 of the scheduled jobs running on the Netflix Data Platform to use notebook-based execution. When we’re done, more than 150,000 Genie jobs will be running through notebooks on our platform every single day.When thinking about the future of analytics tooling, we initially asked ourselves a few basic questions:We also wondered: is there a single tool that can support all of these scenarios?One tool that showed promise was the Jupyter notebook. Notebooks were already used at Netflix for data science but were increasingly being used for other types of workloads too. With its flexible nature and high extensibility, plus its large and vibrant open source community, notebooks was a compelling option. So, we took a deeper look at how we might use it as a common interface for our users.Notebooks are, in essence, managed JSON documents with a simple interface to execute code within. They’re good at expressing iterative units of work via cells, which facilitate reporting and execution isolation with ease. Plus, with different kernels, notebooks can support a wide range of languages and execution patterns. These attributes mean that we can expose any arbitrary level of complexity for advanced users while presenting a more easily followed narrative for consumers — all within a single document. We talk about these attributes and their supporting services more in our previous post. If you haven’t read it yet, it’s a good introduction to the work we’re doing on notebooks, including our motivations and other use cases.We knew that any tooling we chose we would need the ability to schedule our workloads. As the potential of Jupyter notebooks became increasingly clear, we began to look at what it would take to schedule a notebook. The properties of a notebook, while excellent for interactive work, do not readily lend to scheduled execution. If you’re already familiar with notebooks — both their strengths and weaknesses — you may even think we’re a little crazy for moving all of our etl workloads to notebooks.On the surface, notebooks pose a lot of challenges: they’re frequently changed, their cell outputs need not match the code, they’re difficult to test, and there’s no easy way to dynamically configure their execution. Furthermore, you need a notebook server to run them, which creates architectural dependencies to facilitate execution. These issues caused some initial push-back internally at the idea. But that has changed as we’ve brought in new tools to our notebook ecosystem.The biggest game-changer for us is Papermill. Papermill is an nteract library built for configurable and reliable execution of notebooks with production ecosystems in mind. What Papermill does is rather simple. It take a notebook path and some parameter inputs, then executes the requested notebook with the rendered input. As each cell executes, it saves the resulting artifact to an isolated output notebook.Papermill enables a paradigm change in how you work with notebook documents. Since Papermill doesn’t modify the source notebook, we get a functional property added to our definition of work — something which is normally missing in the notebook space. Our inputs, a notebook JSON document and our input parameters, are treated as immutable records for execution that produce an immutable output document. That single output document provides the executed code, the outputs and logs from each code cell, and a repeatable template which can be easily rerun at any point in the future.Another feature of Papermill is its ability to read or write from many places. This enables us to store our output notebook somewhere with high durability and easy access in order to provide a reliable pipeline. Today we default to storing our output notebooks to an s3 bucket managed by Commuter, another nteract project which provides a read-only display of notebooks.Output notebooks can thus become isolated records on whichever system best supports our users. This makes analyzing jobs or related work as easy as linking to a service or S3 prefix. Users can take those links and use them to debug issues, check on outcomes, and create new templates without impacting the original workflows.Additionally, since Papermill controls its own runtime processes, we don’t need any notebook server or other infrastructure to execute against notebook kernels. This eliminates some of the complexities that come with hosted notebook services as we’re executing in a simpler context.To further improve notebook reliability, we push our notebooks into git and only promote them to production services after we run tests against those notebooks using Papermill. If a notebook becomes too complex to easily test, we have the local repository into which we can consolidate code in a more traditional package. This allows us to gain the benefits of normal CI tooling in promoting notebooks as traditional code, but still allow us to explore and iterate with notebooks as an integration tool.Our notebooks thus became versioned, pushed as immutable records to a reliable data store before and after execution, tested before they’re made available, and made parameterizable for specialization at runtime. The user-friendly-but-unreliable notebook format is now made reliable for our data pipelines, and we’ve gained a key improvement over a non-notebook execution pattern: our input and outputs are complete documents, wholly executable and shareable in the same interface.Even with a platform supporting the testing, versioning, and presentation of notebooks we were still missing a key component to enable users to run work on a periodic basis with triggered executions — or more concisely, we needed a scheduling layer. Executing a notebook through a web interface is great for visual and reactive feedback for users, but once you have something working you need a tool to do that execution on your behalf.The execution side of this equation is made easy with Papermill. We can compute runtime parameters and inject them into a notebook, run the notebook, and store the outcomes to our data warehouse. This architecture decouples parameterized notebooks from scheduling, providing flexibility in choosing a scheduler. Thus just about any cron string and/or event consuming tool can enable running the work we’ve setup so far.This means that so long as a few basic capabilities are present, scheduling notebooks is easy. Instead, you’ll want to spend effort here on choosing the secondary attributes of the scheduler that you care most about. You may want to reuse a tool already familiar to your team, or make a choice to satisfy other operational needs. If you don’t have a preferred scheduler or haven’t used one before, Airflow is an open source tool that can serve this role well.In our case, the secondary attributes we cared about were:These requirements left us with a handful of potential options to consider, including both open and closed source solutions. After thoroughly exploring our options, we chose a scheduler developed at Netflix called Meson. Meson is a general purpose workflow orchestration and scheduling framework for executing ML pipelines across heterogeneous systems. One of the major factors for us choosing Meson is its deep support for Netflix’s existing cloud-based infrastructure, including our data platform.With a scheduler in place, how would this to look to a developer? Let’s explore a hypothetical data workflow. Suppose we want to aggregate video plays by device type to understand which devices our members use to watch content. Because we’re global, we need to split our aggregates by region so we can understand the most popular devices in each part of the world. And, once the results are ready each day, we want to push the updated report to our analysts.To start, we’ll need a schedule for our workflow. Let’s say daily at 2 AM. Most schedulers accept crontab as a schedule trigger, so a single 0 2 * * * string satisfies this requirement.Next, we need to break our work into logical units of work. We’ll want to collect our data, aggregate it, and report back to the user the results. To express this work we’ll define a DAG with each individual job represented as a node in the graph, and each edge represents the next job to run upon success.In this scenario, we would need four notebooks. One to collect our input data. One to enhance our raw data with geographical information. One to be parameterized for each region. And one to push our results to a report. Our aggregate notebook, for example, might have a parameterized execution such as:We have a few lines of code to execute a simple SQL statement. You can see that in cell [4] we have our injected parameters from Papermill overwriting the default region_code. The run_date is already what we want, so we’ll keep the default instead of overwriting it.The scheduler then executes a simple command to run the notebook.Done! Pretty easy, isn’t it? Now, this is a contrived example and may not reflect how our data engineers would actually go about this work, but it does help demonstrate how everything fits together in a workflow.Another important aspect to consider when bringing new technologies to a platform is the ability to debug and support its users. With notebooks, this is probably the most beneficial aspect of our scheduler system.Let’s dig into how we would deal with a failure. Say something went wrong in our example notebook from earlier. How might we debug and fix the issue? The first place we’d want to look is the notebook output. It will have a stack trace, and ultimately any output information related to an error.Here we see that our job couldn’t find the ‘genie.typo’ hostname. That’s probably not a user input error, so we’ll likely need to change the template to have the correct hostname. In a traditional scheduler situation, you’d need to either create a mock of the job execution environment or try making changes and resubmitting a similar job. Here instead we simply take the output notebook with our exact failed runtime parameterizations and load it into a notebook server.With a few iterations and looking at our job library methods, we can quickly find a fix for the failure.Now that it’s fixed, this template can be pushed to the source notebook path. Any future executions, including retrying the failed job, will pick up and run the updated template.At Netflix we’ve adopted notebooks as an integration tool, not as a library replacement. This means we needed to adopt good integration testing to ensure our notebooks execute smoothly and don’t frequently run into bugs. Since we already have a pattern for parameterizing our execution templates, we repeat these interactions with dummy inputs as a test of linear code paths.What this means is that we’re not using notebooks as code libraries and consequently aren’t pressing for unit level tests on our notebooks, as those should be encapsulated by the underlying libraries. Instead, we promote guiding principles for notebook development:When followed, these guidelines make it easy for us to guide and support our users across a wide spectrum of use-cases and underlying technologies.With the choices and technologies outlined thus far, we’ve been able to start building the shared experience we described at the beginning of this post. Adoption internally at Netflix has been growing quickly this year, and we’re ahead of schedule to completely replacing several of our pre-existing ETL and reporting patterns.But we’re far from done. We still have work we want to do to improve the development experience and ease of adoption. Namely, we want better code review patterns for notebooks using tools like nbdime, more integration of CI and platform tools to notebooks, and easier ways to schedule and share notebooks. These, and many more, useful improvements will help make notebooks a common and easy entry-point for cross-functional development at Netflix. If you’d like to help us with these efforts, our team is currently hiring for multiple roles.If you’re curious to hear more, you can catch us at JupyterCon in NYC this week, where several of us are speaking about Jupyter notebooks, scheduling, and new open source libraries.There are more ways to learn from Netflix Data and we’re happy to share:You can also stay up to date with nteract via their mailing list and blog!This post is part two in a series on notebooks at Netflix we’ll be releasing over the coming weeks. You can follow us on Medium for more from Netflix and check out the currently released articles below:",https://medium.com/netflix-techblog/scheduling-notebooks-348e6c14cfd6?source=collection_home---4------11---------------------,0,netflix,,NULL,2018-08-22
Beyond Interactive: Notebook Innovation at Netflix,"By Michelle Ufford, M Pacer, Matthew Seal, and Kyle KelleyNotebooks have rapidly grown in popularity among data scientists to become the de facto standard for quick prototyping and exploratory analysis. At Netflix, we’re pushing the boundaries even further, reimagining what a notebook can be, who can use it, and what they can do with it. And we’re making big investments to help make this vision a reality.In this post, we’ll share our motivations and why we find Jupyter notebooks so compelling. We’ll also introduce components of our notebook infrastructure and explore some of the novel ways we’re using notebooks at Netflix.If you’re short on time, we suggest jumping down to the Use Cases section.Data powers Netflix. It permeates our thoughts, informs our decisions, and challenges our assumptions. It fuels experimentation and innovation at unprecedented scale. Data helps us discover fantastic content and deliver personalized experiences for our 130 million members around the world.Making this possible is no small feat; it requires extensive engineering and infrastructure support. Every day more than 1 trillion events are written into a streaming ingestion pipeline, which is processed and written to a 100PB cloud-native data warehouse. And every day, our users run more than 150,000 jobs against this data, spanning everything from reporting and analysis to machine learning and recommendation algorithms. To support these use cases at such scale, we’ve built an industry-leading Data Platform which is flexible, powerful, and complex (by necessity). We’ve also built a rich ecosystem of complementary tools and services, such as Genie, a federated job execution service, and Metacat, a federated metastore. These tools simplify the complexity, making it possible to support a broader set of users across the company.User diversity is exciting, but it comes at a cost: the Netflix Data Platform — and its ecosystem of tools and services — must scale to support additional use cases, languages, access patterns, and more. To better understand this problem, consider 3 common roles: analytics engineer, data engineer, and data scientist.Generally, each role relies on a different set of tools and languages. For example, a data engineer might create a new aggregate of a dataset containing trillions of streaming events — using Scala in IntelliJ. An analytics engineer might use that aggregate in a new report on global streaming quality — using SQL and Tableau. And that report might lead to a data scientist building a new streaming compression model — using R and RStudio. On the surface, these seem like disparate, albeit complementary, workflows. But if we delve deeper, we see that each of these workflows has multiple overlapping tasks:data exploration — occurs early in a project; may include viewing sample data, running queries for statistical profiling and exploratory analysis, and visualizing datadata preparation — iterative task; may include cleaning, standardizing, transforming, denormalizing, and aggregating data; typically the most time-intensive task of a projectdata validation — recurring task; may include viewing sample data, running queries for statistical profiling and aggregate analysis, and visualizing data; typically occurs as part of data exploration, data preparation, development, pre-deployment, and post-deployment phasesproductionalization — occurs late in a project; may include deploying code to production, backfilling datasets, training models, validating data, and scheduling workflowsTo help our users scale, we want to make these tasks as effortless as possible. To help our platform scale, we want to minimize the number of tools we need to support. But how? No single tool could span all of these tasks; what’s more, a single task often requires multiple tools. When we add another layer of abstraction, however, a common pattern emerges across tools and languages: run code, explore data, present results.As it happens, an open source project was designed to do precisely that: Project Jupyter.Project Jupyter began in 2014 with a goal of creating a consistent set of open-source tools for scientific research, reproducible workflows, computational narratives, and data analytics. Those tools translated well to industry, and today Jupyter notebooks have become an essential part of the data scientist toolkit. To give you a sense of its impact, Jupyter was awarded the 2017 ACM Software Systems Award — a prestigious honor it shares with Java, Unix, and the Web.To understand why the Jupyter notebook is so compelling for us, consider the core functionality it provides:The Jupyter protocol provides a standard messaging API to communicate with kernels that act as computational engines. The protocol enables a composable architecture that separates where content is written (the UI) and where code is executed (the kernel). By isolating the runtime from the interface, notebooks can span multiple languages while maintaining flexibility in how the execution environment is configured. If a kernel exists for a language that knows how to communicate using the Jupyter protocol, notebooks can run code by sending messages back and forth with that kernel.Backing all this is a file format that stores both code and results together. This means results can be accessed later without needing to rerun the code. In addition, the notebook stores rich prose to give context to what’s happening within the notebook. This makes it an ideal format for communicating business context, documenting assumptions, annotating code, describing conclusions, and more.Of our many use cases, the most common ways we’re using notebooks today are: data access, notebook templates, and scheduling notebooks.Notebooks were first introduced at Netflix to support data science workflows. As their adoption grew among data scientists, we saw an opportunity to scale our tooling efforts. We realized we could leverage the versatility and architecture of Jupyter notebooks and extend it for general data access. In Q3 2017 we began this work in earnest, elevating notebooks from a niche tool to a first-class citizen of the Netflix Data Platform.From our users’ perspective, notebooks offer a convenient interface for iteratively running code, exploring output, and visualizing data — all from a single cloud-based development environment. We also maintain a Python library that consolidates access to platform APIs. This means users have programmatic access to virtually the entire platform from within a notebook. Because of this combination of versatility, power, and ease of use, we’ve seen rapid organic adoption for all user types across our entire platform.Today, notebooks are the most popular tool for working with data at Netflix.As we expanded platform support for notebooks, we began to introduce new capabilities to meet new use cases. From this work emerged parameterized notebooks. A parameterized notebook is exactly what it sounds like: a notebook which allows you to specify parameters in your code and accept input values at runtime. This provides an excellent mechanism for users to define notebooks as reusable templates.Our users have found a surprising number of uses for these templates. Some of the most common ones are:One of the more novel ways we’re leveraging notebooks is as a unifying layer for scheduling workflows.Since each notebook can run against an arbitrary kernel, we can support any execution environment a user has defined. And because notebooks describe a linear flow of execution, broken up by cells, we can map failure to particular cells. This allows users to describe a short narrative of execution and visualizations that we can accurately report against when running at a later point in time.This paradigm means we can use notebooks for interactive work and smoothly move to scheduling that work to run recurrently. For users, this is very convenient. Many users construct an entire workflow in a notebook, only to have to copy/paste it into separate files for scheduling when they’re ready to deploy it. By treating notebooks as a logical workflow, we can easily schedule it the same as any other workflow.We can schedule other types of work through notebooks, too. When a Spark or Presto job executes from the scheduler, the source code is injected into a newly-created notebook and executed. That notebook then becomes an immutable historical record, containing all related artifacts — including source code, parameters, runtime config, execution logs, error messages, and so on. When troubleshooting failures, this offers a quick entry point for investigation, as all relevant information is colocated and the notebook can be launched for interactive debugging.Supporting these use cases at Netflix scale requires extensive supporting infrastructure. Let’s briefly introduce some of the projects we’ll be talking about.nteract is a next-gen React-based UI for Jupyter notebooks. It provides a simple, intuitive interface and offers several improvements over the classic Jupyter UI, such as inline cell toolbars, drag and droppable cells, and a built-in data explorer.Papermill is a library for parameterizing, executing, and analyzing Jupyter notebooks. With it, you can spawn multiple notebooks with different parameter sets and execute them concurrently. Papermill can also help collect and summarize metrics from a collection of notebooks.Commuter is a lightweight, vertically-scalable service for viewing and sharing notebooks. It provides a Jupyter-compatible version of the contents API and makes it trivial to read notebooks stored locally or on Amazon S3. It also offers a directory explorer for finding and sharing notebooks.Titus is a container management platform that provides scalable and reliable container execution and cloud-native integration with Amazon AWS. Titus was built internally at Netflix and is used in production to power Netflix streaming, recommendation, and content systems.We explore this architecture in our follow-up blog post, Scheduling Notebooks at Netflix. For the purposes of this post, we’ll just introduce three of its fundamental components: storage, compute, and interface.The Netflix Data Platform relies on Amazon S3 and EFS for cloud storage, which notebooks treat as virtual filesystems. This means each user has a home directory on EFS, which contains a personal workspace for notebooks. This workspace is where we store any notebook created or uploaded by a user. This is also where all reading and writing activity occurs when a user launches a notebook interactively. We rely on a combination of [workspace + filename] to form the notebook’s namespace, e.g. /efs/users/kylek/notebooks/MySparkJob.ipynb. We use this namespace for viewing, sharing, and scheduling notebooks. This convention prevents collisions and makes it easy to identify both the user and the location of the notebook in the EFS volume.We can rely on the workspace path to abstract away the complexity of cloud-based storage from users. For example, only the filename of a notebook is displayed in directory listings, e.g. MySparkJob.ipynb. This same file is accessible at ~/notebooks/MySparkJob.ipynb from a terminal.When the user schedules a notebook, the scheduler copies the user’s notebook from EFS to a common directory on S3. The notebook on S3 becomes the source of truth for the scheduler, or source notebook. Each time the scheduler runs a notebook, it instantiates a new notebook from the source notebook. This new notebook is what actually executes and becomes an immutable record of that execution, containing the code, output, and logs from each cell. We refer to this as the output notebook.Collaboration is fundamental to how we work at Netflix. It came as no surprise then when users started sharing notebook URLs. As this practice grew, we ran into frequent problems with accidental overwrites caused by multiple people concurrently accessing the same notebook . Our users wanted a way to share their active notebook in a read-only state. This led to the creation of Commuter. Behind the scenes, Commuter surfaces the Jupyter APIs for /files and /api/contents to list directories, view file contents, and access file metadata. This means users can safely view notebooks without affecting production jobs or live-running notebooks.Managing compute resources is one of the most challenging parts of working with data. This is especially true at Netflix, where we employ a highly-scalable containerized architecture on AWS. All jobs on the Data Platform run on containers — including queries, pipelines, and notebooks. Naturally, we wanted to abstract away as much of this complexity as possible.A container is provisioned when a user launches a notebook server. We provide reasonable defaults for container resources, which works for ~87.3% of execution patterns. When that’s not enough, users can request more resources using a simple interface.We also provide a unified execution environment with a prepared container image. The image has common libraries and an array of default kernels preinstalled. Not everything in the image is static — our kernels pull the most recent versions of Spark and the latest cluster configurations for our platform. This reduces the friction and setup time for new notebooks and generally keeps us to a single execution environment.Under the hood we’re managing the orchestration and environments with Titus, our Docker container management service. We further wrap that service by managing the user’s particular server configuration and image. The image also includes user security groups and roles, as well as common environment variables for identity within included libraries. This means our users can spend less time on infrastructure and more time on data.Earlier we described our vision for notebooks to become the tool of choice for working with data. But this presents an interesting challenge: how can a single interface support all users? We don’t fully know the answer yet, but we have some ideas.We know we want to lean into simplicity. This means an intuitive UI with a minimalistic aesthetic, and it also requires a thoughtful UX that makes it easy to do the hard things. This philosophy aligns well with the goals of nteract, a React-based frontend for Jupyter notebooks. It emphasizes simplicity and composability as core design principles, which makes it an ideal building block for the work we want to do.One of the most frequent complaints we heard from users is the lack of native data visualization across language boundaries, especially for non-Python languages. nteract’s Data Explorer is a good example of how we can make the hard things simpler by providing a language-agnostic way to explore data quickly.You can see Data Explorer in action in this sample notebook on MyBinder. (please note: it may take a minute to load)We’re also introducing native support for parametrization, which makes it easier to schedule notebooks and create reusable templates.Although notebooks are already offering a lot of value at Netflix, we’ve just begun. We know we need to make investments in both the frontend and backend to improve the overall notebook experience. Our work over the next 12 months is focused on improving reliability, visibility, and collaboration. Context is paramount for users, which is why we’re increasing visibility into cluster status, kernel state, job history, and more. We’re also working on automatic version control, native in-app scheduling, better support for visualizing Spark DataFrames, and greater stability for our Scala kernel. We’ll go into more detail on this work in a future blog post.Netflix has long been a proponent of open source. We value the energy, open standards, and exchange of ideas that emerge from open source collaborations. Many of the applications we developed for the Netflix Data Platform have already been open sourced through Netflix OSS. We are also intentional about not creating one-off solutions or succumbing to “Not Invented Here” mentality. Whenever possible, we leverage and contribute to existing open source projects, such as Spark, Jupyter, and pandas.The infrastructure we’ve described relies heavily on the Project Jupyter ecosystem, but there are some places where we diverge. Most notably, we have chosen nteract as the notebook UI for Netflix. We made this decision for many reasons, including alignment with our technology stack and design philosophies. As we push the limits of what a notebook can do, we will likely create new tools, libraries, and services. These projects will also be open sourced as part of the nteract ecosystem.We recognize that what makes sense for Netflix does not necessarily make sense for everyone. We have designed these projects with modularity in mind. This makes it possible to pick and choose only the components that make sense for your environment, e.g. Papermill, without requiring a commitment to the entire ecosystem.As a platform team, our responsibility is to enable Netflixers to do amazing things with data. Notebooks are already having a dramatic impact at Netflix. With the significant investments we’re making in this space, we’re excited to see this impact grow. If you’d like to be a part of it, check out our job openings.Phew! Thanks for sticking with us through this long post. We’ve just scratched the surface of what we’re doing with notebooks. This post is part one in a series on notebooks at Netflix we’ll be releasing over the coming weeks. You can follow us on Medium for more from Netflix and check out the currently released articles below:We’re thrilled to sponsor this year’s JupyterCon. If you’re attending, check out one of the 5 talks by our engineers, or swing by our booth to talk about Jupyter, nteract, or data with us.There are more ways to learn from Netflix Data and we’re happy to share:You can also stay up to date with nteract via their mailing list and blog!",https://medium.com/netflix-techblog/notebook-innovation-591ee3221233?source=collection_home---4------12---------------------,0,netflix,"docker,backend,jupyter,cloud,python",NULL,2018-08-16
Netflix Cloud Security: Detecting Credential Compromise in AWS,"Will Bengtson, Netflix Security Tools and OperationsCredential compromise is an important concern for anyone operating in the cloud. The problem becomes more obvious over time, as organizations continue to adopt cloud resources as part of their infrastructure without maintaining an accompanying capability to observe and react to these compromises. The associated impacts to these compromises vary widely as well. Attackers might use this for something as straightforward as stealing time and CPU on your instances to mine Bitcoin, but it could be a lot worse; credential compromise could lead to deletion of infrastructure and stolen data.We on the Netflix Security Tools and Operations team want to share a new methodology for detecting temporary security credential use outside of your AWS environment. Consider your AWS environment to be “all those AWS resources that are associated with your AWS accounts.”In this post, we’ll show you how to detect compromised AWS instance credentials (STS credentials) outside of your environment. Keep in mind, however, that you could do this with other temporary security credentials, such as ECS, EKS, etc.Attackers understand where your applications run, as well as common methods of detecting credential compromise. When attacking AWS, attackers will often try to use your captured AWS credentials from within their AWS account. Perhaps you’re already paying attention to invocations of the “dangerous” AWS API calls in your environment — which is a great first step — but attackers know what will get your attention, and are likely to try innocuous API calls first. The obvious next step is to determine if API calls are happening from outside of your environment. Right now, because the more general AWS IP space is well-known, it’s easy to detect if API calls originate from outside of AWS. If they originate from AWS IPs other than your own, however, you’ll need some extra magic. That’s the methodology we’re publicizing here.To understand our method, you first need to know how AWS passes credentials to your EC2 instance, and how to analyze CloudTrail entries record by record.We’ll first build a data table of all EC2 assumed role records, culled from CloudTrail. Each table entry shows the instance ID, assumed role, IP address of the API call, and a TTL entry (the TTL helps keep the table lean). We can quickly determine if the caller is within our AWS environment by examining the source IP address of the API call from an instance.When you launch an EC2 instance with an IAM Role, the AWS EC2 service assumes the role specified for the instance and passes those temporary credentials to the EC2 metadata service. This AssumeRole action appears in CloudTrail with the following key fields:We can determine the Amazon Resource Name (ARN) for these temporary instance credentials from this Cloud Trail log. Note that AWS refreshes credentials in the EC2 metadata service every 1–6 hours.When we see an AssumeRole action by the EC2 service, let’s store it in a table with the following columns:We can get the Instance-Id from the requestParameters.roleSessionName field. For each AssumeRole action, let’s check to see if a row already exists in our table. If not, we will create one.If the row exists, let’s update the TTL to keep it around. At this point we update the TTL; since the instance is still up and running, we don’t want this entry to expire. A safe TTL in this case is 6 hours, due to the periodic refreshing of instance credentials within AWS, but you may decide to make it longer. You can construct the AssumedRole-Arn by taking the requestParameters.roleArn and requestParameters.roleSessionName from the AssumeRole CloudTrail record.For example, the resulting AssumedRole-Arn for the above entry is:This AssumeRole-Arn becomes your userIdentity.arn entry in CloudTrail for all calls that use these temporary credentials.Now that we have a table of Instance-IDs and AssumeRole-ARNs, we can start analyzing each CloudTrail record using these temporary credentials. Each instance-id/session row starts without an IP address to lock to (remember, we claimed that with this method, you won’t need to know all your IP addresses in advance).For each CloudTrail event, we will analyze the type of record to make sure it came from an assumed role. You can do this by checking the value of userIdentity.type and making sure it equals AssumedRole. If it is AssumedRole, we will grab the userIdentity.arn field which is equivalent to the AssumeRole-Arn column in the table. Since the userIdentity.arn has the requestParameters.roleSessionName in the value, we can extract the instance-id and do a lookup in the table to see if a row exists. If the row exists, we then check to see if there are any IPs that this AssumeRole-Arn is locked to. If there aren’t any, then we update the table with the sourceIPAddress from the record and this becomes our IP address that all calls should come from. And here’s the key to the whole method: If we see a call with a sourceIPAddress that doesn’t match the previously observed IP, thenFor CloudTrail events that do not have a corresponding row in the table, we’ll just have to discard these, because we can’t make a decision without a corresponding entry in the table. However, we’ll only face this shortcoming for up to six hours, due to the way AWS handles temporary instance credentials within EC2. After that point we’ll have all of the AssumeRole entries for our environment, and we won’t need to discard any events.To prevent false positives, you’ll want to consider a few edge cases that impact this approach:If you look in your CloudTrail records, you may find that you see a sourceIPAddress that shows up as <servicename>.amazonaws.com outside of the AssumeRole action mentioned earlier. You will want to account for these appearing and trust AWS in these calls. You might still want to keep track of these and provide informational alerting.When you make an API call in a VPC that has a VPC endpoint for your service, the sourceIPAddress will show up as a private IP address instead of the public IP address assigned to your instance or your VPC NAT Gateway. You will most likely need to account for having a [public IP, private IP] list in your table for a given instance-id/AssumeRole-Arn row.You might have a use case where you attach additional ENI(s) to your EC2 instance or associate a new address through use of an Elastic IP (EIP). In these cases, you will see additional IP(s) show up in CloudTrail records for your AssumedRole-Arn. You will need to account for these actions in order to prevent false positives. One way to address this edge case is to inspect the CloudTrail records which associate new IPs to instances and create a table that has a row for each time a new IP was associated with the instance. This will account for the number of potential IP changes that you come across in CloudTrail. If you see a sourceIPAddress that does not match your lock IP, check to see if there was a call that resulted in a new IP for your instance. If so, add this IP to your IP column in your AssumeRole-Arn table entry, remove the entry in the additional table where you track associations, and do not alert.You might be asking the question: “Since we set the lock IP to the first API call seen with the credentials, isn’t there a case where an attacker’s IP is set to the lock IP?” Yes, there is a slight chance that due to this approach you add an attacker’s IP to the lock table because of a compromised credential. In this rare case, you will detect a “compromise” when your EC2 instance makes its first API call after the lock of the attacker’s IP. To minimize this rare case, you might add a script that executes the first time your AWS instance boots and makes an AWS API call that is known to be logged in CloudTrail.The methodology we’ve shared here requires a high level of familiarity with CloudTrail, and how AssumeRole calls are logged. However, there are several advantages, including scalability, as your AWS environment grows and your number of accounts increases, and simplicity, since with this method you needn’t maintain a full list of IP addresses allocated to your account. Do bear in mind the “defense in depth” truism: this should only constitute one “layer” of your security tactics in AWS.Be sure to let us know if you implement this, or something better, in your own environment.Will Bengtson, for Netflix Security Tools and Operations",https://medium.com/netflix-techblog/netflix-cloud-security-detecting-credential-compromise-in-aws-9493d6fd373a?source=collection_home---4------13---------------------,0,netflix,"docker,backend,cloud",NULL,2018-08-08
Pseudo Localization @ Netflix,"by Tim BrandallIn the past 8 years Netflix has transformed from an English-only product, to now supporting 26 languages and growing. As we add language support for our members residing in 190 different countries, scaling globalization at Netflix has never been more important. Along the way we’ve built out countless solutions to help us achieve globalization at scale. In this article we’ll focus on one my team has been working on, Pseudo Localization.The problem we set out to solve was simple: Expansion of text due to translation causes most of the UI layout issues we detect during localization testing.When translating into other languages, the translated text could be up to 40% longer than the English. This is more prevalent in German, Hebrew, Polish, Finnish, and Portuguese. Take this real-world example from our German UI :Translated into German, this becomes much longer! :Put in the context of the UI, we see problems:This is one example among many. The source of the problem is that our UIs are designed in English, and assume English string lengths, line heights, and glyphs. More often than not, when those strings are translated we will see expansion that causes layout issues. When the product is translated into 26 languages, like ours is, you potentially end up with 26 defects that need to be logged, managed, and resolved; all the while we had the opportunity to fix the issue at the English design phase, before translation ever started.Enter Pseudo Localization. Pseudo Localization is a way to simulate translation of English UI strings, without waiting for, or going to the effort of doing real translation. Think of it as a fake translation that remains readable to an English speaking developer, and allows them to test for translation related expansion, among other important things. Here’s an example of our Netflix Pseudo Localization in action on iOS:It helps to break down what we’re doing here, using the following string as an example:After passing through our Pseudo Localization algorithm it becomes this:Here are the various elements of the transform:We are lucky enough to leverage our cloud based Global String Repository, for this effort. We apply the Pseudo Localization transformations on any string requested from the repository, before it is handed off to the client. The transformation logic resides in our Netflix Internationalization library, NFi18n, and is available by API to all other Netflix services. The beauty of this solution is that it can be applied to all of our supported UIs.One of the biggest challenges we faced was text that displays on our UIs but doesn’t originate from our Global Strings Repository. Examples of this type of text would be movie/show titles, synopsis, cast names, maturity ratings, to name a few. This “movie metadata” lives in various systems across Netflix so we had some investigation to do to figure out where, and when was the best place and time to apply our Pseudo Localization transforms on this metadata. Having all this additional text pseudo localized was important because without the pseudo localized metadata, the experience felt incomplete, half pseudo localized, half English:Going into this project, we knew that driving implementation across the various UI development teams at Netflix would be critical to success. It didn’t matter how good our solution was if nobody was using it. We put a lot of investment into the following areas:We correctly assumed that architecting, and implementing the solution would not be half of the battle, I would argue it was even less. The real work starts while advocating, influencing, educating, and convincing development teams to fundamentally change the way they work. We did this by showing the impact that Pseudo Localization can have and the amounts of defects it can eradicate, that they fix a UI layout issue once, instead of 26 times. Already we are seeing UI engineers catching and fixing UI layout issues that previously we would have caught post-translation in multiple languages. Now they can simply find and fix it once themselves.About 6 weeks after the initial rollout to development teams, we surveyed all our developers. Areas we touched on were:Based on the survey, the resounding theme was readability. While we had maintained readability, we had introduced additional overhead in parsing the text on screen while reading. Because of this we will look to simplify our transforms to something less egregious, while still retaining the useful elements of Pseudo Localization. We also heard feedback about the expansion text we add, “one two three four…”, it feels unnatural and can cause confusion around if the text is expansion text, or if it’s a placeholder / variable in the UI. As a result we will investigate other ways of simulating expansion, one option is to multiply vowels to achieve the same result, e.g.:Before: [ƒîกี้ð Ĥéļþ Öกี้ļîกี้é one two]After: [ƒîîîกี้ð Ĥéééļþ ÖÖกี้ļîîîกี้ééé]If you’re interested in working on high impact projects like the one we’ve talked about here, I have openings on my Internationalization team. Check out the role we have posted, and feel free to connect with me on LinkedIn!",https://medium.com/netflix-techblog/pseudo-localization-netflix-12fff76fbcbe?source=collection_home---4------14---------------------,0,netflix,python,NULL,2018-08-06
Netflix Cloud Security SIRT releases Diffy: A Differencing Engine for Digital Forensics in the Cloud,"Forest Monsen and Kevin Glisson, Netflix Security Intelligence and Response TeamThe Netflix Security Intelligence and Response Team (SIRT) announces the release of Diffy under an Apache 2.0 license. Diffy is a triage tool to help digital forensics and incident response (DFIR) teams quickly identify compromised hosts on which to focus their response, during a security incident on cloud architectures.Digital Forensics and Incident Response (DFIR) teams work in a variety of environments to quickly address threats to the enterprise. When operating in a cloud environment, our ability to work at scale, with imperative speed, becomes critical. Can we still operate? Do we have what we need?When moving through systems, attackers may leave artifacts — signs of their presence — behind. As an incident responder, if you’ve found one or two of these on disk or in memory, how do you know you’ve found all the instances touched by the attackers? Usually this is an iterative process; after finding the signs, you’ll search for more on other instances, then use what you find there to search again, until it seems like you’ve got them all. For DFIR teams, quickly and accurately “scoping a compromise” is critical, because when it’s time to eradicate the attackers, it ensures you’ll really kick them out.Since we don’t yet have a system query tool broadly deployed to quickly and easily interrogate large groups of individual instances (such as osquery), we realized in cases like these we would have some difficulty in determining exactly which instances needed closer examination, and which we could leave for later.We’ve scripted solutions using SSH, but we’ve also wanted to create an easier, more repeatable way to address the issue.Diffy finds outliers among a group of very similar hosts (e.g. AWS Auto Scaling Groups) and highlights those for a human investigator, who can then examine those hosts more closely. More importantly, Diffy helps an investigator avoid wasting time in forensics against hosts that don’t need close examination.How does Diffy do this? Diffy implements two methods to find outliers: a “functional baseline” method (implemented now), and a “clustering” method (to be implemented soon).How does the “functional baseline” method work?When is the functional baseline useful?How does the “clustering” method work?When is the clustering method useful?In environments supporting continuous integration or continuous delivery (CI/CD) such as ours, software is frequently deployed through a process involving first the checkout of code from a source control system, followed by the packaging of that code into a form combined (“baked”) into a system virtual machine (VM) image. The VM is then copied to a cloud provider, and started up as a VM instance in the cloud architecture. You can read more about this process in “How We Build Code at Netflix.”Diffy provides an API for application owners to call at deploy time, after those virtual machine instances begin serving traffic. When activated, Diffy deploys a system configuration and management tool called osquery to the instance (if it isn’t already present) and collects a baseline set of observations from the system by issuing SQL commands. We do this on virtual machines, but osquery can do this on containers as well.When an incident occurs, an incident responder can use Diffy to interrogate an ASG: first pulling the available baseline, next gathering current observations from all instances there, and finally comparing all instances within the ASG against that baseline. Instances that differ from the baseline in interesting, security-relevant ways are highlighted, and presented to the investigator for follow-up. If the functional baseline wasn’t previously collected, Diffy can rely solely on the clustering method. We’re not settled on the algorithm yet, but we see Diffy collecting observations from all instances in an ASG, and using the algorithm to identify outliers.In today’s cloud architectures, automation wins. Digital forensics and incident response teams need straightforward help to help them respond to compromises with swift action, quickly identifying the work ahead. Diffy can help those teams.We’ve characterized Diffy as one of our “Skunkworks” projects, meaning that the project is under active development and we don’t expect to be able to provide support, or a public commitment to improve the software. To download the code, visit https://github.com/Netflix-Skunkworks/diffy. If you’d like to contribute, take a look at our Contributor Guidelines at https://diffy.readthedocs.io/ to get started on your plugin and send us a pull request. Oh, and we’re hiring — if you’d like to help us solve these sorts of problems, take a look at https://jobs.netflix.com/teams/security, and reach out!",https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698?source=collection_home---4------15---------------------,0,netflix,,NULL,2018-07-17
Evolution of Application Data Caching : From RAM to SSD,"In 2013, we introduced EVCache a distributed in-memory caching solution based on memcached that offers low-latency, high-reliability caching and storage. It is well integrated with AWS and EC2, a Netflix OSS project, and in many occasions it is termed as “the hidden microservice”. Since then, EVCache has become a fundamental tier-0 service storing petabytes of data and hundred of billions of items, performing trillions of operations per day, has the ability to persist the data to the disk, and has a footprint of thousands of servers in three AWS regions.With the advent of Netflix global cloud architecture we are able to serve requests for a Netflix customer from any AWS region where we are deployed. The diagram below shows the logical structure of our multi-region deployment and the default routing of member traffic to AWS region.As we started moving towards the Global Cloud, we had a three times increase in the data that needed to be replicated and cached in each region. We also needed to move this data swiftly and securely across all regions. Supporting these features came at a considerable increase in cost and complexities. Our motivation was to provide a global caching solution which was not only fast but was also cost effective.Storing large amounts of data in volatile memory (RAM) is expensive. Modern disk technologies based on SSD are providing fast access to data but at a much lower cost when compared to RAM. Hence, we wanted to move part of the data out of memory without sacrificing availability or performance. The cost to store 1 TB of data on SSD is much lower than storing the same amount in RAM.We observed during experimentation that RAM random read latencies were rarely higher than 1 microsecond whereas typical SSD random read speeds are between 100–500 microseconds. For EVCache our typical SLA (Service Level Agreement) is around 1 millisecond with a default timeout of 20 milliseconds while serving around 100K RPS. During our testing using the storage optimized EC2 instances (I3.2xlarge) we noticed that we were able to perform over 200K IOPS of 1K byte items thus meeting our throughput goals with latency rarely exceeding 1 millisecond. This meant that by using SSD (NVMe) we were able to meet our SLA and throughput requirements at a significantly lower cost.EVCache Moneta was our first venture at using SSD to store data. The approach we chose there was to store all the data on SSD (RocksDB) and the active/hot data in RAM (Memcached). This approach reduced the size of most Moneta based clusters over 60% as compared to their corresponding RAM-only clusters. It worked well for Personalization & Recommendation use cases where the personalization compute systems periodically compute the recommendations for every user and use EVCache Moneta to store the data. This enabled us to achieve a significant reduction in cost for personalization storage clusters.However, we were unable to move some of the large online and customer facing clusters as we hit performance and throughput issues while overwriting existing data (due to compactions) on RocksDB. We would also exceed the desired SLA at times. As we were working towards solving these issues, Memcached External Storage (extstore), which had taken a different approach in using NVMe based storage devices, was announced.Memcached provides an external storage shim called extstore, that supports storing of data on SSD (I2) and NVMe (I3). extstore is efficient in terms of cost & storage device utilization without compromising the speed and throughput. All the metadata (key & other metadata) is stored in RAM whereas the actual data is stored on flash.With extstore we are able to use the storage device completely and more efficiently which we could not do achieve with Moneta. On Moneta based systems we could use at most 50% of the disc capacity as we had to ensure an old item could be deleted (FIFO compaction) only after it was written again. This meant we had could end up with a copy of new and old data for every item thus having a need for 50% disc utilization. Since we did not have need for storing duplicate records in extstore we are able to reduced the cost of extstore based EVCache clusters significantly. At this point, most of the EVCache clusters are scaled to meet network demands rather than storage demands. This has been quite a remarkable achievement.By moving from Moneta based clusters to extstore we are also able to take full advantage of the asynchronous metadump command (lru_crawler), which allows us to iterate through all of the keys in an instance. We use this to warm up a new cluster when we deploy a new version of memcached or scale the clusters up or down. By taking advantage of this command we can also take snapshot of the data at regular intervals or whenever we need. This ensures data in EVCache is durable and highly available in case of a disaster.The performance is also consistent compared to Moneta and rarely exceeds our SLA. Below is a log of disc access via iosnoop for read operations from one of the production cluster which is used to store users personalized recommendations.Below is a histogram plot of the read latencies from the log above. The majority of reads are around 100 microseconds or less.Below is the average read latency of one of cache comparing Moneta (blue) vs extstore (read). extstore latencies are consistently lower than Moneta for similar load across both the instances.With extstore we are able to handle all types of workloads whether it is a read heavy, write heavy or balanced. We are also able to handle data sets ranging from gigabytes to petabytes while maintaining consistent performance.It has been quite a journey to move from Moneta to extstore and as of now we have moved all our production clusters running Moneta to extstore. We have also been able to move some of the large RAM based memcached clusters to considerably smaller extstore clusters. The new architecture for EVCache Server running extstore is allowing us to continue to innovate in ways that matter. There’s still much to do and If you want to help solve this or similar big problems in cloud architecture, join us.— Shashi Madappa, Sridhar Enugula on behalf of the High Performance Data Team",https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690?source=collection_home---4------16---------------------,0,netflix,"docker,machine,machinelearning,python",NULL,2018-07-12
Auto Scaling Production Services on Titus,"By Andrew Leung, Amit Joshi, and the rest of the Titus team.Over the past three years, Netflix has been investing in container technology. A large part of this investment has been around Titus, Netflix’s container management platform that was open sourced in April of 2018. Titus schedules application containers to be run across a fleet of thousands of Amazon EC2 instances.Early on Titus focused on supporting simple batch applications and workloads that had a limited set of feature and availability requirements. As several internal teams building microservices wanted to adopt containers, Titus began to build scheduling support for service applications. However, supporting services, especially those in the customer critical path, required Titus to provide a much richer set of production ready features. Since Netflix’s migration to AWS began almost a decade earlier, microservices have been built atop EC2 and heavily leverage AWS and internal Netflix infrastructure services. The set of features used by an internal service then drove if or how that service could leverage Titus.One of the most commonly used service features is auto scaling. Many microservices are built to be horizontally scalable and leverage Amazon EC2 Auto Scaling to automatically add or remove EC2 instances as the workload changes. For example, as people on the east coast of the U.S. return home from work and turn on Netflix, services automatically scale up to meet this demand. Scaling dynamically with demand rather than static sizing helps ensure that services can automatically meet a variety of traffic patterns without service owners needing to size and plan their desired capacity. Additionally, dynamic scaling enables cloud resources that are not needed to be used for other purposes, such as encoding new content.As services began looking at leveraging containers and Titus, Titus’s lack of an auto scaling feature became either a major hurdle or blocker for adoption. Around the time that we were investigating building our own solution, we engaged with the AWS Auto Scaling team to describe our use case. As a result of Netflix’s strong relationship with AWS, this discussion and several follow ups led to the design of a new AWS Application Auto Scaling feature that allows the same auto scaling engine that powers services like EC2 and DynamoDB to power auto scaling in a system outside of AWS like Titus.This design centered around the AWS Auto Scaling engine being able to compute the desired capacity for a Titus service, relay that capacity information to Titus, and for Titus to adjust capacity by launching new or terminating existing containers. There were several advantages to this approach. First, Titus was able to leverage the same proven auto scaling engine that powers AWS rather than having to build our own. Second, Titus users would get to use the same Target Tracking and Step Scaling policies that they were familiar with from EC2. Third, applications would be able to scale on both their own metrics, such as request per second or container CPU utilization, by publishing them to CloudWatch as well as AWS-specific metrics, such as SQS queue depth. Fourth, Titus users would benefit from the new auto scaling features and improvements that AWS introduces.The key challenge was enabling the AWS Auto Scaling engine to call the Titus control plane running in Netflix’s AWS accounts. To address this, we leveraged AWS API Gateway, a service which provides an accessible API “front door” that AWS can call and a backend that could call Titus. API Gateway exposes a common API for AWS to use to adjust resource capacity and get capacity status while allowing for pluggable backend implementations of the resources being scaled, such as services on Titus. When an auto scaling policy is configured on a Titus service, Titus creates a new scalable target with the AWS Auto Scaling engine. This target is associated with the Titus Job ID representing the service and a secure API Gateway endpoint URL that the AWS Auto Scaling engine can use. The API Gateway “front door” is protected via AWS Service Linked Roles and the backend uses Mutual TLS to communicate to Titus.Configuring auto scaling for a Titus service works as follows. A user creates a service application on Titus, in this example using Spinnaker, Netflix’s continuous delivery system. The figure below shows configuring a Target Tracking policy for a Node.js application on the Spinnaker UI.The Spinnaker policy configuration also defines which metrics to forward to CloudWatch and the CloudWatch alarm settings. Titus is able to forward metrics to CloudWatch using Atlas, Netflix’s telemetry system. These metrics include those generated by the application and the container-level system metrics collected by Titus. When metrics are forwarded to Atlas they include information that associates them with the service’s Titus Job ID and whether Atlas should also forward them to CloudWatch.Once a user has selected policy settings on Spinnaker, Titus associates the service with a new scalable resource within the AWS Auto Scaling engine. This process is shown in the figure below. Titus configures both the AWS Auto Scaling policies and CloudWatch alarms for the service. Depending on the scaling policy type, Titus may explicitly create the CloudWatch alarm or AWS automatically may do it, in the case of Target Tracking policies.As service apps running on Titus emit metrics, AWS analyzes the metrics to determine whether CloudWatch alarm thresholds are being breached. If an alarm threshold has been breached, AWS triggers the alarm’s associated scaling actions. These actions result in calls to the configured API Gateway endpoints to adjust instance counts. Titus responds to these calls by scaling up or down the Job accordingly. AWS monitors both the results of these scaling requests and how metrics change.Providing an auto scaling feature that allowed Titus users to configure scaling policies the same way they would on EC2 greatly simplified adoption. Rather than coupling the adoption of containers with new auto scaling technology, Titus was able to provide the benefits of using containers with well tested auto scaling technology that users and their tools already understood. We followed the same pattern of leveraging existing AWS technology instead of building our own for several Titus features, such as networking, security groups, and load balancing. Additionally, auto scaling drove Titus availability improvements to ensure it was capable of making fast, online capacity adjustments. Today, this feature powers services that many Netflix customers interact with every day.Up until today, Titus has leveraged this functionality as a private AWS feature. We are happy that AWS has recently made this feature generally available to all customers as Custom Resource Scaling. Beyond container management platforms like Titus, any resource that needs scaling, like databases or big data infrastructure, can now leverage AWS Auto Scaling. In addition to helping drive key functionality for Titus, we are excited to see Netflix’s collaboration with AWS yield new features for general AWS customers.",https://medium.com/netflix-techblog/auto-scaling-production-services-on-titus-1f3cd49f5cd7?source=collection_home---4------17---------------------,0,netflix,"react,frontend",NULL,2018-07-09
Netflix at Spark+AI Summit 2018,"By Faisal SiddiqiApache Spark has been an immensely popular big data platform for distributed computing. Netflix has been using Spark extensively for various batch and stream-processed workloads. A substantial list of use cases for Spark computation come from the various applications in the domain of content recommendations and personalization. A majority of the machine learning pipelines for member personalization run atop large managed Spark clusters. These models form the basis of the recommender system that backs the various personalized canvases you see on the Netflix app including, title relevance ranking, row selection & sorting, and artwork personalization among others.Spark provides the computation infrastructure to help develop the models through data preparation, feature extraction, training, and model selection. The Personalization Infrastructure team has been helping scale Spark applications in this domain for the last several years. We believe strongly in sharing our learnings with the broader Spark community and at this year’s Spark +AI Summit in San Francisco, we had the opportunity to do so via three different talks on projects using Spark at Netflix scale. This post summarizes the three talks.Fact Store for Netflix Recommendations (Nitin Sharma, Kedar Sadekar)The first talk cataloged our journey building training data infrastructure for personalization models — how we built a fact store for extracting features in an ever-evolving landscape of new requirements. To improve the quality of our personalized recommendations, we try an idea offline using historical data. Ideas that improve our offline metrics are then pushed as A/B tests which are measured through statistically significant improvements in core metrics such as member engagement, satisfaction, and retention. The heart of such offline analyses are historical facts (for example, viewing history of a member, videos in ‘My List’ etc) that are used to generate features required by the machine learning model. Ensuring we capture enough fact data to cover all stratification needs of various experiments and guarantee that the data we serve is temporally accurate is an important requirement.In the talk, we presented the key requirements, evolution of our fact store design, its push-based architecture, the scaling efforts, and our learnings.We discussed how we use Spark extensively for data processing for this fact store and delved into the design tradeoffs of fast access versus efficient storage.Near Real-time Recommendations with Spark Streaming (Elliot Chow, Nitin Sharma)Many recommendations for the personalization use cases at Netflix are precomputed in a batch processing fashion, but that may not be quick enough for time sensitive use cases that need to take into account member interactions, trending popularity, and new show launch promotions. With an ever-growing Netflix catalog, finding the right content for our audience in near real-time is a necessary element to providing the best personalized experience.Our second talk delved into the realtime Spark Streaming ecosystem we have built at Netflix to provide this near-line ML Infrastructure. This talk was contextualized by a couple of product use cases using this near-real-time (NRT) infrastructure, specifically how we select the personalized video to present on the Billboard (large canvas at the top of the page), and how we select the personalized artwork for any title given the right canvas. We also reflected upon the lessons learnt while building a high volume infrastructure on top of Spark Streaming.With regards to the infrastructure, we talked about:Spark-based Stratification library for ML use cases (Shiva Chaitanya)Our last talk introduced a specific Spark based library that we built to help with stratification of the training sets used for offline machine learning workflows. This allows us to better model our users’ behaviors and provide them great personalized video recommendations.This library was originally created to implement user selection algorithms in our training data snapshotting infrastructure, but it has evolved to cater to the general-purpose stratification use cases in ML pipelines. The main idea here is to be able to provide a mechanism for down-sampling the data set while still maintaining the desired constraints on the data distribution. We described the flexible stratification API on top of Spark Dataframes.Choosing Spark+Scala gave us strong type safety in a distributed computing environment. We gave some examples of how, using the library’s DSL one can easily express complex sampling rules.These talks presented a few glimpses of the Spark usage from the Personalization use cases at Netflix. Spark is also used for many other data processing, ETL, and analytical uses in many other different domains in Netflix. Each domain brings its unique sets of challenges. For the member-facing personalization domain, the infrastructure needs to scale at the level of member scale. That means, for our over 125 million members and each of their active profiles, we need to personalize our content and do so reasonably fast for it to be relevant and timely.While Spark provides a great horizontally-scalable compute platform, we have found that using some of the advanced features, like code-gen for example, at our scale often poses interesting technical challenges. As Spark’s popularity grows, the project will need to continue to evolve to meet the growing hunger for truly big data sets and do a better job at providing transparency and ease of debugging for the workloads running on it.This is where sharing lessons from one organization can help benefit the community-at-large. We are happy to share our experiences at such conferences and welcome the ongoing interchange of ideas on making Spark better for modern ML and big data infrastructure use cases.If you are interested in joining such efforts, we’d love to hear from you as we look to accelerate our focus in areas of compute infrastructure for personalization and data systems for personalization.",https://medium.com/netflix-techblog/netflix-at-spark-ai-summit-2018-5304749ed7fa?source=collection_home---4------18---------------------,0,netflix,"docker,backend,css,cloud,frontend",NULL,2018-06-21
Simplifying Media Innovation at Netflix with Archer,"By Naveen Mareddy, Frank San Miguel, Rick Wong, Mangala Prabhu and Olof JohanssonImagine you are developing a quick prototype to sift through all the frames of the movie Bright to find the best shots of Will Smith with an action-packed background. Your goal is to get the computer vision algorithm right with high confidence without worrying about:In the past, our developers have had to think of all of these things and as you can see, it’s quite taxing when the goal is to simply get the algorithm right. In this blog post, we will share our journey into how we built a platform called Archer where everything is handled transparently enabling the users to dive right into the algorithm.We are Media Cloud Engineering (MCE). We enable high scale media processing which includes media transcoding, trailer generation, and high-quality image processing for artwork. Our compute farm runs tens of thousands of EC2 instances to crank through dynamic workloads. Some examples of compute-hungry use cases include A/B tests, catalog-wide re-encoding for shot-based encoding, and high-quality title images. We handle large-scale distributed computing aspects of the media compute platform and partner closely with Encoding Technologies team for the media standards and codecs.Before Archer, distributed media processing in the cloud was already possible with an in-house developed media processing platform, codename Reloaded. Despite its power and flexibility, development in the Reloaded platform required careful design of dynamic workflow, data model, and distributed workers while observing software development best practices, continuous integration (CI), deployment orchestration, and a staged release train. Although these are the right things to do for feature rollout, it is an impediment and distraction for researchers who just wanted to focus on their algorithms. To gain agility and shield themselves from the distractions of cloud deployment our users were running their experiments on local machines as much as possible. But here the scale was limited. They eventually needed to run their algorithms against a large content catalog to get a better signal.We looked into distributed computing frameworks like Apache Spark, Kubernetes, and Apache Flink. These frameworks were missing important features like first class support for media objects, custom docker image for each execution, or multi-tenant cluster support with fair resource balancing.Then we realized that we could combine the best attributes from the Reloaded with the patterns found in the popular distributed computing frameworks and the synthesis mitigated the difficulties mentioned previously, providing an easy-to-use platform that runs at scale for ad-hoc experiments and certain types of production use cases.Archer is an easy to use MapReduce style platform for media processing that uses containers so that users can bring their OS-level dependencies. Common media processing steps such as mounting video frames are handled by the platform. Developers write three functions: split, map and collect; and they can use any programming language. Archer is explicitly built for simple media processing at scale, and this means the platform is aware of media formats and gives white glove treatment for popular media formats. For example, a ProRes video frame is a first class object in Archer and splitting a video source into shot based chunks [1] is supported out of the box (a shot is a fragment of the video where the camera doesn’t move).Many innovative apps have been built using Archer, including an application that detects dead pixels caused by defective digital cameras, an app that uses machine learning (ML) to tag audio and an app that performs automated quality control (QC) for subtitles. We’ll get to more examples later.From a 10,000 foot view, Archer has multiple components to run jobs. Everything starts with a REST API to accept the job requests. The workflow engine then picks the request and drives the MapReduce workflow, dispatching work as messages to the priority queue. Application workers listen on the queue and execute the media processing functions supplied by the user. Given the dynamic nature of the work, Archer uses a queue aware scaler to continuously shift resources to ensure all applications get enough compute resources. (See Archer presentation at @Scale 2017 conference for a detailed overview and demo).Simplicity in Archer is made possible with features like efficient access to large files in the cloud, rapid prototyping with arbitrary media files and invisible infrastructure.MapReduce style — In Archer, users think of their processing job as having three functions: split, map and collect. The job of the split function is to split media into smaller units. The map function applies a media processing algorithm to each split. The collect function combines the results from the map phase. Users can implement all three functions with the programming language of their choice or use built-in functions. Archer provides built-in functions for common tasks such as a shot based video frame splitter and a concatenating collector. It’s very common to build an application by only implementing map function and use built-ins for splitter and collector. Archer users contribute reusable functions to the platform as built-ins.Video frames as images — Most computer vision (CV) algorithms like to work with JPEG/PNG images to detect complex features like motion estimation and camera shot detection. Video source formats use custom compression techniques to represent original sources, and decoding is needed to convert from the source format to images. To avoid the need to repeat the same code to decode video frames (different for each source format), Archer has a feature to allow users to pick image format, quality, and crop params during job submission.Container-based runtime — Archer users package their application as a docker image. They run the application locally or in the cloud in the same way. The container-based local development allows users to get an application into a working state quickly and iterate rapidly after that, then with a few commands, the application can be run in the cloud at scale. The Docker-based environment allows users to install operating system (OS) dependencies of their choice and each application can choose their OS-dependencies independent of other applications. For example, experiments run in Archer may install snapshot versions of media tools like ffmpeg and get quick feedback while production apps will depend on released versions. Archer uses Titus (container management platform at Netflix) to run containers at scale.Access to content catalog — Most Archer applications need access to media sources from the Netflix content catalog. The Archer job API offers a content selector that lets users select the playable of their choice as input for their job execution. For example, you can run your algorithm against the UHD video source of the film Bright by just knowing the movie id. There is no need to worry about the location of the video source in the cloud or media format of the source.Local development — The Developer Productivity team at Netflix has built a tool called Newt (Netflix Workflow Toolkit) to simplify local developer workflows. Archer uses Newt to provide a rich command line interface to make local development easy. Starting a new Archer job or downloading results is just a command away. These commands wrap local docker workflows and interactions with the Archer job API. It’s also easy to build applications in the programming language of choice.With a simple platform like Archer, our engineers are free to dream about ideas and realize them in a matter of hours or days. Without Archer to do the heavy lifting, we may not have attempted some of these innovations. Our users leveraged tens of millions of CPU hours to create amazing applications. Some examples:Archer is still in active development, and we are continually extending its capabilities and scale. The more experience we have with it, the more possibilities we see. Here are some of the items on our roadmapIn an upcoming blog post, we will be writing about the secure media storage service that underpins Archer and other projects at Netflix.The Archer platform is still relatively new. But the concept is daily being validated by the many teams at Netflix who are adopting it and producing innovative advances in the Netflix product. Enthusiasm and usage are growing, and so also is our need for engineering talent. If you are excited to work on large-scale distributed computing problems in media processing and think out of the box by applying machine learning and serverless concepts, we are hiring (MCE, Content Engineering). Also, check out research.netflix.com to learn more about research & data science at Netflix.[1] S. Bhattacharya, A. Prakash, and R. Puri, Towards Scalable Automated Analysis of Digital Video Assets for Content Quality Control Applications, SMPTE 2017 Annual Technical Conference, and Exhibition, Hollywood & Highland, Los Angeles, California, 2017",https://medium.com/netflix-techblog/simplifying-media-innovation-at-netflix-with-archer-3f8cbb0e2bcb?source=collection_home---4------19---------------------,0,netflix,"java,python",NULL,2018-06-20
Growth Engineering at Netflix — Accelerating Innovation,"by Eric EiswerthMillions of people visit Netflix every day. Many of them are already Netflix members, looking to enjoy their favorite movies and TV shows, and we work hard to ensure they have a great experience. Others are not yet members, and are looking to better understand our service before signing up.These prospective members arrive from over 190 countries around the world, and each person arrives with a different set of preferences and intentions. Perhaps they want to see what all the buzz is about and learn what Netflix is, or perhaps they already know what Netflix is and can’t wait to sign up and try out the service. Marketing, social, PR, and word of mouth all help to create awareness and convert that into demand. Growth Engineering collects this demand by helping people sign up, while optimizing for key business metrics such as conversion rate, retention, revenue, etc. We do this by building, maintaining, and operating the backend services that support the signup and login flows that work across mobile phones, tablets, computers and connected televisions.Let’s take a look at what the Netflix sign up experience looks like for two different customers in two different parts of the world, each with different device types and payment methods. Barb is signing up on a set-top-box (STB) device in the United States and prefers to have her billing done through her cable provider. While Riko is signing up on an iPhone 7 in Japan and prefers to use a credit card.The customer experience is remarkably different in each of these cases, but the goal is the same. We seek to offer the best possible signup experience to our prospective members while at the same time, remaining extremely lean, agile and efficient in our implementation of these disparate experiences.Offering an amazing signup experience for thousands of devices in over 190 countries is an incredibly challenging and rewarding task.The signup funnel is where demand is collected. In general, the signup funnel consists of four parts:In the signup funnel, we have a short time to get to know our users and we want to help them sign up as efficiently and effectively as possible. How do we know if we’re succeeding at meeting these goals? We experiment constantly. We use A/B testing in order to learn and improve how users navigate the signup funnel. This enables Growth Engineering to be a lean team that has a tremendous and measurable impact on the business.Every visit to the signup funnel is an opportunity to improve the experience for visitors wanting to learn more about Netflix. We’ve learned from experimentation that different customers have different needs and expectations.Using a TV remote control to navigate the signup flow can be an onerous and time-consuming task. E.g. by leveraging our partnerships, we are able to offer a signup experience with almost no use of the remote control keypad. This enables us to offer a simple and convenient signup experience with integrated billing. The end result is a lower friction signup flow that has improved user experience and business metrics.Browsers offer additional conveniences that can be leveraged. In particular, local payment options (e.g. paying using direct debit or local credit cards) and browser autofill enable us to offer an optimized signup experience that lets customers sign up for Netflix and start watching great content in just a few minutes.As these examples highlight, there are many attributes that can be used to optimize a particular flow. By experimenting with different partnerships, payment methods, and user experiences, we are able to affect the membership base growth rate and ultimately, revenue.Growth Engineering owns the business logic and protocols that allow our UI partners to build lightweight and flexible applications for almost any platform (e.g., iOS, Android, Smart TVs, browsers). Our services speak a custom JSON protocol over HTTP. The protocol is stateless and offers a minimal set of primitives and conventions that enable rapid development of features on almost any platform.Before diving into core concepts, it’s useful to see where Growth Engineering’s services live within the Netflix microservice ecosystem. Typically, these microservices are implemented in Java and are deployed to AWS on EC2 virtual machines.Growth Engineering owns multiple services that each provide a specific function to the signup funnel. The Orchestration Service is responsible for validating upstream requests, orchestrating calls to downstream services, and composing JSON responses during a signup flow. We assume requests will fail and use libraries like Hystrix to ensure we are latency and fault tolerant. This enables our customers to have an extremely resilient and reliable sign up experience.Let’s walk through what it looks like to register for Netflix with a partner-integrated STB device.Step 1: Request the registration pageThe green diamonds and arrows show a successful request path for the registration page.Step 2: JSON responseThe UI can then interpret this response accordingly and render a UI as such:Step 3: Send form details to the server and create an accountStep 4: JSON ResponseAs you can see, there is a lot of complexity abstracted away in a simple attempt to register for Netflix. In general, processing a request consists of 3 steps:The JSON protocol also enables Growth Engineering to be a source of truth for all events pertaining to the signup funnel. This enables us to centrally collect and monitor all the core sign up related business metrics, thus enabling us to be nimble day-to-day.As the stewards of the business logic for the signup funnel, Growth Engineering has an incredibly important role at Netflix. Our work directly affects the membership growth rate and as a result, directly impacts Netflix revenue. Although Netflix is more than two years into our journey as a fully global entertainment company, we are only just beginning to understand many of the complicated and intricate consumer preferences that will inform the next set of experiments aimed at improving the signup funnel. We are just beginning to unlock user experience improvements in our international markets.Netflix has over 125 million members worldwide. The number of global broadband households is over 1 billion and the number of daily internet users is over 4 billion. Growth Engineering is key to making Netflix more accessible for people around the world. Join our team and help us shape the future of global customer acquisition at Netflix.",https://medium.com/netflix-techblog/growth-engineering-at-netflix-accelerating-innovation-90eb8e70ce59?source=collection_home---4------20---------------------,0,netflix,"java,python",NULL,2018-06-18
Metacat: Making Big Data Discoverable and Meaningful at Netflix,"by Ajoy Majumdar, Zhen LiMost large companies have numerous data sources with different data formats and large data volumes. These data stores are accessed and analyzed by many people throughout the enterprise. At Netflix, our data warehouse consists of a large number of data sets stored in Amazon S3 (via Hive), Druid, Elasticsearch, Redshift, Snowflake and MySql. Our platform supports Spark, Presto, Pig, and Hive for consuming, processing and producing data sets. Given the diverse set of data sources, and to make sure our data platform can interoperate across these data sets as one “single” data warehouse, we built Metacat. In this blog, we will discuss our motivations in building Metacat, a metadata service to make data easy to discover, process and manage.The core architecture of the big data platform at Netflix involves three key services. These are the execution service (Genie), the metadata service, and the event service. These ideas are not unique to Netflix, but rather a reflection of the architecture that we felt would be necessary to build a system not only for the present, but for the future scale of our data infrastructure.Many years back, when we started building the platform, we adopted Pig as our ETL language and Hive as our ad-hoc querying language. Since Pig did not natively have a metadata system, it seemed ideal for us to build one that could interoperate between both.Thus Metacat was born, a system that acts as a federated metadata access layer for all data stores we support. A centralized service that our various compute engines could use to access the different data sets. In general, Metacat serves three main objectives:It is worth noting that other companies that have large and distributed data sets also have similar challenges. Apache Atlas, Twitter’s Data Abstraction Layer and Linkedin’s WhereHows (Data Discovery at Linkedin), to name a few, are built to tackle similar problems, but in the context of the respective architectural choices of the companies.Metacat is a federated service providing a unified REST/Thrift interface to access metadata of various data stores. The respective metadata stores are still the source of truth for schema metadata, so Metacat does not materialize it in its storage. It only directly stores the business and user-defined metadata about the datasets. It also publishes all of the information about the datasets to Elasticsearch for full-text search and discovery.At a higher level, Metacat features can be categorized as follows:Multiple query engines like Pig, Spark, Presto and Hive are used at Netflix to process and consume data. By introducing a common abstraction layer, datasets can be accessed interchangeably by different engines. For example: A Pig script reading data from Hive will be able to read the table with Hive column types in Pig types. For data movement from one datastore to another, Metacat makes the process easy by helping in creating the new table in the destination data store using the destination table data types. Metacat has a defined list of supported canonical data types and has mappings from these types to each respective data store type. For example, our data movement tool uses the above feature for moving data from Hive to Redshift or Snowflake.The Metacat thrift service supports the Hive thrift interface for easy integration with Spark and Presto. This enables us to funnel all metadata changes through one system which further enables us to publish notifications about these changes to enable data driven ETL. When new data arrives, Metacat can notify dependent jobs to start.Metacat stores additional business and user-defined metadata about datasets in its storage. We currently use business metadata to store connection information (for RDS data sources for example), configuration information, metrics (Hive/S3 partitions and tables), and tables TTL (time-to-live) among other use cases. User-defined metadata, as the name suggests, is a free form metadata that can be set by the users for their own usage.Business metadata can also be broadly categorized into logical and physical metadata. Business metadata about a logical construct such as a table is considered as logical metadata. We use metadata for data categorization and for standardizing our ETL processing. Table owners can provide audit information about a table in the business metadata. They can also provide column default values and validation rules to be used for writes into the table.Metadata about the actual data stored in the table or partition is considered as physical metadata. Our ETL processing stores metrics about the data at job completion, which is later used for validation. The same metrics can be used for analyzing the cost + space of the data. Given two tables can point to the same location (like in Hive), it is important to have the distinction of logical vs physical metadata because two tables can have the same physical metadata but have different logical metadata.As consumers of the data, we should be able to easily browse through and discover the various data sets. Metacat publishes schema metadata and business/user-defined metadata to Elasticsearch that helps in full-text search for information in the data warehouse. This also enables auto-suggest and auto-complete of SQL in our Big Data Portal SQL editor. Organizing datasets as catalogs helps the consumer browse through the information. Tags are used to categorize data based on organizations and subject areas. We also use tags to identify tables for data lifecycle management.Metacat, being a central gateway to the data stores, captures any metadata changes and data updates. We have also built a push notification system around table and partition changes. Currently, we are using this mechanism to publish events to our own data pipeline (Keystone) for analytics to better understand our data usage and trending. We also publish to Amazon SNS. We are evolving our data platform architecture to be an event-driven architecture. Publishing events to SNS allows other systems in our data platform to “react” to these metadata or data changes accordingly. For example, when a table is dropped, our S3 warehouse janitor services can subscribe to this event and clean up the data on S3 appropriately.The Hive metastore, backed by an RDS, does not perform well under high load. We have noticed a lot of issues around writing and reading of partitions using the metatore APIs. Given this, we no longer use these APIs. We have made improvements in our Hive connector that talks directly to the backed RDS for reading and writing partitions. Before, Hive metastore calls to add a few thousand partitions usually timed out, but with our implementation, this is no longer a problem.We have come a long way on building Metacat, but we are far from done. Here are some additional features that we still need to work on to enhance our data warehouse experience.As we continue to develop features to support our use cases going forward, we’re always open to feedback and contributions from the community. You can reach out to us via Github or message us on our Google Group. We hope to share more of what our teams are working on later this year!And if you’re interested in working on big data challenges like this, we are always looking for great additions to our team. You can see all of our open data platform roles here.",https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520?source=collection_home---4------21---------------------,0,netflix,"docker,backend,json,python",NULL,2018-06-14
The End of Video Coding?,"In the IEEE Signal Processing Magazine issue November 2006 article “Future of Video Coding and Transmission” Prof. Edward Delp started by asking the panelists “Is video coding dead? Some feel that, with the higher coding efficiency of the H.264/MPEG-4 . . . perhaps there is not much more to do. I must admit that I have heard this compression is dead argument at least four times since I started working in image and video coding in 1976.”People were postulating that video coding was dead more than four decades ago. And yet here we are in 2018, organizing the 33rd edition of Picture Coding Symposium (PCS).Is image and video coding dead? From the standpoint of application and relevance, video compression is very much alive and kicking and thriving on the internet. The Cisco white paper “The Zettabyte Era: Trends and Analysis (June 2017)” reported that in 2016, IP video traffic accounted for 73% of total IP traffic. This is estimated to go up to 82% by 2021. Sandvine reported in the “Global Internet Phenomena Report, June 2016” that 60% of peak download traffic on fixed access networks in North America was accounted for by four VOD services: Netflix, YouTube, Amazon Video and Hulu. Ericsson’s “Mobility Report November 2017” estimated that for mobile data traffic in 2017, video applications occupied 55% of the traffic. This is expected to increase to 75% by 2023.As for industry involvement in video coding research, it appears that the area is more active than ever before. The Alliance for Open Media (AOM) was founded in 2015 by leading tech companies to collaborate on an open and royalty-free video codec. The goal of AOM was to develop video coding technology that was efficient, cost-effective, high quality and interoperable, leading to the launch of AV1 this year. In the ITU-T VCEG and ISO/IEC MPEG standardization world, the Joint Video Experts Team (JVET) was formed in October 2017 to develop a new video standard that has capabilities beyond HEVC. The recently-concluded Call for Proposals attracted an impressive number of 32 institutions from industry and academia, with a combined 22 submissions. The new standard, which will be called Versatile Video Coding (VVC), is expected to be finalized by October 2020.Like many global internet companies, Netflix realizes that advancements in video coding technology are crucial for delivering more engaging video experiences. On one end, many people are constrained by unreliable networks or limited data plans, restricting the video quality that can be delivered with current technology. On the other side of the spectrum, premium video experiences like 4K UHD, 360-degree video and VR, are extremely data-heavy. Video compression gains are necessary to fuel the adoption of these immersive video technologies.So how will we get to deliver HD quality Stranger Things at 100 kbps for the mobile user in rural Philippines? How will we stream a perfectly crisp 4K-HDR-WCG episode of Chef’s Table without requiring a 25 Mbps broadband connection? Radically new ideas. Collaboration. And forums like the Picture Coding Symposium 2018 where the video coding community can share, learn and introspect.Influenced by our product roles at Netflix, exposure to the standardization community and industry partnerships, and research collaboration with academic institutions, we share some of our questions and thoughts on the current state of video coding research. These ideas have inspired us as we embarked on organizing the special sessions, keynote speeches and invited talks for PCS 2018.Let’s innovate beyond block-based hybrid encoding.MPEG-2, VC1, H.263, H.264/AVC, H.265/HEVC, VP9, AV1 — all of these standards were built on the block-based hybrid video coding structure. Attempts to veer away from this traditional model have been unsuccessful. In some cases (say, distributed video coding), it was because the technology was impractical for the prevalent use case. In most other cases, however, it is likely that not enough resources were invested in the new technology to allow for maturity. Unfortunately, new techniques are evaluated against the state-of-the-art codec, for which the coding tools have been refined from decades of investment. It is then easy to drop the new technology as “not at-par.” Are we missing on better, more effective techniques by not allowing new tools to mature? How many redundant bits can we squeeze out if we simply stay on the paved path and iterate on the same set of encoding tools?The community needs better ways to measure video quality.In academic publications, standardization activities, and industry codec evaluations, PSNR remains the gold standard for evaluating encoding performance. And yet every person in the field will tell you that PSNR does not accurately reflect human perception. Encoding tools like adaptive quantization and psycho-visual optimization claim to improve visual quality but fare worse in terms of PSNR. So researchers and engineers augment the objective measurements with labor-intensive visual subjective tests. Although this evaluation methodology has worked for decades, it is infeasible for large scale evaluation, especially, if the test set spans diverse content and wide quality ranges. For the video codec community to innovate more quickly, and more accurately, automated video quality measurements that better reflect human perception should be utilized. These new metrics have to be widely agreed upon and adopted, so it is necessary that they open and independently verifiable. Can we confidently move video encoding technology without solving the problem of automated video quality assessment first?Encouraging new ideas means discussing with new people.I (Anne) attended my first MPEG meeting three years ago where I presented an input document on Netflix use cases for future video coding. I claimed that for the Netflix application, encoding complexity increase is not a concern if it comes with significant compression improvement. We run compute on the cloud and have no real-time requirements. I was asked by the Chair, “How much complexity increase is acceptable?” I was not prepared for the question, so did some quick math in my mind estimating an upper bound and said “At the worst case 100X.” The room of about a hundred video standardization experts burst out laughing. I looked at the Chair perplexed, and he says, “Don’t worry they are happy that they can try-out new things. People typically say 3X.” We were all immersed in the video codec space and yet my views surprised them and vice versa.The video coding community today is composed of research groups in academia, institutions active in video standardization, companies implementing video codec technologies and technology and entertainment companies deploying video services. How do we foster more cross-pollination and collaboration across these silos to positively lift all boats?In the spirit of stimulating more perplexed looks that will then hopefully lead to more “aha!” moments, we have organized a series of “Bridging the Gap” sessions for PCS 2018. The talks and panel discussion aim to connect PCS researchers with related fields and communities.To address some of the critical questions in video compression today, we have also organized Special Sessions on Machine Learning of Image and Video Compression, Image and Video Quality Assessment with Industry Applications, and Content Preparation and Compression for VR. In addition, we will have excellent keynote talks by Prof. Vivienne Sze from ​Massachusetts Institute of Technology, Prof. Al Bovik from The University of Texas at Austin, and Prof. Yao Wang from ​New York University.We hope that Picture Coding Symposium 2018 will build bridges, spark stimulating discussions and foster groundbreaking innovation in video and image coding. Join us in San Francisco to help shape the future of video coding!",https://medium.com/netflix-techblog/the-end-of-video-coding-40cf10e711a2?source=collection_home---4------22---------------------,0,netflix,,NULL,2018-06-13
Lessons from Building Observability Tools at Netflix,"Our mission at Netflix is to deliver joy to our members by providing high-quality content, presented with a delightful experience. We are constantly innovating on our product at a rapid pace in pursuit of this mission. Our innovations span personalized title recommendations, infrastructure, and application features like downloading and customer profiles. Our growing global member base of 125 million members can choose to enjoy our service on over a thousand types of devices. If you also consider the scale and variety of content, maintaining the quality of experience for all our members is an interesting challenge. We tackle that challenge by developing observability tools and infrastructure to measure customers’ experiences and analyze those measurements to derive meaningful insights and higher-level conclusions from raw data. By observability, we mean analysis of logs, traces, and metrics. In this post, we share the following lessons we have learned:We started our tooling efforts with providing visibility into device and server logs, so that our users can go to one tool instead of having to use separate data-specific tools or logging into servers. Providing visibility into logs is valuable because log messages include important contextual information, especially when errors occur.However, at some point in our business growth, storing device and server logs didn’t scale because the increasing volume of log data caused our storage cost to balloon and query times to increase. Besides reducing our storage retention time period, we addressed scalability by implementing a real-time stream processing platform called Mantis. Instead of saving all logs to persistent storage, Mantis enables our users to stream logs into memory, and keep only those logs that match SQL-like query criteria. Users also have the choice to transform and save matching logs to persistent storage. A query that retrieves a sample of playback start events for the Apple iPad is shown in the following screenshot:Once a user obtains an initial set of samples, they can iteratively refine their queries to narrow down the specific set of samples. For example, perhaps the root cause of an issue is found from only samples in a specific country. In this case, the user can submit another query to retrieve samples from that country.The key takeaway is that storing all logs in persistent storage won’t scale in terms of cost and acceptable query response time. An architecture that leverages real-time event streams and provides the ability to quickly and iteratively identify the relevant subset of logs is one way to address this problem.As applications migrated to a microservices architecture, we needed insight into the complex decisions that microservices are making, and an approach that would correlate those decisions. Inspired by Google’s Dapper paper on distributed request tracing, we embarked on implementing request tracing as a way to address this need. Since most inter-process communication uses HTTP and gRPC (with the trend for newer services to use gRPC to benefit from its binary protocol), we implemented request interceptors for HTTP and gRPC calls. These interceptors publish trace data to Apache Kafka, and a consuming process writes trace data to persistent storage.The following screenshot shows a sample request trace in which a single request results in calling a second tier of servers, one of which calls a third-tier of servers:The smaller squares beneath a server indicate individual operations. Gray-colored servers don’t have tracing enabled.A distributed request trace provides only basic utility in terms of showing a call graph and basic latency information. What is unique in our approach is that we allow applications to add additional identifiers to trace data so that multiple traces can be grouped together across services. For example, for playback request traces, all the requests relevant to a given playback session are grouped together by using a playback session identifier. We also implemented additional logic modules called analyzers to answer common troubleshooting questions. Continuing with the above example, questions about a playback session might be why a given session did or did not receive 4K video, or why video was or wasn’t offered with High Dynamic Range.Our goal is to increase the effectiveness of our tools by providing richer and more relevant context. We have started implementing machine learning analysis on error logs associated with playback sessions. This analysis does some basic clustering to display any common log attributes, such as Netflix application version number, and we display this information along with the request trace. For example, if a given playback session has an error log, and we’ve noticed that other similar devices have had the same error with the same Netflix application version number, we will display that application version number. Users have found this additional contextual information helpful in finding the root cause of a playback error.In summary, the key learnings from our effort are that tying multiple request traces into a logical concept, a playback session in this case, and providing additional context based on constituent traces enables our users to quickly determine the root cause of a streaming issue that may involve multiple systems. In some cases, we are able to take this a step further by adding logic that determines the root cause and provides an English explanation in the user interface.Besides analysis of logging and request traces, observability also involves analysis of metrics. Because having users examine many logs is overwhelming, we extended our offering by publishing log error counts to our metrics monitoring system called Atlas, which enables our users to quickly see macro-level error trends using multiple dimensions, such as device type and customer geographical location. An alerting system also allows users to receive alerts if a given metric exceeds a defined threshold. In addition, when using Mantis, a user can define metrics derived from matching logs and publish them to Atlas.Next, we have implemented statistical algorithms to detect anomalies in metrics trends, by comparing the current trend with a baseline trend. We are also working on correlating metrics for related microservices. From our work with anomaly detection and metrics correlation, we’ve learned how to define actionable alerting beyond just basic threshold alerting. In a future blog post, we’ll discuss these efforts.We store data used by our tools in Cassandra, Elasticsearch, and Hive. We chose a specific database based primarily on how our users want to retrieve a given data type, and the write rate. For observability data that is always retrieved by primary key and a time range, we use Cassandra. When data needs to be queried by one or more fields, we use Elasticsearch since multiple fields within a given record can be easily indexed. Finally, we observed that recent data, such as up to the last week, is accessed more frequently than older data, since most of our users troubleshoot recent issues. To serve the use case where someone wants to access older data, we also persist the same logs in Hive but for a longer time period.Cassandra, Elasticsearch, and Hive have their own advantages and disadvantages in terms of cost, latency, and queryability. Cassandra provides the best, highest per-record write and read rates, but is restrictive for reads because you must decide what to use for a row key (a unique identifier for a given record) and within each row, what to use for a column key, such as a timestamp. In contrast, Elasticsearch and Hive provide more flexibility with reads because Elasticsearch allows you to index any field within a record, and Hive’s SQL-like query language allows you to match against any field within a record. However, since Elasticsearch is primarily optimized for free text search, its indexing overhead during writes will demand more computing nodes as write rate increases. For example, for one of our observability data sets, we initially stored data in Elasticsearch to be able to easily index more than one field per record, but as the write rate increased, indexing time became long enough that either the data wasn’t available when users queried for it, or it took too long for data to be returned. As a result, we migrated to Cassandra, which had shorter write ingestion time and shorter data retrieval time, but we defined data retrieval for the three unique keys that serve our current data retrieval use cases.For Hive, since records are stored in files, reads are relatively much slower than Cassandra and Elasticsearch because Hive must scan files. Regarding storage and computing cost, Hive is the cheapest because multiple records can be kept in a single file, and data isn’t replicated. Elasticsearch is most likely the next more expensive option, depending on the write ingestion rate. Elasticsearch can also be configured to have replica shards to enable higher read throughput. Cassandra is most likely the most expensive, since it encourages replicating each record to more than one replica in order to ensure reliability and fault tolerance.As usage of our observability tools grows, users have been continually asking for new features. Some of those new feature requests involve displaying data in a view customized for specific user groups, such as device developers, server developers, and Customer Service. On a given page in one of our tools, some users want to see all types of data that the page offers, whereas other users want to see only a subset of the total data set. We addressed this requirement by making the page customizable via persisted user preferences. For example, in a given table of data, users want the ability to choose which columns they want to see. To meet this requirement, for each user, we store a list of visible columns for that table. Another example involves a log type with large payloads. Loading those logs for a customer account increases the page loading time. Since only a subset of users are interested in this log type, we made loading these logs a user preference.Examining a given log type may require domain expertise that not all users may have. For example, for a given log from a Netflix device, understanding the data in the log requires knowledge of some identifiers, error codes, and some string keys. Our tools try to minimize the specialized knowledge required to effectively diagnose problems by joining identifiers with the data they refer to, and providing descriptions of error codes and string keys.In short, our learning here is that customized views and helpful context provided by visualizations that surface relevant information are critical in communicating insights effectively to our users.Our observability tools have empowered many teams within Netflix to better understand the experience we are delivering to our customers and quickly troubleshoot issues across various facets such as devices, titles, geographical location, and client app version. Our tools are now an essential part of the operational and debugging toolkit for our engineers. As Netflix evolves and grows, we want to continue to provide our engineers with the ability to innovate rapidly and bring joy to our customers. In future blog posts, we will dive into technical architecture, and we will share our results from some of our ongoing efforts such as metrics analysis and using machine learning for log analysis.If any of this work sounds exciting to you, please reach out to us!— Kevin Lew (@kevinlew15) and Sangeeta Narayanan (@sangeetan)",https://medium.com/netflix-techblog/lessons-from-building-observability-tools-at-netflix-7cfafed6ab17?source=collection_home---4------23---------------------,0,netflix,,NULL,2018-06-04
Open Sourcing Zuul 2,"We are excited to announce the open sourcing of Zuul 2, Netflix’s cloud gateway. We use Zuul 2 at Netflix as the front door for all requests coming into Netflix’s cloud infrastructure. Zuul 2 significantly improves the architecture and features that allow our gateway to handle, route, and protect Netflix’s cloud systems, and helps provide our 125 million members the best experience possible. The Cloud Gateway team at Netflix runs and operates more than 80 clusters of Zuul 2, sending traffic to about 100 (and growing) backend service clusters which amounts to more than 1 million requests per second. Nearly all of this traffic is from customer devices and browsers that enable the discovery and playback experience you are likely familiar with.This post will overview Zuul 2, provide details on some of the interesting features we are releasing today, and discuss some of the other projects that we’re building with Zuul 2.For context, here’s a high-level diagram of Zuul 2’s architecture:The Netty handlers on the front and back of the filters are mainly responsible for handling the network protocol, web server, connection management and proxying work. With those inner workings abstracted away, the filters do all of the heavy lifting. The inbound filters run before proxying the request and can be used for authentication, routing, or decorating the request. The endpoint filters can either be used to return a static response or proxy the request to the backend service (or origin as we call it). The outbound filters run after a response has been returned and can be used for things like gzipping, metrics, or adding/removing custom headers.Zuul’s functionality depends almost entirely on the logic that you add in each filter. That means you can deploy it in multiple contexts and have it solve different problems based on the configurations and filters it is running.We use Zuul at the entrypoint of all external traffic into Netflix’s cloud services and we’ve started using it for routing internal traffic, as well. We deploy the same core but with a substantially reduced amount of functionality (i.e. fewer filters). This allows us to leverage load balancing, self service routing, and resiliency features for internal traffic.The Zuul code that’s running today is the most stable and resilient version of Zuul yet. The various phases of evolving and refactoring the codebase have paid dividends and we couldn’t be happier to share it with you.Today we are releasing many core features. Here are the ones we’re most excited about:We are also working on some features that will be coming soon, including:We would love to hear from you and see all the new and interesting applications of Zuul. For instructions on getting started, please visit our wiki page.Internally, there are several major features that we’ve been working on but have not open sourced yet. Each one deserves its own blog post, but let’s go over them briefly.The most widely-used feature by our partners is self service routing. We provide an application and API for users to create routing rules based on any criteria in the request URL, path, query params, or headers. We then publish these routing rules to all the Zuul instances.The main use case is for routing traffic to a specific test or staging cluster. However, there are many use cases for real production traffic. For example:As you can see we use self service routing extensively and are increasing the customizability and scope of routes to allow for even more use cases.Another major feature we’ve worked on is making load balancing to origins more intelligent. We are able to route around failures, slowness, GC issues, and various other things that crop up often when running large amounts of nodes. The goal of this work is to increase resiliency, availability, and quality of service for all Netflix services.We have several cases that we handle:When new origin instances start up, we send them a reduced amount of traffic for some time, until they’re warmed up. This was an issue we observed for applications with large codebases and huge metaspace usage. It takes a significant amount of time for these apps to JIT their code and be ready to handle a large amount of traffic.We also generally bias the traffic to older instances and if we happen to hit a cold instance that throttles, we can always retry on a warm one. This gives us an order of magnitude improvement in availability.Errors happen all the time and for varying reasons, whether it’s because of a bug in the code, a bad instance, or an invalid configuration property being set. Fortunately, as a proxy, we can detect errors reliably — either we get a 5xx error or there are connectivity problems to the service.We track error rates for each origin and if the error rate is high enough, it implies the entire service is in trouble. We throttle retries from devices and disable internal retries to allow the service to recover. Moreover, we also track successive failures per instance and blacklist the bad ones for a period of time.With the above approaches we send less traffic to servers in a cluster that are throttling or refusing connections, and lessened the impact by retrying those failed requests on other servers.We’re now rolling out an additional approach where we aim to avoid overloading servers in the first place. This is achieved by allowing origins to signal to Zuul their current utilization, which Zuul then uses as a factor in its load-balancing choices — leading to reduced error rates, retries, and latency.The origins add a header to all responses stating their utilization as a percentage, along with a target utilization they would like to have across the cluster. Calculating the percentage is completely up to each application and engineers can use whatever metric suits them best. This allows for a general solution as opposed to us trying to come up with a one-size-fits-all approach.With this functionality in place, we assign a score (combination of instance utilization and other factors like the ones above) to each instance and do a choice-of-two load balancing selection.As we grew from just a handful of origins to a new world where anyone can quickly spin up a container cluster and put it behind Zuul, we found there was a need to automatically detect and pinpoint origin failures.With the help of Mantis real time event streaming, we built an anomaly detector that aggregates error rates per service and notifies us in real time when services are in trouble. It takes all of the anomalies in a given time window and creates a timeline of all the origins in trouble. We then create a contextual alert email with the timeline of events and services affected. This allows an operator to quickly correlate these events and orient themselves to debug a specific app or feature, and ultimately find the root cause.In fact, it was so useful that we expanded it to send notifications to the origin teams themselves. We’ve also added more internal applications, other than Zuul, and can build a much more extensive timeline of events. This has been a huge help during production incidents and helps operators quickly detect and fix problems before they cascade into massive outages.We hope to open source as many of the above features as we can. Keep watching the tech blog for more depth on them in the future. If you want to help us solve these kinds of problem, please check out our jobs site.— Arthur Gonigberg ( @agonigberg ), Mikey Cohen (@moldfarm ), Michael Smith (@kerumai ), Gaya Varadarajan ( @gaya3varadhu ), Sudheer Vinukonda ( @apachesudheerv ), Susheel Aroskar (@susheelaroskar )",https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3?source=collection_home---4------0---------------------,0,netflix,,NULL,2018-05-21
Full Cycle Developers at Netflix — Operate What You Build,"The year was 2012 and operating a critical service at Netflix was laborious. Deployments felt like walking through wet sand. Canarying was devolving into verifying endurance (“nothing broke after one week of canarying, let’s push it”) rather than correct functionality. Researching issues felt like bouncing a rubber ball between teams, hard to catch the root cause and harder yet to stop from bouncing between one another. All of these were signs that changes were needed.Fast forward to 2018. Netflix has grown to 125M global members enjoying 140M+ hours of viewing per day. We’ve invested significantly in improving the development and operations story for our engineering teams. Along the way we’ve experimented with many approaches to building and operating our services. We’d like to share one approach, including its pros and cons, that is relatively common within Netflix. We hope that sharing our experiences inspires others to debate the alternatives and learn from our journey.Edge Engineering is responsible for the first layer of AWS services that must be up for Netflix streaming to work. In the past, Edge Engineering had ops-focused teams and SRE specialists who owned the deploy+operate+support parts of the software life cycle. Releasing a new feature meant devs coordinating with the ops team on things like metrics, alerts, and capacity considerations, and then handing off code for the ops team to deploy and operate. To be effective at running the code and supporting partners, the ops teams needed ongoing training on new features and bug fixes. The primary upside of having a separate ops team was less developer interrupts when things were going well.When things didn’t go well, the costs added up. Communication and knowledge transfers between devs and ops/SREs were lossy, requiring additional round trips to debug problems or answer partner questions. Deployment problems had a higher time-to-detect and time-to-resolve due to the ops teams having less direct knowledge of the changes being deployed. The gap between code complete and deployed was much longer than today, with releases happening on the order of weeks rather than days. Feedback went from ops, who directly experienced pains such as lack of alerting/monitoring or performance issues and increased latencies, to devs, who were hearing about those problems second-hand.To improve on this, Edge Engineering experimented with a hybrid model where devs could push code themselves when needed, and also were responsible for off-hours production issues and support requests. This improved the feedback and learning cycles for developers. But, having only partial responsibility left gaps. For example, even though devs could do their own deployments and debug pipeline breakages, they would often defer to the ops release specialist. For the ops-focused people, they were motivated to do the day to day work but found it hard to prioritize automation so that others didn’t need to rely on them.In search of a better way, we took a step back and decided to start from first principles. What were we trying to accomplish and why weren’t we being successful?The purpose of the software life cycle is to optimize “time to value”; to effectively convert ideas into working products and services for customers. Developing and running a software service involves a full set of responsibilities:We had been segmenting these responsibilities. At an extreme, this means each functional area is owned by a different person/role:These specialized roles create efficiencies within each segment while potentially creating inefficiencies across the entire life cycle. Specialists develop expertise in a focused area and optimize what’s needed for that area. They get more effective at solving their piece of the puzzle. But software requires the entire life cycle to deliver value to customers. Having teams of specialists who each own a slice of the life cycle can create silos that slow down end-to-end progress. Grouping differing specialists together into one team can reduce silos, but having different people do each role adds communication overhead, introduces bottlenecks, and inhibits the effectiveness of feedback loops.To rethink our approach, we drew inspiration from the principles of the devops movement. We could optimize for learning and feedback by breaking down silos and encouraging shared ownership of the full software life cycle:“Operate what you build” puts the devops principles in action by having the team that develops a system also be responsible for operating and supporting that system. Distributing this responsibility to each development team, rather than externalizing it, creates direct feedback loops and aligns incentives. Teams that feel operational pain are empowered to remediate the pain by changing their system design or code; they are responsible and accountable for both functions. Each development team owns deployment issues, performance bugs, capacity planning, alerting gaps, partner support, and so on.Ownership of the full development life cycle adds significantly to what software developers are expected to do. Tooling that simplifies and automates common development needs helps to balance this out. For example, if software developers are expected to manage rollbacks of their services, rich tooling is needed that can both detect and alert them of the problems as well as to aid in the rollback.Netflix created centralized teams (e.g., Cloud Platform, Performance & Reliability Engineering, Engineering Tools) with the mission of developing common tooling and infrastructure to solve problems that every development team has. Those centralized teams act as force multipliers by turning their specialized knowledge into reusable building blocks. For example:Empowered with these tools in hand, development teams can focus on solving problems within their specific product domain. As additional tooling needs arise, centralized teams assess whether the needs are common across multiple dev teams. When they are, collaborations ensue. Sometimes these local needs are too specific to warrant centralized investment. In that case the development team decides if their need is important enough for them to solve on their own.Balancing local versus central investment in similar problems is one of the toughest aspects of our approach. In our experience the benefits of finding novel solutions to developer needs are worth the risk of multiple groups creating parallel solutions that will need to converge down the road. Communication and alignment are the keys to success. By starting well-aligned on the needs and how common they are likely to be, we can better match the investment to the benefits to dev teams across Netflix.By combining all of these ideas together, we arrived at a model where a development team, equipped with amazing developer productivity tools, is responsible for the full software life cycle: design, development, test, deploy, operate, and support.Full cycle developers are expected to be knowledgeable and effective in all areas of the software life cycle. For many new-to-Netflix developers, this means ramping up on areas they haven’t focused on before. We run dev bootcamps and other forms of ongoing training to impart this knowledge and build up these skills. Knowledge is necessary but not sufficient; easy-to-use tools for deployment pipelines (e.g., Spinnaker) and monitoring (e.g., Atlas) are also needed for effective full cycle ownership.Full cycle developers apply engineering discipline to all areas of the life cycle. They evaluate problems from a developer perspective and ask questions like “how can I automate what is needed to operate this system?” and “what self-service tool will enable my partners to answer their questions without needing me to be involved?” This helps our teams scale by favoring systems-focused rather than humans-focused thinking and automation over manual approaches.Moving to a full cycle developer model requires a mindset shift. Some developers view design+development, and sometimes testing, as the primary way that they create value. This leads to the anti-pattern of viewing operations as a distraction, favoring short term fixes to operational and support issues so that they can get back to their “real job”. But the “real job” of full cycle developers is to use their software development expertise to solve problems across the full life cycle. A full cycle developer thinks and acts like an SWE, SDET, and SRE. At times they create software that solves business problems, at other times they write test cases for that, and still other times they automate operational aspects of that system.For this model to succeed, teams must be committed to the value it brings and be cognizant of the costs. Teams need to be staffed appropriately with enough headroom to manage builds and deployments, handle production issues, and respond to partner support requests. Time needs to be devoted to training. Tools need to be leveraged and invested in. Partnerships need to be fostered with centralized teams to create reusable components and solutions. All areas of the life cycle need to be considered during planning and retrospectives. Investments like automating alert responses and building self-service partner support tools need to be prioritized alongside business projects. With appropriate staffing, prioritization, and partnerships, teams can be successful at operating what they build. Without these, teams risk overload and burnout.To apply this model outside of Netflix, adaptations are necessary. The common problems across your dev teams are likely similar — from the need for continuous delivery pipelines, monitoring/observability, and so on. But many companies won’t have the staffing to invest in centralized teams like at Netflix, nor will they need the complexity that Netflix’s scale requires. Netflix’s tools are often open source, and it may be compelling to try them as a first pass. However, other open source and SaaS solutions to these problems can meet most companies needs. Start with analysis of the potential value and count the costs, followed by the mindset-shift. Evaluate what you need and be mindful of bringing in the least complexity necessary.The tech industry has a wide range of ways to solve development and operations needs (see devops topologies for an extensive list). The full cycle model described here is common at Netflix, but has its downsides. Knowing the trade-offs before choosing a model can increase the chance of success.With the full cycle model, priority is given to a larger area of ownership and effectiveness in those broader domains through tools. Breadth requires both interest and aptitude in a diverse range of technologies. Some developers prefer focusing on becoming world class experts in a narrow field and our industry needs those types of specialists for some areas. For those experts, the need to be broad, with reasonable depth in each area, may be uncomfortable and sometimes unfulfilling. Some at Netflix prefer to be in an area that needs deep expertise without requiring ongoing breadth and we support them in finding those roles; others enjoy and welcome the broader responsibilities.In our experience with building and operating cloud-based systems, we’ve seen effectiveness with developers who value the breadth that owning the full cycle requires. But that breadth increases each developer’s cognitive load and means a team will balance more priorities every week than if they just focused on one area. We mitigate this by having an on-call rotation where developers take turns handling the deployment + operations + support responsibilities. When done well, that creates space for the others to do the focused, flow-state type work. When not done well, teams devolve into everyone jumping in on high-interrupt work like production issues, which can lead to burnout.Tooling and automation help to scale expertise, but no tool will solve every problem in the developer productivity and operations space. Netflix has a “paved road” set of tools and practices that are formally supported by centralized teams. We don’t mandate adoption of those paved roads but encourage adoption by ensuring that development and operations using those technologies is a far better experience than not using them. The downside of our approach is that the ideal of “every team using every feature in every tool for their most important needs” is near impossible to achieve. Realizing the returns on investment for our centralized teams’ solutions requires effort, alignment, and ongoing adaptations.The path from 2012 to today has been full of experiments, learning, and adaptations. Edge Engineering, whose earlier experiences motivated finding a better model, is actively applying the full cycle developer model today. Deployments are routine and frequent, canaries take hours instead of days, and developers can quickly research issues and make changes rather than bouncing the responsibilities across teams. Other groups are seeing similar benefits. However, we’re cognizant that we got here by applying and learning from alternate approaches. We expect tomorrow’s needs to motivate further evolution.Interested in seeing this model in action? Want to be a part of exploring how we evolve our approaches for the future? Consider joining us.By Philip Fisher-Ogden, Greg Burrell, and Dianne Marsh",https://medium.com/netflix-techblog/full-cycle-developers-at-netflix-a08c31f83249?source=collection_home---4------1---------------------,0,netflix,"java,python",NULL,2018-05-17
Multi-Cloud Continuous Delivery with Spinnaker report now available.,"by Emily Burns, Asher Feldman, Rob Fletcher, Tomas Lin, Justin Reynolds, Chris Sanden and Rob ZienertWe’re pleased to announce the release of our O’Reilly report, Continuous Delivery with Spinnaker. The report is available to download for free on the Spinnaker website. ( Pdf | Epub | Mobi )At Netflix, we’ve built and use Spinnaker as a platform for continuous integration and delivery. It’s used to deploy over 95% of Netflix infrastructure in AWS, comprised of hundreds of microservices and thousands of deployments every day.Encoded within Spinnaker are best practices for high availability, as well as integrations with Netflix tools like Chaos Monkey, ChAP Chaos Automation Platform, Archeius, Automated Canary Analysis and Titus. With Spinnaker, developers at Netflix build and manage pipelines that automate their delivery process to cloud VMs, containers, CDNs and even hardware OpenConnect devices.We first built Spinnaker to commoditize delivery for internal teams so they can manage their deployments. Our active open source community helped validate Spinnaker’s cloud-first, application-centric view of delivery by contributing tools, stages and cloud provider integrations.We were motivated to write this report as a high-level introduction to help engineers better understand how Netflix delivers production changes and the way Spinnaker features help simplify continuous delivery to the cloud. The report covers converting a delivery process into pipelines that can safely deploy to Kubernetes and Amazon EC2, adopting and extending Spinnaker, and ways to leverage advanced features like automated canary analysis and declarative delivery. We hope you like it.If you would like a physical copy of the report, members of the Spinnaker team will have them on hand at the following upcoming conferences:",https://medium.com/netflix-techblog/multi-cloud-continuous-delivery-with-spinnaker-report-now-available-6040ba83b765?source=collection_home---4------2---------------------,0,netflix,docker,NULL,2018-05-15
Netflixにおける日本語字幕の導入,"(Please note that this article is a localized (to Japanese) version of a corresponding tech blog article in the English language)Netflixでは、2015年9月の日本における配信サービス開始時から日本語字幕を提供しています。 今回のブログでは、日本語字幕提供に至るまでの技術的な取り組みについて説明します。 字幕ソースファイルの仕様、字幕ソースファイルからNetflix配信用字幕への変換モデル、Netflixにおける日本語字幕の納品モデルなどを取り上げます。さらに、W3C字幕規格Timed Text Markup Language 2 (TTML2)導入に向けた対応についても触れます。2014年の終盤にかけて、Netflixでは2015年9月に予定していた日本での配信開始に向け、技術的な機能実現を進めていました。 当時、日本市場で展開している他社ストリーミングサービスの字幕品質が問題となっていることは十分認識していました。 その上でNetflixの高い品質基準を維持するために、日本における上質な動画配信サービスの慣例となるべく、日本語字幕にとっての”必須”機能をすべて導入する覚悟で準備を始めました。 それらは、下記必要条件への追加条件として導入したものです。市場調査、日本語言語およびメディア関連の専門家アドバイスをまとめた結果、日本語字幕に関する5つの必須機能が明らかになりました。 これから説明するその5つの機能とは、ルビ、傍点、縦書き、斜体、縦中横 (縦字幕の数字を横書きで表示) です。 これらの機能を実現することは、従来の課題を一層複雑にする大きなチャレンジとなりました。ルビは特定の言葉を説明するためのものです。 たとえば、なじみのない言葉や外来語、スラングの意味を伝えたり、珍しい漢字またはあまり知られていない漢字にふりがなを付けるために使用します。 また、視聴者がコンテンツをより深く理解し楽しめるように、訳文の文化的背景を説明する場合もあります。 ルビ表示は通常、字幕文字よりも小さなフォントサイズを使い、1行のみの字幕、あるいは2行字幕の1行目には、文字の上にルビを振ります。 2行字幕の2行目にルビが存在する場合、文字の下にルビを振ります。ルビは2行字幕の行間には決して配置しません。どちらの行の文字を説明しているのか分かりづらくなるためです。図1に示すルビの例は、”All he ever amounted to was chitlins.”というセリフの字幕に振られたものです。単語”chitlins”*の訳語にその音訳のルビを振ることで、視聴者はセリフのキーワードと訳語を関連付けることができます。 上述したように、ルビは2行字幕の行間には決して配置しません。 図2に示すのは、2行字幕の正しいルビの振り方です。 万一、3行の字幕を必要とする場合、1行目と2行目は文字の上に、また3行目は文字の下にルビを振ります。傍点は単語や言葉を強調するため、上または下に配置するもので、英文における斜体と同じ役割です。 言外の意味を伝えるのに役立ち、翻訳をより豊かで力強いものにします。 図3に示す傍点の例は、”I need someone to talk to.”というセリフの字幕に振られたものです。上図の例の字幕では、傍点は単語”talk”の訳語の文字上に振られています。 単語を強調することで、このシーンで話者が特定の人しか知らない情報の提供者を必要としていることが伝わります。縦字幕は、主に動画の画面上に表示されている文字との重なりを避けるために使用します。 英語字幕における画面上部への表示に相当するものです。 図4に例を示します。日本語のタイポグラフィでは、縦書き文字の中に横書きの短い数字やアルファベット文字が含まれることがよくあります。 これを縦中横と呼びます。 縦に並べるのではなく、半角文字を横並びに配置することで読みやすくなり、字幕1行の中により多くの文字を入れることができます。 図5に示す例は、”It’s as if we are still 23 years old”というセリフの字幕です。 この例では、半角数字”23""が縦中横になっています。斜体は他言語におけるイタリック体と同様、ナレーション、画面外のセリフ、および強制字幕に使用します。 ただし、日本語字幕の場合、横字幕と縦字幕では斜体の傾き方向が異なる点が独特です。さらに傾きの角度も一定とは限りません。 図6と図7に例を示します。エンターテインメント業界での字幕アセットは、主に、構造化テキスト/バイナリファイルまたはレンダリング画像の2形式のどちらかです。 Netflixのコンテンツ取り込みシステム用には、常に前者の形式をお願いしてきました。 それにはいくつかの理由があります。 まず、クライアントによって字幕機能に差があるため、1つのソースから多様なクライアントアセットを作成する必要があります。 また、テキスト形式の字幕ソースファイルは将来性も確保されています。 つまり、新しいデバイス機能が次々と市場に出ても、テキスト形式なら、Netflixが所有する膨大な字幕アセットのバックカタログへ問題なく適用できるのです。 たとえば、HDRデバイスで再生されるHDRコンテンツに字幕を表示する場合、白い文字が最大白色の鏡面ハイライトにならないよう、輝度ゲインを指定することが推奨されます。 テキスト形式の字幕ソースを使用すれば、輝度ゲインをサポートするクライントプロフィールに対応した字幕表示を容易に処理できます。 一方、画像形式の字幕ソースを取り込んだ場合、同様の処理をクライアントアセットに適用するのは非常に困難です。 さらに、解析のための検索性や自然言語処理の観点からも、不透明な画像形式アセットに比べて、はるかに優れています。テキスト形式の字幕ソースが必須条件であることを前提に、日本語に利用できるオプションを検討した結果、Videotron Lambda (LambdaCap形式とも呼ばれる) を日本語字幕として唯一使用可能なモデルとして選択しました。 これにはいくつかの理由がありますが、 分析した結果、LambdaCap形式には次のような特徴があることがわかったからです:このように、日本でのNetflix配信開始時にはVideotron Lambdaを選択していましたが、長期的には決して優れたオプションではありませんでした。 業界の標準形式ではなく、仕様面において曖昧な点があるためです。 LambdaCap形式は日本語字幕の必須機能をサポートする一方で、TTML1のようなウェブプラットフォーム規格がサポートする基本的な機能の一部が含まれていないことがあります。 そうしたサポート外の機能には、色、フォント情報、さまざまなレイアウトやコンポジションのプリミティブなどが含まれます。 また、Netflixのエコシステムにおける再生デバイスへの納品モデルとしても、LambdaCap形式を使用しないことにしました。 さらにこの時期、タイムテキスト作業グループ (TTWG) がTTML規格の第2バージョンであるTTML2に取り組んでいました。 TTML2の目的の1つは、日本語字幕を主要対象とした多言語字幕をサポートすることでした。 そこで、NetflixでTTML2規格化に向けてTTWGと協力することになり、すでに蓄積した経験に基づいて仕様の完成、および後述する導入作業を行ったのです。 こうして、最終的にはTTML2が、Netflixの字幕処理パイプラインにおける全ソース形式の正準表現になりました。表1は上記の日本語字幕の必須機能と、TTML2が提供する構造間のマッピングを要約したものです。 また、Netflixの日本語字幕カタログ全体における上記機能の使用統計と、Netflixエコシステムの理想モードも示しています。 現時点で未使用の機能や使用頻度が低いものは、今後、より広く使用されるものと予測されます。† 次のセクションでは、特にサポートされている値に関する各機能の詳細を説明します。tts:rubyこのスタイル属性は、ルビ自体とルビが振られた文字幅の定義を含む、ルビコンテンツの構造的な特徴を指定しています。 tts:rubyに関連付けられた値の範囲は、対応するHTMLマークアップ要素にマッピングされます。 TTMLサンプルで示しているように、”container”マークアップはルビの振られた文字とルビを含み、”base”と”text”はそれぞれ文字とルビをマークアップします。 このサンプルのレンダリングは、図8になるはずです。tts:rubyPositionこのスタイル属性は、文字に相対するブロック進行方向の寸法内でのルビの配置を指定しています。 日本語字幕の場合、tts:rubyPosition=”top”とtts:rubyPosition=”bottom”では想定外の行替えに対応できないため、理想的ではないことが分かりました。ルビが2行目にあるときは文字の下に置く必要があるためです。 そのような場合、tts:rubyPosition=”auto”はその名のとおり、自動的に対応してくれます。 次のTTMLサンプルでその例を示しています。 このサンプルのレンダリングを図9で示します。 現在、”auto”の動作は厳密には2行イベントにのみ指定できるものであり、2行イベントの2行目に予測しない改行が起きた場合は適用されません。 NetflixではTTML2で説明されている”outside”の現在の動作が正しいモデルであると考えるため、”auto”を廃止して代わりに”outside”を採用する可能性もあります。tts:rubyAlignこのスタイル属性は、ルビコンテナが生成したインラインエリア内のルビの配置を指定しています。 Netflixでの日本語字幕の経験から、tts:rubyAlignの推奨値を”center”にしています。次に示すのは上記TTMLサンプルに対応するものです。文字の幅がルビより大きい場合と、その逆の場合の動作の2つの例を表示しています。 どちらの例でも、”の所だ” (ユニコード文字3文字) の文字とルビのアラインメントは”center”です。例1この例 (図10) では、ルビの幅が文字の幅よりも小さくレンダリングされています。例2この例 (図11) では、ルビの幅が文字の幅よりも大きくレンダリングされています。 どちらの例も、文字に対してルビは中央揃えになっています。tts:rubyReserveこの機能の目的は、文字のみの字幕からルビ付き字幕 (またはその逆) に移動する際に、ブロックの進行方向に沿って文字の配置の時間的整合性を保持することです。 この機能はまた、傍点を使用する際、時間が変わっても文字のアラインメントを保つためにも使用できます。上記TTMLサンプルのレンダリングが図12です。 tts:rubyReserveを有効にした場合、時間が経過しても字幕の相対移動は起こりません。tts:rubyReserveを有効にしていない場合、時間が経過すると字幕文字のベースラインに相対移動が起こるため、快適ではない視聴体験が発生してしまいます。 図13に示すのは、1つ目の字幕の文字 (左側) と2つ目の字幕の2行目の文字 (右側) との間に起きている、上下相対移動の例です。標準の縦書きNetflixでは縦書きモードを指定する際に、tts:writingModeを利用します。 次に縦書き (縦書き用の句読点や記号を含む) の例を示します。以下はTTMLサンプルおよび対応するレンダリング (図14) です。縦書きのルビ  上記TTMLサンプルで示したように、縦書きモードにおけるルビのマークアップは横書きモードの場合と変わりはありません。tts:textCombine縦中横機能を実行するにはtts:textCombineを使用します。 次のTTMLサンプルがその例です。 図15でわかる通り、この機能を使うと字幕の読みやすさが増します。tts:textEmphasis傍点をレンダリングするのに使用します。 この機能は前のセクションに記載したTTMLサンプルでも指定されています。 図16は、同サンプルに対応するレンダリングです。tts:fontShear日本語のタイポグラフィにはイタリック体のフォントはありません。 斜体を表示するには、グリフの幾何変換を実行します。 共通値tts:fontShearは約15度の回転に相当します。 前述したように、斜体の傾き方向は横字幕と縦字幕で異なります。 次のTTMLサンプルのレンダリング図17と図18を確認してください。日本語の字幕機能を特定し、ソース形式を選択して、日本語字幕アセットの納品仕様を作成しました。 次の課題はクライアントへの字幕納品でした。 日本語字幕の取り込みはVideotron Lambda形式で行っていましたが、前述の理由でLambdaCap形式はクライアントモデルには適していないと考えるようになりました。 また、テキスト形式の字幕をクライアントへ納品することが望ましい一方で、大多数のNetflix対応デバイスは複雑な日本語字幕機能をサポートしていませんでした。 こうした問題により、日本では画像形式の字幕を使用することにしました。そして複雑な日本語字幕のレンダリング作業のすべてを、Netflixにおけるバックエンドのトランスコード処理で対応することにしたのです。  画像形式の字幕をレンダリングするためには日本語字幕のレンダリングエンジンの導入が必要となり、これが次の課題となりました。 このプロジェクトには、日本語タイポグラフィの専門家でありW3C TTML仕様エディターでもあるグレン・アダムス氏の協力を仰ぎました。 そのコラボレーションの成果が、Netflixによる出資およびSkynavの開発から実現したオープンソースソフトウェアTimed Text Toolkit (TTT) プロジェクトです。 TTTは日本語字幕の必須機能すべてをサポートし、TTML2ファイルの検証とレンダリングを行う完全なツールセットを提供します。 W3C TTML系の派生規格であるIMSC1 (Internet Media Subtitles and Captions: インターネットメディア用字幕とキャプション) は、インターネットベースの字幕とキャプションの納品アプリケーションを対象にしたものです。 IMSC1仕様の開発段階でTTTを完全な基準組み込みとして使用することができたため、TTTはIMSC1仕様の推奨基準となりました。 同様に、TTTはTTML2の完全な組み込みを提供するため、TTML2仕様の実現がさらに可能になります。TTTを使用し、LambdaCap形式ソースファイルをTTML2正準表現に変換した後、字幕を画像としてレンダリングする字幕処理パイプラインを導入しました。 図19で示している通り、cap2ttモジュールを使用してLambdaCap形式ファイルをTTML2ドキュメントに変換します。 TTML2ドキュメントはttpeによって一連の画像シーケンスに変換されます。 字幕画像の時間シーケンスは、それら画像のタイミングと位置情報を含むアーカイブにパッケージされます。 そして、そのアーカイブがNetflixエコシステムのデバイスに納品されるのです。 また、こうした画像セットを異なる解像度で作成し、多様な画面サイズやデバイスフォームファクターに対応しています。さまざまな理由から、長期的に望ましい納品システムは画像形式よりもテキスト形式だと言えます。 たとえば、画像形式よりテキスト形式のほうがファイルサイズが小さいため帯域効率がアップすることや、字幕のフォント、フォントサイズ、色をエンドユーザーが柔軟に選択できることなどがその利点です。 Netflixエコシステムの再生デバイスがさらに日本語字幕の機能をサポートできるようになり、TTML2規格が定着すれば、今後はテキスト形式の納品モデルに移行する予定です。 Netflixではこれに向け、さまざまなTTML2機能に対して最上級のサポートを提供し、デバイスに最適な融通性の高い字幕レンダリングエンジンの開発を現在進めています。— ロヒット・プリ、シリル・コンコラト、デヴィッド・ロンカ、ユミ・ディーター(*) 「ハウス・オブ・カード 野望の階段」で使われたスラングで、”チトリンズ”とも呼ばれるアメリカ南部のブタの小腸を材料にした料理。 こうした言葉は視聴者にとってなじみがないことが多い。(†) 表に含まれていないその他のTTML2日本語機能 (tts:rubyOverhangなど) は、Netflixにおける日本語字幕機能のサポートには必要ありません。",https://medium.com/netflix-techblog/netflix%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E6%97%A5%E6%9C%AC%E8%AA%9E%E5%AD%97%E5%B9%95%E3%81%AE%E5%B0%8E%E5%85%A5-b8c8c4fa299e?source=collection_home---4------3---------------------,0,netflix,,NULL,2018-05-13
Engineers Making Movies (AKA Open Source Test Content),"by Andy Schuler and Matthew DonatoIf you’re a die-hard Netflix fan, you may have already stumbled onto the dark and brooding story of a timeless struggle between man and nature, water and fountain, ball and tether known much more plainly as Example Short. In the good old days of 2010, Example Short served us well to evaluate our streaming profiles with maximum resolutions of 1920x1080, frame rates of 30 frames per second (fps) and colorimetry within the BT. 709 gamut.Of course streaming technology has evolved a great deal in the past eight years. Our technical and artistic requirements now include higher quality source formats with frame rates up to 60 fps, high dynamic range and wider color gamuts up to P3-D65, audio mastered in Dolby Atmos and content cinematically comparable to your regularly unscheduled television programming.Furthermore, we’ve been able to freely share Example Short and its successors with partners outside of Netflix, to provide a common reference for prototyping bleeding-edge technologies within entertainment, technology and academic circles without compromising the security of our original and licensed programming.If you’re only familiar with Example Short, allow us to get you up to speed.Our first test title contains a collection of miscellaneous live action events in 1920x1080 resolution with BT. 709 colorimetry and an LtRt Stereo mix. It was shot natively at four frame rates: 23.976, 24, 25 and 29.97 fps.As the demand for more pixels increased, so did appropriate test content. El Fuente was shot in 4K at both 48 and 59.94 fps to meet increasing resolution and frame rate requirements.Chimera is technically comparable to El Fuente, but its scenes are more representative of existing titles. The dinner scene pictured here attempts to recreate a codec-challenging sequence from House of Cards.Following the industry shift from “more pixels” to “better pixels,” we produced Meridian, our first test title to tell a story. Meridian was mastered in Dolby Vision high dynamic range (HDR) with a P3-D65 color space and PQ (perceptual quantizer) transfer function. It also contained a Dolby Atmos mix, multiple language tracks and subtitles. You can read more about Meridian on Variety and download the entire SDR Interoperable Master Package (IMP) for yourself.We felt a need to include animated content in our test title library, so we partnered with the Open Movie Project and Fotokem’s Keep Me Posted to re-grade Cosmos Laundromat, an award-winning short film in Dolby Vision HDR.While our latest test title is technically comparable to Meridian, we’ve chosen to archive its assets and share them to the open source community along with the final deliverables. The production process is described as follows.The idea sparked when we observed construction workers completing one of two new buildings on the Netflix campus. We believed the dark shadows of steel beams juxtaposed perpendicularly to the horizon and the setting sun, contrasted against the glowing light from a welder’s arc would push the dynamic range of any camera to its limits. For our shoot, we acquired a Sony PMW-F55 and the AXS-R5 RAW recorder to shoot 16-bit RAW SQ and maintain the highest dynamic range in capture.After joining forces with the construction crew, and locating a sharp, talented welder actor, Sparks, a slice-of-life title following the day of said welder was born.From a technical standpoint, the final deliverable for Sparks is on par with the technical specifications of Meridian, captured at a resolution of 4096x2160, frame rate of 59.94 fps and mastered in Dolby Vision HDR at 4,000 cd/m². To crank things up a notch, we’re delving more into the technical specifics of the workflow, mastering in the open source Academy Color Encoding System (ACES) format, and sharing several production assets.We captured just over an hour of footage and cut it down to about 3 and a half minutes in Adobe Premiere Pro, editing the Sony RAW SQ format directly, but the real fun began with our high dynamic range color grading session.Though we began with a first pass color grade on a Sony BVM-X300, capable of reproducing 1000 cd/m², we knew it was critical to finesse our grade on a much more capable display. To accomplish this, we teamed up with Dolby Laboratories in Sunnyvale, CA to create our master color grade using the Pulsar: Dolby’s well-characterized P3-D65 PQ display capable of reproducing luminance levels up to 4000 cd/m². Though at the time atypical for a Dolby Vision workflow, we chose to master our content in ACES so that we could later create derivative output formats for infinitely many displays. With the exception of our first pass color grade, our post-production workflow is outlined as follows:Starting with our RAW footage (red) we’ve applied various transforms and operations (blue) to create intermediate formats (yellow) and ultimately, deliverables (green). The specific software and utilities we’ve used in practice are labeled in gray.Our aim was to create enough intermediates so that a creative could start working with their preferred tool at any stage in the pipeline. This is an open invitation for those who who are eager to download and experiment with the source and intermediate materials.For example, you could begin with the graded ACES master. Following the steps in the dashed grey box, you could apply a different RRT and ODT to create a master using a specific colorimetry and transfer function. While we graded our ACES footage monitoring at 4000 cd/m² and rendered a Dolby Vision master (P3-D65 PQ), we welcome you to create a different output format or backtrack to the non-graded frame sequence and regrade the footage entirely.For us, creating the Dolby Vision master also required the use of Dolby’s mezzinator tool to create a JPEG-2000 sequence in an MXF container, which was packaged into an IMP along with our original stereo audio mix. We used this Dolby Vision master as a source format to our encoding pipeline, from which we derive Dolby Vision, HDR10 and SDR bitstreams.We’ve selected key assets in the production process to share with the open source community. This includes the original Sony RAW files, ACES Non-Graded Archival Master (NAM), ACES Graded Archival Master (GAM), Dolby Vision Video Display Master (VDM), and the final IMP, which can be downloaded from the Open Source Assets section. These assets correspond to the workflow diagram as follows:From an encoding perspective, Sparks is highly representative of some of the most difficult titles to encode from our catalog. The high spatial frequency and fast motion content in Sparks gives it a complexity that is on par with some of our grainiest titles shot on film, such as Breaking Bad, The Meyerowitz Stories and Lawrence of Arabia. For example, a 1920x1080 H264 AVCMain encode of Sparks required a bitrate of 12568 kbps to achieve a VMAF score of 91.47, where a spatially simplistic animation like BoJack Horseman required only 1,673 kbps to achieve a comparable quality (VMAF=91.10) at the same resolution. For more context on these bitrates, see our article on Video Multimethod Assessment Fusion (VMAF).You can browse the directory structure of our open source assets on aws, and even download the assets through your web browser. We’ve included sample encodes, IMPs and many production assets not only from Sparks but also from Meridian and Cosmos Laundromat. For downloading large files and long frame sequences, you may wish to use command line tools such as aws cli. An asset manifest and detailed instructions can be found in our readme.Though Sparks was released in 2017, we have certainly not sat idly watching our test content transcode! (You could say that it’s like watching 18% neutral gray paint dry.) We’ve been actively producing Nocturne, a culmination of the technical and visual features that have been added to our service since 2010. Stay tuned for Part 2.This article was updated on 11 May 2018 at 15:55. In the workflow diagrams, “OCES” is now replaced with “Dolby Vision Master.” Additional acronyms are also now expanded.This article was again updated on 14 May 2018 at 14:17. In the workflow diagrams, dashed lines are added to a color grading node to indicate non-destructive viewing transforms to the Pulsar.",https://medium.com/netflix-techblog/engineers-making-movies-aka-open-source-test-content-f21363ea3781?source=collection_home---4------4---------------------,0,netflix,,NULL,2018-05-11
Announcing the Netflix Research Website,"We’re pleased to announce that we’ve launched a new website at research.netflix.com that provides an overview of the research that we do here at Netflix. We have many amazing researchers working on a variety of hard problems and are happy to share some of our work with the world.Netflix embraces innovation and has been investing in research to power that innovation for many years. This started with an early focus in areas like recommendations and experimentation but has now expanded to several other research areas and application domains in our business including studio production, marketing, and content delivery. To maximize the impact of our research, we do not centralize research into a separate organization. Instead, we have many teams that pursue research in collaboration with business teams, engineering teams, and other researchers. While this has worked well internally, we have found that it can be difficult to navigate for people outside Netflix who may want to understand our work, connect with our people, or find job opportunities. Thus, we’ve created this website to provide a broad overview of our research. We hope that it provides more insight into some of the areas we work in, the research that we’ve done, and the challenges we face in continuing to make Netflix better.The site also is a resource for the various publications, blog posts, and talks that we’ve done across these research and business areas. Because our research is focused on improving our product and business, the publications represent a small fraction of the volume of research we conduct at Netflix on an ongoing basis. You can also see from our publications that we’re pretty focused on the applied side of the research spectrum, though we do also pursue fundamental research that we think has the potential for high impact, such as improving our understanding of causality in our data and systems. We also seek to keep engaged in the research community by participating in conferences and organizing research-oriented events.Going forward we expect our research efforts at Netflix to continue to grow as we keep finding new and better ways to entertain the world. We’ll push forward by discovering new and better ways to personalize more dimensions of our product, using natural language processing and computer vision to build a deeper understanding of content in all phases of its production cycle, and pushing for even better quality in our streaming experience. Expect to see new articles being published, new events announced, and new areas being added on the site as we continue this adventure.You can also follow us on Twitter at @NetflixResearch.By Justin Basilico and Nirmal Govind on behalf of the Netflix Research team",https://medium.com/netflix-techblog/netflix-research-website-767408c50404?source=collection_home---4------5---------------------,0,netflix,,NULL,2018-05-09
"Titus, the Netflix container management platform, is now open source","by Amit Joshi, Andrew Leung, Corin Dwyer, Fabio Kung, Sargun Dhillon, Tomasz Bak, Andrew Spyker, Tim BozarthToday, we are open-sourcing Titus, our container management platform.Titus powers critical aspects of the Netflix business, from video streaming, recommendations and machine learning, big data, content encoding, studio technology, internal engineering tools, and other Netflix workloads. Titus offers a convenient model for managing compute resources, allows developers to maintain just their application artifacts, and provides a consistent developer experience from a developer’s laptop to production by leveraging Netflix container-focused engineering tools.Over the last three years, Titus evolved initially from supporting batch use cases, to running services applications (both internal, and ultimately critical customer-facing). Through that evolution, container use at Netflix has grown from thousands of containers launched per week to as many as three million containers launched per week in April 2018. Titus hosts thousands of applications globally over seven regionally isolated stacks across tens of thousands of EC2 virtual machines. The open-sourcing of Titus shares the resulting technology assembled through three years of production learnings in container management and execution.Over the past few years of talking about Titus, we’ve been asked over and over again, “When will you open source Titus?” It was clear that we were discussing ideas, problems, and solutions that resonated with those at a variety of companies, both large and small. We hope that by sharing Titus we are able to help accelerate like-minded teams, and to bring the lessons we’ve learned forward in the container management community.Multiple container management platforms (Kubernetes, Mesosphere DC/OS, and Amazon ECS) have been adopted across the industry during the last two years, driving different benefits to a wide class of use cases. Additionally, a handful of web-scale companies have developed solutions on top of Apache Mesos to meet the unique needs of their organizations. Titus shares a foundation of Apache Mesos and was optimized to solve for Netflix’s production needs.Our experience talking with peers across the industry indicates that other organizations are also looking for some of the same technologies in a container management platform. By sharing the code as open source, we hope others can help the overall container community absorb those technologies. We would also be happy for the concepts and features in Titus to land in other container management solutions. This has an added benefit for Netflix in the longer term, as it will provide us better off-the-shelf solutions in the future.And finally, a part of why we are open-sourcing is our desire to give back and share with the community outside Netflix. We hope open sourcing will lead to active engagements with other companies who are working on similar engineering challenges. Our team members also enjoy being able to present their work externally and future team members can learn what they have an opportunity to work on.To ensure we are investing wisely at Netflix, we stay well aware of off-the-shelf infrastructure technologies. In addition to the aforementioned container orchestration front, we also stay deeply connected with the direction and challenges of the underlying container runtime technologies such as Docker (Moby, container-d) and CRI-O. We regularly meet with engineering teams at the companies both building these solutions as well as the teams using them in their production infrastructures. By balancing the knowledge of what is available through existing solutions with our needs, we believe Titus is the best solution for container management at Netflix.A few of those key reasons are highlighted below:The first is a tight integration between Titus and both Amazon and Netflix infrastructure. Given that Netflix infrastructure leverages AWS so broadly, we decided to seamlessly integrate, and take advantage of as much functionality AWS had to offer. Titus has advanced ENI and security group management support spanning not only our networking fabric but also our scheduling logic. This allows us to handle ENIs and IPs as resources and ensure safe large scale deployments that consider EC2 VPC API call rate limits. Our IAM role support, which allows secure EC2 applications to run unchanged, is delivered through our Amazon EC2 metadata proxy. This proxy also allows Titus to give a container specific metadata view, which enables various application aspects such as service discovery. We have leveraged AWS Auto Scaling to provide container cluster auto scaling with the same policies that would be used for virtual machines. We also worked with AWS on the design of IP target groups for Application Load Balancers, which brings support for full IP stack containers and AWS load balancing. All these features together enable containerized applications to transparently integrate with internal applications and Amazon services.In order to incrementally enable applications to transition to containers while keeping as many systems familiar as possible, we decided to leverage existing Netflix cloud platform technologies, making them container aware. We choose this path to ensure a common developer and operational approach between VMs and containers. This is evident through our Spinnaker enablement, support in our service discovery (Eureka), changes in our telemetry system (Atlas), and performance insight technologies.Next is scale, which has many dimensions. First, we run over a thousand different applications, with some being very compute heavy (media encoding), some being critical Netflix customer facing services, some memory and GPU heavy (algorithm training), some being network bound (stream processing), some that are happy with resource over commitment (big data jobs) and some that are not. We launch up to a half million containers and 200,000 clusters per day. We also rotate hundreds of thousands of EC2 virtual machines per month to satisfy our elastic workloads. While there are solutions that help solve some of these problems, we do not believe there are off-the-shelf solutions that can take on each of these scale challenges.Finally, Titus allows us to quickly and nimbly add features that are valuable as our needs evolve, and as we grow to support new use-cases. We always try to maintain a philosophy of “just enough” vs “just in case” with the goal of keeping things as simple and maintainable as possible. Below are a few examples of functionality we’ve been able to quickly develop in response to evolving business and user needs:In the scheduling layer, we support advanced concepts such as capacity management, agent management, and dynamic scheduling profiles. Capacity management ensures all critical applications have the capacity they require. Agent management provides multiple functions required to support a fleet of thousands of hosts. Agent management is inclusive of host registration and lifecycle, automatic handling of failing hosts, and autoscaling hosts for efficiency. We have dynamic scheduling profiles that understand the differences needed in scheduling between application types (customer facing services vs. internal services vs. batch) and differences in scheduling needed during periods of normal or degraded health. These scheduling profiles help us optimize scheduling considering real world trade-offs between reliability, efficiency and job launch time latencies.In container execution, we have a unique approach to container composition, Amazon VPC networking support, isolated support for log management, a unique approach to vacating decommissioned nodes, and an advanced operational health check subsystem. For container composition, we inject our system services into containers before running the user’s workload in the container. We classify container networking traffic using BPF and perform QoS using HTB/ECN ensuring we provide highly performant, burstable as well as sustained throughput to every container. We isolate log uploading and stdio processing within the container’s cgroup. Leveraging Spinnaker, we are able to offload upgrade node draining operations in an application specific way. We have operationalized the detection and remediation of kernel, container runtime, EC2, and container control plane health issues. For our security needs, we run all containers with user namespaces, and provide transparent direct user access to only the container.Titus is designed to satisfy Netflix’s complex scalability requirements, deep Amazon and Netflix infrastructure integration, all while giving Netflix the ability to quickly innovate on the exact scheduling and container execution features we require. Hopefully, by iterating our goals in detail you can see how Titus’s approach to container management may apply to your use cases.In the fourth quarter of 2017, we opened up Titus’s source code to a set of companies that had similar technical challenges as Netflix in the container management space. Some of these companies were looking for a modern container batch and service scheduler on Mesos. Others were looking for a container management platform that was tightly integrated with Amazon AWS. And others still were looking for a container management platform that works well with NetflixOSS technologies such as Spinnaker and Eureka.By working with these companies to get Titus working in their AWS accounts, we learned how we could better prepare Titus for being fully open sourced. Those experiences taught us how to disconnect Titus from internal Netflix systems, the level of documentation needed to get people started with Titus, and what hidden assumptions we relied on in our EC2 configuration.Through these partnerships, we received feedback that Titus really shined due to our Amazon AWS integration and the production focused operational aspects of the platform. We also heard how operating a complex container management platform (like Titus) is going to be challenging for many.With all these learnings in mind, we strived to create the best documentation possible to getting Titus up and running. We’ve captured that information on the Titus documentation site.Open sourcing Titus marks a major milestone after over three years of development, operational battle hardening, customer focus, and sharing/collaboration with our peers. We hope that this effort can help others with the challenges they are facing, and bring new options to container management across the whole OSS community.In the near future we will keep feature development in Titus well aligned with Netflix’s product direction. We plan to share our roadmap in case others are interested in seeing our plans and contributing. We’d love to hear your feedback. We will be discussing Titus at our NetflixOSS meetup this evening and will post the video later in the week.Conference talks: Dockercon 2015, QCon NYC 2016, re:Invent 2016, QCon NYC 2017, re:Invent 2017, and Container World 2018Articles: Netflix techblog posts (1, 2), and ACM Queue",https://medium.com/netflix-techblog/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436?source=collection_home---4------6---------------------,0,netflix,"java,python",NULL,2018-04-18
Automated Canary Analysis at Netflix with Kayenta,"by Michael Graff and Chris SandenToday, in partnership with Google, we have open sourced Kayenta, a platform for Automated Canary Analysis (ACA). Kayenta leverages lessons learned over the years of delivering rapid and reliable changes into production at Netflix. It is a crucial component of delivery at Netflix as it reduces the risk from making changes in our production environment. In addition, Kayenta has increased developer productivity by providing engineers with a high degree of trust in their deployments.A canary release is a technique to reduce the risk from deploying a new version of software into production. A new version of software, referred to as the canary, is deployed to a small subset of users alongside the stable running version. Traffic is split between these two versions such that a portion of incoming requests are diverted to the canary. This approach can quickly uncover any problems with the new version without impacting the majority of users.The quality of the canary version is assessed by comparing key metrics that describe the behavior of the old and new versions. If there is significant degradation in these metrics, the canary is aborted and all of the traffic is routed to the stable version in an effort to minimize the impact of unexpected behavior.At Netflix, we augment the canary release process and use three clusters, all serving the same traffic with different amounts:The production cluster receives the majority of traffic, while the baseline and canary each receive a small amount. How this delineation of traffic routing occurs depends on the type of traffic, but a typical configuration leverages a load balancer to add the baseline and canary instances into the regular pool of existing instances.Note: while it’s possible to use the existing production cluster rather than creating a baseline cluster, comparing a newly created canary cluster to a long-lived production cluster could produce unreliable results. Creating a brand new baseline cluster ensures that the metrics produced are free of any effects caused by long-running processes.Spinnaker, our continuous delivery platform, handles the lifecycle of the baseline and canary clusters. Moreover, Spinnaker runs one or more iterations of the canary analysis step and makes the decision to continue, rollback, or, in some cases, prompt manual intervention to proceed. If the new version is determined to be safe the deployment is allowed to continue, and the production change is fully rolled out into a new cluster. If not, Spinnaker will abort the canary process and all traffic will be routed to the production cluster.Canary analysis was initially a manual process for engineers at Netflix. A developer or release engineer would look at graphs and logs from the baseline and canary servers to see how closely the metrics (HTTP status codes, response times, exception counts, load avg, etc.) matched. If the data looked reasonable, a manual judgment was made to move forward or to roll back.Needless to say, this approach didn’t scale and was not reliable. Each canary meant several hours spent staring at graphs and combing through logs. This made it difficult to deploy new builds more than once or twice a week. Visually comparing graphs made it difficult to see subtle differences between the canary and baseline. Our first attempt at automating canary analysis was a script that was very specific to the application it was measuring. We next attempted to generalize this process and introduced our first version of automated canary analysis more than 5 years ago. Kayenta is an evolution of this system and is based on lessons we have learned over the years of delivering rapid and reliable changes into production at Netflix.Kayenta is our next-generation automated canary analysis platform and is tightly integrated with Spinnaker. The Kayenta platform is responsible for assessing the risk of a canary release and checks for significant degradation between the baseline and canary. This is comprised of two primary stages: metric retrieval and judgment.This stage retrieves the key metrics from the baseline and canary clusters. These metrics are typically stored in a time-series database with a set of tags or annotations which identify if the data was collected from the canary or the baseline.Kayenta takes a configuration file which defines the metric queries. These metrics are combined with a scope (“for this cluster and this time range”) and are used to query one of the available metric sources. The results are then passed to the judge for analysis.Kayenta currently supports the following metric sources: Prometheus, Stackdriver, Datadog, and Netflix’s Atlas. In addition, different metric sources can be combined in a single analysis, i.e., some metrics may come from one source while other metrics can come from another.This stage compares the metrics collected from the baseline and canary. The output is a decision as to whether the canary passed or failed, i.e., was there a significant degradation in the metrics. Towards this end, there are four main steps as part of judgment which are outlined below.Data ValidationThe goal of data validation is to ensure that, prior to analysis, there is data for the baseline and canary metrics. For example, if the metric collection stage returns an empty array for either the baseline or canary metric the data validation step will mark the metric as “NODATA” and the analysis moves onto the next metric.Data CleaningThe data cleaning step prepares the raw metrics for comparison. This entails handling missing values from the input. There are different strategies for handling missing values based on the type of metric. For example, missing values, represented as NaNs, may be replaced with zeros for error metrics while they may be removed for other types of metrics.Metric ComparisonThe metric comparison step is responsible for comparing the canary and baseline data for a given metric. The output of this step is a classification for each metric indicating if there is a significant difference between the canary and baseline.More specifically, each metric is classified as either “Pass”, “High”, or “Low”. A classification of “High” indicates that the canary metric is meaningfully higher than the baseline metric. The following screenshot shows an example where the metric Latency 50th was classified as “High”.The primary metric comparison algorithm in Kayenta uses confidence intervals, computed by the Mann-Whitney U test, to classify whether a significant difference exists between the canary and baseline metrics.Score ComputationAfter each metric has been classified a final score is computed. This score represents how similar the canary is to the baseline. This value is used by Spinnaker to determine if the canary should continue or roll back.The score is calculated as the ratio of metrics classified as “Pass” out of the total number of metrics. For example, if 9 out of 10 metrics are classified as “Pass” then the final canary score would be 90%. While there are more complex scoring methodologies we bias towards techniques which are simple to understand.In addition to open sourcing the Kayenta platform, we are also releasing the Spinnaker UI components which integrate Kayenta. This includes a component which integrates the canary score into the Spinnaker pipeline execution details as shown in the image below.Users can drill down into the details of a canary result and view them in various ways using the Canary Report. The report gives a breakdown of the results by metric and displays the input data.For example, the following report shows a canary score of 58%. A number of metrics were classified as “High” resulting in a lower score. By selecting a specific metric, users can get a view of the input data used for judgment. Having detailed insight into why a canary release failed is crucial in building confidence in the system.Within Kayenta, the output of the metric retrieval and judgment stages is archived. This allows for new metric comparison algorithms and judges to be run on previously collected data leading to rapid experimentation.In addition, metric sources, judges, configuration storage, and result storage are all pluggable. Kayenta is designed to allow new metric and judgment systems to be plugged in as needed.A REST endpoint is provided to perform CRUD operations on configurations and retrieve canary results. This REST endpoint is used by Spinnaker pipelines to run an analysis, and is also available for use outside of Spinnaker. While we have heavily integrated with Spinnaker, Kayenta is able to run without any other Spinnaker components, having only Redis as a dependency.Kayenta is much more flexible than our previous solution and is easier for application owners to configure. We have removed much of the complexity of setting proper thresholds and other hand-tuning, and instead rely on superior algorithms to classify whether a significant difference exists between the canary and baseline metrics.Additionally, our legacy system had many special flags which were combined in various ways, but would later be unclear as to the intent of using them. Kayenta is more focused on semantic meaning of a metric, and will extend this further to set appropriate defaults for metrics such as “error” and “resource usage.”We are in the middle of migrating from our legacy system to Kayenta. Currently, Kayenta runs approximately 30% of our production canary judgments, which amounts to an average of 200 judgments per day. Over the next few months, we plan on migrating all internal users to KayentaThe following are some ways you can learn more about Kayenta and contribute to the project:",https://medium.com/netflix-techblog/automated-canary-analysis-at-netflix-with-kayenta-3260bc7acc69?source=collection_home---4------7---------------------,0,netflix,,NULL,2018-04-10
Netflix FlameScope,"We’re excited to release FlameScope: a new performance visualization tool for analyzing variance, perturbations, single-threaded execution, application startup, and other time-based issues. It has been created by the Netflix cloud performance engineering team and just released as open source, and we welcome help from others to develop the project further. (If it especially interests you, you might be interested in joining Netflix to work on it and other projects.)FlameScope combines a subsecond-offset heatmap for navigating a profile with flame graphs. This profile can be of CPU samples or other events. Since it’s visual, it’s best demonstrated by the following one minute video:There is also a longer video of examples here.If you’re familiar with flame graphs, you’ll know they show an entire profile at once, which can span one minute. That’s good for analyzing steady workloads, but often there are small perturbations or variation during that minute that you want to know about, which become a needle-in-a-haystack search when shown with the full profile. FlameScope solves this by starting with a subsecond-offset heat map to visualize these perturbations, then lets you select them for study with a flame graph. In other words, you can select an arbitrary continuous time-slice of the captured profile, and visualize it as a flame graph.You might not be familiar with subsecond-offset heat maps. They work as shown in figure 1, which has a mock ten row heat map, where:Imagine you have an event timestamp of 11.25 seconds. The x coordinate will be the 11th column, and the y coordinate will be the row that’s one quarter from the bottom. The more events that occurred around 11.25 seconds, the darker that block will be drawn.Here’s an example, with annotations showing the steps for selecting a range:There’s a number of interesting things from this production CPU profile. The CPUs are busier between 0 and 5 seconds, shown as darker colors. Around the 34 and 94 second mark (sounds like a 60 second periodic task), the CPUs also become busier, but for a shorter duration. And there are occasional bursts of heavy CPU activity for about 80 milliseconds, shown as short dark red stripes.All of these details can be selected in FlameScope, which will then draw a flame graph just for that range. Here’s one of the short red stripes:Ah, that’s Java garbage collection.Getting started instructions are listed (and will be updated) on the github repository here. The quickest way to get started is:FlameScope comes with a sample profile to browse (where application code has been redacted with ‘x’ characters). Here’s how to create new profiles on Linux, which can be added to the examples directory of FlameScope for browsing:That example shows a two minute CPU profile, sampling at 49 Hertz on all CPUs. Any perf output with stack traces can be browsed with FlameScope, including tracing events such as block I/O, context switches, page faults, etc. Since the profile output can get large, it can also be compressed with gzip (flamescope can read .gz).Why sample at 49 Hertz? Because 50 Hertz may sample in lock-step with a timed activity, and over- or under-count. Why roughly 50 Hertz in the first place? It’s not too slow and not too fast. Too slow and we don’t have enough samples to paint across FlameScope’s 50 row heatmap (the row count can be changed). Too fast and the overhead of sampling can slow down the application.Runtimes like Java can require extra steps to profile using perf correctly, which have been documented in the past for generating flame graphs (including here). Since you may have already been running these steps, you might have a library of old profiles (perf script output) that you can now explore using FlameScope.Since FlameScope reads Linux perf profiles, I already have a collection from prior investigations. Here are some screenshots, showing variation that I did not know about at the time.Not all profiles are this interesting. Some do just look like TV static: a steady workload of random request arrivals and consistent latency. You can find out with FlameScope.FlameScope was created by the Netflix cloud performance team, so far involving Vadim Filanovsky, myself, Martin Spier, and our manager, Ed Hunter, who has supported the project. The original issue was a microservice that was suffering latency spikes every 15 minutes or so, cause unknown. Vadim found it corresponded to an increase in CPU utilization that lasted only a few seconds. He had tried collecting CPU flame graphs to explain this further, but a) could not reliably capture a one minute flame graph covering the issue, as the onset of it kept fluctuating; and b) capturing a two or three minute flamegraph didn’t help either, and the issue was “drowned” in the normal workload profile. Vadim asked me for help.Since I had a two minute profile to begin with, I began by slicing it into ten second ranges, and creating a flame graph for each. This approach looked promising as it revealed variation, so I sliced it even further down to one second windows. Browsing these short windows solved the problem and found the issue, however, it had become a laborious task. I wanted a quicker way.Subsecond-offset heat maps were something I invented many years ago, but they haven’t seen much adoption or use so far. I realized they would be a great way to navigate a profile, allowing variance to be visualized not just for whole seconds but also for fractions within a second. I did a quick prototype which proved the idea worked, and discussed turning it into a real tool with Martin. The annotated heat map in this post shows Vadim’s original profile, and the issue was the CPU activity in the first few seconds.Martin has done most of the architecture design and coding for FlameScope, which includes his newer d3-based version of FlameGraphs: d3-flame-graph, and his d3-based heat maps: d3-heatmap2. It’s great to see them come together in FlameScope.There’s more features we have planned, and we’ll add tickets to github in case others would like to help. They include more interactive features, such as palette selection and data transformations. There should be a button to save the final flame graph as a stand alone SVG. Other profile sources can be supported, not just Linux perf. And there should be a way to show the difference between the selected range and the baseline (the whole profile).If you’re reading this months in the future, some of these extra features may already exist: check out the latest on https://github.com/Netflix/flamescope.FlameScope was developed by Martin Spier and Brendan Gregg, Netflix cloud performance engineering team. Blog post by Brendan Gregg.",https://medium.com/netflix-techblog/netflix-flamescope-a57ca19d47bb?source=collection_home---4------8---------------------,0,netflix,,NULL,2018-04-04
Women Living on the Edge Event,"by Karen Casella, Sangeeta Narayanan, Kim Trott, Gaya Varadarajan, Haripriya MurthyNetflix’s Edge Engineering team recently hosted a Women in Technology event where we shared some of the challenges that come with delivering over 1 billion streaming hours per week to delight more than 100M users around the globe. We welcomed senior women technologists and allies to our campus for networking and dinner at Netflix HQ.For those that were not able to join us, we hope that you find value in the content of our lightning talks. Each Netflix woman technologist shared context about her team’s piece of the Edge architecture and how we engineer our systems to ensure that Netflix can remain a highly available, resilient and performant service.",https://medium.com/netflix-techblog/women-living-on-the-edge-event-709f49f521df?source=collection_home---4------9---------------------,0,netflix,,NULL,2018-04-02
Data Science and the Art of Producing Entertainment at Netflix,"by Ritwik Kumar, Vinith Misra, Jen Walraven, Lavanya Sharan, Bahareh Azarnoush, Boris Chen, Nirmal GovindNetflix has released hundreds of Originals and plans to spend $8 billion over the next year on content. Creators of these stories pour their hearts and souls into turning ideas into joy for our viewers. The sublime art of doing this well is hard to describe, but it necessitates a careful orchestration of creative, business and technical decisions. Here we will focus on the latter two — business & technical decisions like planning budgets, finding locations, building sets, and scheduling guest actors that enable the creative act of connecting with viewers.Each production is a mountain of operational and logistical challenges that consumes and produces tremendous amounts of data. At Netflix’s scale, this is further amplified to levels seldom encountered before in the history of entertainment. This has created opportunities to organize, analyze and model this data that are equally singular in history. This is where data science can aid the art of producing entertainment.From the moment a show is pitched and before it shows up on our service, it goes through a few broad stages that are depicted in Figure 1. Studio Production refers to Pre-Production (planning, budgeting, etc.), Production (principal photography), Post Production (editing, sound mixing, etc.), Localization & Quality Control (subtitle creation, snuffing out technical glitches, etc.). In the rest of this blog, we will follow a title’s journey through these stages and examine some questions that data science can help answer.During Pre-Production, producers and executives are tasked with critical decisions such as: do we shoot in Georgia or in Gibraltar? Do we hire a thousand extras or lean on VFX? Do we keep a 10-hour workday or a 12-hour workday? Each of these choices can have massive impact on cost, timeline and creative outcome of the project. Traditionally, these decisions are rooted in human experience and intuition. Let us see how one can supplement these with data derived insights.The problem we are describing is one of cost estimation: given various attributes about a production, estimate how much it will cost. These attributes should characterize both the content (genre, similarity to other titles, etc.) as well as the aforementioned production decisions (geographical location, production appetite, schedule, etc.). A production team could use this model as a sandbox to answer the central question of prep: which combination of production decisions stays most true to the creative vision while also staying under budget?The core challenge with building any such model is data sparsity. A production executive may want to evaluate Atlanta, Georgia as a shooting location for a big budget fantasy epic, but we may not have much historical data about Atlanta. Furthermore, the mechanisms by which location impacts cost may be complex and difficult to infer from data, even in popular shooting locations.One solution is to lean heavily on domain knowledge and expertise. Rather than attempting to learn thousands of parameters in a black box, we carefully construct networks that reflect our intuition about the problem space, and place strong priors on any parameters we seek to learn.For instance, suppose our goal is to model the ratio R_XY in certain production costs between geographic locations X (Atlanta) and Y (New Orleans). There are hundreds of such costs for any given production, and some vary more with location than others. Despite the sparsity in the data, one can efficiently model the fractional change in each of these costs by organizing them hierarchically (illustrated in Figure 2), placing them in a model that reflects this hierarchy, and, finally, putting priors on these ratios that reflect domain expertise. Such a model allows production executives to play around with ‘what-if’ scenarios and make informed decisions about critical aspects of a production.Once the pre-production minefield of decisions has been navigated, next we get to tackle the challenge of putting a plan into action. This marks the start of the Production stage (Figure 1). This is where it takes sorting through a mountain of logistical and operational challenges to enable the creative act of principal photography. These challenges are perhaps best illustrated by the task of Scheduling, which dictates how various resources are orchestrated for principal photography. We examine Scheduling next.A multi-episode show can easily have hundreds of scenes, shot with hundreds of on- and off-screen individuals, over dozens of locations, spanning many months. Efficiently and effectively scheduling this symphony — the task of the first assistant director (1st AD) — is the central logistical challenge in film and TV production, and it is as much an art form as the performances one sees on-screen. When done manually, it is not uncommon for a 1st AD to spend hundreds of hours building a schedule.At its core, a schedule is an ordering of scenes for each day of principal photography. A 1st AD’s job is to create such an ordering that respects various constraints and objectives: e.g. “We only have Actor X for one week.” Many of these considerations are fundamentally human judgments, but there is also room for automation to provide suggestions, or to assist with the more mechanical side of things. In particular, mathematical optimization can help generate rough schedules to inform early-stage production planning.Let’s describe a simplistic model that attempts to roughly capture the most basic scheduling considerations. Suppose our production consists of N scenes being shot by a single unit, over the course of D days, in L locations, and suppose we are given a rough estimate for the time each scene will take to shoot. To formulate this as an optimization problem, we need to specify variables, constraints, and an objective.Variables:Sample Constraints:Objective (very crude approximations!):Even with a simplistic model like this, one can generate reasonable-looking schedules within minutes. Such schedules are also useful for early stage planning (e.g. budgeting) and as starting points for more refined scheduling. For expert users, models of this sort can even be used as interactive tools to selectively adjust portions of a schedule.After principal photography is complete, for a typical show, there could still be 100 to 200 tasks like editing, sound mixing, color correction etc. that need to be completed before it is ready for the screen. Coordination and tracking of these tasks is tackled in Post Production (Figure 1). For many TV shows and films, Post Production easily ends up consuming far more time than principal photography. For instance, ‘Apocalypse Now’ famously took over two years of post production before it was ready for the screens. Let us examine how data science can help Post Production next.As Post Production teams track hundreds of tasks per TV show or film, identifying bottlenecks and blockers that span multiple titles becomes an even greater challenge. Leveraging data science to equip teams with the ability to slice, dice and visualize this data at scale can help identify anomalies and opportunities for interventions in their complex, multilayered cross-team workflows.As an illustration, let us look at the Sankey diagram below representing a generic post production asset workflow. This captures the progression of assets — like VFX shots, daily film clips, or final cuts — through a review process. In this example things flow pretty nicely, but we see drop-offs(red boxes) at the Blocked, In Progress, and Received stages. With this detail, teams can dive in to understand what is blocked or undelivered, using additional tools to evaluate these gaps on specific productions.Giving users the ability to choose different filters (production team, geography, facility, etc.) and track trends over time builds an understanding of workflow patterns we might expect in the future, helping us plan more efficiently. Continuing our Sankey example, as we move toward Q4 it looks like the gaps in Blocked, In Progress, and Received even out and we’ve added more assets to the workflow. However, we might want to review the Q4 gap following the ‘In Review’ stage. This could lead teams to increase their staff, focus on technical improvements, or rebalance workload toward the end of the year.As Netflix content production expands globally, geographic visuals are increasingly becoming an important part of analyzing studio data. By overlaying multiple dimensions across geographies, we can surface key insights on resource availability and dependencies, unlocking efficiencies across our production catalog.A map like the one below might help teams better anticipate expected production delivery patterns for specific types of assets. Circles represent locations involved in production, and lines represent the movement of assets between locations.Locations with larger circles and a high number of edges will experience a high influx of deliveries and may need extra planning to ensure all supporting resources are in place. For Assets with complex delivery patterns (e.g. Asset Type 4 above), we may have an opportunity to further optimize.At the end of Post Production, if things go just right, we have a show or a movie ready for the screen. Quite importantly, at Netflix, this metaphorical screen is truly global. Millions of members, across 190 countries consume our content across over 20 languages. Thus localizing content to make it ‘travel’ across the globe is an important part of Studio Production. We examine it next.Localizing content originally created in, let’s say, German for the English-speaking US market is a complex creative process. Doing it well so that the content connects emotionally with viewers across the globe requires great effort and time.Our localization teams craft an in-depth plan to create a seamless localized experience for our content. This includes developing a relationship with the content, assessing localization complexities, and providing specific creative guidance. For example, we try to cast voice actors in each language that sound like the original cast.We strive to make all our content accessible to as many viewers as possible. Not surprisingly, time, talent availability, and technical constraints force us to sequence the available localization resources carefully. To make matters more complex, these decisions often need to be made many months before content is released on our service.We can turn to data to support such decisions. Historical viewing trends inform us how our content is consumed across a range of languages and markets. If a piece of content is more popular in a language A than language B, we may sequence our efforts for A before B. For upcoming shows, this turns into the following data science problem: predict the per-language consumption for each show k months before it is released.How do we solve this prediction problem? If we can define a notion of distance between shows, we can leverage historical data to make predictions about a new show’s per-language consumption based on the per-language consumption of “similar” shows. Some features we might consider when constructing such a distance metric include genre, language (both the original language of the content as well as the localized language), and whether the localized content was consumed as dubbed audio or as subtitles.Even a simple predictive model, as outlined here, can be helpful for guiding and scaling our content localization efforts as we expand our slate of localized content.The typical next stop in a show’s journey towards launch (Figure 1) is Quality Control (QC). Managing QC workflows for multimodal (audio, video & text) creative products like scripted or unscripted shows, films, documentaries, etc. is a challenging task. Data Science plays an important role here by optimizing QC workflows using predictive modeling. You may read more about the specifics in our previous blog post on this subject.Finally, we have a movie or TV show that’s ready to launch on Netflix! We’ve looked at only a few specific examples of problems where data science can help during the Studio Production stages, but the possibilities are limitless as Netflix ramps up original content creation.It is not often that one gets to witness transformation of an entire industry. Opportunity to be an agent for that change is even rarer. Netflix has been that agent on a few different occasions over its short history. We believe we are at the cusp of another such transformation in the world of content creation. Working with Netflix Studio’s business, technical and creative partners to transform a century-old industry with data science is challenging, but truly invigorating. If you are interested in being part of this refreshingly new endeavor with data, please contact Ritwik Kumar or check out the Studio Production Science & Analytics positions on the Netflix jobs site.",https://medium.com/netflix-techblog/studio-production-data-science-646ee2cc21a1?source=collection_home---4------10---------------------,0,netflix,,NULL,2018-03-27
Using Machine Learning to Improve Streaming Quality at Netflix,"by Chaitanya EkanadhamOne of the common questions we get asked is: “Why do we need machine learning to improve streaming quality?” This is a really important question, especially given the recent hype around machine learning and AI which can lead to instances where we have a “solution in search of a problem.” In this blog post, we describe some of the technical challenges we face for video streaming at Netflix and how statistical models and machine learning techniques can help overcome these challenges.Netflix streams to over 117M members worldwide. Well over half of those members live outside the United States, where there is a great opportunity to grow and bring Netflix to more consumers. Providing a quality streaming experience for this global audience is an immense technical challenge. A large portion of this is engineering effort required to install and maintain servers throughout the world, as well as algorithms for streaming content from those servers to our subscribers’ devices. As we expand rapidly to audiences with diverse viewing behavior, operating on networks and devices with widely varying capabilities, a “one size fits all” solution for streaming video becomes increasingly suboptimal. For example:We need to adapt our methods for these different, often fluctuating conditions to provide a high-quality experience for existing members as well as to expand in new markets. At Netflix, we observe network and device conditions as well as aspects of the user experience (e.g., video quality) we were able to deliver for every session, allowing us to leverage statistical modeling and machine learning in this space. A previous post described how data science is leveraged for distributing content on our servers worldwide. In this post we describe some technical challenges we face on the device side.Network quality is difficult to characterize and predict. While the average bandwidth and round trip time supported by a network are well-known indicators of network quality, other characteristics such as stability and predictability make a big difference when it comes to video streaming. A richer characterization of network quality would prove useful for analyzing networks (for targeting/analyzing product improvements), determining initial video quality and/or adapting video quality throughout playback (more on that below).Below are a few examples of network throughput measured during real viewing sessions. You can see they are quite noisy and fluctuate within a wide range. Can we predict what throughput will look like in the next 15 minutes given the last 15 minutes of data? How can we incorporate longer-term historical information about the network and device? What kind of data can we provide from the server that would allow the device to adapt optimally? Even if we cannot predict exactly when a network drop will happen (this could be due to all kinds of things, e.g. a microwave turning on or going through a tunnel while streaming from a vehicle), can we at least characterize the distribution of throughput that we expect to see given historical data?Since we are observing these traces at scale, there is opportunity to bring to bear more complex models that combine temporal pattern recognition with various contextual indicators to make more accurate predictions of network quality.One useful application of network prediction is to adapt video quality during playback, which we describe in the following section.Movies and shows are often encoded at different video qualities to support different network and device capabilities. Adaptive streaming algorithms are responsible for adapting which video quality is streamed throughout playback based on the current network and device conditions (see here for an example of our colleagues’ research in this area). The figure below illustrates the setup for video quality adaptation. Can we leverage data to determine the video quality that will optimize the quality of experience? The quality of experience can be measured in several ways, including the initial amount of time spent waiting for video to play, the overall video quality experienced by the user, the number of times playback paused to load more video into the buffer (“rebuffer”), and the amount of perceptible fluctuation in quality during playback.These metrics can trade off with one another: we can choose to be aggressive and stream very high-quality video but increase the risk of a rebuffer. Or we can choose to download more video up front and reduce the rebuffer risk at the cost of increased wait time. The feedback signal of a given decision is delayed and sparse. For example, an aggressive switch to higher quality may not have immediate repercussions, but could gradually deplete the buffer and eventually lead to a rebuffer event on some occasions. This “credit assignment” problem is a well-known challenge when learning optimal control algorithms, and machine learning techniques (e.g., recent advances in reinforcement learning) have great potential to tackle these issues.Another area in which statistical models can improve the streaming experience is by predicting what a user will play in order to cache (part of) it on the device before the user hits play, enabling the video to start faster and/or at a higher quality. For example, we can exploit the fact that a user who has been watching a particular series is very likely to play the next unwatched episode. By combining various aspects of their viewing history together with recent user interactions and other contextual variables, one can formulate this as a supervised learning problem where we want to maximize the model’s likelihood of caching what the user actually ended up playing, while respecting constraints around resource usage coming from the cache size and available bandwidth. We have seen substantial reductions in the time spent waiting for video to start when employing predictive caching models.Netflix operates on over a thousand different types of devices, ranging from laptops to tablets to Smart TVs to mobile phones to streaming sticks. New devices are constantly entering into this ecosystem, and existing devices often undergo updates to their firmware or interact with changes on our Netflix application. These often go without a hitch but at this scale it is not uncommon to cause a problem for the user experience — e.g., the app will not start up properly, or playback will be inhibited or degraded in some way. In addition, there are gradual trends in device quality that can accumulate over time. For example, a chain of successive UI changes may slowly degrade performance on a particular device such that it was not immediately noticeable after any individual change.Detecting these changes is a challenging and manually intensive process. Alerting frameworks are a useful tool for surfacing potential issues but oftentimes it is tricky to determine the right criteria for labeling something as an actual problem. A “liberal” trigger will end up with too many false positives, resulting in a large amount of unnecessary manual investigation by our device reliability team, whereas a very strict trigger may miss out on the real problems. Fortunately, we have history on alerts that were triggered as well as the ultimate determination (made by a human) of whether or not the issue was in fact real and actionable. We can then use this to train a model that can predict the likelihood that a given set of measured conditions constitutes a real problem.Even when we’re confident we’re observing a problematic issue, it is often challenging to determine the root cause. Was it due to a fluctuation in network quality on a particular ISP or in a particular region? An internal A/B experiment or change that was rolled out? A firmware update issued by the device manufacturer? Is the change localized to a particular device group or specific models within a group? Statistical modeling can also help us determine root cause by controlling for various covariates.By employing predictive modeling to prioritize device reliability issues, we’ve already seen large reductions in overall alert volume while maintaining an acceptably low false negative rate, which we expect to drive substantial efficiency gains for Netflix’s device reliability team.The aforementioned problems are a sampling of the technical challenges where we believe statistical modeling and machine learning methods can improve the state of the art:Solving these problems is central to Netflix’s strategy as we stream video under increasingly diverse network and device conditions. If these problems excite you and/or you’re interested in bringing machine learning to this exciting new space, please contact me or check out these science and analytics or software engineering postings!",https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f?source=collection_home---4------11---------------------,0,netflix,,NULL,2018-03-22
Launching the Netflix Public Bug Bounty Program,"Netflix’s goal is to deliver joy to our 117+ million members around the world, and it’s the security team’s job to keep our members, partners and employees secure. We have been engaging with the security community to achieve this goal through programs like responsible disclosure and private bug bounty over the past 5 years. We are now publicly launching our bug bounty program through the Bugcrowd platform to continue improving the security of our products and services while strengthening our relationship with the community.We first started our responsible vulnerability disclosure program in 2013 to provide an avenue for researchers to report security issues to us. To date, we have received and remediated 190 valid issues from this program. Once we felt comfortable with our processes around handling external reports efficiently, we dipped our toe in the bug bounty space with a private program launch in September 2016. Over the past 18 months, we have gradually increased the scope as well as the number of researchers in the program. We started our program with a more limited scope and 100 of Bugcrowd’s top researchers. In preparation for our public launch, we have increased our scope dramatically over the last year and have now invited over 700 researchers. We have attempted to fine tune things like triage quality, response time and researcher interactions to build a quality program that researchers like to participate in.Since the launch of our private bug bounty program, we have received 145 valid submissions (out of 275 total) of various criticality levels across the Netflix services. These submissions have helped us improve our external security posture and identify systemic security improvements across our ecosystem. We have also made efforts to stay engaged with our researchers via events such as a Defcon Meet and Greet and a recent bug bash. We work closely with researchers to evaluate the impact of a vulnerability and reward accordingly. So far, the highest reward in our program is a $15,000 payout for a critical vulnerability.Netflix has a unique culture of Freedom and Responsibility that enables us to run an effective bug bounty program. Engineers at Netflix have a high degree of ownership for the security of their products and this helps us address reports quickly. Our security engineers also have the autonomy and freedom to make reward decisions quickly based on the reward matrix and bug severity. This ultimately helps create an efficient and seamless experience for researchers which is important for engagement in the program.Netflix works with security researchers that participate in our program to understand and attempt to acknowledge reports quickly, within seven days of submission. Our current report acknowledgement average is 2.7 days. We also recognize researcher contributions on our Security Researcher Hall of Fame if they are the first to report the issue and we make a code or configuration change based on the report. We pay researchers for unique vulnerabilities that meet the guidelines of our scope as soon as we validate them. Finally, through our public program we will allow coordinated disclosure when appropriate for valid, remediated submissions. Please see our program terms for all details. We are so excited to launch our Public Program and we hope to expand our researcher community.",https://medium.com/netflix-techblog/netflixbugbounty-ae3bf4489def?source=collection_home---4------12---------------------,0,netflix,,NULL,2018-03-21
Engineering to Improve Marketing Effectiveness (Part 1),"“Make people so excited about our content that they sign up and watch”- Kelly Bennett, Netflix Chief Marketing OfficerThis statement has become the driving force on our Advertising Technology (AdTech) team. With a slate of great original content to promote, Netflix has a unique opportunity to use both earned and paid media to create the excitement among people all over the world. Netflix is now available in 190 countries and advertises globally in dozens of languages, for hundreds of pieces of content, using millions of promotional assets.The AdTech team’s charter is to help make it easy for our marketing partners to spend their time and money wisely through experimentation and automation. This involves deep partnership with marketing, operations, finance, science and analytics groups to drive improvements across the board. This is the first in a series of blog posts where we share all the ways we partner with our marketing team all the way from collaborative asset creation and assembly of advertisements to optimizing campaigns on programmatic channels.Context and Culture:The Netflix marketing team believes the best ways to create demand for Netflix is to promote high quality, exclusive content that can only be watched on Netflix. If we are successful at creating demand for our original content, new members will sign up. As part of this process, we are more successful if we create and collect on this demand (acquisition marketing) in about the right proportion by market.Choosing which titles and markets to support remains a mix of art and science, with our creative teams working hand in hand with our technology teams to create winning formulas. Marketing makes top level strategy decisions on the set of markets to advertise in, the set of titles that need attention, and the creative behind the title in partnership with the director/showrunner.The AdTech team helps our marketing partners execute against that strategy in the following ways:We are proud of Netflix’s data driven culture, as you can read about here and here. Just as we improve our Netflix product through A/B testing, our marketing team embraces experimentation to help guide and improve human judgement. The more we can create tools and processes that streamline our approach, the more our talented teams can focus on helping great stories reach the right audiences. Our philosophy is “every dollar we spend is a dollar we can learn from”. The AdTech team ultimately seeks to create technology that will enable our partners to spend more of their time on strategic and creative decisions, while we use experimentation to guide Netflix’s instincts on the best tactical path forward.Optimizing for IncrementalityNetflix aims to use paid media to drive incremental effectiveness. Some cohorts of people are likely to sign up for Netflix anyway (e.g. due to a friend’s recommendation) and we would rather not show ads to them to the extent we are able to control for it. Instead, what we are most interested in is having our marketing focus on people who have not yet made up their mind about Netflix. This overall strategy has heavily shaped our philosophy and work as you will find in subsequent blog posts.At a high level, we can model the marketing lifecycle at Netflix to these four steps:This article will go in details on first step of Creative Development & Localization. We will give an overview of the work to support our operations team who helps create millions of assets (trailers, artwork, etc.) that are used in Netflix marketing.Creative Development & Localization at ScaleThe rate at which Netflix is growing, marketing all of the Netflix Original titles in dozens of languages, with many concepts & message types will ultimately result in millions of marketing assets. We need our marketing, social & PR teams to come together to scale our content campaigns globally around the world. This will only be possible when we build a streamlined, robust asset creation & delivery pipeline to automate all the processes involved. This is where the Digital Marketing Infrastructure team enters! Our charter is to create applications and services that help Netflix Marketing optimize and automate their processes and scale their operations to deliver a large number of video, digital and print assets for all Netflix marketing campaigns being worked on across the globe.To put it in simple words — any Netflix trailers and digital artwork that you see on YouTube, Facebook, Snapchat, Instagram, Twitter and other social media platforms or on TV are created and localized using tools built by the team. We even touch some of those physical billboards and posters you see at various freeways and traffic lights and on buses and trains all over the world.So how are these trailers created?In the Audio Visual (AV) world, it all starts with the Marketing Creative team working with external agencies to create a set of trailers for a particular title. A title can be a Series like Stranger Things or a movie like Bright, or a Stand up Comedy like Dave Chappelle’s or a Documentary like 13th. After a few rounds of creative review and feedback, the trailer is finalized for the original language of the title. Regions then need this trailer in various combinations of subtitles, dubs, ratings card, Netflix logo in specific locations on the screen, etc. Here is an example showing these various combinations for a frame from Ozark.The Marketing team works with multiple partner agencies to build these localized video files for these trailers. The assets are then encoded to specifications (file type, resolution, encoding) of the social platform where they will be delivered.To give an insight into the numbers, the following graphic gives an idea on the number of video assets that were created for marketing Bright. We ended up with a total of over 5000 different files covering different languages and various ad formats.Our Marketing team spends a lot of time creating, producing, testing, and sometimes redelivering these assets due to video, subtitle, dubbing issues. We also incur expenses for getting these done by external agencies to help scale out this operation today. Moving forward, we would love to be able to gather metrics during the marketing campaign to identify bottlenecks, be able to compare campaigns for various titles and provide much needed visibility across global teams.How do we help?We started our work by looking through the Marketing workflow and identifying key automation points in the process and started building applications/services to help optimize. We are building:As we make progress on each of these tools individually, the real value will be realized when all of these tools start interacting with each other and doing automated handoffs for the assets from one tool to another minimizing human involvement for lower order decisions. Here is how it will all come together.We have set ourselves with the goal of making 2018 the year when all the moving pieces start to work together and we expect to see tremendous gains on the operations side in terms of time, effort and resources.The team is growing fast and rightly so, as the benefit of automating and optimizing most of these workflows will save us a lot of manual hours and expense. That is the only way forward for us to scale. We are hiring for several open positions in the AdTech team to help us architect and build these systems. A chance to make an impact of this magnitude is rare. If you are interested in working on these complex challenges to disrupt the entertainment industry and shape its future, we would love for you to be a part of the team!As mentioned earlier, Creative Development & Localization is just the first stage in the Marketing Asset Creation and Delivery pipeline. There are a lot of interesting opportunities and challenges in the whole process.Our follow up articles will go in depth on the next stages of the marketing lifecycle. So stay tuned!",https://medium.com/netflix-techblog/engineering-to-improve-marketing-effectiveness-part-1-a6dd5d02bab7?source=collection_home---4------13---------------------,0,netflix,"bitcoin,blockchain,java,python",NULL,2018-03-14
Netflix Hack Day — Winter 2018,"by Tom Richards, Ruslan Meshenberg, Leslie Posada, Daniel Jacobson, Kaely CoonAbout a week ago, Netflix hosted another successful Hack Day. This event gives Netflix employees a chance to take a break from everyday work, collaborate with new people, and experiment with new technologies. It’s also where product ideas can get sparked.Like previous Hack Days, we saw a wide range of projects. Everything from hacks designed around improving the product, to increasing our internal efficiency, to some that were just meant for having fun. Below, we’re sharing videos produced by the hackers of some of our favorite hacks from the event. If you’re curious, you can also check out highlights from our past events: August 2017, January 2017, May 2016, November 2015, March 2015, February 2014, & August 2014.While we’re excited about the creativity and thought put into these hacks, they may never end up becoming part of the Netflix product. However, we still believe in the value of this work, and in sharing it in the spirit of both the event and our culture of open innovation.Thanks again to the teams who assembled another round of really impressive hacks in just 24 hours.As a nod to a well-known spacefaring industrialist, we thought it’d be fun to see if we could watch Netflix in space. We did this with an iPhone with downloaded Netflix content, a GoPro camera to watch the phone and the Earth, some heaters (lithium batteries don’t work well at -60°C / -76°F!), and an avionics package which measured GPS location, altitude, temperature, and pressure. All of this was hoisted to the heavens by a helium-filled meteorological balloon which reached an altitude of approximately 35,000 meters / 115,000 feet, at which you can see the curvature of the earth and the blackness of outer space. A special thanks goes to the Stanford Student Space Initiative for helping us out with this hack.By Tony Edwards, Micah RansdellAltered Carbon is set in Bay City, a futuristic version of San Francisco filled with towering buildings and neon advertisements. For Hack Day I created a virtual 3D Bay City which is also a functional Netflix home page. The city is procedurally generated based on your Netflix account. The glowing billboards advertise titles recommended for you. Use your flying car to explore the city. When you see a title you want to watch, you can open it to play on Netflix.By Evan BrowningMillions of people around the world take crowded public transit to work. Watching landscape video on a mobile device while commuting means you’re often elbowing your neighbors as you hold your arm out to rotate the screen. Our hack lets you watch full screen portrait video, while still letting you pan and scan around the video by tilting the device or swiping on the screen. It’s more polite without sacrificing control.By John Fox, Astha Trivedi",https://medium.com/netflix-techblog/netflix-hack-day-winter-2018-b36ee09699d6?source=collection_home---4------14---------------------,0,netflix,"python,php,html,css,frontend",NULL,2018-03-13
Project Nimble: Region Evacuation Reimagined,"We are proud to present Nimble: the evolution of the Netflix failover architecture that makes region evacuation an order of magnitude faster. At Netflix, our goal is to be there for our customers whenever they want to come and watch their favorite shows. A lot of the work we do centers around making our systems ever more available, and averting or limiting customer-facing outages. One of the most important tools in our toolbox is to route traffic away from an AWS region that is unhealthy. Because Netflix continues to grow quickly, we are now at a point where even short or partial outages affect many of our customers. So it’s critical that we are able to route traffic away from a region quickly when needed. This article describes how we re-imagined region failover from what used to take close to an hour to less than 10 minutes, all while remaining cost neutral.The history of region evacuation at Netflix is captured in three prior articles. While traffic failovers have been an important tool at our disposal for some time, Nimble takes us to the next level by optimizing the way in which we use existing capacity to migrate traffic. As part of our project requirements, we wanted minimal changes to core infrastructure, no disruptions to work schedules, and no onerous maintenance requirements dropped on other engineering teams at the company.When we set out on this journey, we began by breaking down the time it took then to do a traffic failover, about 50 minutes:All of the above steps add up to about 50 minutes, which we considered unacceptably long. Remember that we are operating at a scale of 117+M customers who, together, watch 140 million hours of content every day! 50 minutes of a broken experience impacts a lot of people. What we needed was something that was much faster.We set ourselves an aggressive goal of being able to fail over traffic in less than 10 minutes. In order to hit that kind of speed, we needed to eliminate the long poles. We needed services to start up instantly and be ready to take traffic without a warm-up period. If we could meet that requirement, regional failover would consist purely of flipping DNS records and letting the network move users over.Our services maintain homeostatic balance using autoscaling policies. Each service runs as an ASG (Auto Scaling Group). If a service is CPU-bound, for instance, we may choose to add more instances to its group when the average CPU usage crosses a threshold, and remove some instances when average CPU usage drops below another, lower threshold. Given the years since our migration to the cloud, this is a mechanism Netflix’s dev teams are now operationally familiar with, and is well-understood during normal and crisis operations.If we attempted to modify groups to run “cold” by pre-calculating needed capacity to absorb a failover from another region, we would need to make significant changes. Either we would need to change the signals that teams used for autoscaling into something centralized, giving their services instructions divorced from their normal operation, or we would need to alter every autoscaling policy with some kind of linear (or worse) transformation to take into account failover absorption needs. The idea of opening targeted consultations on each of the hundreds of scaling policies at Netflix did not seem like a winning strategy.We also considered simply abandoning autoscaling altogether and pinning to a calculated value, but this would hide performance regressions in the code by absorbing them into a potentially enormous buffer intended for regional evacuation absorption. We would need to come up with some automated way to frequently calculate a desired service size given incoming RPS and scale the buffer based on this metric, but no such mechanism was, as yet, available.We needed to come up with something more clever.Having capacity ready to take traffic seemed like the right solution, but adding it to active services in the front-line would add an operational burden that we didn’t want to incur. How, then, would we keep spare instances at the ready without affecting production? Our solution essentially combines the benefits of extra capacity without the distributed burden of operating it.Netflix bakes AMIs rather than having configuration management prepare instances from a base after launch. We realized that we could keep instances hidden away in shadow ASGs that would work as “dark” groups topping off capacity for the services they were shadowing. We would have to ensure total isolation of these groups from the streaming and metrics analysis path until they were activated, and we’d need to figure out mechanisms to add them to running services when they were needed so that on failover it would look like we’d provisioned new instances spontaneously when called upon.Our setup is based on the relatively unknown detach and attach instance mechanisms that AWS provides for EC2 ASGs. Essentially, we can pluck an instance from the dark autoscaling group and push it into the ether, then make a subsequent EC2 API call to pop it into a running service group. We created an orchestrator that detaches instances, keeps track of them, and attaches them to their intended destinations.It was straightforward to test the detach and attach mechanism with a single ASG, but for a production environment incorporating many ASGs, we would need a much better mechanism to track active services, follow “red/black” pushes, and clone configurations from production services. We leveraged our Netflix Edda and Spinnaker cloud management and deployment tools and their APIs to track changes to frontline ASGs and clone them into dark autoscaling clusters with identical launch configurations automatically when deploy pipelines, rollbacks, or other operations happen. We also needed to predict how many dark instances we need for each service. If we have too few instances for a service, this service may easily get overwhelmed, which can then have negative downstream and upstream effects on other services and eventually on our customers. For this reason, it’s important that we get this prediction right for each and every one of our services. At a high level, the calculation is based on time of day and how much traffic we expect each service in a specific region to see if we were to do a failover.Nearly all Netflix production applications inherit from a base AMI. The base AMI provides a well-known Netflix environment featuring consistent packages and system configuration as well as kernel tuning and a pre-populated set of environment variables. It also autodetects the ASG that a service is in and sets a number of variables corresponding to this — variables we needed to match to the parent service we were shadowing. The base AMI team helped us interject early in the boot process, making sure that all dark instances match the system environment they are shadowing and are blissfully unaware of their actual location.Now we had a mechanism to create dark autoscaling groups and move their instances into production. We had to keep these instances out of the traffic path; otherwise we’d just created a very elaborate mechanism of pinning services high, which we were trying to avoid. Netflix uses two primary mechanisms to send traffic to instances — Amazon’s Elastic Load Balancers (ELBs) and our internal Ribbon system. ELBs attach to one or more ASGs, and because dark instances existed in a different, shadow group from a service’s main ASG, the service’s ELBs never saw the dark instances, and this communication method was thus disabled. In order to prevent Ribbon traffic from communicating with dark capacity, our Runtime team helped us devise a library (included in all of our services through our common platform) to prevent dark capacity from registering as UP with Eureka (our service discovery system), gating them in a STARTING state. In this mode, Ribbon never noticed the instances as ready to take traffic; they would come up at the ready but wait for our signal before registering UP with Eureka.Finally, even when not munching on customer traffic, our instances produce an incredible amount of metrics about their functioning. We needed the instances to be silent, and to that end, we enlisted the help of the Insight Team to help us disable Atlas reporting when we had hit our STARTING gate. This was the final piece of the puzzle: until the transition to a functioning UP status, dark instances were not registered to communicate and reported no metrics, but had in fact gone through their entire startup procedure and were ready to take on traffic at the flip of a switch. So we built the switch.We indeed reached our goal of sub-10 minute failovers. We can now complete the operation in 8 minutes, as opposed to the 50 minutes it used to take. Rolling out all the changes above and the software to orchestrate all of this took a team of two approximately six months. The project timeline is a great example how a small team can make a big difference fast at Netflix.For such a wide-ranging and impactful project, touching all of our control plane services in every AWS region out of which we operate, Nimble’s simplicity allows it to scale. As long as teams have no cross-regional dependencies, Nimble will “just work” for them without extra effort or the need to build in integration or enable any special code. At this time, all streaming-path services are enabled for Nimble by default.The lack of heavy maintenance burden allows the Traffic Team to focus on innovation and the future of Nimble, some of which can be already be seen in how we tackle regional disruptions. We’re looking to explore new ways to use Nimble. For instance, if a service needs an emergency dose of capacity, should we allow them to hit a button and engage any failover capacity? We’re also investigating using Nimble as a basis for quicker autoscaling response: why have AWS start up fresh instances when we have pre-warmed instances ready to go?If answering questions such as these sounds interesting, join us!- Luke Kosewski, Amjith Ramanujam, Niosha Behnam, Aaron Blohowiak, and most recently Katharina Probst",https://medium.com/netflix-techblog/project-nimble-region-evacuation-reimagined-d0d0568254d4?source=collection_home---4------15---------------------,0,netflix,,NULL,2018-03-12
Optimized shot-based encodes: Now Streaming!,"Bad picture quality — blockiness, blurring, distorted faces and objects — can draw you out of that favorite TV show or movie you’re watching on Netflix. In many cases, low bandwidth networks or data caps prevent us from delivering the perfect picture. To address this, the Netflix Video Algorithms team has been working on more efficient compression algorithms that enable Netflix to deliver the same or better picture quality while using less bandwidth. And working together with other engineering teams at Netflix, we update our client applications and streaming infrastructure to support the new video streams and to ensure seamless playback on Netflix devices.To improve our members’ video quality, we developed and deployed per-title encoding in 2015, followed by better mobile encodes for downloads a year later. Our next step was productizing a shot-based encoding framework, called Dynamic Optimizer, resulting in more granular optimizations within a video stream. In this article we describe some of the implementation challenges we overcame in bringing this framework into our production pipeline, and practical results on how it improves video quality for our members.As described in more detail in this blog post, the Dynamic Optimizer analyzes an entire video over multiple quality and resolution points in order to obtain the optimal compression trajectory for an encode, given an optimization objective. In particular, we utilize VMAF, the Netflix subjective video quality metric, as our optimization objective, since our goal is to generate streams at the best perceptual quality.The primary challenge we faced in implementing the Dynamic Optimizer framework in production was retrofitting our parallel encoding pipeline to process significantly more encode units. First, the analysis step for the Dynamic Optimizer required encoding with different resolutions and qualities (QPs), requiring an order of magnitude more complexity. Second, we transitioned from encoding video chunks of about a few minutes long, to video encodes on a per-shot basis. For example, in the original system, a 1-hour episode of Stranger Things results in twenty 3-minute chunks. With shot-based encoding, with an average shot-length of 4 seconds, the same episode requires processing of 900 shots. Assuming each chunk corresponds to a shot (Fig. 1B), the new framework increased the number of chunks by more than two orders of magnitude per encode, per title. This increase exposed system bottlenecks related to the number of messages passed between compute instances. Several engineering innovations were performed to address the limitations and we discuss two of them here: Collation and Checkpoints.While we could have improved the core messaging system to handle such an increase in message volume, it was not the most feasible and expedient solution at that time. We instead adapted our pipeline by introducing collation.In collation, we collate shots together, so that a set of consecutive shots make up a chunk. Now, given that we have flexibility on how such collation occurs, we can group an integer number of shots together so that we produce approximately the same 3-minute chunk duration that we produced initially, under the chunk-based encode model (Fig. 1C). These chunks could be configured to be approximately the same size, which helps with resource allocation for instances previously tuned for encoding of chunks a few minutes long. Within each chunk, the compute instance independently encodes each of the shots, with its own set of defined parameters.Collating independently encoded shots within a chunk led to an additional system improvement we call checkpoints. Previously, if we lost a compute instance (because we had borrowed it and it was suddenly needed for higher priority tasks), we re-encoded the entire chunk. In the case of shots, each shot is independently encoded. Once a shot is completed, it does not need to be re-encoded if the instance is lost while encoding the rest of the chunk. We created a system of checkpoints (Fig. 2) to ensure that each encoded shot and associated metadata are stored immediately after completion. Now, if the same chunk is retried on another compute instance, encoding does not start from scratch but from the shot where it left off, bringing computational savings.In December 2016, we introduced AVCHi-Mobile and VP9-Mobile encodes for downloads. For these mobile encodes, several changes led to improved compression performance over per-title encodes, including longer GOPs, flexible encoder settings and per-chunk optimization. These streams serve as our high quality baseline for H.264/AVC and VP9 encoding with traditional rate control settings.The graph below (Fig. 3) demonstrates how the combination of Dynamic Optimization with shot-based encoding further improves compression efficiency. We plot the bitrate-VMAF curves of our new optimized encodes, referred to as VP9-Opt and AVCHi-Opt, compared toTo construct this graph, we took a sample of thousands of titles from our catalog. For each bitrate, x, (on the horizontal axis), and for each title, we selected the highest quality encode (as expressed by a VMAF score) with bitrate ≤ x. We then averaged VMAF values across all the titles for the given x, which provided one point for each curve in the following figure. Sweeping over all bitrate values x, this resulted in 5 curves, corresponding to the 5 types of encodes discussed above. Assuming stable network conditions, this is the average VMAF quality you will receive on the Netflix service at that particular video bandwidth.Let’s illustrate the reduction in bitrate at equivalent quality, by drawing a horizontal line at VMAF=80 (good quality), which gives us the following bitrates:We can see that, compared to per-title encoding with AVCMain, the optimized encodes require less than half of the bits to achieve the same quality. With VP9-Opt, we can stream the same quality at less than one third of the bits of AVCMain. Compared to AVCHi-Mobile and VP9-Mobile, we save 17% and 30%, respectively.We also examine how visual quality is impacted given the same bandwidth. For example, an average cellular connection bandwidth of 250 kbps, results in the average VMAF values shown in the table below. The optimized encodes provide noticeably better video quality than AVCMain.To illustrate the difference in visual quality, the example below shows a frame from a Chef’s Table episode, taken from different encodes with approximately 250 kbps bitrate. Immediately noticeable is the increased quality in textures (bricks, trees, rocks, water etc). A visually noticeable difference is observed between AVCMain (Fig. 4A, VMAF=58) and AVCHi-Opt (Fig. 4B, VMAF=73). The VP9-Opt frame (Fig. 4C, VMAF=79) looks sharpest.In the following example, we show a detail of the opening scene of 13 Reasons Why, at approximately 250 kbps. For AVCMain (Fig. 5A), the text at the top is hardly legible, deserving a VMAF value of 60. For AVCHi-Opt (Fig. 5B), we see a large jump in quality to a VMAF value of 74. For VP9-Opt (Fig. 5C), the text and edges become crisp, and we get another noticeable increase in quality, which is also reflected in the VMAF value of 81.In the previous section, we illustrated that optimized encodes offer significantly higher compression efficiency than per-title encodes, leading to higher quality at a comparable bitrate, or lower bitrate at the same quality. The question remains whether this translates into an improved experience for our members.Before deploying any new encoding algorithm in production, we thoroughly validate playability of the streams using A/B testing on different platforms and devices. A/B testing provides us with a controlled way to compare the Quality of Experience (QoE) of a treatment cell (our new encodes), to the control cell (existing experience). We ran A/B tests on a wide range of devices and titles to compare our optimized encodes against the existing AVCMain streaming experience. This also allowed us to fine-tune our encoding algorithms and adaptive streaming engine for different platforms.We assessed the impact of optimized encodes on different QoE metrics. Based on the results of A/B testing, we expect the following improvements to our members’ viewing experience:In addition, many of our members have a data cap on their cellular plans. With the new optimized encodes, these members can now stream more hours of Netflix at the same or better quality using the same amount of data. The optimized encodes are also available for our offline downloads feature. For downloadable titles, our members can watch noticeably higher quality video for the same storage.Over the last few months, we have generated AVCHi-Opt encodes for our entire Netflix catalog and started streaming them on many platforms. You can currently enjoy these optimized streams when watching Netflix on iOS, Android, PS4 and XBox One. VP9-Opt streams have been made available for a selection of popular content, and can be streamed on certain Android devices. We are actively testing these new streams on other devices and browsers.Whether you’re watching Chef’s Table on your smart TV with the fastest broadband connection, or Jessica Jones on your mobile device with a choppy cellular network, Netflix is committed to delivering the best picture quality possible. The launch of the new optimized encodes is an excellent example of combining innovative research, effective cross-team engineering and data-driven deployment to bring a better experience to our members.",https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830?source=collection_home---4------16---------------------,0,netflix,"docker,cloud,backend,java,python",NULL,2018-03-09
Dynamic optimizer — a perceptual video encoding optimization framework,"Video encoding has fueled academic research over the past 25 years and enabled compelling products and services. Many companies are built around video encoding and transmission — Netflix and Google’s YouTube are two prime examples of video-centric companies. The fundamentals of video encoding haven’t changed for all these years, as modern video streams are produced with the same type of encoding parameters that have been used since MPEG-1 [1]: a certain frame resolution is chosen, together with a group-of-pictures (GOP) structure that imposes periodic Intra pictures, and a target bitrate that is (approximately) met by either a single-pass or two-passes over the input video frames.Companies have struggled to fine-tune additional parameters in video codecs, creating what is commonly referred to in the industry as a good “recipe”. These recipes have been typically created and customized by human inspection of resulting encodes on a selected set of a few titles and have been kept fixed for a very long time.At the same time, improvements in core video codec tools have led to spectacular reduction in bitrate savings — one can achieve the same quality with an HEVC [2] encoder, while using just a fraction (about 30%) of the bits required by MPEG-1. This improvement, although, which has always been measured using mean-squared-error (MSE), wasn’t always accompanied by equally impressive results, when encodes were evaluated by human observers. The magic number to claim when a new codec was being developed has been “50%”. H264/AVC [3] claimed 50% less bits than MPEG-2 [4] and HEVC claimed 50% less bits than AVC. Yet, in practical systems, these savings never quite materialized — the best estimates on what benefits one sees from an incremental change in video codecs is closer to 40% [5].Parallel to the standardization efforts at ISO and ITU, Google has been developing their own family of royalty-free video codecs; its latest addition was VP9 [6], first introduced in 2013 and finalized in 2014. VP9 built on the earlier success of VP8 and the line of “True Motion” codecs developed by On2 Technologies, acquired by Google in 2010.Keeping in mind that most of the video codec improvements carried a very heavy computational overhead on both decoding and — mainly — encoding complexity, one already understands that the newer and more efficient codecs required an ever increasing complexity in order to be deployed in a commercial video transmission service. One typically sees a factor between 5–10 in encoder complexity increase with each generation of video codecs — while the corresponding increase in decoding complexity is typically by a factor of 2.If one accepts the increased complexity that comes with newer and more efficient codecs, a bigger question is: what can we do at the system level, for example, in the way we connect video frames as input to an encoder or how we use the output of a video decoder to render it on a screen, to further improve the video quality as perceived by human observers who consume all these hours of video today?The keywords in this new approach, presented here, are the following:For the remainder of this tech blog, we assume the reader is familiar with the basics of adaptive streaming, such asInterested readers can refer to a number of available adaptive streaming tutorials, such as this Wiki page [7].In a previous Netflix tech-blog [8], published in Dec. 2015, we described how encoding on the cloud benefits greatly from “chunked” encoding. This translates into breaking a long video sequence, e.g. of 1 hour duration, in multiple chunks, each of a certain duration — for example, 20 chunks, each 3 min. long. We then perform encoding of each chunk independently with a certain encoding recipe, concatenate or “assemble” the encodes and thus obtain an encoded version of the entire video sequence.Among the advantages of chunked encoding, the most important is that it allows for a robust system to be built on the cloud using software video encoding. If and when cloud instances fail to complete a certain encode, it requires re-processing the corresponding chunk only, instead of restarting an entire hour-long video encode. One can also see the reduction in end-to-end delay, since different chunks can be encoded in parallel; thus achieving almost infinite scalability in the overall encoding system.There are some penalties that come with chunked encoding — namely the fact that a video encoder operating over the full hour-long sequence, especially in two-pass mode, can preview what is following and therefore do better long-term bitrate allocation; thus achieving better overall quality at the same bitrate. Yet, the advantages that come from chunked encoding outweigh these penalties.At Netflix, we have been constantly improving video quality for our members all over the world. One major milestone in our continuous efforts has been “Per-title encode optimization”, described in great detail in our techblog, posted in Dec. 2015 [9]. Per-title encode optimization introduced the concept of customizing encoding according to complexity, which translates to proper resolution and bitrate selection for each video sequence we have in our catalog. This provided significant improvement over our previous fixed resolution/bitrate ladder generation, by taking into account the characteristics of video — amount of motion, level of detail, colorfulness — and optimizing coding efficiency by selecting encoding parameters that better fit each title. Another important milestone has been “per-chunk encode optimization”, introduced in Dec. 2016 as part of our “Mobile encodes for downloads” initiative, explained in more detail in this Netflix tech blog [10]. The concept of equalizing rate-distortion slopes, discussed in more detail in a subsequent section, was also used in that work and provided significant improvements. In fact, one can consider the current work a natural extension of the “Per-title encode optimization” and “Per-chunk encode optimization”; we can call it “Perceptual per-shot encode optimization”.In an ideal world, one would like to chunk a video and impose different sets of parameters to each chunk, in a way to optimize the final assembled video. The first step in achieving this perfect bit allocation is to split video in its natural atoms, consisting of frames that are very similar to each other and thus behave similarly to changes to encoding parameters — these are the “shots” that make up a long video sequence. Shots are portions of video with a relatively short duration, coming from the same camera under fairly constant lighting and environment conditions. It captures the same or similar visual content, for example, the face of an actor standing in front of a tree and — most important — it is uniform in its behavior when changing coding parameters. The natural boundaries of shots are established by relatively simple algorithms, called shot-change detection algorithms, which check the amount of differences between pixels that belong to consecutive frames, as well as other statistics. When that amount of difference exceeds certain fixed or dynamically adapted threshold, a new shot boundary is announced.There are cases, such as cross-fades or other visual effects that can be applied on the boundary between two consecutive shots, which can be dealt with by more sophisticated algorithms.The end result of a shot-change detection algorithm is a list of shots and their timestamps. One can use the resulting shots as the basic encoding block, instead of a fixed-length chunk. That provides for a few really unique opportunities:In another Netflix tech-blog [11], published in June 2016, we explained the Video Multi-method Assessment Fusion (VMAF) quality metric, developed in-house and then open-sourced for the entire video community to benefit.Key features of VMAF are the following:A seminal paper by Ortega and Ramchandran [12] in 1998 showed how to address optimality when dealing with multiple choices in image and video coding.Assuming that an image consists of N units that need to be codedOne can thus consider the following system:This complete system is called “Dynamic Optimizer” and the framework produces Netflix’s newest generation of encodes.10 representative titles from the Netflix catalog were selected and encoded using the VP9-libvpx video codec.In terms of temporal aggregation, we have implemented various pooling methods, two of them corresponding to the quality-to-distortion mappings introduced earlier, i.e. linear and inversely proportional mapping. We refer to them as arithmetic mean average VMAF (LVMAF) and harmonic mean averaged VMAF (HVMAF).These two methods, LVMAF and HVMAF temporal quality aggregation, produced very high quality encoded sequences — allowing for more aggressive or more conservative temporal quality fluctuations in the combined video sequence, respectively.Encoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” held in Oct. 2016. Based on that study, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. We reproduce key results from that study in the Appendix.We compared results obtained by the dynamic optimizer against the best possible fixed-QP VP9-libvpx encoding. The methodology followed and various parameters chosen for this experiment are summarized in the following table.The corresponding gains obtained by the dynamic optimizer, expressed both in terms of % bitrate savings at the same visual quality and in terms of improvement in HVMAF scores at the same average bitrate, are as follows:The result was an average bitrate savings of 17.1% over the best possible fixed-QP encoding of the entire video sequence when using HVMAF as quality metric. The improvement when using PSNR is even higher: 22.5% bitrate savings on average.In this comparison, computational complexity remained constant between the baseline and dynamic optimizer results, since obtaining the convex hull of fixed-QP encodes for an entire sequence requires the same complexity as that for the dynamic optimizer. Thus, this represents a lower-bound on the amount of improvement introduced by the dynamic optimizer.If we use a more common baseline, such as the 2-pass VBR configuration with CPU=1, good, AQ-mode=2 encoding recipe in VP9-libvpx, the improvement by the dynamic optimizer is much larger: over 50% bitrate savings on average, in terms of HVMAF. One needs to keep in mind, although, that computational complexity of the dynamic optimizer solution is much higher in that case.Based on what was presented earlier, one can immediately understand that there is nothing codec-specific in the dynamic optimizer framework. In order to confirm this, a set of shorter clips were encoded with H.264/AVC, HEVC and VP9-libvpx, with the following experimental set-up:One can notice that the dynamic optimizer improves all three codecs by approximately 28–38%. Keep in mind that these improvements are not comparing performance between codecs but rather how each one of these codecs can be improved by using the dynamic optimizer framework. A more thorough comparison of state-of-the-art video codecs, using the dynamic optimizer as high-level encoding framework, will be published in the upcoming weeks.Dynamic optimizer is an optimization framework for video encoding. Its key features are the following:Its key advantages are the following:We’ve implemented the dynamic optimizer framework in our encoding pipeline, leveraging our scalable cloud infrastructure and under-utilized cloud instances during non-peak streaming hours [13],[14]. We’ve applied this encoding system to AVC-High and VP9 streams, improving our members’ video quality as well as saving bandwidth. Stay tuned for another tech blog describing our implementation and results!This work is the collective result of the entire Video Algorithms team at Netflix. I would like to personally thank Anne Aaron, Chao Chen, Jan De Cock, Rich Gerber, Liwei Guo, Zhi Li, Megha Manohara, Aditya Mavlankar, Anush Moorthy, Andrey Norkin, Kyle Swanson and David Ronca for all their contributions.[1] ISO/IEC 11172–2:1993 “Information technology — Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s — Part 2: Video”[2] ISO/IEC 23008–2:2013 “Information technology — High efficiency coding and media delivery in heterogeneous environments — Part 2: Video”[3] ISO/IEC 14496–10:2014 “Information technology — Coding of audio-visual object — Part 10: Advanced Video Coding”[4] ISO/IEC 13818–2:2013 “Information technology — Generic coding of moving pictures and associated audio information — Part 2: Video”[5] J. De Cock, A. Mavlankar, A. Moorthy and A. Aaron, “A large-scale video codec comparison of x264, x265 and libvpx for practical VOD applications”, Proc. of the SPIE 9971, Applications of Digital Image Processing XXXIX, 997116 (27 Sep. 2016)[6] A. Grange, P. de Rivaz, and J. Hunt, “VP9 Bitstream and Decoding Process Specification”, Google, 2016[7] “Adaptive bitrate streaming”, Wikipedia — The Free Encyclopedia, https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming[8] A. Aaron and D. Ronca, “High quality video encoding at scale,” The NETFLIX tech blog, Dec. 9, 2015, link: http://techblog.netflix.com/2015/12/high-quality-video-encoding-at-scale.html[9] A. Aaron, Z. Li, M. Manohara, J. De Cock and D. Ronca, “Per-title encode optimization”, The NETFLIX tech blog, Dec. 14, 2015, link: http://techblog.netflix.com/2015/12/per-title-encode-optimization.html[10] A. Norkin, J. De Cock, A. Mavlankar and A. Aaron, “More Efficient Mobile Encodes for Netflix Downloads”, The NETFLIX tech blog, Dec. 1, 2016, link: https://medium.com/netflix-techblog/more-efficient-mobile-encodes-for-netflix-downloads-625d7b082909[11] Z. Li, A. Aaron, I. Katsavounidis, A. Moorthy, and M. Manohara, “Toward a practical perceptual video quality metric,” The NETFLIX tech blog, June 5, 2016, link: http://techblog.netflix.com/2016/06/toward-practical-perceptual-video.html[12] A. Ortega and K. Ramchandran, “Rate-distortion methods for image and video compression: An overview,” IEEE Signal Processing Magazine, vol. 15, no. 6, pp. 23–50, 1998[13] A. Park, D. Derlinger and C. Watson “Creating your own EC2 spot market,” The NETFLIX tech blog, Sep. 28, 2015, link: http://techblog.netflix.com/2015/09/creating-your-own-ec2-spot-market.html[14] R. Wong, D. Derlinger, A. Shiroor, N. Mareddy, F. San Miguel, R. Gallardo and M. Prabhu “Creating your own EC2 spot market — part 2,” The NETFLIX tech blog, Nov. 23, 2015, link: http://techblog.netflix.com/2015/11/creating-your-own-ec2-spot-market-part-2.htmlEncoding parameters used in VP9-libvpx were taken from a previous study; its findings were presented at Netflix’s “Open house on royalty-free codecs” in Oct. 2016. Based on that study, which used the same set of 10 full-titles for testing as those chosen for the first experiment reported earlier, the best configuration to use is “fixed-QP, AQ-mode=0, CPU=0, best”, shown to produce highest quality both in terms of PSNR and VMAF quality metrics. The following figures show the effect in terms of average BD-rate loss when choosing different parameters in VP9-libvpx encoding.",https://medium.com/netflix-techblog/dynamic-optimizer-a-perceptual-video-encoding-optimization-framework-e19f1e3a277f?source=collection_home---4------17---------------------,0,netflix,python,NULL,2018-03-05
AVA: The Art and Science of Image Discovery at Netflix,"Authored by — Madeline, Lauren, Boris, Tim, Parth, Eugene and ApurvaAt Netflix, the Content Platform Engineering and Global Product Creative teams know that imagery plays an incredibly important role in how viewers find new shows and movies to watch. We take pride in surfacing the unique elements of a story that connect our audiences to diverse characters and story lines. As our Original content slate continues to expand, our technical experts are tasked with finding new ways to scale our resources and alleviate our creatives from the tedious and ever-increasing demands of digital merchandising. One of the ways in which we do this is by harvesting static image frames directly from our source videos to provide a more flexible source of raw artwork.Merchandising stills are static video frames taken directly from the source video content used to broaden the reach of a title on the Netflix service. Within a single one-hour episode of Stranger Things, there are nearly 86,000 static video frames.Traditionally, these merchandising stills are selected by human curators or editors, and require an in-depth expertise of the source content that they’re intended to represent. We know through A/B testing that we can effectively drive increased viewing from expected and unexpected audience groups by exploring as many representations of a title as possible. When it comes to title key art, we like to test many artistic representations of a title in order to find the “right” artwork for the right audience. While this presents an exciting opportunity for innovation and testing, it simultaneously presents a very challenging expectation to scale this experience across every title in our growing global catalog.AVA is a collection of tools and algorithms designed to surface high quality imagery from the videos on our service. A single season of average TV show (about 10 episodes) contains nearly 9 million total frames. Asking creative editors to efficiently sift through that many frames of video to identify one frame that will capture an audience’s attention is tedious and ineffective. We set out to build a tool that quickly and effectively identifies which frames are the best moments to represent a title on the Netflix service.To achieve this goal, we first came up with objective signals that we can measure for each and every frame of the video using Frame Annotations. As result, we can collect an effective representation of each frame of the video. Subsequently, we created ranking algorithms that allows us to rank a subset of frames that meets aesthetic, creative and diversity objectives to represent content accurately for various canvases of our product.As part of our automation pipeline, we process and annotate many different variables on each individual frame of video to best derive what the frame contains, and to understand why it is or isn’t important to the story. In order to scale horizontally and have predictable SLA for a growing catalog of content, we utilized the Archer framework to process our videos more efficiently. Archer allowed us to split the videos into smaller sized chunks that could each be processed in parallel. This has enabled us to scale by lending efficiency to our video processing pipelines, and allowing us to integrate more and more content intelligence algorithms into our tool sets.Every frame of video in a piece of content is processed through a series of computer vision algorithms to gather objective frame metadata, latent representation of frame, as well as some of the contextual metadata that those frame(s) contain. The annotation properties that we process and apply to our video frames can be roughly grouped into 3 main categories:Typically these properties are objective, measurable, and mostly contained at the pixel-level. Some examples of visual properties are brightness, color, contrast, and motion blur.Contextual metadata is comprised of a combination of elements that are aggregated to derive meaning from the actions or movement of the actors, objects and camera in the frame. Some examples include;Composition metadata refers to a special set of heuristic characteristics that we’ve identified and defined based on some of the core principles in photography, cinematography and visual aesthetic design. Some examples of composition are rule-of-third, depth-of-field and symmetry.After we’ve processed and annotated every frame in a given video, the next step is to surface “the best” image candidates from those frames through an automated artwork pipeline. That way, when our creative teams are ready to begin work for a piece of content, they are automatically provided with a high quality image set to choose from. Below, we outline some of the key elements we use to surface the best images for a given title.ActorsActors play a very important role in artwork. One way we identify the key character for a given episode is by utilizing a combination of face clustering and actor recognition to prioritize main characters and de-prioritize secondary characters or extras. To accomplish this, we trained a deep-learning model to trace facial similarities from all qualifying candidate frames tagged with frame annotation to surface and rank the main actors of a given title without knowing anything about the cast members.Beyond cast, we also take into account pose, facial landmarks, and the overall position of characters for a given cast member.Frame DiversityCreative and visual diversity is a highly subjective discipline, as there are many different ways to perceive and define diversity in imagery. In the context of this solution, image diversity more specifically refers to the algorithms ability to capture the heuristic variance that naturally occurs within a single movie or episode. In doing so, we hope to provide designers and creatives with a scalable mechanism to quickly understand which visual elements are most representative of the title, and which elements are misrepresentative of the title. Some of the visual heuristic variables that we’ve incorporated into AVA to surface a diverse image set for a title include elements such as camera shot types (long shot vs medium shot), visual similarity (rule of thirds, brightness, contrast), color (colors that are most prominent), and saliency maps (to identify negative space and complexity). By combining these heuristic variables, we can effectively cluster image frames based on a custom vector for diversity. Furthermore, by incorporating several vectors, we’re able to construct a diversity index against which all candidate imagery for a given episode or movie, can be scored.Filters for MaturityFor content sensitivity and audience maturity reasons, we also needed to make sure we excluded frames containing harmful or offensive elements. Examples of editorial exclusion criteria are things like; sex/nudity, text, logos/unauthorized branding, and violence/gore. In order to de-prioritize frames containing these elements, we incorporated the probability of each of these variables as vectors, allowing us to quantify and ultimately attribute a lower score for these frames.We additionally included elements such as title genre, content format, maturity rating, etc. as secondary elements or minor features and as feedback to the model for ranking prediction.In this techblog, we’ve provided an overview of our unique approach to surfacing meaningful images from video and enabling our creative teams to design stunning artwork every single day. AVA is a collection of tools and algorithms encapsulating the key intersections of computer vision combined with the core principles of filmmaking and photo editing.Stay tuned for a follow up blog in which we’ll dive into programmatic artwork composition, an exciting new solution that’s responsible for much of the artwork you see on the Netflix service today!Thank you.If you have great or innovative ideas come join us on the Content Platform Engineering team!",https://medium.com/netflix-techblog/ava-the-art-and-science-of-image-discovery-at-netflix-a442f163af6?source=collection_home---4------18---------------------,0,netflix,,NULL,2018-02-07
Fast JSON API serialization with Ruby on Rails,"by Shishir Kakaraddi, Srinivas Raghunathan, Adam Gross and Ryan JohnstonWe are pleased to announce the open source release of the Fast JSON API gem geared towards Ruby on Rails applications.IntroductionFast JSONAPI is aimed at providing all the major functionality that Active Model Serializer (AMS) provides, along with an emphasis on speed and performance by meeting a benchmark requirement of being 25 times faster than AMS. The gem also enforces performance testing as a discipline.AMS is a great gem and fast_jsonapi was inspired by it when it comes to declaration syntax and features. AMS begins to slow down, however, when a model has one or more relationships. Compound documents, AKA sideloading, on those models makes AMS slow down further. Throw in a need for infinite scroll on the UI, and AMS’s slowness starts becoming visible to users.Why optimize serialization?JSON API serialization is often one of the slowest parts of many well implemented Rails APIs. Why not provide all the major functionality that AMS provides with greater speed?Features:How do you write a serializer using Fast JSONAPI?We like the familiar way Active Model Serializers lets us declare our serializers. Declaration syntax of fast_jsonapi is similar to AMS.How fast is it compared to Active Model Serializers?Performance tests indicate a 25–40x speed gain over AMS, essentially making serialization time negligible on even fairly complex models. Performance gain is significant when the number of serialized records increases.Don’t believe us? You can run the benchmark tests for yourself. Refer to readme.DependencyJSON API is the anti-bikeshedding tool.Future WorkWe plan to add more features to the gem. We welcome suggestions, improvements, corrections and additional tests.",https://medium.com/netflix-techblog/fast-json-api-serialization-with-ruby-on-rails-7c06578ad17f?source=collection_home---4------19---------------------,0,netflix,"python,react,docker,backend,frontend,javascript,xml,ruby",NULL,2018-02-01
Scaling Time Series Data Storage — Part I,"The growth of internet connected devices has led to a vast amount of easily accessible time series data. Increasingly, companies are interested in mining this data to derive useful insights and make data-informed decisions. Recent technology advancements have improved the efficiency of collecting, storing and analyzing time series data, spurring an increased appetite to consume this data. However this explosion of time series data can overwhelm most initial time series data architectures.Netflix, being a data-informed company, is no stranger to these challenges and over the years has enhanced its solutions to manage the growth. In this 2-part blog post series, we will share how Netflix has evolved a time series data storage architecture through multiple increases in scale.Netflix members watch over 140 million hours of content per day. Each member provides several data points while viewing a title and they are stored as viewing records. Netflix analyzes the viewing data and provides real time accurate bookmarks and personalized recommendations as described in these posts:Viewing history data increases along the following 3 dimensions:As Netflix streaming has grown to 100M+ global members in its first 10 years there has been a massive increase in viewing history data. In this blog post we will focus on how we approached the big challenge of scaling storage of viewing history data.The first cloud-native version of the viewing history storage architecture used Cassandra for the following reasons:In the initial approach, each member’s viewing history was stored in Cassandra in a single row with row key:CustomerId. This horizontal partitioning enabled effective scaling with member growth and made the common use case of reading a member’s entire viewing history very simple and efficient. However as member count increased and, more importantly, each member streamed more and more titles, the row sizes as well as the overall data size increased. Over time, this resulted in high storage and operation cost as well as slower performance for members with large viewing history.The following figure illustrates the read and write flows of the initial data model:One viewing record was inserted as a new column when a member started playing a title. That viewing record was updated after member paused or stopped the title. This single column write was fast and efficient.Whole row read to retrieve all viewing records for one member: The read was efficient when the number of records per member was small. As a member watched more titles, the number of viewing records increased. Reading rows with a large number of columns put additional stress on Cassandra that negatively impacted read latencies.Time range query to read a time slice of a member’s data: This resulted in the same inconsistent performance as above depending on the number of viewing records within the specified time range.Whole row read via pagination for large viewing history: This was better for Cassandra as it wasn’t waiting for all the data to be ready before sending it back. This also avoided client timeouts. However it increased overall latency to read the whole row as the number of viewing records increased.Let’s look at some of the Cassandra internals to understand why our initial simple design slowed down. As the data grew, the number of SSTables increased accordingly. Since only recent data was in memory, in many cases both the memtables and SSTables had to be read to retrieve viewing history. This had a negative impact on read latency. Similarly Compaction took more IOs and time as the data size increased. Read repair and Full column repair became slower as rows got wider.Cassandra performed very well writing viewing history data but there was a need to improve the read latencies. To optimize read latencies, at the expense of increased work during the write path, we added an in-memory sharded caching layer (EVCache) in front of Cassandra storage. The cache was a simple key value store with the key being CustomerId and value being the compressed binary representation of viewing history data. Each write to Cassandra incurred an additional cache lookup and on cache hit the new data was merged with the existing value. Viewing history reads were serviced by the cache first. On a cache miss, the entry was read from Cassandra, compressed and then inserted in the cache.With the addition of the caching layer, this single Cassandra table storage approach worked very well for many years. Partitioning based on CustomerId scaled well in the Cassandra cluster. By 2012, the Viewing History Cassandra cluster was one of the biggest dedicated Cassandra clusters at Netflix. To scale further, the team needed to double the cluster size. This meant venturing into uncharted territory for Netflix’s usage of Cassandra. In the meanwhile, Netflix business was continuing to grow rapidly, including an increasing international member base and forthcoming ventures into original content.It became clear that a different approach was needed to scale for growth anticipated over the next 5 years. The team analyzed the data characteristics and usage patterns, and redesigned viewing history storage with two main goals in mind:For each member, viewing history data is divided into two sets:LiveVH and CompressedVH are stored in different tables and are tuned differently to achieve better performance. Since LiveVH has frequent updates and small number of viewing records, compactions are run frequently and gc_grace_seconds is small to reduce number of SSTables and data size. Read repair and full column family repair are run frequently to improve data consistency. Since updates to CompressedVH are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.New viewing records are written to LiveVH using the same approach as described earlier.To get the benefit of the new design, the viewing history API was updated with an option to read recent or full data:While reading viewing history records from LiveVH, if the number of records is over a configurable threshold then the recent viewing records are rolled up, compressed and stored in CompressedVH via a background task. Rolled up data is stored in a new row with row key:CustomerId. The new rollup is versioned and after being written is read to check for consistency. Only after verifying the consistency of the new version, the old version of rolled up data is deleted. For simplicity there is no locking during rollup and Cassandra takes care of resolving very rare duplicate writes (i.e., the last writer wins).As shown in figure 2, the rolled up row in CompressedVH also stores metadata information like the latest version, object size and chunking information (more on that later). The version column stores a reference to the latest version of rolled up data so that reads for a CustomerId always return only the latest rolled up data. The rolled up data is stored in a single column to reduce compaction pressure. To minimize the frequency of rollups for members with frequent viewing pattern, just the last couple of days worth of viewing history records are kept in LiveVH after rollup and the rest are merged with the records in CompressedVH during rollup.For the majority of members, storing their entire viewing history in a single row of compressed data resulted in good performance during the read flows. For a small percentage of members with very large viewing history, reading CompressedVH from a single row started to slow down due to similar reasons as described in the first architecture. There was a need to have an upper bound on the read and write latencies for this rare case without negatively impacting the read and write latencies for the common case.To solve for this, we split the rolled up compressed data into multiple chunks if the data size is greater than a configurable threshold. These chunks are stored on different Cassandra nodes. Parallel reads and writes of these chunks results in having an upper bound on the read and write latencies even for very large viewing data.As figure 3 indicates, rolled up compressed data is split into multiple chunks based on a configurable chunk size. All chunks are written in parallel to different rows with row key:CustomerId$Version$ChunkNumber. Metadata is written to its own row with row key:CustomerId after successful write of the chunked data. This bounds the write latency to two writes for rollups of very large viewing data. In this case the metadata row has an empty data column to enable fast read of metadata.To make the common case (compressed viewing data is smaller than the configurable threshold) fast, metadata is combined with the viewing data in the same row to eliminate metadata lookup overhead as shown in figure 2.The metadata row is first read using CustomerId as the key. For the common case, the chunk count is 1 and the metadata row also has the most recent version of rolled up compressed viewing data. For the rare case, there are multiple chunks of compressed viewing data. Using the metadata information like version and chunk count, different row keys for the chunks are generated and all chunks are read in parallel. This bounds the read latency to two reads.The in-memory caching layer was enhanced to support chunking for large entries. For members with large viewing history, it was not possible to fit the entire compressed viewing history in a single EVCache entry. So similar to the CompressedVH model, each large viewing history cache entry is broken into multiple chunks and the metadata is stored along with the first chunk.By leveraging parallelism, compression, and an improved data model, the team was able to meet all of the goals:The team achieved ~6X reduction in data size, ~13X reduction in system time spent on Cassandra maintenance, ~5X reduction in average read latency and ~1.5X reduction in average write latency. More importantly, it gave the team a scalable architecture and headroom to accommodate rapid growth of Netflix viewing data.In the next part of this blog post series, we will explore the latest scalability challenges motivating the next iteration of viewing history storage architecture. If you are interested in solving similar problems, join us.",https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-i-ec2b6d44ba39?source=collection_home---4------20---------------------,0,netflix,python,NULL,2018-01-23
Integrating imperative APIs into a React application,"by Sriram RaoThe Netflix TV app plays videos not only when users choose to watch a title, but also while browsing to find something great to enjoy. When we rewrote our TV user interface in React, we set out to improve the developer experience of integrating video playback into the UI so we could more rapidly experiment with various video-centric user experiences.A key piece was to expose a declarative interface to the underlying imperative video playback API that is more expressive, easier to extend and better hides the complexity of interacting with a stateful system thus fitting more elegantly into our React application.A common method of playing video in React involves rendering the <video> element and then using a reference to invoke imperative actions. A good example of this is skipping to a different location within the current video. We don’t actually have a <video> element available to our UI, just an imperative video player API in the Netflix device platform. We decided to map this imperative/stateful API onto a fully declarative/stateless React component because it fit into React better as JSX is declarative, made it possible to hold the state of video in application state, helped avoid the use of refs, and enabled easier layering of functionality via higher-order components (for example, to add text-to-speech).So how did we get rid of the imperative actions? Instead of orchestrating imperative function calls, we send props to our video component that describe the desired target state of video playback. Our video component detects changes in props and decides what video player API functions to call.Our video component signals when there are changes in video state (e.g., when transitioning from loading to playing) and changes in playback position. Handlers for these changes are passed down in props.A component that wraps an imperative API with a declarative facade should be capable of translating changes in input into the correct sequence of imperative API calls.This is how you can tell our video component to start playback in JSX:The playbackState prop is the desired final state. The onVideoStateChanged and onReportedTimeChanged props are handlers for our video component to signal changes in state of video and playback location respectively.The requested video plays irrespective of the current state of playback. If the video is not already playing, it starts. If the video is paused, it un-pauses. Our video component invokes the right set of imperative API calls to achieve this.To pause or stop video, the caller sets the playbackState prop to ""paused"" or ""stopped"".Our video component communicates the state of video playback (like loading, playing, paused, etc) so that surrounding UI elements can adapt to or reflect that state.We found that a component fits well into declarative UI if it:An example of the latter is that a switch from one playing video to another will trigger the following state changes in the Video Player:But the loading of the new video is more relevant to our UI, so it can react by showing a loading indicator. Our video component will smooth this out to:In most cases extending the API is matter of introducing additional properties. Here’s what it looks like when extended for an audio track, a subtitle track and volume.Extending the API to describe the target value of a constantly changing attribute of the system isn’t straightforward. Such attributes require two properties to describe it.For example, a single time property isn’t enough because it can become stale very quickly. After video starts at the position specified by the time property, the actual playback location and the time property drift apart. There is also no way to make the video seek back to the position specified by the time property because the value of this property won’t change.Our solution is to use two time properties — time and reportedTime — where reportedTime serves as a proxy for the actual playback location and helps tackle the drift between the two. You can think of time as the setter and reportedTime as the getter (but one that is delivered periodically via an event). During playback, the UI sets time to the reportedTime it gets so both values stay the same. When the UI wants to skip to a different location it sets time to the target time. Changes in desired position can be detected because time will not match reportedTime.Here’s an illustration of how this works. Say our video component is rendered with every update to reportedTime (which by the way is not a requirement). Since time and reportedTime advance together it is effectively a no-op. When time differs from reportedTime, a skip is initiated, and no more time updates are sent by our video component. Any renders during the skip are also effectively no-ops if these props don’t change. After the skip completes, time updates resume causing time and reportedTime to be in sync again.The UI can stop tracking the current state and instead focus on describing the target state. Our video component always gets the full and latest target state in props each time which means:The UI also doesn’t have to care about order of operations. It can change multiple properties and expect that the they will get applied in the right order, for instance when audio track is changed and playback state is changed from “paused” to “playing”.A fully declarative API isn’t without drawbacks. Property bloat is one to watch out for. Modeling of dynamic attributes can be non-intuitive and can result in additional render cycles caused by an increase in state update frequency (like the reportedTime change event would). Our experience is that these aren’t roadblocks and haven’t stopped us from using our video component even on low-end devices.Creating a declarative facade over an imperative API in the form of a React component makes integration into a UI easier. But this can also mean that the declarative facade might have to account for variance in platform / device level support of features and plan for fallback experiences. If you own the imperative API, consider turning that imperative API into a declarative one.We did exactly that by replacing our video player’s imperative API with a single setTargetState method that takes the same props that our video component does. This has allowed our video component to treat our video player like a react component, passing the props to it and letting it detect and react to changes in the props. It has facilitated better separation of concerns, both between pieces of software and between teams. The video player is in a much better position to decide the actions needed to get from the current state to the desired target state because it has access to the internal state of the player.There are alternatives even if you don’t own the imperative API. For instance you could encapsulate this variance outside React boundary, in a layer between the declarative component and the actual imperative API.As we build new UI concepts for the TV experience we need to solve for tough UI challenges. Our declarative video component has made richer video integrations into our TV UI easier and faster.Do you want to help us invent the future user interface for the living room? Join us if this sparks your curiosity.",https://medium.com/netflix-techblog/integrating-imperative-apis-into-a-react-application-1257e1b45ac6?source=collection_home---4------22---------------------,0,netflix,"react,vue,frontend,javascript,angular",NULL,2017-12-19
Distributing Content to Open Connect,"In previous posts we’ve talked about how we calculate, predict, and use content popularity for Open Connect to maximize the efficiency of our content delivery network and other data science challenges in this space. We also recently talked about improvements we made in the server throughput space.In this blog post, we will dig deeper into how we place content on Open Connect Servers, (also referred to as Open Connect Appliances or OCAs in other literature), including our hashing strategies and how we deal with heterogeneous server clusters. This work is a result of continued collaboration with the Open Connect and Science & Analytics teams at Netflix.Content Placement refers to the decisions we make on a daily basis about which content gets deployed to which servers in a given cluster. (Refer back to our earlier blog for an overview of why these decisions are important.)In general, to maximize traffic from a cluster we should place the most popular content from the catalog onto the cluster. It also makes sense to load balance the popular content over each server in the cluster. A secondary goal is for the allocation to be reasonably stable day-over-day and as stable as possible when servers are added to or removed from a cluster. And finally, this allocation algorithm needs to be reasonable in terms of compute requirements.We use Consistent Hashing to distribute content across multiple servers as follows. Imagine a ring with all numbers from 0 to N (Figure 1). Server IDs S1 to Sn are hashed over this ring. The space in the ring that precedes h(Si) and succeeds the previous hash is said to be owned by Si (Figure 2). Content IDs C1 to Cm are hashed over the same ring. Cj is assigned to the server that owns that part of the ring where h(Cj) lands (Figure 3).In addition, we hash every server ID (S1 to Sn) 1000 times to generate a reasonably equal distribution of content and also to facilitate fair re-hashing when the cluster changes. Using the Uniform Consistent Hashing approach, we assign the same weight to every server. Finally, we find as many owners as we need replicas for a particular piece of content.Using this approach, day-over-day churn is minimized. Content that is added to or removed from the catalog impacts only the server that needs to download or delete this piece of content. When a server is added into a cluster, 1000 new slices are distributed over the ring, where the new server takes over content roughly equally from the other servers. Similarly, when a server is removed from a cluster, its 1000 slices are removed, and it passes on the content ownership roughly equally to the rest of the servers in the cluster.We found that the Uniform Consistent Hashing approach can be sub-optimal in our environment due to the additional layer of complexity that is introduced by our heterogeneous fleet of servers. Our servers fall into one of two general categories — Storage and Flash. These two server types have very different characteristics. Storage servers consist of mostly spinning disks, can hold upwards of 200 TB, and generate ~40 Gbps of throughput. Flash servers (all SSD disks) can generate up to ~100 Gbps but can hold only up to 18 TB of content. For small to medium ISP co-locations, we ship Storage servers only. Our IX and large ISP co-locations consist of a Flash server tier for most traffic and a Storage server tier for storing the entire catalog.Our hardware team builds newer revisions of these servers with ever-increasing capability. For maximum flexibility, we need to enable newer servers to serve alongside older ones without compromising on resource utilization. Additionally, one or more drives on servers can fail. We disable such drives automatically and this leads to disk space differences even among servers with the same hardware type. Overall, these complexities mean that servers in the same cluster have different levels of storage and throughput capacities.Uniform Consistent Hashing works great when servers are homogenous. But it tends to over- or under-utilize resources in the heterogeneous case.Differing Storage: For storage clusters with different disk capacities (for example, four 100 TB servers and one 50 TB server in a single cluster), Uniform Consistent Hashing drops about 1/5th of content from the 250th to 500th TB mark and therefore would create a gap in stored popular content (a “content hole” in our terminology). In certain cases, content holes can lead to the content not being available for streaming.Differing Throughput: In 2016, we built servers that could generate 100 Gbps of throughput with 18 TB drives. Most of our Flash servers in production are 40 Gbps with 12 TB disks. Uniform Consistent Hashing cannot combine these two types of servers into a single cluster, because the traffic attracted to a server would generally be proportional to storage size — 3:2. The target traffic proportions needs to be roughly 5:2.The solution to these issues is a new algorithm we developed called Heterogeneous Cluster Allocation (HCA). The HCA algorithm is used to more intelligently distribute content across heterogeneous servers to make better use of hardware resources.HCA addresses the above cases by altering the allocation protocol. The basic idea is simple — keep the consistent hashing, but use a model to come up with allocation weights when placing content on different individual servers. Weights are effected by changing the number of slices hashed onto the consistent hashing ring on a per-server basis.We have two criteria that need to be satisfied:A simple weighted consistent hashing algorithm — assigning different weights to each machine — could satisfy one or the other constraint, but not both. To satisfy both criteria, we needed to use two different sets of allocation weights — one for popular content, and another for less popular content. The HCA algorithm is a systematic procedure for doing this.The HCA algorithm allocates content in two stages, each with its own weighted consistent hash ring. To configure it, we must specify weights for each server in each stage and the catalog depth D (“cutoff”) where we switch from stage 1 to stage 2. Given the storage and throughput specification of each server, a popularity curve for the cluster’s region, and a candidate cutoff D, we formulate and solve an optimization problem that either yields a set of allocation weights satisfying both criteria above or determines that cutoff D is infeasible (no configuration satisfies the constraints).While it is possible that no HCA configuration exists satisfying both criteria for some cluster and popularity curve combination, we find in practice that there is usually a wide range of cutoffs D that are feasible. For the final HCA configuration, we choose the cutoff D* that induces the least amount of churn for content that crosses the cutoff — for example, if the cutoff is at catalog depth D, and a particular downloadable was below D in the popularity ranking one night and after D the next due to popularity changes, it would be allocated in different rings on consecutive days, and may shuffle to a different server. We choose the cutoff where the probability of its shuffling is smallest.We also need to handle the case that the cluster configuration has changed — for example, when an OCA is added or removed from the cluster. This scenario could also induce churn if the reconfiguration of HCA changes the cutoff D* or the token numbers. To mitigate this, we can scale up or down the token numbers in each zone (only their ratio matters, not the absolute number) to cause the smallest churn between reconfiguration.Using the HCA algorithm to distribute content to Open Connect servers has shown a clear benefit, with content holes substantially reduced and significant improvements in load balance in clusters that are clearly heterogeneous.We are always evaluating and improving our popularity algorithms and storage strategies. If these kinds of large scale challenges sound interesting to you, check out our latest job postings!",https://medium.com/netflix-techblog/distributing-content-to-open-connect-3e3e391d4dc9?source=collection_home---4------23---------------------,0,netflix,"java,python",NULL,2017-12-12
Implementing Japanese Subtitles on Netflix,"Japanese subtitles were first made available on the Netflix service as a part of the Japanese launch in September 2015. This blog post provides a technical description of the work we did leading up to this launch. We cover topics including our specification for source subtitle files, the transformation model from source subtitle files to those deliverable on the Netflix service, the model for delivery of Japanese subtitles on our service as well as the interaction of our work with the upcoming W3C subtitle standard, Timed Text Markup Language 2 (TTML2).Towards the end of 2014, we were working on the technical features for the planned September 2015 launch of Netflix in Japan. At the time, we were mindful that other streaming services that were operating in the Japanese market had received criticism for providing a substandard subtitle experience. Armed with this knowledge, and our desire to maintain the high Netflix standard of quality, we made a decision to implement rigorous support for all of the “essential” Japanese subtitle features — those that would be customary in a premium Japanese video service. This was on top of our following existing subtitle requirements:Through a combination of market research, and advice from Japanese language and media experts, we identified five essential Japanese subtitle features. These features (described below) include rubies, boutens, vertical text, slanted text, and “tate-chu-yoko” (horizontal numbers in vertical text). From our perspective, this prelude was indicative of the complexity of the challenge we had at our hand.Ruby annotations describe the base text that they are associated with. They help explain the meaning of unfamiliar, foreign, or slang words/phrases AND/OR convey pronunciation of kanji characters that are rare and/or unknown. They can help provide cultural context to a translation which allows the viewer to enjoy the content with a deeper understanding. Common practice in subtitling is to display rubies in a font size that is smaller relative to the base text and to place them above the base character for single-line subtitles, and for the first line of two-line subtitles. Rubies are placed below the base character if appearing on the second line of a two-line subtitle. Rubies should never be placed between two lines as it is difficult to discern which base character they should be associated with. Figure 1 shows a ruby example for the dialogue “All he ever amounted to was chitlins.”The base text translates the word “chitlins”*, while the ruby provides transliteration of the word “chitlins” so that viewers can more closely associate the keyword of the dialogue to the translation. As mentioned above, rubies should never be placed in between two lines. Figure 2 shows the proper placement for rubies with two-line subtitles. In the unlikely event that a subtitle spans 3 lines, it is preferable to have the rubies on top of each line, except for the last line where they should be at the bottom.Boutens are dots placed above or below a word or phrase that act as literal points of emphasis, equivalent to the use of italics in English. Boutens can help express implied meanings which provide a richer and more dynamic translation. Figure 3 shows a bouten example for the dialogue: “I need someone to talk to.”This subtitle has boutens above the word for “talk”. In the context of this scene, placing emphasis on this word allows the viewer to understand the implication that the speaker needs someone to provide him/her with privileged information.Vertical subtitles are generally used to avoid overlap with on-screen text present in the video. This is the Japanese equivalent to putting subtitles at the top of the screen. This is illustrated in Figure 4.In Japanese typography, vertical text often includes short runs of horizontal numbers or Latin text. This is referred to as tate-chu-yoko. Instead of stacking the characters vertically, half-width characters are placed side-by-side to enhance legibility and allow more characters to be placed on a single subtitle line. This is illustrated in the Figure 5, for the dialogue, “It’s as if we are still 23 years old”. In this example, subtitle, the number “23” uses half-width numeric characters, and employ the tate-chu-yoko functionality.Slanted text is used in similar fashion as italics/oblique text in other languages — for narration, off-screen dialogue, and forced narratives. One unique feature in Japanese subtitles however is that italics slant is in different directions for horizontal vs. vertical subtitles; furthermore, the angle of the slant is not necessarily constant, but may vary. This is illustrated in Figure 6 and Figure 7.Subtitle assets in the entertainment industry are primarily present in one of two forms — structured text/binary files or rendered images. Netflix has always required the former for its content ingestion system. There are several reasons for this requirement. First, different clients have different subtitle capabilities, requiring us to be able to produce many variations of client assets from a single source. In addition, text subtitle sources are future-proof. That is, as new device capabilities emerge, we can apply those to our large back-catalog of subtitle assets. As an example, when displaying subtitles on an HDR device playing HDR content, it is useful to specify the luminance gain so that white text is not a max-white specular highlight. With text sources, we can easily go back and reprocess to produce subtitles for a client profile that supports luminanceGain. If we had ingested image sources, on the other hand, it would have been difficult to add this sort of functionality to client assets. Further, image assets are opaque while text assets are a lot more amenable for searchability and natural language processing based analysis purposes.With text sources as a “must-have” requirement, we reviewed the available options for Japanese, and Videotron Lambda (also called ‘LambdaCap’ format) was chosen as the only workable model for Japanese subtitles. There were several reasons for this decision. From our analysis, we determined that the LambdaCap format:Although we chose the Videotron Lambda model for the Japanese launch, it did not seem like a great long-term option. It is not a de jure industry standard, and there are some ambiguities in the specification. The LambdaCap format supports the essential Japanese subtitling features very well but lacks in some of the rudimentary features supported in Web Platform standards such as TTML1. Examples of such features include color, font information, and also various layout and composition primitives. In addition, we chose to not use LambdaCap as the delivery model to the playback devices in the Netflix eco-system. Concurrently, the timed text working group (TTWG) was working on the second version of the TTML standard (TTML2). One of the stated objectives of TTML2 was the ability to support global subtitles — Japanese subtitles being a key use case. This became a basis for us to collaborate with TTWG on the TTML2 standardization effort including help complete the specification using our experience as well as the implementations described below. TTML2 became the canonical representation for all source formats in our subtitle processing pipeline.Table 1 summarizes the mapping between the essential Japanese subtitling features described above and the constructs offered by TTML2. It also shows the usage statistics for these features across the Netflix Japanese subtitles catalog as well as the preferred mode for their usage in the Netflix eco-system. The other features not yet used or not used significantly are expected to be used more widely in the future†. The following sections provide details on each feature, in particular regarding the supported values.This styling attribute specifies structural aspects of ruby content including mechanisms that define carriage of ruby base text as well as ruby annotations. The range of values associated with tts:ruby map to corresponding HTML markup elements. As shown in the associated TTML snippet, a ruby “container” markup encompasses both ruby base and annotation text, while ruby “base” and “text” respectively markup base text and annotation text. Figure 8 shows the expected rendering associated with this snippet.This styling attribute specifies positioning of rubies in the block progression dimension relative to the base text. We observed that for the Japanese subtitles use case, tts:rubyPosition=“top” and tts:rubyPosition=“bottom” are less than ideal because they do not provide for unanticipated word wrapping, in which case the second line of text should ideally have rubies below. True to its name, the behavior of tts:rubyPosition=“auto” automatically covers these semantics. This is illustrated in the accompanying TTML snippet. Figure 9 illustrates the expected rendering associated with this snippet. We also note that the behavior of “auto” is currently only specified for exactly two-line events, and will not cover the use case of unanticipated line break on the second line of a two-line event. We believe that that the current behavior described in TTML2 for “outside” is the correct model, and perhaps “auto” could be retired in favor of “outside”.This styling attribute specifies the position of ruby text within the inline area that is generated by the ruby container. Given our experience with Japanese subtitles, prefered value of tts:rubyAlign is “center”.The illustrations below were obtained from the above TTML snippet and they serve to describe the behavior in two cases, when the base text is wider than the ruby text and vice-versa. In both cases, the base text corresponds to ‘の所だ’ (3 Unicode characters) and the ruby alignment value is “center”.Case 1In this case (shown in Figure 10), the rendered width of the ruby text is smaller than that of the base text.Case 2In this case (shown in Figure 11), the width of the ruby text is greater than the width of the base text. We note that in both the cases, the ruby text is centered with respect to the base text.The intent of this feature is to maintain temporal consistency in placement of the base text along the block progression direction as we move from subtitles with only base text to those with base text that is annotated with rubies (and vice versa). We note that this feature can also be used to preserve base text alignment across time when boutens are used.The above TTML snippet results in the rendering shown in Figure 12. We note that with tts:rubyReserve enabled, there is no relative movement of subtitles over time.When tts:rubyReserve is not enabled, the baseline of the base text moves over time resulting in a jarring user experience. This is shown in Figure 13 where there is relative vertical movement between base text from the first subtitle (shown on the left) and the second line of base text from second subtitle (shown on the right).Netflix relies on the use of tts:writingMode to indicate the vertical writing mode. Below is a sample illustration of vertical text (including specific vertical punctuation and symbols), and a TTML snippet as well as the corresponding rendering (see Figure 14) is provided below.As was shown in the above TTML snippet, the indication of ruby markup in vertical writing mode is no different from that in the horizontal writing mode.tts:textCombine is used to realize the “tate-chu-yoko” feature. This is shown in the accompanying TTML snippet. This feature helps increase legibility of subtitles as is shown in the illustration in Figure 15.This is used for rendering boutens. The TTML snippet in the previous section covers this feature also. The adjacent illustration (Figure 16) shows the corresponding rendering.Japanese typography does not have italics fonts. This behavior is realized by performing geometric transformations on glyphs. The common value of tts:fontShear corresponds to a rotation by nearly 15°. As was noted earlier, the direction of slant varies depending upon the writing mode. This is indicated in the Figure 17 and Figure 18 that are based on the following TTML snippet.Having identified the Japanese feature set and selected a source format, we were able to produce a delivery specification for Japanese subtitle assets. Delivering subtitles to clients was the next problem. Though we were ingesting Japanese subtitles in Videotron Lambda format, for reasons described earlier, we did not feel that LambdaCap format was suitable as a client model. In addition, though we prefer delivering text subtitles to our clients, the large installed base of Netflix Ready Devices could not support the complex Japanese subtitle features. Based on these challenges, we decided to use image subtitles for Japan, and move all of the complexity of rendering Japanese subtitles to our transcoding backend.This still left us with the challenge of implementing a Japanese subtitle rendering engine to render the image subtitles. For this project, we engaged with Glenn Adams, a Japanese typography expert, and the editor of the W3C TTML specification. The result of our collaboration is the open source software (OSS) Timed Text Toolkit (TTT) project, funded by Netflix, and developed by Skynav. TTT supports all of the essential Japanese subtitle features and in fact, provides a complete set of tools for validation and rendering of TTML2 files. IMSC1 (Internet Media Subtitles and Captions) is a derivative standard from the W3C TTML family, intended for Internet based subtitles and captions delivery applications. When the IMSC1 specification was in development, we were able to use TTT as a full reference implementation, thus helping to drive it to recommendation status. Likewise, TTT provides a full implementation of TTML2, and thus moves us much closer to the completion of that specification.Using TTT, we implemented a subtitle processing pipeline that converts the source LambdaCap files to a canonical TTML2 representation which was followed by rendering of these subtitles onto images. As shown in Figure 19, the cap2tt module is used to transform a LambdaCap file into a TTML2 document. The TTML2 document is converted into a sequence of images by ttpe. The time sequence of subtitle images is packaged into an archive that also contains timing and positioning information for these images. This archive is delivered to devices in the Netflix eco-system. We also note that these image sets are produced at various target resolutions to cover various screen sizes and device form factors.A text-based delivery system is preferable relative to an image-based system in the longer term for various reasons. These include bandwidth efficiency reasons because of the smaller file sizes corresponding to the former case as well as end-user flexibility in terms of choice of font, font size, as well as choice of color. As devices in the Netflix playback eco-system gain more capabilities in terms of their support for Japanese subtitle features, and as the TTML2 standard gathers traction, we intend to migrate to the text-based delivery model. To that end, we are developing a nimble, device-optimized subtitle rendering engine that offers first class support for various TTML2 features.— by Rohit Puri, Cyril Concolato, David Ronca and Yumi Deeter(*) Slang word used in “House of Cards”, also known as “Chitterlings”, southern US food usually made from the small intestines of a pig. Such word is unlikely to be known to the audience.(†) Other TTML2 Japanese features (such as tts:rubyOverhang) not listed in the table are not necessary for the support of Japanese features in the Netflix use-case.",https://medium.com/netflix-techblog/implementing-japanese-subtitles-on-netflix-c165fbe61989?source=collection_home---4------0---------------------,0,netflix,"css,react,frontend,javascript,python",NULL,2017-12-11
Artwork Personalization at Netflix,"By Ashok Chandrashekar, Fernando Amat, Justin Basilico and Tony JebaraFor many years, the main goal of the Netflix personalized recommendation system has been to get the right titles in front each of our members at the right time. With a catalog spanning thousands of titles and a diverse member base spanning over a hundred million accounts, recommending the titles that are just right for each member is crucial. But the job of recommendation does not end there. Why should you care about any particular title we recommend? What can we say about a new and unfamiliar title that will pique your interest? How do we convince you that a title is worth watching? Answering these questions is critical in helping our members discover great content, especially for unfamiliar titles. One avenue to address this challenge is to consider the artwork or imagery we use to portray the titles. If the artwork representing a title captures something compelling to you, then it acts as a gateway into that title and gives you some visual “evidence” for why the title might be good for you. The artwork may highlight an actor that you recognize, capture an exciting moment like a car chase, or contain a dramatic scene that conveys the essence of a movie or TV show. If we present that perfect image on your homepage (and as they say: an image is worth a thousand words), then maybe, just maybe, you will give it a try. This is yet another way Netflix differs from traditional media offerings: we don’t have one product but over a 100 million different products with one for each of our members with personalized recommendations and personalized visuals.In previous work, we discussed an effort to find the single perfect artwork for each title across all our members. Through multi-armed bandit algorithms, we hunted for the best artwork for a title, say Stranger Things, that would earn the most plays from the largest fraction of our members. However, given the enormous diversity in taste and preferences, wouldn’t it be better if we could find the best artwork for each of our members to highlight the aspects of a title that are specifically relevant to them?As inspiration, let us explore scenarios where personalization of artwork would be meaningful. Consider the following examples where different members have different viewing histories. On the left are three titles a member watched in the past. To the right of the arrow is the artwork that a member would get for a particular movie that we recommend for them.Let us consider trying to personalize the image we use to depict the movie Good Will Hunting. Here we might personalize this decision based on how much a member prefers different genres and themes. Someone who has watched many romantic movies may be interested in Good Will Hunting if we show the artwork containing Matt Damon and Minnie Driver, whereas, a member who has watched many comedies might be drawn to the movie if we use the artwork containing Robin Williams, a well-known comedian.In another scenario, let’s imagine how the different preferences for cast members might influence the personalization of the artwork for the movie Pulp Fiction. A member who watches many movies featuring Uma Thurman would likely respond positively to the artwork for Pulp Fiction that contains Uma. Meanwhile, a fan of John Travolta may be more interested in watching Pulp Fiction if the artwork features John.Of course, not all the scenarios for personalizing artwork are this clear and obvious. So we don’t enumerate such hand-derived rules but instead rely on the data to tell us what signals to use. Overall, by personalizing artwork we help each title put its best foot forward for every member and thus improve our member experience.At Netflix, we embrace personalization and algorithmically adapt many aspects of our member experience, including the rows we select for the homepage, the titles we select for those rows, the galleries we display, the messages we send, and so forth. Each new aspect that we personalize has unique challenges; personalizing the artwork we display is no exception and presents different personalization challenges. One challenge of image personalization is that we can only select a single piece of artwork to represent each title in each place we present it. In contrast, typical recommendation settings let us present multiple selections to a member where we can subsequently learn about their preferences from the item a member selects. This means that image selection is a chicken-and-egg problem operating in a closed loop: if a member plays a title it can only come from the image that we decided to present to that member. What we seek to understand is when presenting a specific piece of artwork for a title influenced a member to play (or not to play) a title and when a member would have played a title (or not) regardless of which image we presented. Therefore artwork personalization sits on top of the traditional recommendation problem and the algorithms need to work in conjunction with each other. Of course, to properly learn how to personalize artwork we need to collect a lot of data to find signals that indicate when one piece of artwork is significantly better for a member.Another challenge is to understand the impact of changing artwork that we show a member for a title between sessions. Does changing artwork reduce recognizability of the title and make it difficult to visually locate the title again, for example if the member thought was interested before but had not yet watched it? Or, does changing the artwork itself lead the member to reconsider it due to an improved selection? Clearly, if we find better artwork to present to a member we should probably use it; but continuous changes can also confuse people. Changing images also introduces an attribution problem as it becomes unclear which image led a member to be interested in a title.Next, there is the challenge of understanding how artwork performs in relation to other artwork we select in the same page or session. Maybe a bold close-up of the main character works for a title on a page because it stands out compared to the other artwork. But if every title had a similar image then the page as a whole may not seem as compelling. Looking at each piece of artwork in isolation may not be enough and we need to think about how to select a diverse set of images across titles on a page and across a session. Beyond the artwork for other titles, the effectiveness of the artwork for a title may depend on what other types of evidence and assets (e.g. synopses, trailers, etc.) we also display for that title. Thus, we may need a diverse selection where each can highlight complementary aspects of a title that may be compelling to a member.To achieve effective personalization, we also need a good pool of artwork for each title. This means that we need several assets where each is engaging, informative and representative of a title to avoid “clickbait”. The set of images for a title also needs to be diverse enough to cover a wide potential audience interested in different aspects of the content. After all, how engaging and informative a piece of artwork is truly depends on the individual seeing it. Therefore, we need to have artwork that highlights not only different themes in a title but also different aesthetics. Our teams of artists and designers strive to create images that are diverse across many dimensions. They also take into consideration the personalization algorithms which will select the images during their creative process for generating artwork.Finally, there are engineering challenges to personalize artwork at scale. One challenge is that our member experience is very visual and thus contains a lot of imagery. So using personalized selection for each asset means handling a peak of over 20 million requests per second with low latency. Such a system must be robust: failing to properly render the artwork in our UI brings a significantly degrades the experience. Our personalization algorithm also needs to respond quickly when a title launches, which means rapidly learning to personalize in a cold-start situation. Then, after launch, the algorithm must continuously adapt as the effectiveness of artwork may change over time as both the title evolves through its life cycle and member tastes evolve.Much of the Netflix recommendation engine is powered by machine learning algorithms. Traditionally, we collect a batch of data on how our members use the service. Then we run a new machine learning algorithm on this batch of data. Next we test this new algorithm against the current production system through an A/B test. An A/B test helps us see if the new algorithm is better than our current production system by trying it out on a random subset of members. Members in group A get the current production experience while members in group B get the new algorithm. If members in group B have higher engagement with Netflix, then we roll-out the new algorithm to the entire member population. Unfortunately, this batch approach incurs regret: many members over a long period of time did not benefit from the better experience. This is illustrated in the figure below.To reduce this regret, we move away from batch machine learning and consider online machine learning. For artwork personalization, the specific online learning framework we use is contextual bandits. Rather than waiting to collect a full batch of data, waiting to learn a model, and then waiting for an A/B test to conclude, contextual bandits rapidly figure out the optimal personalized artwork selection for a title for each member and context. Briefly, contextual bandits are a class of online learning algorithms that trade off the cost of gathering training data required for learning an unbiased model on an ongoing basis with the benefits of applying the learned model to each member context. In our previous unpersonalized image selection work, we used non-contextual bandits where we found the winning image regardless of the context. For personalization, the member is the context as we expect different members to respond differently to the images.A key property of contextual bandits is that they are designed to minimize regret. At a high level, the training data for a contextual bandit is obtained through the injection of controlled randomization in the learned model’s predictions. The randomization schemes can vary in complexity from simple epsilon-greedy formulations with uniform randomness to closed loop schemes that adaptively vary the degree of randomization as a function of model uncertainty. We broadly refer to this process as data exploration. The number of candidate artworks that are available for a title along with the size of the overall population for which the system will be deployed informs the choice of the data exploration strategy. With such exploration, we need to log information about the randomization for each artwork selection. This logging allows us to correct for skewed selection propensities and thereby perform offline model evaluation in an unbiased fashion, as described later.Exploration in contextual bandits typically has a cost (or regret) due to the fact that our artwork selection in a member session may not use the predicted best image for that session. What impact does this randomization have on the member experience (and consequently on our metrics)? With over a hundred millions members, the regret incurred by exploration is typically very small and is amortized across our large member base with each member implicitly helping provide feedback on artwork for a small portion of the catalog. This makes the cost of exploration per member negligible, which is an important consideration when choosing contextual bandits to drive a key aspect of our member experience. Randomization and exploration with contextual bandits would be less suitable if the cost of exploration were high.Under our online exploration scheme, we obtain a training dataset that records, for each (member, title, image) tuple, whether that selection resulted in a play of the title or not. Furthermore, we can control the exploration such that artwork selections do not change too often. This gives a cleaner attribution of the member’s engagement to specific artwork. We also carefully determine the label for each observation by looking at the quality of engagement to avoid learning a model that recommends “clickbait” images: ones that entice a member to start playing but ultimately result in low-quality engagement.In this online learning setting, we train our contextual bandit model to select the best artwork for each member based on their context. We typically have up to a few dozen candidate artwork images per title. To learn the selection model, we can consider a simplification of the problem by ranking images for a member independently across titles. Even with this simplification we can still learn member image preferences across titles because, for every image candidate, we have some members who were presented with it and engaged with the title and some members who were presented with it and did not engage. These preferences can be modeled to predict for each (member, title, image) tuple, the probability that the member will enjoy a quality engagement. These can be supervised learning models or contextual bandit counterparts with Thompson Sampling, LinUCB, or Bayesian methods that intelligently balance making the best prediction with data exploration.In contextual bandits, the context is usually represented as an feature vector provided as input to the model. There are many signals we can use as features for this problem. In particular, we can consider many attributes of the member: the titles they’ve played, the genre of the titles, interactions of the member with the specific title, their country, their language preferences, the device that the member is using, the time of day and the day of week. Since our algorithm selects images in conjunction with our personalized recommendation engine, we can also use signals regarding what our various recommendation algorithms think of the title, irrespective of what image is used to represent it.An important consideration is that some images are naturally better than others in the candidate pool. We observe the overall take rates for all the images in our data exploration, which is simply the number of quality plays divided by the number of impressions. Our previous work on unpersonalized artwork selection used overall differences in take rates to determine the single best image to select for a whole population. In our new contextual personalized model, the overall take rates are still important and personalization still recovers selections that agree on average with the unpersonalized model’s ranking.The optimal assignment of image artwork to a member is a selection problem to find the best candidate image from a title’s pool of available images. Once the model is trained as above, we use it to rank the images for each context. The model predicts the probability of play for a given image in a given a member context. We sort a candidate set of images by these probabilities and pick the one with the highest probability. That is the image we present to that particular member.To evaluate our contextual bandit algorithms prior to deploying them online on real members, we can use an offline technique known as replay [1]. This method allows us to answer counterfactual questions based on the logged exploration data (Figure 1). In other words, we can compare offline what would have happened in historical sessions under different scenarios if we had used different algorithms in an unbiased way.Replay allows us to see how members would have engaged with our titles if we had hypothetically presented images that were selected through a new algorithm rather than the algorithm used in production. For images, we are interested in several metrics, particularly the take fraction, as described above. Figure 2 shows how contextual bandit approach helps increase the average take fraction across the catalog compared to random selection or non-contextual bandits.After experimenting with many different models offline and finding ones that had a substantial increase in replay, we ultimately ran an A/B test to compare the most promising personalized contextual bandits against unpersonalized bandits. As we suspected, the personalization worked and generated a significant lift in our core metrics. We also saw a reasonable correlation between what we measured offline in replay and what we saw online with the models. The online results also produced some interesting insights. For example, the improvement of personalization was larger in cases where the member had no prior interaction with the title. This makes sense because we would expect that the artwork would be more important to someone when a title is less familiar.With this approach, we’ve taken our first steps in personalizing the selection of artwork for our recommendations and across our service. This has resulted in a meaningful improvement in how our members discover new content… so we’ve rolled it out to everyone! This project is the first instance of personalizing not just what we recommend but also how we recommend to our members. But there are many opportunities to expand and improve this initial approach. These opportunities include developing algorithms to handle cold-start by personalizing new images and new titles as quickly as possible, for example by using techniques from computer vision. Another opportunity is extending this personalization approach across other types of artwork we use and other evidence that describe our titles such as synopses, metadata, and trailers. There is also an even broader problem: helping artists and designers figure out what new imagery we should add to the set to make a title even more compelling and personalizable.If these types of challenges interest you, please let us know! We are always looking for great people to join our team, and, for these types of projects, we are especially excited by candidates with machine learning and/or computer vision expertise.[1] L. Li, W. Chu, J. Langford, and X. Wang, “Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms,” in Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, New York, NY, USA, 2011, pp. 297–306.",https://medium.com/netflix-techblog/artwork-personalization-c589f074ad76?source=collection_home---4------1---------------------,0,netflix,,NULL,2017-12-07
Innovating Faster on Personalization Algorithms at Netflix Using Interleaving,"By Juliette Aurisset, Michael Ramm, Joshua ParksThe Netflix experience is powered by a family of ranking algorithms, each optimized for a different purpose. For instance, the Top Picks row on the homepage makes recommendations based on a personalized ranking of videos, and the Trending Now row also incorporates recent popularity trends. These algorithms, along with many others, are used together to construct personalized homepages for over 100 million members.At Netflix, we strive to continually improve our recommendations. The development process begins with the creation of new ranking algorithms and the evaluation of their performance offline. We then leverage A/B testing to conduct online measurements of core evaluation metrics that align closely with our business objective of maximizing member satisfaction. Such metrics include month-to-month subscription retention and member streaming hours. As the ranking algorithms and the overall Netflix product become optimized, discerning wins in these metrics requires increasingly large sample sizes and long experiment durations.To accelerate the pace of algorithm innovation, we have devised a two-stage online experimentation process. The first stage is a fast pruning step in which we identify the most promising ranking algorithms from a large initial set of ideas. The second stage is a traditional A/B test on the pared-down set of algorithms to measure their impact on longer-term member behavior. In this blog post, we focus on our approach to the first stage: an interleaving technique that unlocks our ability to more precisely measure member preferences.Increasing the rate of learning by testing a broad set of ideas quickly is a major driver of algorithm innovation. We have expanded the number of new algorithms that can be tested by introducing an initial pruning stage of online experimentation that satisfies two properties:We have achieved the above using an interleaving technique (cf. Chapelle et al.) that dramatically speeds up our experimentation process (see Fig. 2). The first stage finishes in a matter of days, leaving us with a small group of the most promising ranking algorithms. The second stage uses only these select algorithms, which allows us to assign fewer members to the overall experiment and to reduce the total experiment duration compared to a traditional A/B test.To develop intuition around the sensitivity gain that interleaving offers, let’s consider an experiment to determine whether Coke or Pepsi is preferred within a population. If we use traditional A/B testing, we might randomly split the population into two groups and perform a blind trial. One group would be offered only Coke, and the second group would be offered only Pepsi (with neither drink having identifiable labels). At the conclusion of the experiment, we could determine whether there is a preference for Coke or Pepsi by measuring the difference in soda consumption between the two groups, along with the extent of uncertainty in this measurement, which can tell us if there is a statistically significant difference.While this approach works, there may be opportunities for refining the measurement. First, there is a major source of measurement uncertainty: the wide variation in soda consumption habits within the population, ranging from those who hardly consume any soda to those who consume copious amounts. Second, heavy soda consumers may represent a small percentage of the population, but they could account for a large percentage of overall soda consumption. Therefore, even a small imbalance in heavy soda consumers between the two groups may have a disproportionate impact on our conclusions. When running online experiments, consumer internet products often face similar issues related to their most active users, whether it is in measuring a change to a metric like streaming hours at Netflix or perhaps messages sent or photos shared in a social app.As an alternative to traditional A/B testing, we can use a repeated measures design for measuring preference for Coke or Pepsi. In this approach, the population would not be randomly split. Rather, each person would have the option of either Coke or Pepsi (with neither brand having identifiable labels but yet still being visually distinguishable). At the conclusion of the experiment, we could compare, at the level of a person, the fraction of soda consumption for Coke or Pepsi. In this design, 1) we remove the uncertainty contributed by the wide range in population-level soda-consumption habits, and 2) by giving every person equal weight, we reduce the possibility that the measurement is materially affected by an imbalance in heavy soda consumers.At Netflix, we use interleaving in the first stage of experimentation to sensitively determine member preference between two ranking algorithms. The figure below depicts the differences between A/B testing and interleaving. In traditional A/B testing, we choose two groups of subscribers: one to be exposed to ranking algorithm A and another to B. In interleaving, we select a single set of subscribers who are exposed to an interleaved ranking generated by blending the rankings of algorithms A and B. This allows us to present choices side-by-side to the user to determine their preference of ranking algorithms. (Members are not able to distinguish between which algorithm recommended a particular video.) We calculate the relative preference for a ranking algorithm by comparing the share of hours viewed, with attribution based on which ranking algorithm recommended the video.When generating an interleaved set of videos from two ranking algorithms A and B for a row on the Netflix homepage, we have to consider the presence of position bias: the probability of a member playing a video decreases as we go from left to right. For interleaving to yield valid measurements, we must ensure that at any given position in a row, a video is equally likely to have come from ranking algorithm A or B.To address this, we have been using a variant of team draft interleaving, which mimics the process of how team selection occurs for a friendly sports match. In this process, two team captains toss a coin to determine who picks first. They then alternate picks, with each captain selecting the player who is highest on their preference list and is still available. This process continues until team selection is complete. Applying this analogy to interleaving for Netflix recommendations, the videos represent the available players and ranking algorithms A and B represent the ordered preferences of the two team captains. We randomly determine which ranking algorithm contributes the first video to the interleaved list. The ranking algorithms then alternate, with each algorithm contributing their highest-ranked video that is still available (see Fig. 4). The member preference for ranking algorithm A or B is determined by measuring which algorithm produced the greater share of hours viewed within the interleaved row, with views attributed to the ranker that contributed the video.The first requirement that we laid out for using interleaving in a two-stage online experimentation process was that it needs to reliably identify the better ranking algorithm with a considerably smaller sample size. To evaluate how well interleaving satisfies this requirement, we turned to a case in which two ranking algorithms A and B were of known relative quality: ranker B is better than ranker A. We then ran an interleaving experiment in parallel with an A/B test using these 2 rankers.To compare the sensitivity of interleaving vs. A/B testing, we computed both the interleaving preference and A/B metrics at various sample sizes using bootstrap subsampling. In performing the bootstrap analysis, we either simulated assigning N users to the interleaving cell or N/2 users to each cell of the traditional A/B experiment. If we were to randomly guess which ranker is better, the probability of disagreeing with the true preference would be 50%. When this probability is 5%, we are achieving 95% power to detect the difference in ranker quality. Therefore, a metric that crosses this threshold with a fewer number of subscribers is the more sensitive one.Figure 5 shows the results from our analysis. We compare the interleaving preference with two metrics typically used in the A/B setting: overall streaming and an algo-specific engagement metric. The sensitivity of metrics used to evaluate A/B tests can vary over a wide range. We find that interleaving is very sensitive: it requires >100× fewer users than our most sensitive A/B metric to achieve 95% power.Our second requirement was that the metrics measured in the interleaving stage need to be aligned with our traditional A/B test metrics. We now evaluate whether the interleaving preference is predictive of a ranker’s performance in the subsequent A/B test.The figure below shows the change in the interleaving preference metric versus the change in the A/B metric compared to control. Each data point represents a ranking algorithm that is evaluated against the production ranker, which serves as control. We find that there is a very strong correlation and alignment between the interleaving metric and our most sensitive A/B evaluation metric, giving us confidence that the interleaving preference is predictive of success in a traditional A/B experiment.Interleaving is a powerful technique that has enabled us to accelerate ranking algorithm innovation at Netflix. It allows us to sensitively measure member preference for ranking algorithms and to identify the most promising candidates within days. This has enabled us to quickly test a broad set of new algorithms, and thus increase our rate of learning.While interleaving provides an enormous boost in sensitivity and aligns well with A/B metrics, it does have limitations. First, implementing an interleaving framework can be fairly involved, which presents challenges from an engineering perspective. The presence of business logic can furthermore interfere, which requires building scalable solutions for consistency checks and automated detection of issues. Second, while interleaving enables quick identification of the best ranking algorithms, a limitation is that it is a relative measurement of user preference for a ranking algorithm. That is, it does not allow us to directly measure changes to metrics such as retention.We address the latter limitation by running an A/B experiment in a second phase, where our initial set of ideas has been pruned to the best candidates. This gives us the option to power up the experiment by increasing the sample size per cell, which enables us to perform careful measurements of longer-term member behavior. Addressing these challenges and developing better measurements are aspects that we are continuing to explore.If the work described here sounds exciting to you, please take a look at the jobs page. We are always looking for talented data scientists and researchers to join our team and help innovate on experimentation methods at Netflix. Your work will help shape the product experience for the next 100M members worldwide!",https://medium.com/netflix-techblog/interleaving-in-online-experiments-at-netflix-a04ee392ec55?source=collection_home---4------2---------------------,0,netflix,"java,python",NULL,2017-11-29
Updates on Netflix’s Container Management Platform,"We continue to share lessons learned from scheduling and executing containers in production at scale. This blog posts summarizes not only recent publications on our container management platform (Project Titus) but also future collaboration opportunities.PublicationsWe were honored when the Association for Computing Machinery (ACM) asked for us to write for their bimonthly ACM Queue publication, and recently published our article entitled “Titus: Introducing Containers to the Netflix Cloud”. In the article, we talk about why we designed Titus in the manner we did, and why we chose to introduce containers into our existing cloud native virtual machine architecture for service and batch workloads. We talk about unique aspects of Titus such as AWS and Netflix infrastructure integration, networking, capacity management and approaches to operational challenges. And finally, we share our future plans and expectations for container management at Netflix.ConferencesLast year at re:Invent 2016, we documented how Titus works under the covers. At Mesoscon 2017, we covered how we schedule efficiently on an elastic cloud. During our talk at QCon NYC 2017, we talked about the challenges we have seen operating Titus for production workloads over the last two years. We believe operational issues and scheduling efficiency are key issues to understand regardless of container platform.Open SourceWhile there is some benefit in socializing what we have built and learned operating Titus for two years, we know we can do better. We have heard requests from the community to open source Titus, letting people learn from the exact code we run in production at Netflix.Open sourcing a project requires great amount of work and responsibility, especially for projects as complex as Titus. Also, a healthy open source project requires more than a single company to grow a lasting community. In that spirit, Netflix has started to collaborate with others who have similar challenges as Netflix when running containers. We have found three categories of collaborators that are looking for unique values from Titus. Specifically, those who are looking for battle hardened:We are currently working under a private collaboration model which includes sharing our code privately as we work towards a community driven fully open source project.Upcoming ConferencesAt re:Invent this year, we will be talking at the “NET402: Elastic Load Balancing Deep Dive and Best Practices” session about how we are extending our networking support using Application Load Balancer (ALB) integrations with IP Target Groups. We will also be in attendance at QCon SF 2017 and KubeCon/CloudNativeCon and hope to connect with collaborators.If you are interested in getting in contact with the Titus team or are attending either QCon SF 2017 or KubeCon/CloudNativeCon, please touch base with Andrew Spyker (linkedin, twitter).",https://medium.com/netflix-techblog/updates-on-netflixs-container-management-platform-a91738360bd8?source=collection_home---4------4---------------------,0,netflix,,NULL,2017-11-14
Machine Learning Platform meetup,"By Faisal SiddiqiMachine Learning is making fast inroads into many areas of business and is being employed in an increasingly widening array of commercial applications. At Netflix, ML has been used for several years on our Recommendations and Personalization problems. Over the last couple of years, ML has expanded into a wide range of newer applications here, such as Content Promotion, Content Price Modeling, Programmatic Marketing, Efficient Content Delivery, Adaptive Streaming QoE, to name only a handful. Scaling and speeding up the application of ML is at the top of mind for the multitudes of researchers, engineers, and data scientists working on these salient problems. We are always looking to learn from academic research and industry application in large scale contexts and love to share what we have learnt as part of running the Netflix use cases at scale.Last week we hosted a Machine Learning Platform meetup at Netflix HQ in Los Gatos. We had five talks from top practitioners in the industry and in this post we will provide a brief overview summarizing them. The talks from Google, Twitter, Uber, Facebook and Netflix largely fell into one or both of these themes:Ed Chi kicked off the talks with a presentation on how to learn user and item representation for sparse data in recommender systems. His talk covered two major areas of emphasis — Focused Learning and Factorized Deep Retrieval.In setting up the motivation for the first area, Ed talked of the Tyranny of the Majority in describing the outsized impact of dense sub-zones in otherwise sparse data sets. As as solution to this challenge, he talked about Focused Learning, a sort of divide-and-conquer approach, where you look for subsets of data to focus on and then figure out which models can learn on each subset with differing data density. As an example of this approach, he talked about how using focused and unfocused regularization factors in the cost function, allowed them to model the problem with more success. In the canonical example of movie recommendations, he demonstrated how this approach led to outsized improvements in model prediction quality for sparse data sets (e.g. where Documentaries is one of the most sparse categories).The use case that Factorized Deep Retrieval addresses is about predicting good co-watch patterns of videos when the corpus of items to recommend is huge (YouTube). You want the recommendations to not ignore the fresh content while still ensuring relevance. He presented TensorFlow’s distributed implementation of WALS (weighted alternating least squares) Factorization as the solution they used for picking sparse but relevant candidates out of the huge candidate corpus. Online rankings for serving were staged into a tiered approach with a first pass nominator selecting a reasonably small set. Then subsequent rankers further refined the selection until a small set of highly relevant candidates were chosen and impressed to the user.Ed also talked about several implementation challenges, in sharing, failures, versioning, and coordination, that the TensorFlow team encountered and addressed in the TFX (TensorFlow Extended) system internally in use at Google. They are looking to add some of these features to TF Serving open-source in the future.Next up was Joe Xie from Twitter who started off by setting the stage with the real time use cases of Twitter. He talked about their single/merged online training and prediction pipeline as a backdrop to the parameter server approach his team took to scaling training and serving. Joe talked about how they tackled one area to scale up at a time. He walked through three stages of their parameter server, starting with first decoupling the training and prediction requests, then focusing on scaling up the training traffic and finally scaling up the model size.They were able to increase the prediction queries/sec by 10x by separating training and prediction requests in v1 and increase training set size by 20x in v2. They are working on accepting models 10x larger in size in v3, and looking to explore data-parallel optimizations and feature sharding across their v3 distributed architecture. When asked about whether trainers and predictors are also sharded, Joe mentioned that for now they have sharded the trainers but not the predictors.Alex Sergeev from Uber introduced a new distributed training library they have recently open-sourced — Horovod, for making distributed TensorFlow workloads easier. Alex started off by setting the problem of scaling up model training. With ever increasing data sizes and the need for faster training, they explored distributed TensorFlow. Alex alluded to challenges in using the distributed TensorFlow package out of the box. At some level they felt lack of clarity around which knobs to use. As well, they weren’t quite able to do justice to their GPU utilization when training data at scale via TensorFlow.They explored recent approaches to leveraging data-parallelism in the distributed training space. Motivated by some past work at Baidu and more recently at Facebook (see below), they took a different approach to their deep network training. Conceptually, the training set was split up in chunks of data each of which was processed in parallel by executors, computing gradients. In each pass the gradients are averaged and then fed back into the model.Typically, in TensorFlow the workers compute the gradients and then send them to the Parameter Server(s) for averaging. Horovod utilizes a data-parallel “ring-allreduce” algorithm that obviates the need to have a parameter server. Each of the workers are involved in not only computing the gradients but also averaging them, communicating in a peer-to-peer fashion. Horovod uses NVDIA’s NCCL2 library to optimize bandwidth of communication between workers.Alex talked about how they were able to utilize this for both single GPU as well as multiple GPU cases. He also talked about Tensor Fusion, an approach to smart batching that gave them larger gains on less optimized networks.When asked about distributing the data across GPUs, interestingly, Alex mentioned that in their benchmarks they have seen better scale performance with 8 machines with a single GPU versus a single machine with 8 GPUs.Continuing the thread of GPU optimization for large scale training, Aapo Kyrola from Facebook shared some insights on their experiences working on this problem. As a matter of fact, Aapo described in more detail the case study that was referenced by the Uber presentation on Horovod just prior to this talk.Aapo started off by giving a quick overview of Caffe2, a lightweight ML framework, highlighting how training can be modeled as a directed graph problem. He compared synchronous SGD (stochastic gradient descent) with asynchronous SGD and then proceeded to describe how they pushed the boundaries on sync SGD on Caffe2 with GLOO. The latter is an open-sourced fast library for performing distributed reductions and leverages NVIDIA’s NCCL2 for inter-GPU reductions.This all led to the case study of their recently published milestone achievement — they were able to train a ResNet-50 (a 50 layer residual convolutional net) on the ImageNet-1K dataset in less than 1 hour. Sync SGD was used with a data-parallel approach and the “all-reduce” algorithm from GLOO. The case study described how they were able to tweak the learning rate piece-wise with an 8K mini-batch size to get to near state-of-the-art on error rate metrics.Aapo ended the talk on some lessons learned through this exercise, like how Sync SGD can be pushed quite far on modern GPUs, and how learning rate is an important hyper-parameter for mini-batch size.Netflix’s Benoit Rostykus talked about VectorFlow, a recently open-sourced library for handling sparse data. VectorFlow is designed to be a lightweight neural network library for not-so-deep networks and sparse data.In many ways Benoit’s talk presented a bit of a counter narrative to the rest of the talks. He argued that while there are a lot of applications in ML which are better served by throwing more training data and more layers (eg, images in convolutional nets), there is a surprising number of applications on the other end of the spectrum too. Such applications, like real-time bidding, can be handled in a single machine context, often need simple feed-forward shallow nets, and performance on CPUs can be pushed hard enough to meet the training latency and cost budget.VectorFlow was designed for these contexts and this philosophy informed several design decisions. For example the choice of the language, D, was one such decision. The modern system language was chosen to address the goal of a single language in adhoc as well as production contexts — providing the power of C++ and the simplicity/clarity of Python.Another notable differentiator for VectorFlow is its focus on optimizing for latency instead of throughput. By avoiding large data transfers between RAM and CPU/GPU memory, and avoiding allocations during forward/back-prop, VectorFlow is able to speed things up considerably during training. Benoit was able to demonstrate that for a training set size of 33M rows with a data sparsity of 7%, VectorFlow was able to run each pass of SGD in 4.2 seconds on a single machine with 16 commodity CPU cores.VectorFlow is used in real-time bidding, programmatic advertising, causal inference, and survival regression to name a few applications in Netflix.Benoit scoped out some of the future work on VectorFlow’s roadmap which includes deeper sparsity support, more complex nodes, and more optimizers, without giving up the core mantra behind VectorFlow that Simple > Complex. VectorFlow is open-sourced and available on github.This meetup featured high quality talks and active participation by the audience. It was followed by engaging informal conversations after the talks around the direction of ML in general and deep learning in particular. At Netflix, we are solving many such challenging problems in ML research, engineering and infrastructure at scale as we grow to meet the needs of the next 100 million Netflix members. Join us if you are interested in pushing the state of the art forward.",https://medium.com/netflix-techblog/machine-learning-platform-meetup-ddec090f3c17?source=collection_home---4------6---------------------,0,netflix,"react,frontend,python",NULL,2017-10-18
Developer Experience Lessons Operating a Serverless-like Platform At Netflix — Part II,"By Ludovic Galibert, Vasanth Asokan and Sangeeta NarayananIn Part 1 of this series, we outlined key learnings the Edge Developer Experience team gained from operating the API dynamic scripting platform which provides a serverless or FaaS like experience for client application developers. We addressed the concerns around getting code ready for production deployment. Here, we look at what it takes to deploy it safely and operate it on an ongoing basis.Simplicity and abstraction of operations come with reduced control; have you evaluated the tradeoffs?When a script is deployed to our platform, the rollout is completed within a fixed time interval. This predictability is useful, especially for automated workflows that involve a dependency on the script being fully rolled out; an example being mobile app rollouts gated by the deployment of a script with new functionality. Additionally, we implemented a realtime notification system that allows developers to progressively monitor the state of their deployment. This enables further optimization of such workflows.Given the dynamic nature of serverless scheduling, application instances are more vulnerable to cold start delays caused by JIT-ing or connection priming. This unpredictable latency jitter is typically not an issue if there is a steady state of requests coming in from clients, but is more pronounced in the case of applications that do not receive enough traffic to keep the application instances “warm”. New application deployments are also susceptible to this issue. In order to mitigate this, we designed a feature whereby scripts could be shipped a set of “warmup” handlers as part of the image. Right after instance provisioning, the platform executes these warmup handlers in order to ensure JIT-ing and connection priming can be performed for covered code paths before an instance starts processing requests. Typical monolithic application units are in control of the provisioning lifecycle, and thus contain infrastructure to perform such startup tuning. It is important to ensure that off the shelf serverless platforms provide hooks to retain this facility.Another provisioning consideration is multi-region (datacenter) deployments. Initial versions of our platform only supported instant global deployments. While this simplified the experience for our users by abstracting the notion of regions from them, it deprived them of the ability to test their deployment in stages across regions. More importantly it was a potential availability risk if a bad deployment was rolled out globally. In the end, we evolved our platform to support both global and regional deployments. Users have the option to choose their deployment schedules by region.As a final gate before production, canary deployments and multi-variate testing are key techniques to gain confidence at scale and reduce the risk associated with a new deployment. These capabilities are built into our deployment and routing layers. Users can deploy a reduced size baseline (current production code) and canary (new pre-release code) version of the application, and metrics (such as cpu load and latency) flowing from each are tagged correspondingly according to function. This allows a comparison of application behavior between versions prior to full rollout.To sum up, all the techniques and best practices that help reduce risk of deployments of services are equally applicable to and necessary for script deployments and serverless functions. Also, keep in mind that the simplicity of a serverless experience comes at the cost of control over the application container and its scheduling. For use cases requiring precise control over when the application is ready to take traffic, this could be an important consideration.Smaller, lightweight application units are more vulnerable to system noise; how do you smoothen out the jitter?Our website team redesigned the Netflix website experience in 2015. As part of a modular design, they chose to break down the data access scripts they deployed to our scripting platform into fine grained units which can be thought of as “nano-services”. Over time, this led to an order of magnitude increase in the number of scripts they run.Through the lifecycle of this exercise, we observed a few interesting things:Based on the above experience, we believe that in order to reliably operate applications composed of smaller units, the core concept of increased abstraction should be extended to operational insight and workflows well beyond today’s levels. Here are some ideas influencing our next generation platform that we think would be beneficial more broadly.Taken together, these innovations help provide a more holistic view and control plane, powered by automation. It is important to note that these considerations are not unique to serverless — they just get amplified. Overall, the key is to allow developers to outsource more of the operations to tools with confidence.Large applications now become numerous smaller ones. What are the implications of this increase in dimensionality?As described earlier, breaking up applications into nano-services implies an increase in the number of application units. A compounding factor is the increase in deployed versions that have to be maintained indefinitely as is often the case with consumer facing applications. The combination of these two factors could mean a drastic increase in independent deployments. A prominent example of this is the Android fragmentation, which results in the necessity to maintain multiple application versions in order to run on old devices that cannot upgrade to newer versions of Android. Google is now trying to address the fragmentation problem at the core of Android.So what about maintenance? In Part 1 we mentioned frequent developer commits coupled with CI/CD runs in pre-production environments resulting in a long trail of short-lived deployments. These unused deployments which consume resources come with a maintenance overhead and result in unnecessary cost. Our first attempt to address this problem was to provide accurate and actionable usage reports, as well as self-protecting limits and switches in the platform against accidental over-subscription and abuse. We soon realized that manual clean up was not only tedious, but also error prone — versions that were still taking meaningful traffic were sometimes accidentally removed. This presented an automation opportunity for hands off application lifecycle management. Developers are asked to specify upfront when a particular version can be safely sunset, based on traffic falling below a threshold for a minimum number of days. Versions that fall below the threshold are automatically cleaned up using an off-band system that evaluates eligibility. Additional safety checks and the ability to quickly resurrect deleted versions help reduce the risk associated with the maintenance operations.A variation of the maintenance problem is the requirement to update applications to address security vulnerabilities, performance issues or end-of-life for libraries. While automated reporting is helpful in surfacing applications that need attention, updates are typically a tedious, manual process and are not always performed in a timely manner. An idea we are pursuing here is to facilitate automatic upgrades. Our goal is to apply the updates to an application unit, run it through the canary process and based on the canary score, provide a push button way for the update to be rolled out. We believe this feature will provide a significant productivity win for developers, especially as the number of deployments increases.Serverless makes it easy to do fire and forget deployments but it brings with it increased maintenance considerations. Features designed to eliminate toil become increasingly important even at reasonable scale.Our experience tells us that serverless architectures go a long way in simplifying the process of developing scalable applications in a rapid and cost effective manner. From an operational perspective, they introduce different considerations such as the loss of control over the execution environment and the complexity of managing many smaller deployment units, resulting in the need for much more sophisticated insights and observability solutions. We see the industry headed in this direction and are eagerly looking forward to the innovations in this space.If you have opinions on serverless or want to engage in conversations related to developer experience, we’d love to hear from you! And if you want to help Netflix engineers in their quest to delight millions of customers worldwide, come join our team!",https://medium.com/netflix-techblog/developer-experience-lessons-operating-a-serverless-like-platform-at-netflix-part-ii-63a376c28228?source=collection_home---4------7---------------------,0,netflix,,NULL,2017-10-12
Serving 100 Gbps from an Open Connect Appliance,"In the summer of 2015, the Netflix Open Connect CDN team decided to take on an ambitious project. The goal was to leverage the new 100GbE network interface technology just coming to market in order to be able to serve at 100 Gbps from a single FreeBSD-based Open Connect Appliance (OCA) using NVM Express (NVMe)-based storage.At the time, the bulk of our flash storage-based appliances were close to being CPU limited serving at 40 Gbps using single-socket Xeon E5–2697v2. The first step was to find the CPU bottlenecks in the existing platform while we waited for newer CPUs from Intel, newer motherboards with PCIe Gen3 x16 slots that could run the new Mellanox 100GbE NICs at full speed, and for systems with NVMe drives.Normally, most of an OCA’s content is served from disk, with only 10–20% of the most popular titles being served from memory (see our previous blog, Content Popularity for Open Connect for details). However, our early pre-NVMe prototypes were limited by disk bandwidth. So we set up a contrived experiment where we served only the very most popular content on a test server. This allowed all content to fit in RAM and therefore avoid the temporary disk bottleneck. Surprisingly, the performance actually dropped from being CPU limited at 40 Gbps to being CPU limited at only 22 Gbps!After doing some very basic profiling with pmcstat and flame graphs, we suspected that we had a problem with lock contention. So we ran the DTrace-based lockstat lock profiling tool that is provided with FreeBSD. Lockstat told us that we were spending most of our CPU time waiting for the lock on FreeBSD’s inactive page queue. Why was this happening? Why did this get worse when serving only from memory?A Netflix OCA serves large media files using NGINX via the asynchronous sendfile() system call. (See NGINX and Netflix Contribute New sendfile(2) to FreeBSD). The sendfile() system call fetches the content from disk (unless it is already in memory) one 4 KB page at a time, wraps it in a network memory buffer (mbuf), and passes it to the network stack for optional encryption and transmission via TCP. After the network stack releases the mbuf, a callback into the VM system causes the 4K page to be released. When the page is released, it is either freed into the free page pool, or inserted into a list of pages that may be needed again, known as the inactive queue. Because we were serving entirely from memory, NGINX was advising sendfile() that most of the pages would be needed again — so almost every page on the system went through the inactive queue.The problem here is that the inactive queue is structured as a single list per non-uniform memory (NUMA) domain, and is protected by a single mutex lock. By serving everything from memory, we moved a large percent of the page release activity from the free page pool (where we already had a per-CPU free page cache, thanks to earlier work by Netflix’s Randall Stewart and Scott Long, Jeff Roberson’s team at Isilon, and Matt Macy) to the inactive queue. The obvious fix would have been to add a per-CPU inactive page cache, but the system still needs to be able to find the page when it needs it again. Pages are hashed to the per-NUMA queues in a predictable way.The ultimate solution we came up with is what we call “Fake NUMA”. This approach takes advantage of the fact that there is one set of page queues per NUMA domain. All we had to do was to lie to the system and tell it that we have one Fake NUMA domain for every 2 CPUs. After we did this, our lock contention nearly disappeared and we were able to serve at 52 Gbps (limited by the PCIe Gen3 x8 slot) with substantial CPU idle time.After we had newer prototype machines, with an Intel Xeon E5 2697v3 CPU, PCIe Gen3 x16 slots for 100GbE NIC, and more disk storage (4 NVMe or 44 SATA SSD drives), we hit another bottleneck, also related to a lock on a global list. We were stuck at around 60 Gbps on this new hardware, and we were constrained by pbufs.FreeBSD uses a “buf” structure to manage disk I/O. Bufs that are used by the paging system are statically allocated at boot time and kept on a global linked list that is protected by a single mutex. This was done long ago, for several reasons, primarily to avoid needing to allocate memory when the system is already low on memory, and trying to page or swap data out in order to be able to free memory. Our problem is that the sendfile() system call uses the VM paging system to read files from disk when they are not resident in memory. Therefore, all of our disk I/O was constrained by the pbuf mutex.Our first problem was that the list was too small. We were spending a lot of time waiting for pbufs. This was easily fixed by increasing the number of pbufs allocated at boot time by increasing the kern.nswbuf tunable. However, this update revealed the next problem, which was lock contention on the global pbuf mutex. To solve this, we changed the vnode pager (which handles paging to files, rather than the swap partition, and hence handles all sendfile() I/O) to use the normal kernel zone allocator. This change removed the lock contention, and boosted our performance into the 70 Gbps range.As noted above, we make heavy use of the VM page queues, especially the inactive queue. Eventually, the system runs short of memory and these queues need to be scanned by the page daemon to free up memory. At full load, this was happening roughly twice per minute. When this happened, all NGINX processes would go to sleep in vm_wait() and the system would stop serving traffic while the pageout daemon worked to scan pages, often for several seconds. This had a severe impact on key metrics that we use to determine an OCA’s health, especially NGINX serving latency.The basic system health can be expressed as follows (I wish this was a cartoon):This problem is actually made progressively worse as one adds NUMA domains, because there is one pageout daemon per NUMA domain, but the page deficit that it is trying to clear is calculated globally. So if the vm pageout daemon decides to clean, say 1GB of memory and there are 16 domains, each of the 16 pageout daemons will individually attempt to clean 1GB of memory.To solve this problem, we decided to proactively scan the VM page queues. In the sendfile path, when allocating a page for I/O, we run the pageout code several times per second on each VM domain. The pageout code is run in its lightest-weight mode in the context of one unlucky NGINX process. Other NGINX processes continue to run and serve traffic while this is happening, so we can avoid bursts of pager activity that blocks traffic serving. Proactive scanning allowed us to serve at roughly 80 Gbps on the prototype hardware.TCP Large Receive Offload (LRO), is the technique of combining several packets received for the same TCP connection into a single large packet. This technique reduces system load by reducing trips through the network stack. The effectiveness of LRO is measured by the aggregation rate. For example, if we are able to receive four packets and combine them into one, then our LRO aggregation rate is 4 packets per aggregation.The FreeBSD LRO code will, by default, manage up to 8 packet aggregations at one time. This works really well on a LAN, when serving traffic over a small number of really fast connections. However, we have tens of thousands of active TCP connections on our 100GbE machines, so our aggregation rate was rarely better than 1.1 packets per aggregation on average.Hans Petter Selasky, Mellanox’s 100GbE driver developer, came up with an innovative solution to our problem. Most modern NICs will supply an Receive Side Scaling (RSS) hash result to the host. RSS is a standard developed by Microsoft wherein TCP/IP traffic is hashed by source and destination IP address and/or TCP source and destination ports. The RSS hash result will almost always uniquely identify a TCP connection. Hans’ idea was that rather than just passing the packets to the LRO engine as they arrive from the network, we should hold the packets in a large batch, and then sort the batch of packets by RSS hash result (and original time of arrival, to keep them in order). After the packets are sorted, packets from the same connection are adjacent even when they arrive widely separated in time. Therefore, when the packets are passed to the FreeBSD LRO routine, it can aggregate them.With this new LRO code, we were able to achieve an LRO aggregation rate of over 2 packets per aggregation, and were able to serve at well over 90 Gbps for the first time on our prototype hardware for mostly unencrypted traffic.An RX queue containing 1024 packets from 256 connections would have 4 packets from the same connection in the ring, but the LRO engine would not be able to see that the packets belonged together, because it maintained just a handful of aggregations at once. After sorting by RSS hash, the packets from the same connection appear adjacent in the queue, and can be fully aggregated by the LRO engine.So the job was done. Or was it? The next goal was to achieve 100 Gbps while serving only TLS-encrypted streams.By this point, we were using hardware which closely resembles today’s 100GbE flash storage-based OCAs: four NVMe PCIe Gen3 x4 drives, 100GbE ethernet, Xeon E5v4 2697A CPU. With the improvements described in the Protecting Netflix Viewing Privacy at Scale blog entry, we were able to serve TLS-only traffic at roughly 58 Gbps.In the lock contention problems we’d observed above, the cause of any increased CPU use was relatively apparent from normal system level tools like flame graphs, DTrace, or lockstat. The 58 Gbps limit was comparatively strange. As before, the CPU use would increase linearly as we approached the 58 Gbps limit, but then as we neared the limit, the CPU use would increase almost exponentially. Flame graphs just showed everything taking longer, with no apparent hotspots.We finally had a hunch that we were limited by our system’s memory bandwidth. We used the Intel® Performance Counter Monitor Tools to measure the memory bandwidth we were consuming at peak load. We then wrote a simple memory thrashing benchmark that used one thread per core to copy between large memory chunks that did not fit into cache. According to the PCM tools, this benchmark consumed the same amount of memory bandwidth as our OCA’s TLS-serving workload. So it was clear that we were memory limited.At this point, we became focused on reducing memory bandwidth usage. To assist with this, we began using the Intel VTune profiling tools to identify memory loads and stores, and to identify cache misses.Because we are using sendfile() to serve data, encryption is done from the virtual memory page cache into connection-specific encryption buffers. This preserves the normal FreeBSD page cache in order to allow serving of hot data from memory to many connections. One of the first things that stood out to us was that the ISA-L encryption library was using half again as much memory bandwidth for memory reads as it was for memory writes. From looking at VTune profiling information, we saw that ISA-L was somehow reading both the source and destination buffers, rather than just writing to the destination buffer.We realized that this was because the AVX instructions used by ISA-L for encryption on our CPUs worked on 256-bit (32-byte) quantities, whereas the cache line size was 512-bits (64 bytes) — thus triggering the system to do read-modify-writes when data was written. The problem is that the the CPU will normally access the memory system in 64 byte cache line-sized chunks, reading an entire 64 bytes to access even just a single byte. In this case, the CPU needed to write 32 bytes of a cache line, but using read-modify-writes to handle those writes meant that it was reading the entire 64 byte cache line in order to be able to write that first 32 bytes. This was especially silly, because the very next thing that would happen would be that the second half of the cache line would be written.After a quick email exchange with the ISA-L team, they provided us with a new version of the library that used non-temporal instructions when storing encryption results. Non-temporals bypass the cache, and allow the CPU direct access to memory. This meant that the CPU was no longer reading from the destination buffers, and so this increased our bandwidth from 58 Gbps to 65 Gbps.In parallel with this optimization, the spec for our final production machines was changed from using lower cost DDR4–1866 memory to using DDR4–2400 memory, which was the fastest supported memory for our platform. With the faster memory, we were able to serve at 76 Gbps.We spent a lot of time looking at VTune profiling information, re-working numerous core kernel data structures to have better alignment, and using minimally-sized types to be able to represent the possible ranges of data that could be expressed there. Examples of this approach include rearranging the fields of kernel structs related to TCP, and re-sizing many of the fields that were originally expressed in the 1980s as “longs” which need to hold 32 bits of data, but which are now 64 bits on 64-bit platforms.Another trick we use is to avoid accessing rarely used cache lines of large structures. For example, FreeBSD’s mbuf data structure is incredibly flexible, and allows referencing many different types of objects and wrapping them for use by the network stack. One of the biggest sources of cache misses in our profiling was the code to release pages sent by sendfile(). The relevant part of the mbuf data structure looks like this:The problem is that arg2 fell in the 3rd cache line of the mbuf, and was the only thing accessed in that cache line. Even worse, in our workload arg2 was almost always NULL. So we were paying to read 64 bytes of data for every 4 KB we sent, where that pointer was NULL virtually all the time. After failing to shrink the mbuf, we decided to augment the ext_flags to save enough state in the first cache line of the mbuf to determine if ext_arg2 was NULL. If it was, then we just passed NULL explicitly, rather than dereferencing ext_arg2 and taking a cache miss. This gained almost 1 Gbps of bandwidth.VTune and lockstat pointed out a number of oddities in system performance, most of which came from the data collection that is done for monitoring and statistics.The first example is a metric monitored by our load balancer: TCP connection count. This metric is needed so that the load balancing software can tell if the system is underloaded or overloaded. The kernel did not export a connection count, but it did provide a way to export all TCP connection information, which allowed user space tools to calculate the number of connections. This was fine for smaller scale servers, but with tens of thousands of connections, the overhead was noticeable on our 100GbE OCAs. When asked to export the connections, the kernel first took a lock on the TCP connection hash table, copied it to a temporary buffer, dropped the lock, and then copied that buffer to userspace. Userspace then had to iterate over the table, counting connections. This both caused cache misses (lots of unneeded memory activity), and lock contention for the TCP hash table. The fix was quite simple. We added per-CPU lockless counters that tracked TCP state changes, and exported a count of connections in each TCP state.Another example is that we were collecting detailed TCP statistics for every TCP connection. The goal of these statistics is to monitor the quality of customer’s sessions. The detailed statistics were quite expensive, both in terms of cache misses and in terms of CPU. On a fully loaded 100GbE server with many tens of thousands of active connections, the TCP statistics consumed 5–10% of the CPU. The solution to this problem was to only keep detailed statistics on a small percentage of connections. This dropped CPU used by TCP statistics to below 1%.These changes resulted in a speedup of 3–5 Gbps.The FreeBSD mbuf system is the workhorse of the network stack. Every packet which transits the network is composed of one or more mbufs, linked together in a list. The FreeBSD mbuf system is very flexible, and can wrap nearly any external object for use by the network stack. FreeBSD’s sendfile() system call, used to serve the bulk of our traffic, makes use of this feature by wrapping each 4K page of a media file in an mbuf, each with its own metadata (free function, arguments to the free function, reference count, etc).The drawback to this flexibility is that it leads to a lot of mbufs being chained together. A single 1 MB HTTP range request going through sendfile can reference 256 VM pages, and each one will be wrapped in an mbuf and chained together. This gets messy fast.At 100 Gbps, we’re moving about 12.5 GB/s of 4K pages through our system unencrypted. Adding encryption doubles that to 25 GB/s worth of 4K pages. That’s about 6.25 Million mbufs per second. When you add in the extra 2 mbufs used by the crypto code for TLS metadata at the beginning and end of each TLS record, that works out to another 1.6M mbufs/sec, for a total of about 8M mbufs/second. With roughly 2 cache line accesses per mbuf, that’s 128 bytes * 8M, which is 1 GB/s (8 Gbps) of data that is accessed at multiple layers of the stack (alloc, free, crypto, TCP, socket buffers, drivers, etc).To reduce the number of mbufs in transit, we decided to augment mbufs to allow carrying several pages of the same type in a single mbuf. We designed a new type of mbuf that could carry up to 24 pages for sendfile, and which could also carry the TLS header and trailing information in-line (reducing a TLS record from 6 mbufs down to 1). That change reduced the above 8M mbufs/sec down to less than 1M mbufs/sec. This resulted in a speed up of roughly 7 Gbps.This was not without some challenges. Most specifically, FreeBSD’s network stack was designed to assume that it can directly access any part of an mbuf using the mtod() (mbuf to data) macro. Given that we’re carrying the pages unmapped, any mtod() access will panic the system. We had to augment quite a few functions in the network stack to use accessor functions to access the mbufs, teach the DMA mapping system (busdma) about our new mbuf type, and write several accessors for copying mbufs into uios, etc. We also had to examine every NIC driver in use at Netflix and verify that they were using busdma for DMA mappings, and not accessing parts of mbufs using mtod(). At this point, we have the new mbufs enabled for most of our fleet, with the exception of a few very old storage platforms which are disk, and not CPU, limited.At this point, we’re able to serve 100% TLS traffic comfortably at 90 Gbps using the default FreeBSD TCP stack. However, the goalposts keep moving. We’ve found that when we use more advanced TCP algorithms, such as RACK and BBR, we are still a bit short of our goal. We have several ideas that we are currently pursuing, which range from optimizing the new TCP code to increasing the efficiency of LRO to trying to do encryption closer to the transfer of the data (either from the disk, or to the NIC) so as to take better advantage of Intel’s DDIO and save memory bandwidth.",https://medium.com/netflix-techblog/serving-100-gbps-from-an-open-connect-appliance-cdb51dda3b99?source=collection_home---4------8---------------------,0,netflix,"typescript,html,frontend,angular",NULL,2017-09-29
"Scaling Event Sourcing for Netflix Downloads, Episode 2","by Karen Casella, Phillipa Avery, Robert Reta, Joseph BreuerIn the first episode of this series of posts, we introduced the Netflix downloads project and the use cases that led us to consider a solution based on the event sourcing pattern. In this post, we provide an overview of the general event sourcing pattern and how we applied it to some of these key use cases.When we first started to design the downloads licensing service, the content licensing restrictions had yet to be defined. All we knew was that they were coming, and we needed to be able to adapt to them. So, how do you start to design and implement a service when the requirements are yet to be decided? What if that service is going live in a matter of months to millions of members on a single day with a 6am global press release? You make the system flexible to change. Easy. Right?Anyone who’s familiar with relational databases knows that the phrases ‘flexible’ and ‘easy to change’ are not overly true with regards to the underlying table schema. There are ways to change the schema, but they are not easily accessible, require a deep knowledge of SQL, and direct interaction with the database. In addition, once the data is mutated you lose valuable context to the cause of the change and what the previous state was.Document oriented NoSQL databases are known for providing such flexibility to change, and we quickly moved in that direction as a means to provide a flexible and scalable solution. The document model provides us with the flexibility we need for the data model, but doesn’t provide us with the traceability to determine what caused the data mutation. Given the longevity of the data, we wanted the ability to look back in time to debug a change in state.Event Sourcing is an architectural pattern that is resurfacing lately as a valuable component of a modern distributed microservices ecosystem. Martin Fowler describes the basic pattern as follows: “The fundamental idea of Event Sourcing is that of ensuring every change to the state of an application is captured in an event object, and that these event objects are themselves stored in the sequence they were applied for the same lifetime as the application state itself.”There are many excellent overviews of the Event Sourcing pattern. Two of our favorites include:In short, Event Sourcing is an architectural pattern that maintains a complete transaction history for a data model. Instead of persisting the data model itself, you persist the events that lead to a change in the data. These events are then played in order, building up an aggregate view of the complete data domain. The ability to replay events to any point in time is also an excellent debugging tool that enables us to easily explain why a member’s account is in a particular state and allows us to easily test system variations.The following diagram provides a high-level view of how we applied the Event Sourcing pattern for the Netflix system responsible for enforcing the downloads business rules, with a generic explanation of each component following.The Event Sourcing pattern depends upon three different service layers: commands, events & aggregates.As shown, there are a number of actors involved in implementing the pattern.When a member selects a title to download, the license lifecycle begins:The Netflix client application first requests a license. Once the license is acquired, the Netflix client downloads the content and the member can play their newly downloaded content. Dependent on member actions, the state of the license can change throughout the lifecycle. The member may start, pause, resume or stop viewing the content. The member may remove downloaded content. Each of these actions potentially results in a state change for the license. The license is created, potentially renewed several times, and finally released (deleted), either explicitly by the member, or implicitly based on business rules.There is a large amount of business logic involved in this lifecycle. Maintaining the license state is the job of the event-sourcing based license accounting service, which tracks a complete transaction history for the license, member’s downloaded content and device data models. This allows events to be played back in order, building up an aggregate view of the complete data objects.The Netflix client application makes several different types of requests that are translated into commands, events and aggregates. To support the business requirements for license enforcement, we have three inter-related Aggregates: License, Downloaded Titles and Device. Each of these has its own service, handlers and repository. Following is a description of each of the concepts introduced above as they apply to some key use cases for the Downloads feature.Following is a simple use case of a member obtaining a license for the first time for a particular piece of content.On the initial license request, the client sends a request to the Acquire License Endpoint, with the member identity, along with the title of the content being requested for download, to the License Service:The License Service determines if the requested action is allowed by querying existing Aggregate data and applying appropriate business rules. Since this is the first request for the member on this content, and assuming that device and studio business rules are satisfied, the License Service creates a new empty License Aggregate and a Create License Command to pass on to the License Command Handler:The License Command Handler applies the Create License Command to the License Aggregate and generates a License Created Event:The License Command Handler passes the empty License Aggregate with the License Created Event to the Event Handler, which creates a new License Aggregate:The Event Repository then persists the License Created Event in the Event Store:Finally, the License Repository returns the new License Aggregate to the License Service, which packages the Aggregate information into the response back to the client.Prior to the expiration of the license, the device may request an extension to the existing license, known as license renewal. Renewing a license is similar to the original license acquisition flow, with the major difference being that the current License Aggregate is passed to the License Command Handler along with a Renew License Command. After the License Command Handler generates the appropriate events, The License Event Handler applies the License Renewed Event to the License Aggregate as shown below. Note that the new License Aggregate has an expiration date 30 days beyond the current date. This 30 day renewal represents the business rule for license renewals currently in force. If this were to change, we would make a simple configuration change to the Event Handler.Each time a device requests a new or renewed license from the License Service, the Downloaded Service retrieves the current aggregates for the member and evaluates for business rule validation. For example, one such validation requires some content to only be downloaded twice a year. When a device makes a license request, the License Service checks if the content has been previously downloaded that year. It’s possible to derive this information by retrieving all the license aggregates for the year and filtering on the content Id. This is a lot of processing, and we decided instead to de-normalize the data and create a separate aggregate for the content download, indexed on the member and content Id. This requires new Download Events, Service, Aggregate and Repository.When we receive subsequent events for content, we can check for any previous downloads for the content. Per the sequence diagram below, if the Download Service sees that the member has exceeded the downloads for the content it can reject the request.For our use cases, we have found Event Sourcing to be a very useful pattern for implementing a flexible and robust system. It’s not all sunshine and roses, however, and we definitely made some mistakes and have areas to improve upon (subsequent posts will go into those details). Overall, the flexibility of the architecture has given us the means to rapidly innovate and react to changing requirements, to debug issues with changing data states over time, and to get a brand new service out and serving millions of users around the world in a relatively short time.In the next post of this series, we will provide a deep dive into the implementation details, including the use of data versioning and snapshotting to provide flexibility and scale. Following that, we will share our experiences implementing Event Sourcing, some of the lessons we learned (and the mistakes we made) around testing, scalability and optimization, and introduce some thoughts on future improvements and extensions that we are planning going forward.The team recently presented this topic at QCon New York and you can download the slides and watch the video here. Join us at Velocity New York (October 2–4, 2017) for an even more technical deep dive on the implementation and lessons learned.The authors are members of the Netflix Playback Access team. We pride ourselves on being experts at distributed systems development and operations. And, we’re hiring Senior Software Engineers! Email kcasella@netflix.com or connect on LinkedIn if you are interested.",https://medium.com/netflix-techblog/scaling-event-sourcing-for-netflix-downloads-episode-2-ce1b54d46eec?source=collection_home---4------9---------------------,0,netflix,,NULL,2017-09-18
"Scaling Event Sourcing for Netflix Downloads, Episode 1","by Karen Casella, Phillipa Avery, Robert Reta, Joseph BreuerEarly in 2016, several Netflix teams were asked the question: “What would it take to allow members to download and view content offline on their mobile devices?”For the Playback Licensing team, this meant that we needed to provide a content licensing system that would allow a member’s device to store and decrypt the downloaded content for offline viewing. To do this securely would require a new service to handle a complex set of yet-to-be defined business validations, along with a new API for client and server interactions. Further, we determined that this new service needed to be stateful, when all of our existing systems were stateless.“Great! How long will that take you?”In late November 2016, nine short months after that initial question was asked, Netflix successfully launched the new downloads feature that allows members to download and play content on their mobile devices. Several product and engineering teams collaborated to design and develop this new feature, which launched simultaneously to all Netflix members around the world.This series of posts will outline why and how we built a new licensing system to support the Netflix downloads experience. In this first post of the series, we provide an overview of the Netflix downloads project and the changes it meant for the content licensing team at Netflix. Further posts will dive deeper into the solutions we created to meet these requirements.When a member streams content on Netflix, we deliver data to their device from our back-end servers before the member can commence playing the content. This data is retrieved via a complex device-to-server interaction on our Playback Services systems, which can be summarized as follows.To play a title, the member’s device retrieves all the metadata associated with the content. The response object is known as the Playback Context, and includes data such as the image assets for the content and the URLs for streaming the content (see How Netflix Directs 1/3rd of Internet Traffic for an excellent overview of the streaming playback process and systems). The streamed data is encrypted with Digital Rights Management (DRM) technology and must be decrypted before it can be watched. This is done through the process of licensing, where a device can request a license for a particular title, and the license is then used to decrypt the content on that device. In the streaming case, the license is short-lived, and only able to be used once. When the member finishes watching the content, the license is considered consumed and no longer able to be used for playback.Netflix supports several different DRM technologies to enable licensing for the content. Each of these live in their own microservice, requiring independent scaling and deployment tactics. This licensing tier needs to be as robust and reliable as possible; while many services at Netflix have fallbacks defined to serve a potentially degraded experience in the case of failures or request latency, the licensing services have no fallbacks possible. If licensing goes down, there is no playback. To reduce the risks to availability and resiliency, and to allow for flexible scaling, the licensing services have traditionally been stateless.The downloads flow differs slightly from the streaming one. Similar to the streaming flow, we generate a Playback Context (Metadata) for the downloaded content. Once we have the metadata for the content, we can start the license flow which is depicted as follows:After checking to ensure a title is available for downloading, the member’s device attempts to acquire a license. We then perform several back-end checks to validate if the member is allowed to download the content. If the various business rules are satisfied, we return a license and any additional information used to play the content offline, and the device can then start downloading the encrypted bytes.The license used for downloaded content is also different from streaming — it may be persisted on the device for a longer period of time, and can be reused over multiple playback sessions.Once the title is downloaded to the device, it has a lifecycle as follows:Every time a member presses play on the downloaded content, the application queues up session events to provide information on the viewing experience, and sends those up the next time the member goes online. After a defined duration for each title however, the original license retrieved with the downloaded content expires. At this point, depending on the content, it may be possible to renew the license, which requires the device to ask the back-end for a renewed license. This renewal is also validated against the business rules for downloads and, if successful, allows the content to continue to be played offline. Once the member deletes the content, the license is securely closed (released) which ensures the content can no longer be played offline.Netflix works with a variety of studio partners around the globe to obtain the best content for our members. The restrictions for downloaded content are generally more complex than for streaming, and far more varied amongst the studios. In addition to requirements related to how long a title can be watched, we have a variety of different caps based on the number of downloads for a device or per member, and potentially limitations on how many times the title can be downloaded or watched during a specified period of time.We also have internal business restrictions related to download viewing, such as the number of devices on which content can be downloaded.We must apply all of these restrictions across all the combined movies that a Netflix member downloads. Each time a member downloads a title or wants to extend the viewing time after the initial license expires, we must validate the request against all of the possible restrictions for the partner, taking into account the member’s past download interactions. If a member does not meet any of these requirements, the back-end sends back a response with the reason for why a download request failed.With the introduction of the downloads feature, we needed to reconsider our approach to maintaining state. The downloads feature requires us to validate if a member should be allowed playback based on previous downloading history. We decided to perform this validation when the license was requested, so we needed a new stateful service that the licensing services could consult for validating business rules. We had a short period of time to design this new stateful system, which would enforce business rules and potentially reject licenses according to a yet-to-be defined set of requirements.We had an amazing opportunity to create a new service from scratch, with an existing user base of millions, and a limited time to create it in. Exciting times lay ahead!In the next post of this series, we will discuss the service we created to validate and track downloads usage: an Event-Sourced backed stateful storage microservice. Future posts will deep-dive into the implementation details, including the use of data versioning and snapshotting to provide flexibility and scale.The team recently presented this topic at QCon New York and you can download the slides and watch the video here. Join us at Velocity New York (October 2–4, 2017) for an even more technical deep dive on the implementation and lessons learned.The authors are members of the Netflix Playback Licensing team. We pride ourselves on being experts at distributed systems development and operations. And, we’re hiring Senior Software Engineers! Email kcasella@netflix.com or connect on LinkedIn if you are interested.",https://medium.com/netflix-techblog/scaling-event-sourcing-for-netflix-downloads-episode-1-6bc1595c5595?source=collection_home---4------10---------------------,0,netflix,,NULL,2017-09-11
Netflix Hack Day — Summer 2017,"by Daniel Jacobson, Ruslan Meshenberg, Leslie Posada, and Tom RichardsAbout a week ago, Netflix hosted another great Hack Day. The event gives Netflix employees a chance to take a break from everyday work, have fun, experiment with new technologies, and collaborate with new people.Like previous Hack Days, we saw a wide range of ideas, including hacks designed to improve the product, our internal efficiency, and some that are just meant to be fun. We’ve embedded videos below, produced by the hackers, of some of our favorite hacks from the event. You can also see hacks from our past events: January 2017, May 2016, November 2015, March 2015, Feb. 2014 & Aug. 2014:While we’re excited about the creativity and thought put into these hacks, they may never become part of the Netflix product, internal infrastructure, or used beyond Hack Day. We are posting them here to share the spirit of the event and our culture of innovation.Thanks again to the hackers who, in just 24 hours, assembled really innovative and impressive hacks.A Netflix Kiosk where you can pay, sample, and sign up for NetflixBy Ryan Anklam, Micah Ransdell, Tony Edwards, Neha Samant, Sumana Mohan, Tyler HughesBack in the era of voltaic piles and Edison bulbs, Teleflix utilizes an original 1920s brass AT&T telegraph key and a Raspberry Pi to bring you the one true single button interface for Netflix. The hackers decode Morse code from the key on a Raspberry Pi Zero W and send USB keyboard scancodes to the TV to allow searching and playback of Netflix content, all from the comfort of your armchair. Full write up on the DIY hack here.By Guy Cirino, Alex Wolfe, and Carenina Motion, Christiane PetiteA fun, and slightly creepy, experience for our members where the eyes and heads of characters in our images subtly move around following the movement of your remote or cursor as you make a selection on Netflix.By Kate Pflueger, Juha Turunen, Asim KhanContinue Watching row with a twistBy Kevin Lew and Chris CareyAudiobooks meet old school radio shows by using Audio Descriptions available for lots of great titles. Listen to your favorite movies and shows on Netflix, just like an audiobook! Enjoy rich narration of scenes and action incorporated into the original audio track.By Jordanna Kwok and Glen DavisHere are some photos from the event:",https://medium.com/netflix-techblog/netflix-hack-day-summer-2017-ef3ba81a8a77?source=collection_home---4------11---------------------,0,netflix,"python,php,html,css,frontend",NULL,2017-08-29
Starting the Avalanche,"By Scott Behrens and Bryan PayneWe’d like to introduce you to one of the most devastating ways to cause service instability in modern micro-service architectures: application DDoS. A specially crafted application DDoS attack can cause cascading system failures often for a fraction of the resources needed to conduct a more traditional DDoS attack. This is due to the complex, interconnected relationships between applications. Traditional DDoS attacks focus on exhausting system resources at the network level. In contrast, application layer attacks focus on expensive API calls, using their complex interconnected relationships to cause the system to attack itself — sometimes with a massive effect. In a modern microservice architecture this can be particularly harmful. A sophisticated attacker could craft malicious requests that model legitimate traffic and pass through edge protections such as a web application firewall (WAF).In this blog post, we will discuss an effort at Netflix to identify, test, and remediate application layer DDoS attacks. We will begin with some background on the problem space. Next we will discuss the tools and methods we used to test our systems. Finally, we will discuss steps for making systems more resilient against application layer DDoS attacks. We are also presenting at DEF CON 25 today on the same topics, so if you are attending the conference please stop by.According to Akamai’s Q1 2017 State of the Internet Security report, “less than 1% of all DDoS attacks are application layer”¹. However, this metric underrepresents the impact of these attacks. When an attacker takes the time to craft this style of attack, they can be highly effective. Keeping this in mind, defending against these types of attacks can help ensure that your organization does not have cascading failures if an application layer DDoS attack occurs.Traditional application layer DDoS attacks were focused on the attacker’s work to generate an input compared with the responding system’s work to generate the resulting output. Attacks focused on expensive calls such as database queries or heavy disk I/O with the goal of over utilizing the application until it could no longer service legitimate users. As application architectures have evolved into more complex and distributed systems, we now have additional vectors to focus on like service health checks, queuing/batching, and complex microservice dependencies that may result in failures if one key service becomes unstable.In a modern microservice architecture, application DDoS can be a particularly effective opportunity to cause service instability. To understand why, let’s consider a sample microservice architecture that uses a gateway to interact with a variety of middle tier and backend microservices, as depicted in the figure below.This diagram shows how a single request at the edge can fan out into thousands of requests for the middle tier and backend microservices. If an attacker can identify API calls that have this effect, then it may be possible to use this fan out architecture against the overall service. If the resulting computations are expensive enough, then certain middle tier services could stop working. Depending on the criticality of these services, this could result in an overall service outage.All of this is made possible because the microservice architecture helps the attacker by massively amplifying the attack against internal systems. In summary, a single request in a microservices architecture may generate tens of thousands of complex middle tier and backend service calls.This presents unique challenges for defenders. If your environment takes advantage of a common web application firewall deployment, where the firewall is positioned on the internet facing systems only (such as the API gateway), it may miss an opportunity to block requests that are specifically causing distress to those middle tier and backend services. In addition, this firewall may not know how much work one request to the API gateway will generate for middle tier services, and may not trigger a blacklist until the damage is done. As defenders it is important that we understand how to identify these vulnerable application calls by walking through a framework for discovery and validation.Intuitively, our goal as defenders is to identify which API calls may be vulnerable to a DDoS. These are the calls that require significant resources from the middle tier and backend services. Timing how long API calls take to complete is one way to identify such calls. The most basic and error prone way to do this is to fingerprint API calls from a web browser. This can be done by opening Chrome Developer console, selecting the Network tab, clicking the Preserve log button, and then browsing to the site. After some period of time sort by Time and look at the most latent calls. You will get a screen similar to the image below.This technique may have false positives, including calls that cannot be modified to increase latency. Also, you may end up missing calls that could be manipulated to increase latency. A better technique to identify latent calls may be to monitor request times for middle tier services. Once you have found a latent middle tier service, you should work on reconstructing a request that could be made through the API gateway that would invoke the latent middle tier service.Once you have found some interesting API calls, the next step is to inspect their content. Your goal at this stage should be to find ways to make the calls more expensive. One technique for this is to increase the range of objects requested. For example, in the image below the from and to parameters could potentially be modified to increase the workload on middle tier services.Digging a little deeper, you can often modify many different elements of a request to make it more expensive. The image below shows one example where you can potentially modify the object fields requested, the range, and even the image size.You also want to build out a list of indicators on the health of your test. This will inform you if your test is working and where you may need to scale the test up or down. Typically this will be different HTTP Status codes or latencies observed during testing but it may also be specific headers, response text, stack traces, etc. The image below shows an example list of indicators.Another useful test success indicator is increased latency (such as an HTTP 200 and a 10 second response). You may observe the latency yourself during the test or other users browsing the application. Once you have a good understanding of the types of requests you can send to generate latency and how to measure the indicators of success, you’ll need to tune your test to operate under a web application firewall if that exists in your environment.The ideal traffic flow will be somewhere below when the web application firewall starts blocking, but high enough that the number of requests and work per request causes service instability.To help facilitate this testing on a smaller scale, you can use the Repulsive Grizzly framework. Netflix is releasing this framework through our skunkworks open source program, which means that we are sharing it as a proof of concept but do not anticipate maintaining this code base long term. This framework is written in Python and leverages eventlet for high concurrency. It also supports the ability to round-robin authentication objects, which can be an effective technique to bypass certain web application firewalls.Repulsive Grizzly does not help with identification of application DDoS vulnerabilities. As with all security testing tools, it is important to utilize this only on systems where you are authorized to perform this testing. On these systems, you will first need to identify potential issues as outlined above. Once you have some potential issues to test, Repulsive Grizzly will simplify the testing process.For details on how to use the Repulsive Grizzly framework, see the project’s Github page for documentation.After testing your hypothesis on a smaller scale, you can leverage Cloudy Kraken to scale your test. Cloudy Kraken is an AWS orchestration framework, specifically centered around helping you test your applications at a global scale. Similar to the Repulsive Grizzly framework, Netflix is releasing Cloudy Kraken as a skunkworks open source project.Cloudy Kraken helps maintain a global fleet of test instances and the Repulsive Grizzly tests that run on those instances. It also builds and distributes the test configuration and leverages AWS EC2’s enhanced network drivers. Cloudy Kraken can also scale the test over multiple regions and provides time-synchronization so your test agents run in parallel. The diagram below provides a high level overview of Cloudy Kraken.Cloudy Kraken orchestrates your tests in a developer friendly fashion. This starts with some configuration scripts that define the test. Cloudy Kraken will then create the AWS environment for the test and launch the instances. While the test is underway, Cloudy Kraken will collect data using AWS SNS. Finally, the AWS resources are destroyed at the conclusion of the test. These steps are shown in the diagram below.At Netflix, we wanted to test our findings against a particular API call we identified as being latent. During a Chaos Kong exercise (where Netflix evacuates an entire AWS region while gracefully redirecting customer traffic to other regions), we tested against the production-scale environment in the evacuated region. This provided the rare opportunity to test an application layer DDoS scenario against a production scale environment without any customer impact. Our unique culture encourages us to do what is best for Netflix and it’s customers, and we embraced that freedom to run the test in production to truly understand it’s impact.The test we ran conducted two different attacks over two 5 minute periods. The results of the test confirmed our theory and resulted in an 80% API gateway error rate for the specific region we targeted. Test users who were making requests against that API gateway observed site errors or other exceptions which prevented usage of the site. The graph below shows two spikes in HTTP 503 status codes (depicted in purple), which are correlated to the API gateway health.The best defense for application layer DDoS attacks comes from a collection of security controls and best practices.First and foremost it is critical to know your system. You should understand which microservices impact each aspect of the customer experience. Look for ways to reduce inter-dependencies on those services. If one service becomes unstable the rest of your microservices should continue to operate, perhaps in a degraded state.It is important to have a good understanding of how your services queue and service requests. It may be possible for your middle tier and backend services to limit the batch or object size requested. This can also be done in the client code, and potentially even enforced in the API gateway. Putting a limit on the allowable work per request can significantly reduce the likelihood of exploitation.We also recommend enabling a feedback loop to provide alerts from the middle tier and backend service to your WAF. This will help the WAF know when to block these attacks. In many deployments, the WAF is only monitoring the edge and may not realize the impact of one single request to the API gateway. A WAF should also monitor the volume of cache misses. If an API gateway is constantly performing middle tier service calls due to cache misses, that suggests that the cache is not configured correctly or potential malicious behavior.API gateways and other microservices should prioritize authenticated traffic over unauthenticated traffic. It costs more for the attacker to use authenticated sessions. This can also help to mitigate the impact of an application layer DDoS event on your customers.Finally, ensure you have reasonable client library timeouts and circuit breakers. With reasonable timeouts — and plenty of testing — you can protect your middle tier services from application layer DDoS attacks.ReferencesRepulsive GrizzlyCloudy KrakenNetflix Security Youtube Channel1. “Akamai’s State of the Internet Security Report.” https://www.akamai.com. N.p., 19 Feb. 2017. Web.",https://medium.com/netflix-techblog/starting-the-avalanche-640e69b14a06?source=collection_home---4------13---------------------,0,netflix,python,NULL,2017-07-28
ChAP: Chaos Automation Platform,"We are excited to announce ChAP, the newest member of our chaos tooling family! Chaos Monkey and Chaos Kong ensure our resilience to instance and regional failures, but threats to availability can also come from disruptions at the microservice level. FIT was built to inject microservice-level failure in production, and ChAP was built to overcome the limitations of FIT so we can increase the safety, cadence, and breadth of experimentation.At a high level, the platform interrogates the deployment pipeline for a user-specified service. It then launches experiment and control clusters of that service, and routes a small amount of traffic to each. A specified FIT scenario is applied to the experimental group, and the results of the experiment are reported to the service owner.The best experiments do not disturb the customer experience. In line with the advanced Principles of Chaos Engineering, we run our experiments in production. To do that, we have to put some requests at risk for the sake of protecting our overall availability. We want to keep that risk to a minimum. This raises the question: What is the smallest experiment we can run that still gives us confidence in the result?With FIT, the impact of an experiment affects metrics for the whole system. Statistics for the experimental population are mixed with the remaining population. The experimental population size (and the effect size) has to be large in order to be detectable in the natural noise of the systemHere’s an example chart from a FIT experiment:Can you determine when the experiment ran? Did it have an impact greater than the noise of the system? In order to create bigger differences that were verifiable by humans and machines, users ended up running larger and longer experiments, risking unnecessary disruptions for our customers.To limit this blast radius, in ChAP we take a small subset of traffic and distribute it evenly between a control and an experimental cluster. We wrote a Mantis job that tracks our KPIs just for the users and devices in each of these clusters. This makes it much easier for humans and computers to see when the experiment and control populations’ behaviors diverge.Here’s an example chart from a ChAP experiment:It is much easier to see how the experimental population has diverged from the control even though the impacted population was much smaller than in the FIT experiment.Any change to the production environment changes the resilience of the system. At Netflix, our production environment might see many hundreds of deploys every day. As a result, our confidence in an experimental result quickly diminishes with time.In order to have experiments run unsupervised, we had to make them safe. We designed a circuit breaker for the experiment that would automatically end the experiment if we exceeded a predefined error budget. An automated, ongoing analysis hooks into the same system we use to do canary analysis.Before ChAP, a vulnerability could be identified and fixed, but then regress and cause an incident. To keep our results up-to-date, we have integrated ChAP with Spinnaker, our CI+CD system to run experiments often and continuously. Since rolling out this functionality, we have successfully identified and prevented resiliency-threat regressions.Some failure modes are only visible when the ratio of failures to total requests in a system crosses certain thresholds. The load balancing and request routing for FIT-decorated requests were evenly spread throughout our production capacity. This allowed increased resource consumption to be absorbed by normal operating headroom, which failed to trigger circuit breakers. We call this sort of experiment a “diffuse” experiment. It’s fine for verifying the logical correctness of fallbacks, but not the characteristics of the system during failure at scale.There are critical thresholds that are crossed only when a large portion of requests are highly latent or failing. Some examples:• When a downstream service is latent, thread pools may be exhausted.• When a fallback is more computationally expensive than the happy path, CPU usage increases.• When errors lead to exceptions being logged, lock contention may become a problem in your logging system.• While aggressive retries occur, you may have a self-inflicted, denial-of-service attack.With all this in mind, we want a way to achieve a high ratio of failures or latency while limiting the potential negative impact on users. Similar to how we segregated the KPIs for the experiment and control populations, we want to separate a few machines to experience extreme duress while the rest of the system was unaffected.Let’s say we want to explore how API handles the failure of the Ratings system, which allows people to rate movies by giving them a thumbs-up or thumbs-down. To set up the experiment, we deploy new API clusters that are proportionally scaled to the size of the population we want to study. For an experiment impacting 0.5% of the population on a system with 200 instances, we would spin up experiment and control clusters with one instance each.We then override the request routing for the control and experimental populations to direct just that traffic to the new clusters instead of the regular production cluster. Since only experimental traffic is being sent to the “experiment” cluster, 100% of the requests between API-experiment and Ratings will be impacted. This will verify that API can actually handle any increased load that the failure scenario may cause. We call this class of experiments “concentrated” experiments.The result? ChAP generates emails like the following:With ChAP, we have safely identified mistuned retry policies, CPU-intensive fallbacks, and unexpected interactions between circuit breakers and load balancers.We wrote the book on Chaos Engineering, available for free for a limited time from O’Reilly.Aaron Blohowiak spoke at Velocity 2017 San Jose, on the topic of Precision Chaos.Nora Jones also presented a talk at Velocity San Jose about our experiences with adoption of chaos tools.Join the Chaos Community google group to participate in the discussion, keep up-to-date on evolution of the industry, and announcements about Chaos Community Day.— Ali Basiri, Aaron Blohowiak, Lorin Hochstein, Nora Jones, Casey Rosenthal, Haley Tucker",https://medium.com/netflix-techblog/chap-chaos-automation-platform-53e6d528371f?source=collection_home---4------14---------------------,0,netflix,python,NULL,2017-07-26
Developer Experience Lessons Operating a Serverless-like Platform At Netflix,"By Vasanth Asokan, Ludovic Galibert and Sangeeta NarayananThe Netflix API is based on a dynamic scripting platform that handles thousands of changes per day. This platform allows our client developers to create a customized API experience on over a thousand device types by executing server side adapter code in response to HTTP requests. Developers are only responsible for the adapter code they write; they do not have to worry about infrastructure concerns related to server management and operations. To these developers, the scripting platform in effect, provides an experience similar to that offered by serverless or FaaS platforms. It is important to note that the similarities are limited to the developer experience (DevEx); the runtime is a custom implementation that is not designed to support general purpose serverless use cases. A few years of developing and operating this platform for a diverse set of developers has yielded several DevEx learnings for us. Here’s our biggest takeaway:In serverless, a combination of smaller deployment units and higher abstraction levels provides compelling benefits, such as increased velocity, greater scalability, lower cost and more generally, the ability to focus on product features. However, operational concerns are not eliminated; they just take on new forms or even get amplified. Operational tooling needs to evolve to meet these newer requirements. The bar for developer experience as a whole gets raised.This is the first in a series of posts where we draw from our learnings to outline various aspects that we are addressing in the next generation of our platform. We believe these aspects are applicable to general purpose serverless solutions too. Here we will look at application development, delivery and code composition. Future posts will delve into topics such as deployments, insights, performance, scalability and other operational concerns.How effortless is the local development experience?Our scripting platform allows developers to write functions that contain application logic. Developers upload their code (a script) to the scripting platform which provides the runtime and also handles infrastructure concerns like API routing and scaling. The script is addressable via an HTTP route (aka endpoint) defined by the developer and executes on a remote VM.By definition, a remote runtime model prevents the user’s script from being executable locally, which adds a lot of friction to the develop-test iterations. Even if local changes are somehow seamlessly deployed, turnaround time (even if only a few tens of seconds) is extremely painful for developers.To alleviate this, we have a cloud-based REPL for interactive exploration and execution of scripts. However, we observe that scripts are rarely simple functional units. Over time they tend to become more like nano-services with logic spread across multiple modules and source files — a REPL simply does not scale for real production scripts. Nor does it cover requirements such as supporting a user’s preferred IDE or allowing debugging via breakpoints.We also notice anti-patterns starting to creep-in — developers favor verbose debug logging or other defensive measures just to avoid upload iterations. This also introduces risks like accidental exposure of sensitive data in debug logs. These experiences have led us to prioritize a first class, low latency local development experience with support for live-reload, debugging and emulating the cloud execution environment for the next generation of our platform.Are deployment artifacts portable, easy to version and manage?In our current platform, we focus on three aspects of deployment artifactsPortable, immutable artifacts are necessary in order to ensure that code behaves consistently across environments and can be promoted to production with confidence. Since our platform runs on the JVM, a JAR file was the obvious choice to achieve this.Once built, an artifact needs to be addressable in any environment as it makes its way through the delivery pipeline from lower level environments all the way through to production. The simplest scheme we found was to use a name and an optional version. Given the name and version, any environment or region can address the artifact and serve up the script for execution. While it sounds simple, a human readable naming model frees users up from having to work with opaque system generated resource identifiers that are harder to reason about.We also attach rich metadata to the artifact that includes things like runtime (e.g. Java version), TTL, SCM commit and more. Such metadata powers various use cases. For instance the commit pointer enables traceability of source code across releases and environments and also enables automating upgrade flows. Proactively including such metadata helped unlock solutions to use cases such as lifecycle management of resources, that are unique to serverless.Overall our approach works well for our users, but as with every technology, things sometimes end up being used in ways that vary significantly from the original intent. As an example, since we made version optional, we saw a pattern of teams crafting custom versioning schemes into their resource names, thus unnecessarily reinventing the wheel. Our re-architecture efforts address this in a couple of ways:What are the implications of increased development velocity?It is extremely easy to deploy code changes in our platform and we certainly see developers leverage this capability to its fullest extent. This in turn highlights a few stark differences between pre-production and production environments.As an example, frequent developer commits coupled with CI/CD runs result in nearly 10x greater deployment velocity in pre-production environments. For every 10 to 20 test deployments, only one might make it into production. These test deployments are typically short-lived, but they create a maintenance burden for developers by leaving behind a trail of unused deployments that need to be cleaned up to conserve resources.Another difference is that the volume of traffic is vastly lower than in production. In conjunction with short-lived deployments, this has the unfortunate effect of aggravating cold start issues for the script. As a result, development and test runs often trip over unpredictable execution latencies, and it becomes hard to tune client timeouts in a consistent way. This leads to an undesirable situation where developers rely on production for all their testing, which in turn defeats isolation goals and leads to tests eating into production quotas.Finally, the remote execution model also dictates that a different set of features (like running integration tests, generating code coverage, analyzing execution profiles etc.) are required in pre-production but risky or not viable in production.Given all this, the key learning is to have well defined, isolated and easy to use pre-production environments from the get-go. In the next generation of our platform we are aiming to have the pre-production experience be on par with production, with its own line of feature evolution and innovation.Can fine-grained functions be composed rapidly with confidence?As adoption of our scripting platform grew, so did the requirement for reusing code that implemented shared functionality. Reusing by copy pasting scales only to a certain extent and adds extra operational costs when changes are needed to a common component. We needed a solution to easily compose loosely coupled components dynamically, while providing insight into code reuse. We achieve this by first-classing the notions of dynamic shared libraries and dependency management, coupled with the ability to resolve and chain direct and transitive dependencies at runtime.A key early learning here is that shared module producers and consumers have differing needs around updates. Consumers want tight control over when they pick up changes, while providers want to be highly flexible and decoupled from consumers in rapidly pushing out updates.Using semantic versioning to support specification and resolution of dependencies helps support both sides of this use case. Providers and consumers can negotiate a contract according to their preferences by using strict versions or semantic version ranges.In such a loosely coupled model, sustaining the rate of change at scale requires the ability to gain insight into the dependency chain in both directions. Our platform provides consumers and providers the ability to track updates to shared modules they depend on or deliver.Here’s how such insights help our users:In a future post, we will take a look at operational concerns. Stay tuned!",https://medium.com/netflix-techblog/developer-experience-lessons-operating-a-serverless-like-platform-at-netflix-a8bbd5b899a0?source=collection_home---4------15---------------------,0,netflix,,NULL,2017-07-11
Content Popularity for Open Connect,"There are many reasons why Netflix cares about the popularity of our TV shows and movies. On the Open Connect Content Delivery team, we predict content popularity to maximize our infrastructure efficiency.Some months ago, we blogged about how we use proactive caching to keep the content on our global network of Open Connect Appliances (OCAs) current. We also recently gave an overview of some of the data science challenges where Open Connect and our Science & Algorithms teams collaborate to optimize our CDN. In this post, we delve deeper into one of these areas — how and why we predict content popularity.From the Content Delivery perspective, we view popularity as the number of times a piece of content is watched. We compute it by dividing total bytes streamed from this asset by the size in bytes of the asset.As we described in this blog, the Open Connect global CDN consists of servers that are either physically located in ISP data centers (ISP servers) or IXP data centers (IX servers). We aim to serve as much of the content as possible over the shortest networking path. This maximizes the streaming experience for our members by reducing network latencies.Given the finite amount of disk space available per server and the large size of the entire Netflix catalog, we cannot fit all content in every cluster of co-located servers. Many clusters that are proximally located to end-users (ISP clusters) do not have enough disk capacity to fit the entire Netflix catalog. Therefore, we cache only the most popular content on these clusters.At locations that deliver very large amounts of traffic, we use a tiered infrastructure — high throughput servers (up to 100Gbps) are used to serve very popular content and high capacity storage servers (200TB+) are used to serve the tail of the catalog. We need to rank content based on popularity to properly organize it within these tiers.Within a cluster, we replicate titles over N servers, where N is roughly proportional to the popularity of that content.An extremely popular file, if deployed only on a single server, can overwhelm the resources of that server — while other servers may remain underutilized. This effect is not as pronounced in our deployment environment due to two crucial optimizations:However, we still keep multiple copies for the following reasons.Consistent Hashing is used to allocate content to multiple servers within a cluster. While consistent hashing on its own typically results in a reasonably well-balanced cluster, the absolute traffic variance can be high if every file is served from a single server in a given location.As an example:If we try to distribute from a pile of very large rocks into multiple buckets, even with a great allocation algorithm, it is more likely that the buckets will not all have the same weight. However, if we had a pile of pebbles, then we can balance the weights with higher probability. Analogously, high popularity content (large rocks) can be broken down into less popular content (pebbles) simply by deploying multiple copies of this content.It is desirable to keep servers evenly balanced so that as traffic increases, each server reaches peak utilization at the same overall traffic level. This allows us to maximize the amount of traffic served by the entire cluster.In the event that a server has failed, all of the traffic bound to that server needs to be delivered from other servers in the same cluster — or, from other more distant locations on the network. Staying within the same cluster, therefore minimizing network distance, is much preferable — especially when it comes to very popular content. For this reason, we ensure that we keep multiple replicas of the most popular content in the same cluster.In addition, we replicate some mid-tier content as an insurance against traffic being amplified unexpectedly — for example, because of sudden social media attention for a celebrity.Every title is encoded in multiple formats, or encoding profiles. For example, some profiles may be used by iOS devices and others for a certain class of Smart TVs. There are video profiles, audio profiles, and profiles that contain subtitles.Each audio and video profile is encoded into different levels of quality. For a given title, the higher the number of bits used to encode a second of content (bps), the higher the quality. (For a deeper dive on per-title encode optimization, see this past blog.) Which bitrate you stream at depends on the quality of your network connection, the encoding profiles your device supports, the title itself, and the Netflix plan that you are subscribed to.Finally, we have audio profiles and subtitles available in multiple languages.So for each quadruple of (title, encoding profile, bitrate, language), we need to cache one or more files. As an example, for streaming one episode of The Crown we store around 1,200 files!For a cluster that is set up to service a certain segment of our traffic, caching efficiency is the ratio of bytes served by that cluster versus overall bytes served for this segment of traffic.From the perspective of our ISP partners, we like to measure this on a per-network basis. We also measure this on a per-cluster basis for optimizing our own infrastructure.Maximizing caching efficiency at the closest possible locations translates to lesser network hops. Lesser network hops directly improves user streaming quality, and also reduces the cost of transporting network content for both ISP networks and Netflix. Furthermore, maximizing caching efficiency makes responsible and efficient use of the internet.Although our proactive content updates are downloaded to servers during off-peak hours when streaming traffic is at a minimum, we still strive to minimize the amount of content that has to be updated day-over-day — a secondary metric we call content churn. Less content updates can lead to lower costs for both our ISP partners and Netflix.As described briefly in our earlier blog, we predict future viewing patterns by looking at historical viewing patterns. A simple way to do this could be to look at the content members watched on a given day and assume that the same content will be watched tomorrow. However, this short-sighted view would not lead to a very good prediction. Content popularity can fluctuate, and responding to these popularity fluctuations haphazardly could lead us to unnecessary content churn. So instead, we smooth data collected over multiple days of history to make the best prediction for the next day.We have the following models that compute content popularity at different levels of aggregation:In 2016, we migrated most of our clusters from title level to file level rankings. With this change, we were able to achieve the same caching efficiency with 50% of storage!Orthogonally to the above 2 levels of aggregation, we compute content popularity on a regional level. This is with the intuitive presumption that members from the same country have a similar preference in content.As mentioned above, historical viewing is used for ranking content that has been on Netflix for at least a day. For content that is launching on Netflix for the first time, we look at various internal and external forecasts to come up with a prediction of how a title will perform. We then normalize this with ‘organic’ predictions.For some titles, we adjust this day 1 prediction by how heavily the title will be marketed. And finally, we some time use human judgement to pin certain upcoming titles high in the popularity ranking to ensure that we have adequate capacity to serve them.We are always evaluating and improving our popularity algorithms and storage strategies. If these kinds of large scale challenges sound interesting to you, check out our latest job postings!",https://medium.com/netflix-techblog/content-popularity-for-open-connect-b86d56f613b?source=collection_home---4------17---------------------,0,netflix,"docker,backend,cloud",NULL,2017-06-20
A/B Testing and Beyond: Improving the Netflix Streaming Experience with Experimentation and Data Science,"by Nirmal GovindGolden Globes for The Crown. An Oscar win for The White Helmets. It’s an exciting time to be a Netflix member, with the power to stream such incredible content on a Smart TV or a mobile phone, from the comfort of one’s home or on a daily commute.With a global launch in January 2016 that brought Netflix to over 130 new countries, Netflix is now a truly global Internet TV network, available in almost every part of the world. Our member base is over 100 million strong, and approximately half of our members live outside the US. Since more than 95% of the world’s population is located outside the US, it is inevitable that in the near future, a significant majority of Netflix members will be located overseas. With our global presence, we have the opportunity to watch, learn, and improve the service in every part of the world.A key component of having a great Internet TV service is the streaming quality of experience (QoE). Our goal is to ensure that you can sit back and enjoy your favorite movie or show on Netflix with a picture quality that delights you and a seamless experience without interruptions or errors. While streaming video over the Internet is in itself no small feat, doing it well at scale is challenging (Netflix accounts for more than a third of Internet traffic at peak in North America). It gets even more complex when we’re serving members around the world with not only varying tastes, network infrastructure, and devices, but also different expectations on how they’d like to consume content over the Internet.How do we ensure that the Netflix streaming experience is as enjoyable in São Paulo, Mumbai, and Bangkok as it is in San Francisco, London, or Paris? The engineers and scientists at Netflix continuously innovate to ensure that we can provide the best QoE possible. To enable this, Netflix has a culture of experimentation and data-driven decision making that allows new ideas to be tested in production so we get data and feedback from our members. In this post, I’ll focus on the experimentation that we do at Netflix to improve QoE, including the types of experiments we run, the key role that data science plays, and also how the Netflix culture enables us to innovate via continuous experimentation. The post will not delve into the statistics behind experimentation but I will outline some of the statistical challenges we’re working on in this space.We also use data to build Machine Learning and other statistical models to improve QoE; I will not focus on the modeling aspect here but refer to this blog post for an overview and this post highlights one of our modeling projects. It’s also worth noting that while the focus here is on QoE, we use experimentation broadly across Netflix to improve many aspects of the service, including user interface design, personalized recommendations, original content promotion, marketing, and even the selection of video artwork.Before getting to the experiments that we run, it’s useful to develop some intuition around why a structured approach to experimentation is not just a nice-to-have but a necessary part of innovation.Experiments and empirical observation are the most important part of the scientific method, which allows engineers and scientists to innovate by formulating hypotheses, gathering data from experiments, and making conclusions or formulating new hypotheses. The scientific method emphasizes an iterative learning process, alternating between deduction and induction (see figure below, courtesy of the famous statistician George Box).Deduction is the process of going from an idea or theory to a hypothesis to actual observations/data that can be used to test the hypothesis, while induction is the process of generalizing from specific observations/data to new hypotheses or ideas. Experimentation plays a critical role in collecting data to test hypotheses and enabling the deduction-induction iterations as part of the scientific method.Experimentation is a data-based approach to ensure we understand the impact of the changes we make to our service. In our case, we’d like to understand the impact of a new streaming-related algorithm or a change to an existing algorithm. Usually, we’re interested in answering two questions: 1) How does the change (“treatment”) affect QoE metrics?, and 2) What effect does the change have on member behavior: do our members prefer the new experience or the old one?In general, an experiment allows us to obtain a causal read on the impact of a change. i.e., it allows us to make a claim, with some degree of confidence, that the result we’re seeing was caused by the change we made. In controlled experiments such as A/B tests, proper randomization ensures that the control and treatment groups in a test differ only in the experience or “treatment” they receive, and other factors (that may or may not affect the experiment’s results) are present in equal proportions in both groups. This makes A/B testing a popular approach for running experiments and determining if experience “A” or experience “B” works better.It’s worth noting that experiments help establish causation as opposed to relying on correlation in observed data. In this regard, experimentation may be thought of as being superior to most ML approaches that are based on observational data. We do spend a significant amount of effort in researching and building ML models and algorithms. Carefully exploiting patterns in observed data is powerful for making predictions and also in reaffirming hypotheses, but it’s even more powerful to run experiments to get at causation.Last but not least, experiments are a powerful way to let data guide decision-making. Making decisions based on data from experiments helps avoid the HiPPO (Highest Paid Person’s Opinion) problem, and also ensures that intuition alone does not drive decisions. When combined with human judgment, experiments are a powerful tool to ensure that the best ideas win. Culture plays an important role here, more on that later in the post.There are several aspects that determine QoE, and I’ll provide a brief overview of three key components before getting into the types of experiments we run at Netflix to improve QoE.For each movie or episode of a show we stream, the encoding process creates files at different video quality levels (bitrates), which are then cached on our servers distributed around the world. When a member initiates play, client-side adaptive streaming algorithms select the best bitrate to stream based on network and other considerations, and server-side algorithms determine how best to send packets of data over to the client.Let’s take a closer look at these components, starting with the algorithms that run on a member’s device.A key part of ensuring that members have great QoE is the code that runs on the device used for streaming. Netflix is available on thousands of devices ranging from mobile phones and tablets to game consoles, computers, and Smart TVs. Most of these devices run adaptive streaming algorithms developed by Netflix that decide what bitrate should be selected at various times during a streaming session. These bitrate selection decisions determine the quality of the video on the screen and also directly influence how quickly the local buffer on the device is depleted. When the buffer runs out, playback is interrupted and a “rebuffer” occurs.We obsess over great playback experiences. We want playback to start immediately, at great quality, and we never want playback to stop unexpectedly. But in reality, network effects or last mile connectivity issues may make this impossible to achieve. What we can do is design algorithms that can quickly detect changes in network throughput and make adjustments in real-time to provide the best experience possible.Given the large number of networks, network conditions, and device-level limitations as we serve content to millions of members around the world, it’s necessary to rely on the scientific method to tune existing algorithms and develop new algorithms that can adapt to a variety of scenarios. The adaptive streaming engineers use experimentation to develop and continuously improve the algorithms and configurations that provide the best experience for each streaming session on Netflix.Open Connect is Netflix’s Content Delivery Network (CDN), and it’s responsible for serving the video and audio files needed to play content during a streaming session. At a high level, Open Connect allows us to locate content as close as possible to our members in order to maximize delivery efficiency and QoE. The Open Connect team does this by partnering with Internet Service Providers (ISPs) to localize their Netflix traffic by embedding servers with Netflix content inside the ISP network. Open Connect also peers with ISPs at interconnect locations such as Internet Exchanges around the world. For more on how Open Connect works, check out this blog post.The engineers in Open Connect optimize both the hardware and the software on the servers used to serve Netflix content. This allows us to tune the server configuration, software, and algorithms for the specific purpose of video streaming. For example, caching algorithms determine what content should be stored on servers distributed around the world based on what content is likely to be watched by members served by those servers. Engineers also develop network transport algorithms that determine how packets of data are sent across the internet from the server to a member’s device. For more on some of the problems in this space, refer to this blog post.Similar to adaptive streaming on the client-side, experimentation enables rapid iteration and innovation in Open Connect as we develop new architectures and algorithms for content delivery. There is additional complexity in this area due to nature of the system; in some scenarios, it’s impractical to do a controlled randomized experiment, so we need to adapt experimental techniques to get a causal read. More on this further below.The perceptual quality of content is also an important aspect of streaming and has a direct impact on what’s seen on the screen. Perceptual quality is tied to a process called encoding, which compresses the original “source” files corresponding to a movie or show into smaller files or “encodes” at different bitrates. The encoding algorithms are an active area of innovation at Netflix, and our encoding team has made some significant advancements to provide better perceptual quality at a given network bandwidth or use less bits at a given quality level. More recently, the engineers have been working on more efficient encodes for low-bandwidth streaming.Encoding changes pose a different challenge for experimentation as these changes are usually specific to the content in each movie or show. For example, the effect of an encoding change may be different for animated content versus an action-packed thriller. In addition, it’s also important to ensure that encoding changes are compatible with the client application and the decoders on the devices used to stream Netflix.Before we roll out a new encoding algorithm, which also means re-encoding the entire Netflix catalog, the encoding team runs experiments to validate changes and measure the impact on QoE. The experimental design for such experiments can be challenging due to content-specific interactions and the need to validate on sufficiently diverse content and devices.Let’s take a look at the types of experiments we run at Netflix across the areas outlined above to improve QoE. Broadly, there are three classes of experiments we run to improve QoE and understand the impact of QoE on member behavior.The goal of system experiments is to establish whether a new algorithm, change to an existing algorithm, or a configuration parameter change has the intended effect on QoE metrics. For example, we have metrics related to video quality, rebuffers, play delay (time between initiating playback and playback start), playback errors, etc. The hypotheses for these experiments are typically related to an improvement in one or more of these metrics.System experiments are usually run as randomized A/B experiments. A system test may last a few hours or may take a few days depending on the type of change being made, and to account for daily or weekly patterns in usage and traffic. Our 100 million strong member base allows us to obtain millions of “samples” relatively quickly, and this allows for rapid iteration and multiple system experiments to be run sequentially to optimize the system.From an experimenter’s standpoint, these fast-paced system experiments allow for exploration of new experimentation methodologies. For example, we can test new strategies for allocation to control and treatment groups that allow us to learn quickly. We’re also adapting Response Surface Methodology techniques to build statistical models from experimental data that can reduce the number of iterations needed to achieve a set goal.Testing in this area poses a number of challenges that also motivate our research; below are a couple examples.The distributions of most QoE metrics are not Gaussian and there is a need for hypothesis testing methods that account for such distributions. For this reason, we make heavy use of nonparametric statistical methods in our analysis to establish statistical significance. Nonparametric methods on really large datasets can be rather slow, so this is an active area of research we’re exploring.Furthermore, in these experiments, we typically measure several QoE metrics, some of them correlated, across multiple treatment cells, and need to account for the multiple testing problem.Most of our system experiments are controlled randomized A/B experiments. However, in certain situations where randomization isn’t feasible, we resort to other approaches such as quasi-experiments and causal inference.One area where we’re exploring quasi-experiments is to test changes to algorithms in Open Connect. Consider an Internet Exchange with two identical server (or cache) clusters where one cluster serves member traffic from ISP #1 and the other cluster serves traffic from ISP #2. If we’re interested in testing a new algorithm for filling content on caches, ideally we would run an A/B experiment with one cache cluster being control and the other being the treatment. However, since the traffic to these clusters cannot be randomized (peering relationships are difficult to modify), an A/B experiment is not possible.In such situations, we run a quasi-experiment and apply causal inference techniques to determine the impact of the change. Several challenges abound in this space such as finding a matching control cluster, determining the appropriate functional relationship between treatment and control, and accounting for network effects.Experiments designed to understand the impact of changes on Netflix member behavior are called Consumer Science experiments. Typically, these experiments are run after several iterations of system experiments or quasi-experiments are completed to confirm that the new algorithm or configuration change has the intended effect on QoE metrics. This allows us to study the impact of QoE changes on member behavior: 1) Do members watch more Netflix if they have better video quality or lower rebuffers or faster playback start, and 2) Do they retain better after the free trial month ends and in subsequent months?We can also study the effect on member behavior of making tradeoffs amongst QoE metrics: do members prefer faster playback start (lower play delay) with lower video quality or do they prefer to wait a bit longer but start at a higher quality?Consumer Science experiments typically run for at least one month so we can get a read on member retention after the free month for new members. An interesting challenge with these experiments is to identify segments of the member base that may differ in their expectations around QoE. For example, a change that drastically reduces play delay at the expense of lower initial video quality may be preferred by members in parts of the world with poorer internet connectivity, but the same experience may be disliked by members on stable high-speed internet connections. The problem is made more difficult due to the fact that changes in QoE may be subtle to the member, and it may take a while for behavior changes to manifest as a result of QoE shifts.Last but not least, I’d like to discuss how company culture plays an important role in experimentation. The Netflix culture is based on the core concept of “freedom and responsibility”, combined with having stunning colleagues who are passionate and innovative (there’s more to our culture, check out the Netflix culture deck). When you have highly talented individuals who have lots of great ideas, it’s important to have a framework where any new idea can be developed and tested, and data, not opinion, is used to make decisions. Experimentation provides this framework.Enabling a culture of experimentation requires upfront commitment at the highest level. At Netflix, we look for ways to experiment in as many areas of the business as possible, and try to bring scientific rigor into our decision-making.Data Science has a huge role to play here in ensuring that appropriate statistical rigor is applied as we run experiments that determine the kind of product and service that our members experience. Data Science is also necessary to come up with new ideas and constantly improve how we run experiments at Netflix, i.e. to experiment with our approach to experimentation. Our data scientists are heavily involved in the design, execution, analysis, and decision making for experiments we run, and they also work on advancing experimentation methodology.In addition to the science, it’s also important to have the infrastructure in place to run experiments and analyze them, and we have engineering teams that are focused on improving our experimentation platform. This platform enables automation of the steps needed to kick off an experiment as well as automated generation of analysis reports and visualizations during various phases of the experiment.Netflix is leading the Internet TV revolution and we’re changing how people around the world watch movies and TV shows. Our data scientists and engineers work on hard problems at scale in a fast-paced and fun environment. We entertain millions of people from all walks of life with stories from different cultures, and it is both inspiring and truly satisfying. We’re hiring so reach out if you’d like to join us in this amazing journey![This article is cross-posted on LinkedIn here.]",https://medium.com/netflix-techblog/a-b-testing-and-beyond-improving-the-netflix-streaming-experience-with-experimentation-and-data-5b0ae9295bdf?source=collection_home---4------18---------------------,0,netflix,,NULL,2017-06-13
Evolving the Netflix Data Platform with Genie 3,"by Tom GianosThe big data space continues to change at a rapid pace. Data scientists and analysts have more tools than ever at their disposal whether it be Spark, R, Presto, or traditional engines like Hive and Pig.At Netflix the Big Data Platform team is responsible for making these tools available, reliable and as simple as possible for our users at massive scale. For more information on our overall architecture you can see our talks at Strata 2017, QCon 2016, re:Invent 2016 or find others on our Netflix Data YouTube channel.Genie is one of the core services in the Netflix data platform. It provides APIs for users to access computational resources without worrying about configuration or system state. In the past, we’ve written about the motivations to develop Genie and why we moved to Genie 2. This post is going to talk about new features in the next generation of Genie (i.e., Genie 3) which enable us to keep up with Netflix scale, evolution of tools, and expanding use cases. We will also explore some of our plans for Genie going forward.Genie 3 has been running in production at Netflix since October 2016 serving about 150k jobs per day (~700 running at any given time generating ~200 requests per second on average) across 40 I2.4XL AWS EC2 instances.Within Netflix we use Genie in two primary ways. The primary use case is for users to submit job requests to the jobs API and have the job clients run on the Genie nodes themselves. This allows various systems (schedulers, micro-services, python libraries, etc) at Netflix to submit jobs and access the data in the data warehouse without actually knowing anything about the data warehouse or clusters themselves.A second use case which has evolved over time is to leverage Genie’s configuration repository to set up local working directories for local mode execution. After Genie sets up the working directory, it will return control to the user who can then invoke the run script as needed. We use this method to run REPL’s for various engines like Hive, Spark, etc. which need to capture stdout.While Genie 3 has many new features, we’re going to focus on a few of the bigger ones in this post including:In Genie 2, we spent a lot of time reworking the data model, system architecture and API tier. What this left out was the execution engine which is responsible for configuring and launching jobs after a request is received by the jobs API. The reasoning was the execution piece worked well enough for the use cases that existed at the time. The execution engine revolved around configuring a job directory for each job in a rigid manner. There was a single job execution script which would be invoked when setup was complete for any type of job. This model was limited as the set of tools we needed to use grew and a single script couldn’t cover every case. It became increasingly complex to maintain the script and the code around it.In Genie 3, we’ve rewritten the entire execution engine from the ground up to be a pluggable set of tasks which generate a run script custom for each individual job. This allows the run script to be different based on what cluster, command and application(s) are chosen at runtime by the Genie system. Additionally, since the script is now built up dynamically within the application code, the entire job flow is easier to test and maintain.These changes have resulted in an ability for our team to respond to customer requests more quickly as we can change individual application or command configurations without fear of breaking the entire run script.In Genie 2 every node was treated equally, that is they all would run a set of tasks intended for system wide administration and stability. These tasks included database cleanup, zombie job detection, disk cleanup and job monitoring. This approach was simpler but had some downsides and inefficiencies. For example, all nodes would repeatedly perform the same database cleanup operations unnecessarily. To address this it would be best for cluster wide administration tasks to be handled by a single node within the cluster.Leadership election has been implemented in Genie 3, currently supported via either Zookeeper or statically setting a single node to be the leader via a property. When a node is elected as leader, a certain set of tasks are scheduled to be run. The tasks need only implement a LeadershipTask interface to be registered and scheduled by the system at runtime. They can each be scheduled at times and frequencies independent to each other via either cron based or time delay based scheduling.Genie allows users to run arbitrary code, via job attachments and dependencies, as well as the ability to access and transport data in the data warehouse back to the Genie node. It’s become increasingly important to make every effort to ensure the ability to perform these actions are allowed only by people authorized to do so. We don’t want any users who aren’t administrators changing configurations which could break the system for all other users. We also don’t want anyone not authenticated to be able to access the Genie UI and jobs results as the output directories could have sensitive data.Therefore a long requested set of features have been added in Genie 3 to support application and system security. First, authentication and authorization (authn/authz) have been implemented via Spring Security. This allows us to plugin backend mechanisms for determining who a user is and decouple the decision of authorization from Genie code. Out of the box Genie currently supports SAML based authentication for access to the user interface and OAuth2 JSON Web Token (JWT) support for API access. Other mechanisms could be plugged in if desired.Additionally, Genie 3 supports the ability to launch job processes on the system host as the user who made the request via sudo. Running as users helps prevent a job from modifying another job’s working directory or data since it won’t have system level access.As Genie becomes more flexible the data platform team has moved from installing many of the application binaries directly on the Genie node to having them downloaded at runtime on demand. While this gives us a lot of flexibility to update the application binaries independently of redeploying Genie itself, it adds latency as installing the applications can take time before a job can be run. Genie 3 added a dependency file cache to address this issue. Now when a file system (local, S3, hdfs, etc) is added to Genie it needs to implement a method which determines the last updated time of the file requested. The cache will use this to determine if a new copy of the file needs to be downloaded or if the existing cached version can be used. This has helped to dramatically speed up job startup time while maintaining the aforementioned agility for application binaries.There are many other changes made in Genie 3 including a whole new UI (pictured above), data model improvements, client resource management, additional metrics collection, hypermedia support for the REST APIs, porting the project to Spring Boot and much more. For more information, visit the all new website or check out the Github release milestones. If you want to see Genie 3 in action try out the demo, which uses docker compose so no additional installation or setup necessary beyond docker itself.While a lot of work was done in the Genie 3 release there are still a lot of features we’re looking to add to Genie to make it even better. Here are a few:Genie continues to be an integral part of our data ecosystem here at Netflix. As we continue to develop features to support our use cases going forward, we’re always open to feedback and contributions from the community. You can reach out to us via Github or message on our Google Group. We hope to share more of what our teams are working on later this year!",https://medium.com/netflix-techblog/evolving-the-netflix-data-platform-with-genie-3-598021604dda?source=collection_home---4------19---------------------,0,netflix,"sql,database,backend,python3,python",NULL,2017-06-07
Simone - A Distributed Simulation Service,"By Satyajit Thadeshwar, Mayank Agarwal, Sangeeta Narayanan & Kevin LewHundreds of models of smart TVs, game consoles, mobile devices, and other video streaming devices get shipped with a Netflix app pre-installed. Before shipping these devices, manufacturers need to have the app certified on their device firmware. Certification involves running a series of tests that validate the behavior of the Netflix app under different positive & negative scenarios, and this process is repeated each time a new model of a device is released to the market.Netflix provides its device partners with a scalable and automatable cloud-based testing platform to accomplish this. An integral part of this platform is Simone, a service that allows simulation of different conditions required for testing. Simone is a service that enables configuration, deployment, and execution of simulations within arbitrary domains throughout the Netflix environment.Testing and certifying Netflix apps on devices which talk to services in a cloud-based, distributed environment like Netflix can be hard and error-prone. Without Simone, a tester would need to coordinate a request sent by the Netflix app to the individual service instance where it might land, a process which is tedious and difficult to automate, especially at scale. Additionally, devices at the certification stage are immutable, and we cannot change their request behavior. So we need to simulate various conditions in the Netflix services in order to test the device. For example, we need to simulate the condition where a user has exhausted the maximum number of simultaneous screens allowed based on a subscription plan.Simone also allows testers to certify devices against services deployed in a production environment. The implication of running in production is that there is a potential to adversely impact the customer experience. Simone is designed to minimize the blast radius of simulations and not introduce latency to normal production customer requests. The Architecture section will describe this further.First, we will go over some of the main concepts of Simone. Later, we will see how each of these concepts come together to provide a simulation workflow.Template: The simulation that a service owner exposes is encapsulated in a schema, which is called a Template. A template defines the override behavior and provides information on what arguments it accepts, if any, and under what conditions the override is triggered. Templates are domain specific; they are created and maintained by the service owners. Below is a snippet from a template used to force an error when retrieving a DRM license:Variant: A Variant, which is an immutable instance of a template, is at the core of a simulation. When testers want to create a simulation, they create a Variant of a template which defines the overridden behavior. The service then uses this Variant to provide a simulated response. Below is a sample Variant that tells the service to fail the license request for a playback. This is to simulate the “concurrent stream limit reached” scenario, where more than a specific number of concurrent playbacks are not allowed for a given Netflix service plan.The service which handles the request changes the response based on the arguments specified in the Variant. Each Variant has a set expiration strategy which indicates when a Variant expires. An expiration strategy is needed to control the number of requests a Variant can affect and to clean up unused Variants. Currently, only the execution count is supported, which means “evict this Variant after it has been executed the specified number of times”.Trigger: Notice the trigger and trigger arguments specified in the Variant definition above. A Trigger specifies under what conditions this Variant should be applied. In this case, when a DRM license request originates from a Netflix device which has the ESN “NFXXX-XXX-00000001”, the Variant will be applied. An ESN is a device’s electronic serial number, which is a unique identifier for the device that has Netflix app installed on it.Triggers are defined in such a way that a Variant has a very narrow scope, such as a device ESN or a customer account number. This prevents an incorrectly defined variant from inadvertently affecting normal production requests. Additionally, the trigger implementation adds minimal computation overhead during evaluation for each request and we are continuously looking for ways to reduce it.Below is an architecture diagram of Simone. It is useful to understand the workflow of a Simone simulation.At a high level, there are three main components which are responsible for Simone; shown as highlighted blocks in the architecture diagram above.Simone server is a Java service that provides Create, Read & Delete operations for Variants and Templates. Testers create Variants on the server either via REST APIs — or through Simone Web UI. Simone server stores the Variant and Template data in Cassandra, which is replicated across multiple AWS regions so that testers don’t need to create Variants in each region. The server uses Apache Kafka to make Variants available to all instances of the domain service. The Kafka topic data is also replicated across the same AWS regions, using Apache MirrorMaker.Simone client is the interface through which domain services interact with Simone server to perform the operations mentioned above. Simone client subscribes to a Kafka topic for Variant create & delete events and maintains them in an in-memory cache.Simone Web UI provides the ability to create, view, and delete variants on Simone server. It also provides insights into the lifecycle of a variant and the underlying simulations.As shown in the workflow diagram above, when a Variant is created on Simone server, it publishes a CREATE event with Variant data to a dedicated Kafka topic. Simone client instances running within the context of domain services subscribe to this topic. When a Simone client gets the CREATE event about a Variant, it captures and stores the Variant data it in a local in-memory cache of created Variants. This way, when a production request hits any of these servers, Simone client does not need to make an external request to check if that particular request has any overrides configured. This helps avoid the introduction of additional significant latency in the request path.If the request matches the trigger parameters of a Variant, then Simone client takes over the execution of the template action for that action. This in turn means running the simulation defined in that template. For example, “if a request comes in for this customer account number, send a different, overridden response instead of the regular response”. While executing the simulation, Simone client sends two important messages to Simone server — a synchronous CONSUME request and an APPLY event, which are published to Elasticsearch for querying later.This lifecycle can also be visualized in the Insights view of Simone Web UI as shown in Figure 4 below.Simone Web UI provides users the ability to view existing Templates and associated metadata about those templates. Users can create, delete, and search for Variants through the Web UI.The Web UI also provides insights into the Variant lifecycle and the underlying simulation. In addition to the CONSUME and APPLY events mentioned previously, Simone server also publishes three other events to Elasticsearch — CREATE (when a variant is created), DELETE (when a variant is deleted) and RECEIVED (when a variant is received by a given Simone client instance). The RECEIVE event contains the AWS EC2 instance id of the domain service, which is helpful in troubleshooting issues related to simulations.Now that you have seen the details, let’s walk through our initial example of simulating the concurrent streams error using Simone, and how that helps testing and certification within Netflix.A very simple but useful application of Simone is to force a service to return various types of application errors. For example, Netflix has different streaming plans that allow different maximum numbers of concurrent streams. So a user with 2 Streams plan will only be allowed to watch on 2 devices simultaneously. Without Simone, a user would have to manually play Netflix on more than 2 devices to simulate an error when trying to start playback on a 3rd device.Simone allows a user to create a Variant to force all playback attempts for a device to fail the license request with a “CONCURRENT_STREAM_QUOTA_EXCEEDED”. Below is what that Variant would look like.Once this Variant is created, any playback attempt from the ESN “NFXXX-XXX-00000001” will fail with the error, “CONCURRENT_STREAM_QUOTA_EXCEEDED”. This will result in the user seeing such an error as the one below .To sum up, our goal is to provide our members with the best possible Netflix streaming experience on their devices of choice. Simone is one tool that helps us accomplish that goal by enabling our developers and partners to execute end to end simulations in a complex, distributed environment. Simone has unlocked new use cases in the world of testing and certification and highlighted new requirements as we look to increase the testability of our services. We are looking forward to incorporating simulations into more services within Netflix. If you have an interest in this space, we’d love to hear from you!",https://medium.com/netflix-techblog/https-medium-com-netflix-techblog-simone-a-distributed-simulation-service-b2c85131ca1b?source=collection_home---4------20---------------------,0,netflix,"matlab,python",NULL,2017-06-06
Introducing Aardvark and Repokid,"by Jason Chan, Patrick Kelley, and Travis McPeakToday we are pleased to announce two new open-source cloud security tools from Netflix: Aardvark and Repokid. Used together, these tools are the next logical step in our goal to run a secure, large scale Amazon Web Services (AWS) deployment that accommodates rapid innovation and distributed, high-velocity development. When used together, Aardvark and Repokid help us get closer to the principle of least privilege without sacrificing speed or introducing heavy process. In this blog post we’ll describe the basic problem and why we need tools to solve it, introduce new tools that we’ve developed to tackle the problem, and discuss future improvements to blend the tools seamlessly into our continual operations.IAM Permissions — Inside the CockpitAWS Identity and Access Management (IAM) is a powerful service that allows you to securely configure access to AWS cloud resources. With over 2,500 permissions and counting, IAM gives users fine-grained control over which actions can be performed on a given resource in AWS. However, this level of control introduces complexity, which can make it more difficult for developers. Rather than focusing on getting their application to run correctly they have to switch context to work on knowing the exact AWS permissions the system needs. If they don’t grant necessary permissions, the application will fail. Overly permissive deployments reduce the chances of an application mysteriously breaking, but create unnecessary risk and provide attackers with a large foothold from which they may further penetrate a cloud environment.Rightsizing Permissions — Autopilot for IAMIn an ideal world every application would be deployed with the exact permissions required. In practice, however, the effort required to determine the precise permissions required for each application in a complicated production environment is prohibitively expensive and doesn’t scale. At Netflix we’ve adopted an approach that we believe balances developer freedom and velocity and security best-practices: access profiling and automated and ongoing right-sizing. We allow developers to deploy their applications with a basic set of permissions and then use profiling data to remove permissions that are demonstrably not used. By continually re-examining our environment and removing unused permissions, our environment converges to least privilege over time.Introducing AardvarkAWS provides a service named Access Advisor that shows all of the various AWS services that the policies of an IAM Role permit access to and when (if at all) they were last accessed. Today Access Advisor data is only available in the console, so we created Aardvark to make it easy to retrieve at scale. Aardvark uses PhantomJS to log into the AWS console and retrieve Access Advisor data for all of the IAM Roles in an account. Aardvark stores the latest Access Advisor data in a database and exposes a RESTful API. Aardvark supports threading to retrieve data for multiple accounts simultaneously, and in practice refreshes data for our environment daily in less than 20 minutes.Introducing RepokidRepokid uses the data about services used (or not) by a role to remove permissions that a role doesn’t need. It does so by keeping a DynamoDB table with data about each role that it has seen including: policies, count of permissions (total and unused), whether a role is eligible for repo or if it is filtered, and when it was last repoed (“repo” is shortened from repossess — our verb for the act of taking back unused permissions). Filters can be used to exclude a role from repoing if, for example, if it is too young to have been accurately profiled or it is on a user-defined blacklist.Once a role has been sufficiently profiled, Repokid’s repo feature revises inline policies attached to a role to exclude unused permissions. Repokid also maintains a cache of previous policy versions in case a role needs to be restored to a previous state. The repo feature can be applied to a single role, but is more commonly used to target every eligible role in an account.Future WorkCurrently Repokid uses Access Advisor data (via Aardvark) to make decisions about which services can be removed. Access Advisor data only applies to a service as a whole, so we can’t see which specific service permissions are used. We are planning to extend Repokid profiling by augmenting Access Advisor with CloudTrail. By using CloudTrail data, we can remove individual unused permissions within services that are otherwise required.We’re also working on using Repokid data to discover permissions which are frequently removed so that we can deploy more restrictive default roles.Finally, In its current state Repokid keeps basic stats about the total permissions each role has over time, but we will continue to refine metrics and record keeping capabilities.Extending our Security Automation ToolkitAt Netflix, a core philosophy of the Cloud Security team is the belief that our tools should enable developers to build and operate secure systems as easily as possible. In the past we’ve released tools such as Lemur to make it easy to request and manage SSL certificates, Security Monkey to raise awareness and visibility of common AWS security misconfigurations, Scumblr to discover and manage software security issues, and Stethoscope to assess security across all of a user’s devices. By using these tools, developers are more productive because they can worry less about security details, and our environment becomes more secure because the tools prevent common misconfigurations. With Repokid and Aardvark we are now extending this philosophy and approach to cover IAM roles and permissions.Stay in touch!At Netflix we are currently using both of these tools internally to keep role permissions tightened in our environment. We’d love to see how other organizations use these tools and look forward to collaborating on further development.",https://medium.com/netflix-techblog/introducing-aardvark-and-repokid-53b081bf3a7e?source=collection_home---4------21---------------------,0,netflix,,NULL,2017-06-05
How Data Science Helps Power Worldwide Delivery of Netflix Content,"by Andrew BerglundHave you ever wondered where your video comes from when you watch Netflix? We serve video streams out of our own content delivery network (CDN), called Open Connect, which is tailored to one specific application: delivering internet TV to our members around the world. This system is the cornerstone of every Netflix video experience — serving 100% of our video, over 125 million hours every day, to 100 million members across the globe! In this post, we introduce some of the challenges in the content-delivery space where our data science and engineering teams collaborate to optimize the Netflix service.In order to provide the best video experience to all of our members — with peak traffic of several tens of terabits per second — Open Connect deploys and operates thousands of servers, which we call Open Connect Appliances or OCAs, throughout the world. These OCAs are deployed at internet exchange locations where internet service providers (ISPs) can connect with us and are also offered to ISP partners to embed in their own networks. Embedded appliances can serve a large fraction of the Netflix traffic requested by an ISP’s customers. This architecture benefits ISPs by reducing cost and relieving internet congestion, while providing Netflix members with a high quality, uninterrupted viewing experience.For more detail on how Open Connect works, check out this blog post.Our data scientists work to optimize the streaming quality of experience (QoE) by bringing a rigorous mathematical and statistical approach to algorithm and model development. To address some of the key data science problems in the rapidly growing content delivery space, we are building a new team focused on Content Delivery Network Science & Algorithms. The next sections introduce some of the high-level focus areas for this team.Popularity Prediction and Content CachingAn important priority for Open Connect is to serve traffic from locations as close as possible to the end user and consequently to send as few bytes as possible across the wider internet — this is a little like designing a city where everyone lives near where they work to prevent cross-town traffic congestion. Because our video catalog is too large to store everything at all locations, we need to pre-position the most popular video files at the locations where they are most likely to serve nearby user requests. These techniques are known as edge caching.In order to fully utilize the hardware capacity of our network for serving video during peak (primetime) viewing hours, we proactively cache content. That is, we forecast what will be popular tomorrow and only use disk and network resources for filling during quiet, off-peak hours (this blog post gives more details on how we fill content). This optimization is possible because our robust data on content popularity gives us enough signal to forecast daily demand with high fidelity.From the data science perspective, our goal is to accurately predict popularity and also to use these predictions to prioritize content updates. The prioritization objective is to simultaneously cache the most popular content but also minimize the number of file replacements to reduce fill traffic. For content placement, we do not need to predict popularity all the way to the user level, so we can take advantage of local or regional demand aggregation to increase accuracy. However, we need to predict at a highly granular level in another dimension: there can be hundreds of different files associated with each episode of a show so that we can provide all the encoding profiles and quality levels (bitrates) to support a wide range of devices and network conditions. We need separate predictions for each file because their size and popularity, therefore their cache efficiency, can vary by orders of magnitude.Our work in this area is a combination of time series forecasting, constrained optimization, and high-level network modeling, and our ongoing challenge is to adapt our algorithms to the dynamics of global member preferences, evolving network conditions, and new markets.Optimizing Content Allocation within ClustersAfter we use popularity prediction to decide what content to cache at each location, an important related area of data science work is to optimize the way files are distributed within a cluster of OCAs to maximize hardware utilization. We group OCAs into clusters that function together as logical units for storing content and serving user video requests. If an OCA in a cluster becomes unhealthy — due to traffic overload or any other operational issue — some traffic is steered to an alternative location. Because an imbalance in the traffic served by one machine in a cluster can become a “weak link” with respect to overall health of the cluster, it is important to maintain a balanced traffic load across the individual appliances within a cluster.Cluster performance can be addressed at several layers, including through development of new allocation algorithms for placing content into clusters. A simple way to see how content allocation affects performance is to imagine a bad algorithm that places too much highly popular content on one server — this server will quickly saturate when the other servers are not doing much work at all. To avoid this situation, we distribute content pseudo-randomly, but in a stable and repeatable way (based on consistent hashing). However, content placement with any degree of randomness can still lead to “hot spots” of anomalously high load — a phenomenon exacerbated by power-law traffic patterns or by heterogeneous clusters of different hardware types.We’re working on alternative content distribution algorithms that provide stable and repeatable placement, but are tailored to specific clusters to reduce load imbalance and maximize hardware utilization. Our work in this area involves constrained optimization — servers have finite capacity for both storage and network throughput — and a healthy dose of probability and statistics to model the fluctuations in traffic served under a given allocation algorithm.Long-Term Capacity PlanningNetflix moves fast, and Open Connect is a highly dynamic system. Planning for changes is an important challenge for many teams across the company. Some of the many phenomena that can change the system behavior are catalog changes (new content on the service), member growth, encoding advances, and a dynamic consumer electronics ecosystem (for example, increasing adoption of 4K TVs or growth in mobile usage around the world). Each of these factors can affect how much traffic is served from a network location, as well as how efficiently a cache of fixed size can offload traffic or what hardware designs may be most effective in the future.One of our data science challenges is to combine these various factors into medium- and long-term forecasts to inform capacity planning. Where should additional servers be deployed in anticipation of traffic growth and efficiency changes a year from now? This work involves a combination of demand forecasting, system modeling — combining top-level factors together into a performance model — and resource analysis to identify areas of under- or over-utilization now and in the future.The descriptions above are of current projects, but many more challenges lie just around the corner as our service grows globally, and we continue the close collaboration between Science & Algorithms and Open Connect. Our work is evolving rapidly as we continue to invent the technology of internet television here at Netflix. Our new CDN Science & Algorithms team is dedicated to tackling these problems, and we are on the lookout for strong practitioners in statistics, applied mathematics, and optimization to join us!",https://medium.com/netflix-techblog/how-data-science-helps-power-worldwide-delivery-of-netflix-content-bac55800f9a7?source=collection_home---4------23---------------------,0,netflix,"java,python",NULL,2017-05-25
Automation as a Service — Introducing Scriptflask,"Authors : Fazal Allanabanda and Vilas VeeraraghavanLess than a year ago, we shared our testing wins and the challenges that lie ahead while testing High Impact Titles (HITs) globally. All this while ensuring the pace of innovation at Netflix does not slow down. Since then, we have made significant enhancements to our automation. So in this post, we wanted to talk about our evolution since last year.Our team’s focus still remains on providing test coverage to HITs globally (before and after launch) and ensuring that any A/B test logic is verified thoroughly before test rollout. We achieve these goals using a mix of manual and automated tests.In our previous post, we spoke about building a common set of utilities — shell/python scripts that would communicate with each microservice in the netflix service ecosystem — which gave us an easy way to fetch data and make data assertions on any part of the data service pipeline. However, we began to hit the limits of our model much faster than we expected. In the following section, we talk about some of the obstacles we faced and how we developed a solution to overcome this.As the scope of our testing expanded to cover a substantial number of microservices in the Netflix services pipeline, the issue of scalability became more prominent. We began creating scripts (shell/python) that would serve as utilities to access data from each microservice and quickly ran into issues. Specifically,There are hundreds of microservices that exist in the Netflix ecosystem and many others that constantly are in some stage of evolution to support a new product idea. We wanted to have a way to create test utilities for an application that can be owned by the corresponding application team members and their test counterparts. This will speed up testing and feature teams can craft the utilities in collaboration with us. We can stay in the loop, but the time and resource investment from our team will be reduced.As an integration test team, our focus is not only on creating new tests, but also on making the same tests easy to run for other teams so the product organization as a whole can benefit from the initial effort.However, this means there should be a common language for these tests so when testers/developers/others are trying to run them, they can easily learn to use the tests and its results for their purposes. Most teams focus on their applications, but with a common set of utilities, we can empower testers to write end-to-end tests with very minimal investment of time. This will speed up the velocity of integration testing and development in general.But with the shell/python scripts, it needs to be downloaded from the code repository to try out, debugging usually requires a team member from our group and also data returned from each microservice is not formatted the same, making parsing a non-trivial task for whoever uses the scripts.We have seen a significant benefit already from having other teams use our tools. As an example, during the recent effort to productize thumbs ratings, the QA teams working on the project used many of our utilities to run functional tests before deployment. The MAP team which delivers the landing pages for all device types also has found use for our test utilities to run integration tests. As such, a centralized API will benefit all test teams immensely.Let’s take an example test case — Verify that a test member is presented with a specific title (that is determined by the algorithm as a match for them) and gets the title in the first 3 rows of their Netflix landing page after login. This simple test sentence is easy to check manually, but when it comes to automation, it is not a single step verification. There are multiple services we will need to probe to run assertions for this test. So, we break it up as shown in the diagram below.Each of the above steps requires calling a microservice with a specific set of arguments and then parsing the responses. Each of the steps we call a “simple test verb” since each operation is a single step/REST/HTTP call to a service. As illustrated in the diagram — this test case interfaces with 4 different services (indicated by different colors) which means there are 4 points of possible failure in case of an API change.Our aim was to create higher order “complex test verbs” which would have many such verbs called at the same time. This would effectively mean all of the test steps above (which are simple verbs) can be encapsulated by a higher order complex verb that does everything in one step, like this :The advantage of creating complex test verbs is that it allows integration testers to write very specific tests that can be reused in simple ways, e.g. by developers to verify bug fixes, by other teams to assert specific behavior that they themselves do not have knowledge about/visibility into. Our existing approach had too many obstacles to create these verbs and reach the level of extensibility we wanted from the test framework. We needed an abstraction layer that will allow us to get to our goal.To overcome the scalability, usability and extensibility drawbacks we encountered, we refactored our framework into a first order application in the Netflix ecosystem. Functionally, we had great success with the approach of automating against REST endpoints. Our intention was to further build on this model — thus, Scriptflask served as a logical next step for our test automation. Our existing set of python utility scripts took in a set of parameters, and performed one function in isolation. At its core, Scriptflask is an aggregation of these utilities exposed as REST endpoints.Our infrastructure used to look like this:Our test cases were shell scripts which depended on utilities that would perform one function each. Those utilities then accessed individual microservices and retrieve or modify data for users. In order to get more teams at Netflix to use these utilities, we considered two options:We went with option 2 for the simple reason that it would eliminate the need for version conflicts, and would also eliminate the need for retrieving any of our code, since all interaction could be achieved via http calls.We used the Flask web server to front our python scripts.Our primary requirement was a REST API, and thus we had no interest in building a UI, or have any use for databases, Flask worked perfectly for us. Flask is also utilized by multiple Python applications at Netflix, providing us with easy in-house support. We looked at Django and Pyramid as well, and eventually went with Flask — which offered us the most adaptability and ease of implementation.Scriptflask consists of two components:Scriptflask is scalable because adding to it is easy. Each application owner can add a single utility that interacts with their application, and doing so is relatively simple. It took one of our developers a day to obtain our source code, deploy locally, and implement REST endpoints for a new microservice that they implemented.This significantly reduces test implementation time. As the integration team, we can focus on executing tests and increase our test coverage this way.As a first order application in the Netflix ecosystem, we also take advantage of being able to easily schedule deployments, scaling and Continuous Integration using Spinnaker.Scriptflask makes it very easy to get started in order to implement a REST endpoint — all logging, error handling, data encoding is handled right out of the box, requiring very little custom code to be written. An endpoint is quite straightforward to implement, with the request body being the only component that needs to be implemented — the remainder comes for free.A significant challenge we encountered with our previous iteration of test automation was keeping track of the utilities we had implemented as a team. This is one reason that we included OpenAPI specifications as a must-have feature, when we were gathering requirements for Scriptflask. The OpenAPI implementation makes it much easier for users of Scriptflask to execute operations as well as identify any missing endpoints.Another challenge that we faced with our previous implementation was that other teams weren’t as willing to use our test automation because there was a learning curve involved. We wanted to mitigate this by providing them an easier way to obtain certain tests by converting entire test cases into a REST endpoint that would execute the test, and in the response return the data that was generated or manipulated, as well as the results of any validation.This has now enabled us to expose multiple back-end verification tests for our High Impact Titles (HITs) as REST endpoints to the front-end teams. We have also been able to expose end-to-end tests for one of our microservices as a series of REST endpoints.Another advantage of exposing our test cases as REST endpoints is that they’re language-agnostic, and this enables multiple teams working with different frameworks to incorporate our test cases.Scriptflask significantly reduces the complexity of our integration tests. Each test case is now a simple sequence of REST calls. In some cases, the test itself is a single “complex verb” which can be executed as one step. For a client of Scriptflask, using it is no different that using any other service REST endpoint. For those interested in contributing, installing and getting started with Scriptflask and our integration tests has now become a clean, two step install and a person with minimal knowledge of python can get up and running in less than ten minutes.This helps onboard new testers very quickly to our test framework and lowers the barrier to start contributing to the test effort. We have been able to draw in developers and testers from a variety of feature teams to contribute to Scriptflask and has resulted in a lot of active discussion regarding tests and tools which spurs on more innovation and creativity.Netflix microservices are deployed in multiple AWS regions. Using Scriptflask, we were able to accomplish something we would have never achieved efficiently in our old system — Fallback from one region to another for all services that are in active-active deployments. We have also built accommodation for when the reliability team runs chaos exercises that impact services.The advantage we gain with Scriptflask is that we are able to make intelligent decisions on routing without the tester having to learn and grasp additional logic. This helps relieve the tester from having the responsibility of fighting external variabilities in the system that could have made the test fragile/flaky.As we started moving tests to use Scriptflask, we found a significant decrease in test execution times. In one case, we saw the test execution time drop from 5 mins to 56 seconds. This happened because the complicated filtering and processing logic were delegated to Scriptflask where we were able to optimize many of the most convoluted test steps and that resulted in time savings. This increase in speed results in making the tests focus on assertions which are quick and simple to fetch and verify instead of chasing down false positives caused by systemic inefficiencies.While we did not plan on improving test execution speed as an initial aim, it was a welcome benefit from the whole exercise.Our short-term objective is to promote Scriptflask within the organization and make testing all aspects of the Netflix service pipeline straightforward and intuitive to learn. To do this we will have to:In addition to enhancements that are Netflix focused, we are also considering open sourcing some aspects of Scriptflask.In the long term, more challenges lie ahead of us. We’re always exploring new ways to test and improve the overall velocity of adding new features and accelerating the rollout of A/B tests. If this has piqued your interest, and you wish to join us on this mission, we want to hear from you. Exciting times are ahead!",https://medium.com/netflix-techblog/automation-as-a-service-introducing-scriptflask-17a8e4ad954b?source=collection_home---4------0---------------------,0,netflix,"docker,frontend,javascript",NULL,2017-05-22
Pass the Remote: User Input on TV Devices,"by Andrew EichackerThe Netflix TV team works with device manufacturers to explore new input methods (like your phone!) and improve the screens we watch our favorite shows on. Beyond that, we’re testing the boundaries for content discovery and playback while bringing Netflix to more users around the world.We’ve come a long way from the television dial. From simple remotes to dedicated tablets to waving hands to saying “Hi TV!”, there are a variety of ways that users interact with their TV today. For the sake of this post, we’ll keep it to the two primary input methods: standard and pointer remotes.We use the acronym LRUD to describe input via directional controls — that is: Left, Right, Up, and Down. Of course, there is also a selection button (e.g. OK) and usually a Back button. Users navigate the screen via UI elements that can be focused via a directional key, and subsequent key events are handled with that element as the target. For web developers familiar with accessibility requirements, this might sound similar to tab order, with the additional dimension of directional navigation.Unlike tab order, there is no default handling by the platform — navigational order is defined by UI developers, as if everything has to specify a tabindex. While it might seem simple to navigate to another element in one direction, it can be challenging to maintain order in the midst of dynamic UI layouts and AB tests. One simple approach is to use delegation to let the common parent control the flow of navigation.Working with a device that only supports LRUD is a bit different than a web/mobile app. Maintaining the correct focus for a single element is essential, as the user doesn’t have direct interaction like they do with a mouse or with touch. If focus gets in a bad state, there is no way for the user to recover by clicking or tapping around, so the user will think the app has frozen.Some TVs support a pointer remote, which allows the user to point the remote at their TV to interact with what’s on screen. Pointer navigation should be familiar to most web developers, as it is very similar to mouse/touch input. The UX tends to parallel mouse usage more than touch, such as using a scroll wheel or arrow affordances to navigate lists instead of swiping. Due to the distance to the TV and difficulty holding the pointer still while your arm is in the air, buttons require large targets like a touch-based UI.TV pointer remotes aren’t just mice, however; they also have LRUD buttons. This has the potential to make things rather confusing for the user. If using the pointer to focus one element and LRUD navigates upward, which one remains focused — the element above or the one under the pointer?As a result, most devices have introduced modality to the remote: when using the pointer, only the pointer behavior is respected. If the user starts interacting with LRUD, the pointer is hidden and the interaction switches to LRUD only.There are a couple of things we need to do to support and reinforce this modality. When in pointer mode, focus could be lost by pointing over a non-interactive element, so we must establish a reasonable focus when switching to LRUD mode — either the last-focused element or some screen default. Since pointer scroll affordances only make sense for pointer mode, we hide those in LRUD mode.Over time, we found that handling input was rather cumbersome. Many of our views had custom-built focus handling that broke when the composition of the screen changed, such as with features introduced by AB tests. The UX differed slightly from screen to screen for things like re-establishing focus when switching to LRUD mode or LRUD navigation in asymmetrical layouts. We had a number of bugs that we ran into repeatedly on multiple views, such as:When we set out to build our app with React, we wanted to craft a more robust solution for user input. We landed on 3 core systems: declarative focus, spatial navigation, and focus requests.While moving to React, we tried to rethink a lot of our controls that were historically based on imperative APIs and see how we could design them to be more declarative. This also allows us to avoid the usage of refs to imperatively focus.Instead of divs, our core building block is a Widget. In order to differentiate between any widget and one that could receive focus, we created FocusableWidget, which takes an additional 2 props:FocusableWidgets can be nested, giving a structure to how a FocusableWidget relates to others. A focusPath is just a way to signify a path from the root FocusableWidget all the way down to a leaf, e.g. ‘/app/menu/play’. These will come into play more with the other two systems.Since focus is declared as part of rendering, we can validate and apply the declared focus when rendering all elements completes. This gives us an opportunity to assert on error conditions (e.g. multiple focused widgets, nothing focused).Spatial navigation is intended to make it easier to determine what should be focused by an LRUD event without having to write custom navigation code. The idea is to identify the most-likely desired element in the direction of the key press.The primary concerns with this approach were:Focusable TreePart of the answer for both of these is the focusable tree, which is a structure of FocusableWidgets culled from the widget tree.For performance, this limits the number of elements to only those that could influence the end result. Not by much, in this example, but a full UI has far more Widgets than FocusableWidgets.For correctness, this gives us a way to influence navigation structurally instead of just spatially. If we always want the rate button above to be focused first, for example, we can make the menu a FocusableWidget container. Moving left from related would then focus menu instead of play, which can then drive down focus to its children as it sees fit.Nearest NeighborThe nearest neighbor algorithm itself was also tuned for performance, and was inspired by collision detection algorithms in game programming.Spatial navigation just finds the right element to focus, so we don’t need any fancy algorithms for pointer — we can just use the FocusableWidget the mouse is over. We also save the last focused element so that the focused element can be reset when switching to LRUD mode, making the LRUD/pointer switch a breeze. In all cases, once we have a target element, we can emit a focus request.Once a target is established, a focus request is emitted with the focusPath. This event is handled by the root of the application, which saves the path as part of our application state. This kicks off a new top-down render, communicating the focusPath downward to designate the path to the component that should receive focus.We use a Higher Order Component to convert the path into helpful props (like focused and entered) so that components can modify their visual styles as necessary, and ultimately assign focus to the proper FocusableWidget.With these systems working together, UI developers could compose dynamic views without building custom navigation logic. Less customization means a more consistent UX and a single place to fix problems, avoiding regressions.Allowing components to utilize a single source of truth (the focusPath) avoids issues where individual components try to focus or relinquish focus out of turn. Centralizing the assignment of focus enables validation to find bugs early and provide clear messaging to the developer.We built and tested these systems with a simple UI in an odd layout and a handful of different focusable tree configurations. It was pretty amazing to see LRUD and pointer working perfectly together without a single line of code customizing the navigation. We use it today on millions of TV devices ranging from streaming sticks to high-end game consoles.Does this spark your interest, or do you have a better idea for handling TV input? Join us, and help us make the Netflix experience even better!",https://medium.com/netflix-techblog/pass-the-remote-user-input-on-tv-devices-923f6920c9a8?source=collection_home---4------1---------------------,0,netflix,,NULL,2017-05-17
The Evolution of Container Usage at Netflix,"Containers are already adding value to our proven globally available cloud platform based on Amazon EC2 virtual machines. We’ve shared pieces of Netflix’s container story in the past (video, slides), but this blog post will discuss containers at Netflix in depth. As part of this story, we will cover Titus: Netflix’s infrastructural foundation for container based applications. Titus provides Netflix scale cluster and resource management as well as container execution with deep Amazon EC2 integration and common Netflix infrastructure enablement.This month marks two major milestones for containers at Netflix. First, we have achieved a new level of scale, crossing one million containers launched per week. Second, Titus now supports services that are part of our streaming service customer experience. We will dive deeper into what we have done with Docker containers as well as what makes our container runtime unique.Amazon’s virtual machine based infrastructure (EC2) has been a powerful enabler of innovation at Netflix. In addition to virtual machines, we’ve also chosen to invest in container-based workloads for a few unique values they provide. The benefits, excitement and explosive usage growth of containers from our developers has surprised even us.While EC2 supported advanced scheduling for services, this didn’t help our batch users. At Netflix there is a significant set of users that run jobs on a time or event based trigger that need to analyze data, perform computations and then emit results to Netflix services, users and reports. We run workloads such as machine learning model training, media encoding, continuous integration testing, big data notebooks and CDN deployment analysis jobs many times each day. We wanted to provide a common resource scheduler for container based applications independent of workload type that could be controlled by higher level workflow schedulers. Titus serves as a combination of a common deployment unit (Docker image) and a generic batch job scheduling system. The introduction of Titus has helped Netflix expand to support the growing batch use cases.With Titus, our batch users are able to put together sophisticated infrastructure quickly due to having to only specify resource requirements. Users no longer have to deal with choosing and maintaining AWS EC2 instance sizes that don’t always perfectly fit their workload. Users trust Titus to pack larger instances efficiently across many workloads. Batch users develop code locally and then immediately schedule it for scaled execution on Titus. Using containers, Titus runs any batch application letting the user specify exactly what application code and dependencies are needed. For example, in machine learning training we have users running a mix of Python, R, Java and bash script applications.Beyond batch, we saw an opportunity to bring the benefits of simpler resource management and a local development experience for other workloads. In working with our Edge, UI and device engineering teams, we realized that service users were the next audience. Today, we are in the process of rebuilding how we deploy device-specific server-side logic to our API tier leveraging single core optimized NodeJS servers. Our UI and device engineers wanted a better development experience, including a simpler local test environment that was consistent with the production deployment.In addition to a consistent environment, with containers developers can push new application versions faster than before by leveraging Docker layered images and pre-provisioned virtual machines ready for container deployments. Deployments using Titus now can be done in one to two minutes versus the tens of minutes we grew accustomed to with virtual machines.Both batch and service users can now experiment locally and test more quickly. They can also deploy to production with greater confidence than before. This velocity drives how fast features can be delivered to Netflix customers and therefore is a key reason why containers are so important to our business.We have already covered what led us to build Titus. Now, let’s dig into the details of how Titus provides these values. We will provide a brief overview of how Titus scheduling and container execution supports the service and batch job requirements as shown in the below diagram.Titus handles the scheduling of applications by matching required resources and available compute resources.Service jobs restart failed instances and are autoscaled to maintain a changing level of load. Batch jobs are retried according to policy and run to completion.Titus offers multiple SLA’s for resource scheduling. Titus offers on-demand capacity for ad hoc batch and non-critical internal services by autoscaling capacity in EC2 based on current needs. Titus also offers pre-provisioned guaranteed capacity for user facing workloads and more critical batch. The scheduler does both bin packing for efficiency across larger virtual machines and anti-affinity for reliability spanning virtual machines and availability zones. The foundation of this scheduling is a Netflix open source library called Fenzo.Titus’s container execution, which runs on top of EC2 VMs, integrates with both AWS and Netflix infrastructure. We expect users to use both virtual machines and containers for a long time to come so we decided that we wanted the cloud platform and operational experiences to be as similar as possible. In using AWS we choose to deeply leverage existing EC2 services. We used Virtual Private Cloud (VPC) for routable IPs rather than a separate network overlay. We leveraged Elastic Network Interfaces (ENIs) to ensure that all containers had application specific security groups. Titus provides a metadata proxy that enables containers to get a container specific view of their environment as well as IAM credentials. Containers do not see the host’s metadata (e.g., IP, hostname, instance-id). We implemented multi-tenant isolation (CPU, memory, disk, networking and security) using a combination of Linux, Docker and our own isolation technology.For containers to be successful at Netflix, we needed to integrate them seamlessly into our existing developer tools and operational infrastructure. For example, Netflix already had a solution for continuous delivery, Spinnaker. While it might have been possible to implement rolling updates and other CI/CD concepts in our scheduler, delegating this feature set to Spinnaker allowed for our users to have a consistent deployment tool across both virtual machines and containers. Another example is service to service communication. We avoided reimplementing service discovery and service load balancing. Instead we provided a full IP stack enabling containers to work with existing Netflix service discovery and DNS (Route 53) based load balancing. In each of these examples, a key to the success of Titus was deciding what Titus would not do, leveraging the full value other infrastructure teams provide.Using existing systems comes at the cost of augmenting these systems to work with containers in addition to virtual machines. Beyond the examples above, we had to augment our telemetry, performance autotuning, healthcheck systems, chaos automation, traffic control, regional failover support, secret management and interactive system access. An additional cost is that tying into each of these Netflix systems has also made it difficult to leverage other open source container solutions that provide more than the container runtime platform.Running a container platform at our level of scale (with this diversity of workloads) requires a significant focus on reliability. It also uncovers challenges in all layers of the system. We’ve dealt with scalability and reliability issues in the Titus specific software as well as the open source we depend on (Docker Engine, Docker Distribution, Apache Mesos, Snap and Linux). We design for failure at all levels of our system including reconciliation to drive consistency between distributed state that exists between our resource management layer and the container runtime. By measuring clear service level objectives (container launch start latency, percentage of containers that crash due to issues in Titus, and overall system API availability) we have learned to balance our investment between reliability and functionality.A key part of how containers help engineers become more productive is through developer tools. The developer productivity tools team built a local development tool called Newt (Netflix Workflow Toolkit). Newt helps simplify container development both iteratively locally and through Titus onboarding. Having a consistent container environment between Newt and Titus helps developer deploy with confidence.We run several Titus stacks across multiple test and production accounts across the three Amazon regions that power the Netflix service.When we started Titus in December of 2015, we launched a few thousand containers per week across a handful of workloads. Last week, we launched over one million containers. These containers represented hundreds of workloads. This 1000X increase in container usage happened over a year timeframe, and growth doesn’t look to be slowing down.We run a peak of 500 r3.8xl instances in support of our batch users. That represents 16,000 cores of compute with 120 TB of memory. We also added support for GPUs as a resource type using p2.8xl instances to power deep learning with neural nets and mini-batch.In the early part of 2017, our stream-processing-as-a-service team decided to leverage Titus to enable simpler and faster cluster management for their Flink based system. This usage has resulted in over 10,000 service job containers that are long running and re-deployed as stream processing jobs are changed. These and other services use thousands of m4.4xl instances.While the above use cases are critical to our business, issues with these containers do not impact Netflix customers immediately. That has changed as Titus containers recently started running services that satisfy Netflix customer requests.Supporting customer facing services is not a challenge to be taken lightly. We’ve spent the last six months duplicating live traffic between virtual machines and containers. We used this duplicated traffic to learn how to operate the containers and validate our production readiness checklists. This diligence gave us the confidence to move forward making such a large change in our infrastructure.One of the key aspects of success of Titus at Netflix has been the experience and growth of the Titus development team. Our container users trust the team to keep Titus operational and innovating with their needs.We are not done growing the team yet. We are looking to expand the container runtime as well as our developer experience. If working on container focused infrastructure excites you and you’d like to be part of the future of Titus check out our jobs page.Andrew Spyker, Andrew Leung and Tim BozarthOn behalf of the entire Titus development team",https://medium.com/netflix-techblog/the-evolution-of-container-usage-at-netflix-3abfc096781b?source=collection_home---4------4---------------------,0,netflix,"docker,backend,cloud,ruby",NULL,2017-04-18
Introducing Bolt: On Instance Diagnostic and Remediation Platform,"Last August we introduced Winston, our event driven diagnostic and remediation platform. Winston helps orchestrate diagnostic and remediation actions from the outside. As part of that orchestration, there are multiple actions that need to be performed at an AWS instance(vm) level to collect data or take mitigation steps. We would like to discuss a supporting service called Bolt that helps with instance level action executions. By “action,” we refer to runbook, script or automation code.Netflix does not run its own data centers. We use AWS services for all our infrastructure needs. While we do utilize value added services from AWS (SQS, S3, etc.), much of our usage for AWS is on top of the core compute service provided by Amazon called EC2.As part of operationalizing our fleet of EC2 instances, we needed a simple way of automating common diagnostics and remediation tasks on these instances. This solution could integrate with our orchestration services like Winston to run these actions across multiple instances, making it easy to collect data and aggregate at scale as needed. This solution would became especially important for our instances hosting persistent services like Cassandra which are more long lived than our non-persistent services. For example, when a Cassandra node is running low on disk space, a Bolt action would be executed to analyze if any stale snapshots are laying around and if so, reclaim disk space without user intervention.Bolt is an instance level daemon that runs on every instance and exposes custom actions as REST commands for execution. Developers provide their own custom scripts as actions to Bolt which are version controlled and then seamlessly deployed across all relevant EC2 instance dynamically. Once deployed, these actions are available for execution via REST endpoints through Bolt.Some interesting design decisions when building Bolt wereBelow, we go over some of the more interesting features of Bolt.The main languages used for writing Bolt actions are Python and Bash/Shell. Actually, any scripting language that are installed on the instance can be used (Groovy, Perl, …), as long as a proper shebang is specified.The advantage of using Python is that we provide per-pack virtual environment support which give dependency isolation to the automation. We also provide, through Winston Studio, self-serve dependency management using the standard requirements.txt approach.We chose to extend Winston Studio to supports CRUD & Execute operations for both Winston and Bolt. By providing a simple and intuitive interface on where to upload your Bolt actions, look at execution history, experiment and execute on demand, we ensured that the Fault Detection Engineering team is not the bottleneck for operations associated with Bolt actions.Here is a screenshot of what a single Bolt action looks like. Users (Netflix engineers) can iterate, introspect and manage all aspects of their action via this studio. This studio also implements and exposes the paved path for action deployment to make it easy for engineers to do the right thing to mitigate risks.Users can also look at the previous executions and individual execution details through Winston Studio as shown in the following snapshots.Here is an example requirements.txt file which specifies the pack dependencies (for Python Bolt actions):Similar to Winston, we help ensure that these actions are version controlled correctly and we enforce staged deployment. Here are the 3 stages:Even though this is an internal administrative service, security was a big aspect of building software that installs on all EC2 instances we run. Here are some key security features and decisions we took to harden the service from bad actorsThe decision of choosing an Async API to execute actions allows the ability to run long running diagnostics or remediation actions without blocking the client. It also allows the clients to scale to managing thousands of instances in short interval of time through fire and check back later interface.Here is a simple sequence diagram to illustrate the flow of an action being executed on a single EC2 instance:The Bolt ecosystem consists of both a Python and Java client libraries for integrations..This client library also makes the task of TLS authenticated calls available out of the box as well as implements common client side patterns of orchestration.What do we mean by orchestration? Let say that you want to restart tomcat on 100 instances. You probably don’t want to do it on all instances at the same time, as your service would experience down-time. This is where the serial/parallel nature of an execution comes into play. We support running an action one instance at a time, one EC2 Availability Zone at a time, one EC2 Region at a time, or on all instances in parallel (if this is a read-only action for example). The user decides which strategy applies to his action (the default is one instance at a time, just to be safe).Other notables features include the support for action timeout and the support for killing running actions. Also, the user can see the output (stdout/stderr) while the action is running (doesn’t have to wait for the action to be complete).While Bolt is very flexible as to what action it can perform, the majority of use cases fall into these patterns:Bolt is used as a diagnostic tool to help engineers when their system is not behaving as expected. Also, some team uses it to gather metadata about their instance (disk usage, version of packages installed, JDK version, …). Others use Bolt to get detailed disk usage information when a low disk space alert gets triggered.Remediation is an approach to fix an underlying condition that is rendering a system non-optimal. For example, Bolt is used by our Data Pipeline team to restart the Kafka broker when needed. It is also used by our Cassandra team to free up disk space.In the case of our Cassandra instances, Bolt is used for proactive maintenance (repairs, compactions, Datastax & Priam binary upgrades, …). It is also used for binary upgrades on our Dynomite instances (for Florida and Redis).Here are some usage stats number:Below is a simplified diagram of how Bolt actions are deployed on the instances:It also explains how they are being triggered (either manually from Winston Studio or in response to an event from Winston).The diagram also summarizes how Bolt is stored on each instance:It also shows that we send metrics to Atlas to track usage as well as Pack upgrades failures. This is critical to ensure that we get notified of any issues.Here are some alternatives we looked at before building Bolt.Before Bolt, we were using plain and simple SSH to connect to instances and execute actions as needed. While inbuilt in every VM and very flexible, there were some critical issues with using this model:We discussed the idea of using separate sudo rules, a dedicated set of keys and some form of binary whitelisting to improve the security aspect. This would alleviate our security concerns, but would have limited our agility to add custom scripts (which could contains non whitelisted binaries).In the end, we decided to invest in building technology that allows us to support the instance level use case with the flexibility to add value added features while strengthening the security, reliability and performance challenges we had.AWS EC2 Run is a great option if you need to run remote script on AWS EC2 instances. But in October 2014 (when Bolt was created), EC2 Run was either not created yet, or not public (introduction post was published on October 2015). Since then, we sync quarterly with the EC2 Run team from AWS to see how we can leverage their agent to replace the Bolt agent. But, at the time of writing, given the Netflix specific features of Bolt and it’s integration with our Atlas Monitoring system, we decided not to migrate yet.These are great tools for configuration management/orchestration. Sadly, some of them (Ansible/RunDeck) had dependencies on SSH, which was an issue for the reasons listed above. For Chef and Puppet, we assessed that they were not quite the right tool for what we were trying to achieve (being mostly configuration management tools). As for Salt, adding a middle tier (Salt Master) meant increased risk of down time even with a Multi-Master approach. Bolt doesn’t have any middle tier or master, which significantly reduces the risk of down time and allows better scalability.Below is a glimpse of some future work we plan to do in this space.As an engineer, one of the biggest concern when running extra agent/process is: “Could it take over my system memory/cpu?”. In this regard, we plan to invest into memory/cpu capping as an optional configuration in Bolt. We already reduce processing priority of Bolt and the actions processes with ‘nice’, but memory is currently unconstrained. The current plan is to use cgroups.The Spinnaker team is planning on using Bolt to gather metadata information about running instances as well as other general use-cases that applies to all Netflix micro-services (Restart Tomcat, …). For this reason, we are planning to add Bolt to our BaseAMI, which will automatically make it available on all Netflix micro-services.Since its creation to solve specific needs for our Cloud Database Engineering group to its broader adoption, and soon to be included in all Netflix EC2 instances, Bolt has evolved into a reliable and effective tool.By: Jean-Sebastien Jeannotte and Vinay Shahon behalf of the Fault Detection Engineering team",https://medium.com/netflix-techblog/introducing-bolt-on-instance-diagnostic-and-remediation-platform-176651b55505?source=collection_home---4------5---------------------,0,netflix,"docker,backend,cloud",NULL,2017-04-17
BetterTLS,"At Netflix we run a microservices architecture that has hundreds of independent applications running throughout our ecosystem. One of our goals, in the interest of implementing security in depth, is to have end-to-end encrypted, authenticated communication between all of our services wherever possible, regardless of whether or not it travels over the public internet. Most of the time, this means using TLS, an industry standard implemented in dozens of languages. However, this means that every application in our environment needs a TLS certificate.Bootstrapping the identity of our applications is a problem we have solved, but most of our applications are resolved using internal names or are directly referenced by their IP (which lives in a private IP space). Public Certificate Authorities (CAs) are specifically restricted from issuing certificates of this type (See section 7.1.4.2.1 of the CA/B baseline requirements), so it made sense to use an internal CA for this purpose. As we convert applications to use TLS (e.g., by using HTTPS instead of HTTP) it was reasonably straightforward to configure them to use a truststore which includes this internal CA. However, the question remained of what to do about users accessing their services using a browser. Our internal CA isn’t trusted by browsers out-of-the-box, so what should we do?The most obvious answer is straightforward: “add the CA to browsers’ truststores.” But we were hesitant about this solution. By forcing our users to trust a private CA, they must take on faith that this CA is only used to mint certificates for internal services and is not being used to man-in-the-middle traffic to external services (such as banks, social media sites, etc). Even if our users do take on faith our good behavior, the impact of a compromise to our infrastructure becomes significant; not only could an attacker compromise our internal traffic channels, but all of our employees are suddenly at risk, even when they’re at home.Fortunately, the often underutilized Name Constraints extension provides us a solution to both of these concerns.One powerful (but often neglected) feature of the TLS specification is the Name Constraints extension. This is an extension that can be put on CA certificates which whitelists and/or blacklists the domains and IPs for which that CA or any sub-CAs are allowed to create certificates for. For example, suppose you trust the Acme Corp Root CA, which delegates to various other sub-CAs that ultimately sign certificates for websites. They may have a certificate hierarchy that looks like this:Now suppose that Beta Corp and Acme Corp become partners and need to start trusting each other’s services. Similar to Acme Corp, Beta Corp has a root CA that has signed certificates for all of its services. Therefore, services inside Acme Corp need to trust the Beta Corp root CA. Rather than update every service in Acme Corp to include the new root CA in its truststore, a simpler solution is for Acme Corp to cross-certify with Beta Corp so that the Beta Corp root CA has a certificate signed by the the Acme Root CA. For users inside Acme Corp their trust hierarchy now looks like this.However, this has the undesirable side effect of exposing users inside of Acme Corp to the risk of a security incident inside Beta Corp. If a Beta Corp CA is misused or compromised, it could issue certificates for any domain, including those of Acme Corp.This is where the Name Constraints extension can play a role. When Acme Corp signs the Beta Corp root CA certificate, it can include an extension in the certificate which declares that it should only be trusted to issue certificates under the “betacorp.com” domain. This way Acme Corp users would not trust mis-issued certificates for the “acmecorp.com” domain from CAs under the Beta Corp root CA.This example demonstrates how Name Constraints can be useful in the context of CA cross-certification, but it also applies to our original problem of inserting an internal CA into browsers’ trust stores. By minting the root CA with Name Constraints, we can limit what websites could be verified using that trust root, even if the CA or any of its intermediaries were misused.At least, that’s how Name Constraints should work.The Name Constraints extension lives on the certificate of a CA but can’t actually constrain what a bad actor does with that CA’s private key (much less control what a subordinate CA issues), so even with the extension present there is nothing to stop the bad actor from signing a certificate which violates the constraint. Therefore, it is up to the TLS client to verify that all constraints are satisfied whenever the client verifies a certificate chain.This means that for the Name Constraints extension to be useful, HTTPS clients (and browsers in particular) must enforce the constraints properly.Before relying on this solution to protect our users, we wanted to make sure browsers were really implementing Name Constraints verification and doing so correctly. The initial results were promising: each of the browsers we tested (Chrome, Firefox, Edge, and Safari) all gave verification exceptions when browsing to a site where a CA signed a certificate in violation of the constraints.However, as we extended our test suite beyond basic tests we rapidly began to lose confidence. We created a battery of test certificates which moved the subject name between the certificate’s subject common name and Subject Alternate Name extension, which mixed the use of Name Constraint whitelisting and blacklisting, and which used both DNS names and IP names in the constraint. The result was that every browser (except for Firefox, which showed a 100% pass rate) and every HTTPS client (such as Java, Node.JS, and Python) allowed some sort of Name Constraint bypass.In order to raise awareness around the issues we discovered and encourage TLS implementers to correct them, and to allow them to include some of these tests in their own test suite, we are open sourcing the test suite we created and making it available online. Inspired by badssl.com, we created bettertls.com with the hope that the tests we add to this site can help improve the resiliency of TLS implementations.Before we made bettertls.com public, we reached out to many of the affected vendors and are happy to say that we received a number of positive responses. We’d particularly like to thank Ryan Sleevi and Adam Langley from Google who were extremely responsive and immediately took actions to remediate some of the discovered issues and incorporate some of these test certificates into their own test suite. We have also received confirmation from Oracle that they will be addressing the results of this test suite in Java in an upcoming security release.The source for bettertls.com is available on github, and we welcome suggestions, improvements, corrections, and additional tests!",https://medium.com/netflix-techblog/bettertls-c9915cd255c0?source=collection_home---4------6---------------------,0,netflix,docker,NULL,2017-04-10
Netflix Conductor: Inversion of Control for workflows,"In December 2016, we open sourced Netflix Conductor.We have been working hard since then to add more features, enhance the user interface for monitoring the workflows, and harden the system. We have seen an increase in usage, new use cases, feature requests, and lot of interest from the community. In this post, we will talk about a couple of major features we added to Conductor this quarter.Conductor makes it possible to reuse workflows and embed them as sub workflows. Sub workflows are a great way to promote process reuse and modularity. As Netflix Conductor adoption increased, we observed interesting use cases where business processes were initiated based off the events and state changes in other workflows.Traditionally, applications implemented such use cases using pub/sub systems and publishing events as application state changed and subscribing to interested events to initiate actions.We sought a solution with the following characteristics:Enter Inversion of Control — for workflows. The idea is to be able to chain workflow actions such as starting a workflow, completing a task in a workflow, etc. based on state changes in other workflows.To illustrate this further, let’s take a look at two solutions to trigger a “QC Workflow” after a file has been ingested. Once a file has been ingested, multiple workflows are triggered (owned by separate applications); one of them is a file verification workflow (QC Workflow) that is triggered to identify the type of the file (Audio, Video etc.), run appropriate checks, and start the encoding process for the file.With Conductor, there are two separate ways to achieve this; both the solutions get the job done, but there are subtle differences in both the approaches.Given that sub workflows are tasks, their output can be consumed by other tasks in the File Ingest workflow subsequent to the QC workflow, e.g. outcome of the QC process. Sub workflows thus are useful when there is a tight dependency and coupling between business processes.In the above approach, the File Ingest workflow produces an “Ingest Complete” event. This event can be consumed by one or more event handlers to execute actions — including starting of a workflow.We introduced a new type of task called EVENT. An event task can be added to the workflow definition. When executed, it produces an event in the specified “sink”. A sink is an eventing system such as SQS, Conductor, or any other supported systems. Sinks follow a pluggable architecture where systems such as JMS, Kafka etc. can be added by implementing the required interfaces and making the implementation available in the classpath of the Conductor server JVM.Following the task input / output model of conductor, the input to the EVENT task is calculated and is sent as payload. The sink can be either Conductor or an external system such as SQS. The architecture for supporting various types of Sink is plugin based and new types such as JMS or Kafka can be easily added by implementing the required interfaces and making it available in the JVM classpath for Conductor server.Below is an example of an event task that publishes an event to an SQS queue identified by name.Event handlers are listeners that are executed upon arrival of a specific event. Handlers listen to various event sources including Conductor and SQS/SNS and allows users to plug in other sources via APIs. Conductor supports multiple event handlers per event type. Event handlers are managed with Conductor using the /event API endpoints. It is possible to have multiple event handlers for a single event source.Condition is a JavaScript expression on the payload, which MUST evaluate to true for the event handler to execute the actions. The condition acts as a the filter to selectively take actions on a subset of events matching the criteria. Conditional filters are optional and when not specified all the events from the source are processed.Each Event handler has one or more actions associated with it. When the condition associated with an event is evaluated to “true”, the actions are executed. The supported actions are:Below a sample event handler that listens to an SQS queue and starts the workflow based on the conditionAll the registered event handlers can be inspected in the UI. Updates can be made via REST APIs or through the swagger API page.Conductor now supports JSONPath based data transformation. Using JSON Path in the input configuration to task allows complex data transformation and reduces the need to write one-off tasks that otherwise does the data transformation.https://netflix.github.io/conductor/metadata/#wiring-inputs-and-outputsWhile it’s been a busy quarter, we are not done yet. Looking forward to Q2, our focus is going to be on making it easier to test the workflows for the developers and support for logging task execution context to help understand and troubleshooting workflows spanning multiple workers and applications.If you like the challenges of building distributed systems and are interested in building the Netflix studio ecosystem and the content pipeline at scale, check out our job openings.By Viren Baraiya",https://medium.com/netflix-techblog/netflix-conductor-inversion-of-control-for-workflows-c495621bc1c5?source=collection_home---4------7---------------------,0,netflix,kotlin,NULL,2017-04-06
The Netflix HERMES Test: Quality Subtitling at Scale,"Since Netflix launched globally, the scale of our localization efforts has increased dramatically. It’s hard to believe that just 5 years ago, we only supported English, Spanish and Portuguese. Now we’ve surpassed 20 languages — including languages like Korean, Chinese, Arabic and Polish — and that number continues to grow. Our desire to delight members in “their” language, while staying true to creative intent and mindful of cultural nuances is important to ensure quality. It’s also fueling a need to rapidly add great talent who can help provide top-notch translations for our global members across all of these languages.The need for localization quality at an increasing scale inspired us to build and launch HERMES, the first online subtitling and translation test and indexing system by a major content creator. Before now, there was no standard test for media translation professionals, even though their work touches millions of people’s lives on a daily basis. There is no common registration through a professional organisation which captures the total number of professional media translators worldwide, no license numbers, accreditations, or databases for qualified professionals. For instance, the number of working, professional Dutch subtitlers is estimated to be about 100–150 individuals worldwide. We know this through market research Netflix conducted during our launch in the Netherlands several years ago, but this is a very anecdotal “guesstimate” and the actual number remains unknown to the industry.In the absence of a common registration scheme and standardized test, how do you find the best resources to do quality media translation? Netflix does this by relying on third parties to source and manage localization efforts for our content. But even this method often lacks the precision needed to drive constant improvement and innovation in the media translation space. Each of these vendors recruit, qualify and measure their subcontractors (translators) differently, so it’s nearly impossible for Netflix to maintain a standard across all of them to ensure constant quality at a reliability and scale we need to support our constant international growth. We can measure the company’s success through metrics like rejection rates, on-time rates, etc., but we can’t measure the individual. This is like trying to win the World Cup in soccer and only being able to look at your team’s win/loss record, not knowing how many errors your players are making, blindly creating lineups without scoring averages and not having any idea how big your roster is for the next game. It’s difficult and frustrating to try to “win” in this environment, yet this is largely how Netflix has had to operate in the localization space for the last few years, while still trying to drive improvement and quality.HERMES is emblematic of Hollywood meets Silicon Valley at Netflix, and was developed internally by the Content Localization and Media Engineering teams, with collaboration from renowned academics in the media translation space to create this five part test for subtitlers. The test is designed to be highly scalable and consists of thousands of randomized combinations of questions so that no two tests should be the same. The rounds consist of multiple choice questions given at a specifically timed pace, designed to test the candidate’s ability to:Idioms are expressions that are often times specific to a certain language (“you’re on a roll”, “he bought the farm”) and can be a tough challenge to translate into other languages. There are approximately 4,000 idioms in the English language and being able to translate them in a culturally accurate way is critical to preserving the creative intent for a piece of content. Here’s an example from the HERMES test for translating English idioms into Norwegian:Upon completion, Netflix will have a good idea of the candidate’s skill level and can use this information to match projects with high quality language resources. The real long term value of the HERMES platform is in the issuance of HERMES numbers (H-humbers). This unique identifier is issued to each applicant upon sign-up for the test and will stick with them for the remainder of their career supplying translation services to Netflix. By looking at the quantity of H-Numbers in a given language, Netflix can start to more precisely estimate the size of the potential resource pool for a given language and better project our time needed to localize libraries. Starting this summer, all subtitles delivered to Netflix will be required to have a valid H-Number tied to it. This will allow Netflix to better correlate the metrics associated with a given translation to the individual who did the work.Over time, we’ll be able to use these metrics in concert with other innovations to “recommend” the best subtitler for specific work based on their past performance to Netflix. Much like we recommend titles to our members, we aim to match our subtitlers in a similar way. Perhaps they consider themselves a horror aficionado, but they excel at subtitling romantic comedies — theoretically, we can make this match so they’re able to do their best quality work.Since we unveiled our new HERMES tool two weeks ago, thousands of candidates around the world have already completed the test, covering all represented languages. This is incredible to us because of the impact it will ultimately have on our members as we focus on continually improving the quality of the subtitles on the service. We’re quickly approaching an inflection point where English won’t be the primary viewing experience on Netflix, and HERMES allows us to better vet the individuals doing this very important work so members can enjoy their favorite TV shows and movies in their language.If you’re a professional subtitler interested in taking the test, you can take it here.By Chris Fetner and Denny Sheehan",https://medium.com/netflix-techblog/the-netflix-hermes-test-quality-subtitling-at-scale-dccea2682aef?source=collection_home---4------8---------------------,0,netflix,python,NULL,2017-03-30
Netflix Hack Day — Fall 2018,"by Tom Richards, Carenina Motion, Ruslan Meshenberg, Leslie Posada, and Kaely CoonHack Days are a big deal at Netflix. They’re a chance to bring together employees from all our different disciplines to explore new ideas and experiment with emerging technologies.This Hack Day, there were hacks that ranged from making improvements to the product, to enhancing our internal tools, to just having some fun. We know even the silliest idea can spur something more.Below, you can find videos made by the hackers of some of our favorite hacks from this event. You can also check out highlights from our past events: March 2018, August 2017, January 2017, May 2016, November 2015, March 2015, February 2014 & August 2014.The most important value of our Hack Days is that they support a culture of innovation. We believe in this work, even if it never makes it in the product, and we love to share the creativity and thought put into these ideas.Major credit and thanks goes to all the teams who put together a great round of hacks in 24 hours.Jump to Shark allows you to skip right to the best (and bloodiest) bits and pieces of Sharknado.By Juliano Moraes and Shivaun RobinsonApple’s ARKit is a lot of fun to play with, and has enabled much-loved features like Animoji. We care a lot about Accessibility, so we’re eager to try a hack that would allow people to navigate the iOS app just by moving their eyes. The same technology that enables Face ID is great for accurately tracking eye position and facial expression. We used eye tracking to move the pointer around the screen, and measured the time spent on the same area to trigger the equivalent of a tap. We then used a facial gesture (tongue sticking out) to dismiss a screen. We’re hopeful that this kind of technology will become a part of mainstream Accessibility APIs in the future.By Ben Hands, John Fox, and Steve HendersonEating lunch together is a great way to meet new people and catch up with our coworkers, but sometimes we’re too busy to make the effort and end up eating lunch at our desks. To solve this problem, I created LunchBot. Every morning, LunchBot invites a random group of coworkers to eat lunch together — and checks their calendars to make sure they’re all free at the same time.By Ben Morris",https://medium.com/netflix-techblog/netflix-hack-day-fall-2018-c05dda4b98c1?source=collection_home---4------0---------------------,0,netflix,"css,frontend,html,python,php",NULL,2018-11-07
Netflix Security Monkey on Google Cloud Platform (GCP),"Today we are happy to announce that Netflix Security Monkey has BETA support for tracking Google Cloud Platform (GCP) services. Initially we are providing support for the following GCP services:This work was performed by a few incredible Googlers with the mission to take open source projects and add support for Google’s cloud offerings. Thank you for the commits!GCP support is available in the develop branch and will be included in release 0.9.0. This work helps to fulfill Security Monkey’s mission as the single place to go to monitor your entire deployment.To get started with Security Monkey on GCP, check out the documentation.See Rae Wang, Product Manager on GCP, highlight Security Monkey in her talk, “Gaining full control over your organization’s cloud resources (Google Cloud Next ‘17)”:We released Security Monkey in June 2014 as an open source tool to monitor Amazon Web Services (AWS) changes and alert on potential security problems. In 2014 it was monitoring 11 AWS services and shipped with about two dozen security checks. Now the tool monitors 45 AWS services, 4 GCP services, and ships with about 130 security checks.We plan to continue decomposing Security Monkey into smaller, more maintainable, and reusable modules. We also plan to use new event driven triggers so that Security Monkey will recognize updates much more quickly. With Custom Alerters, Security Monkey will transform from a purely monitoring tool to one that will allow for active response.More Modular:Event Driven:Custom Alerters:We’ll be following up with a future blog post to discuss these changes in more detail. In the meantime, check out Security Monkey on GitHub, join the community of users, and jump into conversation in our Gitter room if you have questions or comments.We appreciate the great community support and contributions for Security Monkey and want to specially thank:By: Patrick Kelley and Mike Grima",https://medium.com/netflix-techblog/netflix-security-monkey-on-google-cloud-platform-gcp-f221604c0cc7?source=collection_home---4------10---------------------,0,netflix,"python,frontend,pandas,react,webpack",NULL,2017-03-13
Netflix Downloads on Android,"by Greg Benson, Francois Goldfain, and Ashish GuptaNetflix is now a global company, so we wanted to provide a viewing experience that was truly available everywhere even when the Internet is not working well. This led to these three prioritized download use cases:From a product perspective, we had many initial questions about how the feature should behave: What bitrate & resolution should we download content at? How much configuration should we offer to users? How will video bookmarks work when offline? How do we handle profiles?We adopted some guiding principles based on general Netflix philosophies about what kind of products we want to create: the Downloads interface should not be so prominent that it’s distracting, and the UX should be as simple as possible.We chose an aggressive timeline for the feature since we wanted to deliver the experience to our members as soon as possible. We aimed to create a great experience with just the right amount of scope, and we could iterate and run A/B tests to improve the feature later on. Fortunately, our Consumer Insights team also had enough time to qualify our initial user-experience ideas with members and non-members before they were built.From an organizational perspective, the downloads feature was a test of coordination between a wide variety of teams. A technical spec was created that represented a balancing act of meeting license requirements, member desires, and security requirements (protecting from fraud). For Android, we used the technical spec to define which pieces of data we’d need to transfer to the client in order to provide a single ‘downloaded video’:We initially looked at Android’s DownloadManager as the mechanism to actually transfer files and data to the client. This component was easy-to-use and handled some of the functionality we wanted. However, it didn’t ultimately allow us to create the UX we needed.We created the Netflix DownloadManager for the following reasons:To store metadata for downloaded titles, our first implementation was a simple solution of serializing and deserializing json blobs to files on disk. We knew there would be problems with this (many objects created, GC churn, not developer-friendly), so while it wasn’t our desired long-term solution, it met our needs to get a prototype off the ground.For our second iteration of managing stored data, we looked at a few possible solutions including built-in SQLite support. We’d also heard a lot about Realm lately and a few companies that had success in using it as a fast and simple data-storage solution. Because we had limited experience with Realm and the downloads metadata case was relatively small and straightforward, we thought it would be a great opportunity to try Realm out.  Realm turned out to be easy to use and has a few benefits we like:Realm also provides straightforward support for versioning of data, which allows data to be migrated from schema to schema if changed as part of an application update. In this case, a RealmMigration can be created which allows for mapping of data.The challenges we had that most impacted our implementation included single thread access for objects and a lack of support for vectors such as List<>.Now that the stability of Realm has been demonstrated in the field with downloads metadata, we are moving forward with adopting it more broadly in the app for generalized video metadata storage.JobScheduler was introduced in Lollipop and allows us to be more resource-efficient in our background processing and network requests. The OS can batch jobs together for an overall efficiency gain. Longer-term, we wanted to build up our experience with this system component since developers will be encouraged more strongly by Google to use it in the future (e.g. Android ‘O’).  For our download use cases, it provided a great opportunity to get low-cost (or effectively free) network usage by creating jobs that would only activate when the user was on an unmetered network. What can our app do in the background?1. Maintenance jobs:2. Resume downloads when connectivity restoredThere were two major issues we found with JobScheduler. The first was how to provide the updates we needed from JobScheduler on pre-Lollipop devices? For these devices, we wrote an abstraction layer over top of the job-scheduling component, and on pre-Lollipop devices we use the system’s Network Connectivity receiver and AlarmManager service to schedule background tasks manually at set times.The second major problem we encountered with JobScheduler was its issue of crashing in certain circumstances (public bug report filed here). While we weren’t able to put in a direct fix for this crash, we were able to determine a workaround whereby we avoided calling JobService.onJobFinished() altogether in certain cases. The job ultimately times out on its own so the cost of operating like this seemed better than permitting the app to crash.There are a number of methods of playing video on Android, varying in their complexity and level of control:Further, playback of offline (non-streaming) content is not supported by the Android system DASH player. It wasn’t the only option, but we felt that downloads were a good opportunity to try Google’s new Android ExoPlayer. The features we liked were:The modularity of ExoPlayer was attractive for us since it allowed us to plug in a variety of DRM solutions. Our previous in-app DRM solution did not support offline licenses so we also needed to provide support for an alternate DRM mechanism.Widevine was selected due to its broad Android support, ability to work with offline licenses, a hardware-based decryption module with a software fallback (suitable for nearly any mobile device), and validation required by Android’s Compatibility Test Suite (CTS).However, this was a difficult migration due to Android fragmentation. Some devices that should have had L3 didn’t, some devices had insecure implementations, and other devices had Widevine APIs that failed whenever we called them. Support was therefore inconsistent, so we had to have reporting in place to monitor these failure rates.If we detect this kind of failure during app init then we have little choice but to disable the Downloads feature on that device since playback would not be possible. This is unfortunate for users but will hopefully improve over time as the operating system is updated on devices.Our encoding team has written previously about the specific work they did to enable high-quality, low-bandwidth mobile encodes using VP9 for Android. However, how did we decide to use VP9 in the first place?Most mobile video streams for Netflix use H.264/AVC with the Main Profile (AVCMain). Downloads were a good opportunity for us to migrate to a new video codec to reduce downloaded content size and pave the way for improved streaming bitrates in the future. The advantages of VP9 encoding for us included:Migrating to support a new video encode had some up-front and ongoing costs, not the least of which was an increased burden placed on our content-delivery system, specifically our Open-Connect Appliances (OCAs). Due to the new encoding formats, more versions of the video streams needed to be deployed and cached in our CDN which required more space on the boxes. This cost was worthwhile for us to provide improved efficiency for downloaded content in the near term, and in the long term will also benefit members streaming on mobile as we migrate to VP9 more broadly.Many teams at Netflix were aligned to work together and release this feature under an ambitious timeline. We were pleased to bring lots of joy to our members around the world and give them the ability to take their favorite shows with them on the go. The biggest proportion of downloading has been in Asia where we see strong traction in countries like India, Thailand, Singapore, Malaysia, Philippines, and Hong Kong.The main suggestion we received for Android was around lack of SD card support, which we quickly addressed in a subsequent release in early 2017. We have now established a baseline experience for downloads, and will be able to A/B test a number of improvements and feature enhancements in coming months.Originally published at techblog.netflix.com on March 8, 2017.",https://medium.com/netflix-techblog/netflix-downloads-on-android-d79db40f1732?source=collection_home---4------11---------------------,0,netflix,"frontend,sass",NULL,2017-03-08
Introducing Netflix Stethoscope,"Netflix is pleased to announce the open source release of Stethoscope, our first project following a User Focused Security approach.The notion of “User Focused Security” acknowledges that attacks against corporate users (e.g., phishing, malware) are the primary mechanism leading to security incidents and data breaches, and it’s one of the core principles driving our approach to corporate information security. It’s also reflective of our philosophy that tools are only effective when they consider the true context of people’s work.Stethoscope is a web application that collects information for a given user’s devices and gives them clear and specific recommendations for securing their systems.If we provide employees with focused, actionable information and low-friction tools, we believe they can get their devices into a more secure state without heavy-handed policy enforcement.We believe that Netflix employees fundamentally want to do the right thing, and, as a company, we give people the freedom to do their work as they see fit. As we say in the Netflix Culture Deck, responsible people thrive on freedom, and are worthy of freedom. This isn’t just a nice thing to say–we believe people are most productive and effective when they they aren’t hemmed in by excessive rules and process.That freedom must be respected by the systems, tools, and procedures we design, as well.By providing personalized, actionable information–and not relying on automatic enforcement–Stethoscope respects people’s time, attention, and autonomy, while improving our company’s security outcomes.If you have similar values in your organization, we encourage you to give Stethoscope a try.It’s important to us that people understand what simple steps they can take to improve the security state of their devices, because personal devices–which we don’t control–may very well be the first target of attack for phishing, malware, and other exploits. If they fall for a phishing attack on their personal laptop, that may be the first step in an attack on our systems here at Netflix.We also want people to be comfortable making these changes themselves, on their own time, without having to go to the help desk.To make this self service, and so people can understand the reasoning behind our suggestions, we show additional information about each suggestion, as well as a link to detailed instructions.We currently track the following device configurations, which we call “practices”:Each practice is given a rating that determines how important it is. The more important practices will sort to the top, with critical practices highlighted in red and collected in a top banner.Stethoscope is powered by a Python backend and a React front end. The web application doesn’t have its own data store, but directly queries various data sources for device information, then merges that data for display.The various data sources are implemented as plugins, so it should be relatively straightforward to add new inputs. We currently support LANDESK (for Windows), JAMF (for Macs), and Google MDM (for mobile devices).In addition to device status, Stethoscope provides an interface for viewing and responding to notifications.For instance, if you have a system that tracks suspicious application accesses, you could choose to present a notification like this:We recommend that you only use these alerts when there is an action for somebody to take–alerts without corresponding actions are often confusing and counterproductive.The Stethoscope user interface is responsive, so it’s easy to use on mobile devices. This is especially important for notifications, which should be easy for people to address even if they aren’t at their desk.We’re excited to work with other organizations to extend the data sources that can feed into Stethoscope. Osquery is next on our list, and there are many more possible integrations.Stethoscope is available now on GitHub. If you’d like to get a feel for it, you can run the front end with sample data with a single command. We also have a Docker Compose configuration for running the full application.We hope that other organizations find Stethoscope to be a useful tool, and we welcome contributions, especially new plugins for device dataOur team, Information Security, is also hiring a Senior UI Engineer at our Los Gatos office. If you’d like to help us work on Stethoscope and related tools, please apply!We’d like to thank ShmooCon for giving us the chance to present this work earlier this year. The slides and video are now both publicly available:— by Jesse Kriss and Andrew WhiteOriginally published at techblog.netflix.com on February 21, 2017.",https://medium.com/netflix-techblog/introducing-netflix-stethoscope-5f3c392368e3?source=collection_home---4------12---------------------,0,netflix,,NULL,2017-02-21
Introducing HubCommander,"by Mike Grima, Andrew Spyker, and Jason ChanNetflix is pleased to announce the open source release of HubCommander, a ChatOps tool for GitHub management.Netflix uses GitHub, a source code management and collaboration site, extensively for both open source and internal projects. The security model for GitHub does not permit users to perform repository management without granting administrative permissions. Management of many users on GitHub can be a challenge without tooling. We needed to provide enhanced security capabilities while maintaining developer agility. As such, we created HubCommander to provide these capabilities in a method optimized for Netflix.Our approach leverages ChatOps, which utilizes chat applications for performing operational tasks. ChatOps is increasingly popular amongst developers, since chat tools are ubiquitous, provide a single context for what actions occurred when and by whom, and also provides an effective means to provide self-serviceability to developers.All Netflix owned GitHub repositories reside within multiple GitHub organizations. Organizations contain the git repositories and the users that maintain them. Users can be added into teams, and teams are given access to individual repositories. In this model, a GitHub user would get invited to an organization from an administrator. Once invited, the user becomes a member of the organization, and is placed into one or more teams.At Netflix, we have several organizations that serve specific purposes. We have our primary OSS organization “Netflix”, our “Spinnaker” organization that is dedicated to our OSS continuous delivery platform, and a skunkworks organization, “Netflix-Skunkworks”, for projects that are in rough development that may or may not become fully-fledged OSS projects, to name a few.One of the biggest challenges with using GitHub organizations is user management. GitHub organizations are individual entities that must be separately administered. As such, the complexity of user management increases with the number of organizations. To reduce complexity, we enforce a consistent permissions model across all of our organizations. This allows us to develop tools to simplify and streamline our GitHub organization administration.The permissions model that we follow is one that applies the principle of least privilege, but is still open enough so that developers can obtain the access they need and move fast. The general structure we utilize is to have all employees placed under an employee’s team that has “push” (write) access to all repositories. We similarly have teams for “bot” accounts to provide for automation. Lastly, we have very few users with the “owner” role, as owners are full administrators that can make changes to the organization itself.While we permit our developers to have write access to all of our repositories, we do not directly permit them to create, delete, or change repository visibility. Additionally, all developers are required to have multi-factor authentication enabled. All of our developers on GitHub have their IDs linked in our internal employee tracking system, and GitHub membership to our organizations is removed when employees leave the company automatically (we have scripts to automate this).We also enable third-party application restrictions on our organizations to only allow specific third party GitHub applications access to our repositories.We want to have self-service tooling that provides an equivalent amount of usability as providing users with administrative access, but without the risk of making all users administrators.Our tooling provides a consistent permissions model across all of our GitHub organizations. It also empowers our users to perform privileged operations on GitHub in a consistent and supported manner, while limiting their individual GitHub account permissions.Because we limited individual GitHub account permissions, this can be problematic for developers when creating repositories, since they also want to update the description, homepage, and even set default branches. Many of our developers also utilize Travis CI for automated builds. Travis CI enablement requires that users be administrators of their repositories, which we do not permit. Our developers also work with teams outside of Netflix to collaborate with on projects. Our developers do not have permissions to invite users to our organizations or to add outside collaborators to repositories. This is where HubCommander comes in.HubCommander is a Slack bot for GitHub organizational management. It provides a ChatOps means for administering GitHub organizations. HubCommander operates by utilizing a privileged account on GitHub to perform administrative capabilities on behalf of our users. Our developers issue commands to the bot to perform their desired actions. This has a number of advantages:Out of the box, HubCommander has the following features:HubCommander is also extendable and configurable. You can develop authentication and command based plugins. At Netflix, we have developed a command plugin which allows our developers to invite themselves to any one of our organizations. When they perform this process, their GitHub ID is automatically linked in our internal employee tracking system. With this linkage, we can automatically remove their GitHub organization membership when they leave the company.Duo is also supported to add additional safeguards for privileged commands. This has the added benefit of protecting against accidental command issuance, as well as the event of Slack credentials getting compromised. With the Duo plugin, issuing a command will also trigger a “Duo push” to the employee’s device. The command only continues to execute if the request is approved. If your company doesn’t use Duo, you can develop your own authentication plugin to integrate with any internal or external authentication system to safeguard commands.Using the bot is as easy as typing !help in the Slack channel. This will provide a list of commands that HubCommander supports:To learn how to issue a specific command, simply issue that command without any arguments. HubCommander will output the syntax for the command. For example, to create a new repository, you would issue the !CreateRepo command:If you are safeguarding commands with Duo (or your own authentication plugin), an example of that flow would look like this:These features are only a starting point, and we plan on adding more soon. If you’d like to extend these features, we’d love contributions to our repository on GitHub.Originally published at techblog.netflix.com on February 7, 2017.",https://medium.com/netflix-techblog/introducing-hubcommander-1774d8f08fc6?source=collection_home---4------13---------------------,0,netflix,,NULL,2017-02-07
Crafting a high-performance TV user interface using React,"by Ian McKayThe Netflix TV interface is constantly evolving as we strive to figure out the best experience for our members. For example, after A/B testing, eye-tracking research, and customer feedback we recently rolled out video previews to help members make better decisions about what to watch. We’ve written before about how our TV application consists of an SDK installed natively on the device, a JavaScript application that can be updated at any time, and a rendering layer known as Gibbon. In this post we’ll highlight some of the strategies we’ve employed along the way to optimize our JavaScript application performance.In 2015, we embarked on a wholesale rewrite and modernization of our TV UI architecture. We decided to use React because its one-way data flow and declarative approach to UI development make it easier to reason about our app. Obviously, we’d need our own flavor of React since at that time it only targeted the DOM. We were able to create a prototype that targeted Gibbon pretty quickly. This prototype eventually evolved into React-Gibbon and we began to work on building out our new React-based UI.React-Gibbon’s API would be very familiar to anyone who has worked with React-DOM. The primary difference is that instead of divs, spans, inputs etc, we have a single “widget” drawing primitive that supports inline styling.Our app runs on hundreds of different devices, from the latest game consoles like the PS4 Pro to budget consumer electronics devices with limited memory and processing power. The low-end machines we target can often have sub-GHz single core CPUs, low memory and limited graphics acceleration. To make things even more challenging, our JavaScript environment is an older non-JIT version of JavaScriptCore. These restrictions make super responsive 60fps experiences especially tricky and drive many of the differences between React-Gibbon and React-DOM.When approaching performance optimization it’s important to first identify the metrics you will use to measure the success of your efforts. We use the following metrics to gauge overall application performance:The strategies outlined below are primarily aimed at improving key input responsiveness. They were all identified, tested and measured on our devices and are not necessarily applicable in other environments. As with all “best practice” suggestions it is important to be skeptical and verify that they work in your environment, and for your use case. We started off by using profiling tools to identify what code paths were executing and what their share of the total render time was; this lead us to some interesting observations.When Babel transpiles JSX it converts it into a number of React.createElement function calls which when evaluated produce a description of the next Component to render. If we can predict what the createElement function will produce, we can inline the call with the expected result at build time rather than at runtime.As you can see we have removed the cost of the createElement call completely, a triumph for the “can we just not?” school of software optimization.We wondered whether it would be possible to apply this technique across our whole application and avoid calling createElement entirely. What we found was that if we used a ref on our elements, createElement needs to be called in order to hook up the owner at runtime. This also applies if you’re using the spread operator which may contain a ref value (we’ll come back to this later).We use a custom Babel plugin for element inlining, but there is an official plugin that you can use right now. Rather than an object literal, the official plugin will emit a call to a helper function that is likely to disappear thanks to the magic of V8 function inlining. After applying our plugin there were still quite a few components that weren’t being inlined, specifically Higher-order Components which make up a decent share of the total components being rendered in our app.We love Higher-order Components (HOCs) as an alternative to mixins. HOCs make it easy to layer on behavior while maintaining a separation of concerns. We wanted to take advantage of inlining in our HOCs, but we ran into an issue: HOCs usually act as a pass-through for their props. This naturally leads to the use of the spread operator, which prevents the Babel plug-in from being able to inline.When we began the process of rewriting our app, we decided that all interactions with the rendering layer would go through declarative APIs. For example, instead of doing:In order to move application focus to a particular Widget, we instead implemented a declarative focus API that allows us to describe what should be focused during render like so:This had the fortunate side-effect of allowing us to avoid the use of refs throughout the application. As a result we were able to apply inlining regardless of whether the code used a spread or not.This greatly reduced the amount of function calls and property merging that we were previously having to do but it did not eliminate it completely.After we had managed to inline our components, our app was still spending a lot of time merging properties inside our HOCs. This was not surprising, as HOCs often intercept incoming props in order to add their own or change the value of a particular prop before forwarding on to the wrapped component.   We did analysis of how stacks of HOCs scaled with prop count and component depth on one of our devices and the results were informative.They showed that there is a roughly linear relationship between the number of props moving through the stack and the render time for a given component depth.Based on our findings we realized that we could improve the performance of our app substantially by limiting the number of props passed through the stack. We found that groups of props were often related and always changed at the same time. In these cases, it made sense to group those related props under a single “namespace” prop. If a namespace prop can be modeled as an immutable value, subsequent calls to shouldComponentUpdate calls can be optimized further by checking referential equality rather than doing a deep comparison. This gave us some good wins but eventually we found that we had reduced the prop count as much as was feasible. It was now time to resort to more extreme measures.After reducing the props moving through our app we were experimenting with other ways to reduce the time spent merging props between HOCs. We realized that we could use the prototype chain to achieve the same goals while avoiding key iteration.In the example above we reduced the 100 depth 100 prop case from a render time of ~500ms to ~60ms. Be advised that using this approach introduced some interesting bugs, namely in the event that this.props is a frozen object . When this happens the prototype chain approach only works if the __proto__ is assigned after the newProps object is created. Needless to say, if you are not the owner of newProps it would not be wise to assign the prototype at all.Once React knows the elements it needs to render it must then diff them with the previous values in order to determine the minimal changes that must be applied to the actual DOM elements. Through profiling we found that this process was costly, especially during mount — partly due to the need to iterate over a large number of style properties.We found that often many of the style values we were setting were never actually changed. For example, say we have a Widget used to display some dynamic text value. It has the properties text, textSize, textWeight and textColor. The text property will change during the lifetime of this Widget but we want the remaining properties to stay the same. The cost of diffing the 4 widget style props is spent on each and every render. We can reduce this by separating out the things that could change from the things that don’t.If we are careful to memoize the memoizedStylesObject object, React-Gibbon can then check for referential equality and only diff its values if that check proves false. This has no effect on the time it takes to mount the widget but pays off on every subsequent render.Taking this idea further, if we know what style props are being set on a particular widget, we can write a function that does the same work without having to iterate over any keys. We wrote a custom Babel plugin that performed static analysis on component render methods. It determines which styles are going to be applied and builds a custom diff-and-apply function which is then attached to the widget props.Internally React-Gibbon looks for the presence of the “special” __update__ prop and will skip the usual iteration over previous and next style props, instead applying the properties directly to the widget if they have changed. This had a huge impact on our render times at the cost of increasing the size of the distributable.Our environment is unique, but the techniques we used to identify opportunities for performance improvements are not. We measured, tested and verified all of our changes on real devices. Those investigations led us to discover a common theme: key iteration was expensive. As a result we set out to identify merging in our application, and determine whether they could be optimized. Here’s a list of some of the other things we’ve done in our quest to improve performance:Building a Netflix TV UI experience that can run on the variety of devices we support is a fun challenge. We nurture a performance-oriented culture on the team and are constantly trying to improve the experiences for everyone, whether they use the Xbox One S, a smart TV or a streaming stick. Come join us if that sounds like your jam!Originally published at techblog.netflix.com on January 12, 2017.",https://medium.com/netflix-techblog/crafting-a-high-performance-tv-user-interface-using-react-3350e5a6ad3b?source=collection_home---4------15---------------------,0,netflix,"frontend,angular,vue",NULL,2017-01-12
Netflix Conductor: A microservices orchestrator,"The Netflix Content Platform Engineering team runs a number of business processes which are driven by asynchronous orchestration of tasks executing on microservices. Some of these are long running processes spanning several days. These processes play a critical role in getting titles ready for streaming to our viewers across the globe.A few examples of these processes are:Traditionally, some of these processes had been orchestrated in an ad-hoc manner using a combination of pub/sub, making direct REST calls, and using a database to manage the state. However, as the number of microservices grow and the complexity of the processes increases, getting visibility into these distributed workflows becomes difficult without a central orchestrator.We built Conductor “as an orchestration engine” to address the following requirements, take out the need for boilerplate in apps, and provide a reactive flow :Conductor was built to serve the above needs and has been in use at Netflix for almost a year now. To date, it has helped orchestrate more than 2.6 million process flows ranging from simple linear workflows to very complex dynamic workflows that run over multiple days.Today, we are open sourcing Conductor to the wider community hoping to learn from others with similar needs and enhance its capabilities. You can find the developer documentation for Conductor here.With peer to peer task choreography, we found it was harder to scale with growing business needs and complexities. Pub/sub model worked for simplest of the flows, but quickly highlighted some of the issues associated with the approach:In a microservices world, a lot of business process automations are driven by orchestrating across services. Conductor enables orchestration across services while providing control and visibility into their interactions. Having the ability to orchestrate across microservices also helped us in leveraging existing services to build new flows or update existing flows to use Conductor very quickly, effectively providing an easier route to adoption.At the heart of the engine is a state machine service aka Decider service. As the workflow events occur (e.g. task completion, failure etc.), Decider combines the workflow blueprint with the current state of the workflow, identifies the next state, and schedules appropriate tasks and/or updates the status of the workflow.Decider works with a distributed queue to manage scheduled tasks. We have been using dyno-queues on top of Dynomite for managing distributed delayed queues. The queue recipe was open sourced earlier this year and here is the blog post.Tasks, implemented by worker applications, communicate via the API layer. Workers achieve this by either implementing a REST endpoint that can be called by the orchestration engine or by implementing a polling loop that periodically checks for pending tasks. Workers are intended to be idempotent stateless functions. The polling model allows us to handle backpressure on the workers and provide auto-scalability based on the queue depth when possible. Conductor provides APIs to inspect the workload size for each worker that can be used to autoscale worker instances.The APIs are exposed over HTTP — using HTTP allows for ease of integration with different clients. However, adding another protocol (e.g. gRPC) should be possible and relatively straightforward.We use Dynomite “as a storage engine” along with Elasticsearch for indexing the execution flows. The storage APIs are pluggable and can be adapted for various storage systems including traditional RDBMSs or Apache Cassandra like no-sql stores.Workflows are defined using a JSON based DSL. A workflow blueprint defines a series of tasks that needs be executed. Each of the tasks are either a control task (e.g. fork, join, decision, sub workflow, etc.) or a worker task. Workflow definitions are versioned providing flexibility in managing upgrades and migration.An outline of a workflow definition:Each task’s behavior is controlled by its template known as task definition. A task definition provides control parameters for each task such as timeouts, retry policies etc. A task can be a worker task implemented by application or a system task that is executed by orchestration server. Conductor provides out of the box system tasks such as Decision, Fork, Join, Sub Workflows, and an SPI that allows plugging in custom system tasks. We have added support for HTTP tasks that facilitates making calls to REST services.JSON snippet of a task definition:Input to a task is a map with inputs coming as part of the workflow instantiation or output of some other task. Such configuration allows for routing inputs/outputs from workflow or other tasks as inputs to tasks that can then act upon it. For example, the output of an encoding task can be provided to a publish task as input to deploy to CDN.JSON snippet for defining task inputs:Let’s look at a very simple encode and deploy workflow:There are a total of 3 worker tasks and a control task (Errors) involved:These three tasks are implemented by different workers which are polling for pending tasks using the task APIs. These are ideally idempotent tasks that operate on the input given to the task, performs work, and updates the status back.As each task is completed, the Decider evaluates the state of the workflow instance against the blueprint (for the version corresponding to the workflow instance) and identifies the next set of tasks to be scheduled, or completes the workflow if all tasks are done.The UI is the primary mechanism of monitoring and troubleshooting workflow executions. The UI provides much needed visibility into the processes by allowing searches based on various parameters including input/output parameters, and provides a visual presentation of the blueprint, and paths it has taken, to better understand process flow execution. For each workflow instance, the UI provides details of each task execution with the following details:Here’s a UI snippet from a kitchen sink workflow used to generate performance numbers:We started with an early version using a simple workflow from AWS. However, we chose to build Conductor given some of the limitations with SWF:Recently announced AWS Step Functions added some of the features we were looking for in an orchestration engine. There is a potential for Conductor to adopt the states language to define workflows.Below are some of the stats from the production instance we have been running for a little over a year now. Most of these workflows are used by content platform engineering in supporting various flows for content acquisition, ingestion and encoding.If you like the challenges of building distributed systems and are interested in building the Netflix studio ecosystem and the content pipeline at scale, check out our job openings.— by Viren Baraiya, Vikram SinghOriginally published at techblog.netflix.com on December 12, 2016.",https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40?source=collection_home---4------17---------------------,0,netflix,"python,cloud,kotlin,docker,java,backend",NULL,2016-12-12
NetflixOSS: Announcing Hollow,"We software engineers often face problems that require the dissemination of a dataset which doesn’t fit the label “big data”. Examples of this type of problem include:When faced with these we usually opt for one of two paths:Scaling each of these paths presents different challenges. Centralizing the data may allow your dataset to grow indefinitely large, but:On the other hand, serializing and keeping a local copy of the data entirely in RAM can allow many orders of magnitude lower latency and higher frequency access, but this approach has scaling challenges that get more difficult as a dataset grows in size:Engineers often select a hybrid approach — cache the frequently accessed data locally and go remote for the “long-tail” data. This approach has its own challenges:At Netflix we’ve realized that this hybrid approach often represents a false savings. Sizing a local cache is often a careful balance between the latency of going remote for many records and the heap requirement of keeping more data local. However, if you can cache everything in a very efficient way, you can often change the game — and get your entire dataset in memory using less heap and CPU than you would otherwise require to keep just a fraction of it. This is where Hollow, Netflix’s latest OSS project comes in.Hollow is a java library and comprehensive toolset for harnessing small to moderately sized in-memory datasets which are disseminated from a single producer to many consumers for read-only access.Hollow focuses narrowly on its prescribed problem set: keeping an entire, read-only dataset in-memory on consumers. It circumvents the consequences of updating and evicting data from a partial cache.Due to its performance characteristics, Hollow shifts the scale in terms of appropriate dataset sizes for an in-memory solution. Datasets for which such liberation may never previously have been considered can be candidates for Hollow. For example, Hollow may be entirely appropriate for datasets which, if represented with json or XML, might require in excess of 100GB.Hollow does more than simply improve performance — it also greatly enhances teams’ agility when dealing with data related tasks.Right from the initial experience, using Hollow is easy. Hollow will automatically generate a custom API based on a specific data model, so that consumers can intuitively interact with the data, with the benefit of IDE code completion.But the real advantages come from using Hollow on an ongoing basis. Once your data is Hollow, it has more potential. Imagine being able to quickly shunt your entire production dataset — current or from any point in the recent past — down to a local development workstation, load it, then exactly reproduce specific production scenarios.Choosing Hollow will give you a head start on tooling; Hollow comes with a variety of ready-made utilities to provide insight into and manipulate your datasets.How many nines of reliability are you after? Three, four, five? Nine? As a local in-memory data store, Hollow isn’t susceptible to environmental issues, including network outages, disk failures, noisy neighbors in a centralized data store, etc. If your data producer goes down or your consumer fails to connect to the data store, you may be operating with stale data — but the data is still present and your service is still up.Hollow has been battle-hardened over more than two years of continuous use at Netflix. We use it to represent crucial datasets, essential to the fulfillment of the Netflix experience, on servers busily serving live customer requests at or near maximum capacity. Although Hollow goes to extraordinary lengths to squeeze every last bit of performance out of servers’ hardware, enormous attention to detail has gone into solidifying this critical piece of our infrastructure.Three years ago we announced Zeno, our then-current solution in this space. Hollow replaces Zeno but is in many ways its spiritual successor.As before, the timeline for a changing dataset can be broken down into discrete data states, each of which is a complete snapshot of the data at a particular point in time. Hollow automatically produces deltas between states; the effort required on the part of consumers to stay updated is minimized. Hollow deduplicates data automatically to minimize the heap footprint of our datasets on consumers.Hollow takes these concepts and evolves them, improving on nearly every aspect of the solution.Hollow eschews POJOs as an in-memory representation — instead replacing them with a compact, fixed-length, strongly typed encoding of the data. This encoding is designed to both minimize a dataset’s heap footprint and to minimize the CPU cost of accessing data on the fly. All encoded records are packed into reusable slabs of memory which are pooled on the JVM heap to avoid impacting GC behavior on busy servers.Hollow datasets are self-contained — no use-case specific code needs to accompany a serialized blob in order for it to be usable by the framework. Additionally, Hollow is designed with backwards compatibility in mind so deployments can happen less frequently.Because Hollow is all in-memory, tooling can be implemented with the assumption that random access over the entire breadth of the dataset can be accomplished without ever leaving the JVM heap. A multitude of prefabricated tools ship with Hollow, and creation of your own tools using the basic building blocks provided by the library is straightforward.Core to Hollow’s usage is the concept of indexing the data in various ways. This enables O(1) access to relevant records in the data, allowing for the construction of powerful access patterns, whether or not they were originally anticipated while designing the data model.Tooling for Hollow is easy to set up and intuitive to understand. You’ll be able to gain insights into your data about things you didn’t know you were unaware of.Hollow can make you operationally powerful. If something looks wrong about a specific record, you can pinpoint exactly what changed and when it happened with a simple query into the history tool. If disaster strikes and you accidentally publish a bad dataset, you can roll back your dataset to just before the error occurred, stopping production issues in their tracks. Because transitioning between states is fast, this action can take effect across your entire fleet within seconds.Hollow has been enormously beneficial at Netflix — we’ve seen server startup times and heap footprints decrease across the board in the face of ever-increasing metadata needs. Due to targeted data modeling efforts identified through detailed heap footprint analysis made possible by Hollow, we will be able to continue these performance improvements.In addition to performance wins, we’ve seen huge productivity gains related to the dissemination of our catalog data. This is due in part to the tooling that Hollow provides, and in part due to architectural choices which would not have been possible without it.Everywhere we look, we see a problem that can be solved with Hollow. Today, Hollow is available for the whole world to take advantage of.Hollow isn’t appropriate for datasets of all sizes. If the data is large enough, keeping the entire dataset in memory isn’t feasible. However, with the right framework, and a little bit of data modeling, that threshold is likely much higher than you think.Documentation is available at http://hollow.how, and the code is available on GitHub. We recommend diving into the quick start guide — you’ll have a demo up and running in minutes, and a fully production-scalable implementation of Hollow at your fingertips in about an hour. From there, you can plug in your data model and it’s off to the races.Once you get started, you can get help from us directly or from other users via Gitter, or by posting to Stack Overflow with the tag “hollow”.— by Drew KoszewnikOriginally published at techblog.netflix.com on December 5, 2016.",https://medium.com/netflix-techblog/netflixoss-announcing-hollow-5f710eefca4b?source=collection_home---4------18---------------------,0,netflix,"machine,docker,python,machinelearning",NULL,2016-12-05
More Efficient Mobile Encodes for Netflix Downloads,"Last January, Netflix launched globally, reaching many new members in 130 countries around the world. In many of these countries, people access the internet primarily using cellular networks or still-developing broadband infrastructure. Although we have made strides in delivering the same or better video quality with less bits (for example, with per-title encode optimization), further innovation is required to improve video quality over low-bandwidth unreliable networks. In this blog post, we summarize our recent work on generating more efficient video encodes, especially targeted towards low-bandwidth Internet connections. We refer to these new bitstreams as our mobile encodes.Our first use case for these streams is the recently launched downloads feature on Android and iOS.We are introducing two new types of mobile encodes — AVCHi-Mobile and VP9-Mobile. The enhancements in the new bitstreams fall into three categories: (1) new video compression formats, (2) more optimal encoder settings, and (3) per-chunk bitrate optimization. All the changes combined result in better video quality for the same bitrate compared to our current streams (AVCMain).Many Netflix-ready devices receive streams which are encoded using the H.264/AVC Main profile (AVCMain). This is a widely-used video compression format, with ubiquitous decoder support on web browsers, TVs, mobile devices, and other consumer devices. However, newer formats are available that offer more sophisticated video coding tools. For our mobile bitstreams we adopt two compression formats: H.264/AVC High profile and VP9 (profile 0). Similar to Main profile, the High profile of H.264/AVC enjoys broad decoder support. VP9, a royalty-free format developed by Google, is supported on the majority of Android devices, Chrome, and a growing number of consumer devices.High profile of H.264/AVC shares the general architecture of H.264/AVC Main profile and among other features, offers other tools that increase compression efficiency. The tools from High profile that are relevant to our use case are:VP9 has a number of tools which bring improvements in compression efficiency over H.264/AVC, including:Apart from using new coding formats, optimizing encoder settings allows us to further improve compression efficiency. Examples of improved encoder settings are as follows:In our parallel encoding pipeline, the video source is split up into a number of chunks, each of which is processed and encoded independently. For our AVCMain encodes, we analyze the video source complexity to select bitrates and resolutions optimized for that title. Whereas our AVCMain encodes use the same average bitrate for each chunk in a title, the mobile encodes optimize the bitrate for each individual chunk based on its complexity (in terms of motion, detail, film grain, texture, etc). This reduces quality fluctuations between the chunks and avoids over-allocating bits to chunks with less complex content.In this section, we evaluate the compression performance of our new mobile encodes. The following configurations are compared:The results were obtained on a sample of 600 full-length popular movies or TV episodes with 1080p source resolution (which adds up to about 85 million frames). We encode multiple quality points (with different resolutions), to account for different bandwidth conditions of our members.In our tests, we calculate PSNR and VMAF to measure video quality. The metrics are computed after scaling the decoded videos to the original 1080p source resolution. To compare the average compression efficiency improvement, we use Bjontegaard-delta rate (BD-rate), a measure widely used in video compression. BD-rate indicates the average change in bitrate that is needed for a tested configuration to achieve the same quality as the anchor. The metric is calculated over a range of bitrate-quality points and interpolates between them to get an estimate of the relative performance of two configurations.The graph below illustrates the results of the comparison. The bars represent BD-rate gains, and higher percentages indicate larger bitrate savings.The AVCHi-Mobile streams can deliver the same video quality at 15% lower bitrate according to PSNR and at 19% lower bitrate according to VMAF. The VP9-Mobile streams show more gains and can deliver an average of 36% bitrate savings according to PSNR and VMAF. This demonstrates that using the new mobile encodes requires significantly less bitrate for the same quality.Viewing it another way, members can now receive better quality streams for the same bitrate. This is especially relevant for members with slow or expensive internet connectivity. The graph below illustrates the average quality (in terms of VMAF) at different available bit budgets for the video bitstream. For example, at 1 Mbps, our AVCHi-Mobile and VP9-Mobile streams show an average VMAF increase of 7 and 10, respectively, over AVC-Main. These gains represent noticeably better visual quality for the mobile streams.Last month, we started re-encoding our catalog to generate the new mobile bitstreams and the effort is ongoing. The mobile encodes are being used in the brand new downloads feature. In the near future, we will also use these new bitstreams for mobile streaming to broaden the benefit for Netflix members, no matter how they’re watching.— by Andrey Norkin, Jan De Cock, Aditya Mavlankar, and Anne AaronOriginally published at techblog.netflix.com on December 1, 2016.",https://medium.com/netflix-techblog/more-efficient-mobile-encodes-for-netflix-downloads-625d7b082909?source=collection_home---4------19---------------------,0,netflix,,NULL,2016-12-01
Netflix Chaos Monkey Upgraded,"We are pleased to announce a significant upgrade to one of our more popular OSS projects. Chaos Monkey 2.0 is now on github!Years ago, we decided to improve the resiliency of our microservice architecture. At our scale it is guaranteed that servers on our cloud platform will sometimes suddenly fail or disappear without warning. If we don’t have proper redundancy and automation, these disappearing servers could cause service problems.The Freedom and Responsibility culture at Netflix doesn’t have a mechanism to force engineers to architect their code in any specific way. Instead, we found that we could build strong alignment around resiliency by taking the pain of disappearing servers and bringing that pain forward. We created Chaos Monkey to randomly choose servers in our production environment and turn them off during business hours. Some people thought this was crazy, but we couldn’t depend on the infrequent occurrence to impact behavior. Knowing that this would happen on a frequent basis created strong alignment among our engineers to build in the redundancy and automation to survive this type of incident without any impact to the millions of Netflix members around the world. We value Chaos Monkey as a highly effective tool for improving the quality of our service. Now Chaos Monkey has evolved. We rewrote the service for improved maintainability and added some great new features. The evolution of Chaos Monkey is part of our commitment to keep our open source software up to date with our current environment and needs.Chaos Monkey 2.0 is fully integrated with Spinnaker, our continuous delivery platform.Service owners set their Chaos Monkey configs through the Spinnaker apps, Chaos Monkey gets information about how services are deployed from Spinnaker, and Chaos Monkey terminates instances through Spinnaker.Since Spinnaker works with multiple cloud backends, Chaos Monkey does as well. In the Netflix environment, Chaos Monkey terminates virtual machine instances running on AWS and Docker containers running on Titus, our container cloud.Integration with Spinnaker gave us the opportunity to improve the UX as well. We interviewed our internal customers and came up with a more intuitive method of scheduling terminations. Service owners can now express a schedule in terms of the mean time between terminations, rather than a probability over an arbitrary period of time. We also added grouping by app, stack, or cluster, so that applications that have different redundancy architectures can schedule Chaos Monkey appropriate to their configuration. Chaos Monkey now also supports specifying exceptions so users can opt out specific clusters. Some engineers at Netflix use this feature to opt out small clusters that are used for testing.Chaos Monkey can now be configured for specifying trackers. These external services will receive a notification when Chaos Monkey terminates an instance. Internally, we use this feature to report metrics into Atlas, our telemetry platform, and Chronos, our event tracking system. The graph below, taken from Atlas UI, shows the number of Chaos Monkey terminations for a segment of our service. We can see chaos in action. Chaos Monkey even periodically terminates itself.Netflix only uses Chaos Monkey to terminate instances. Previous versions of Chaos Monkey allowed the service to ssh into a box and perform other actions like burning up CPU, taking disks offline, etc. If you currently use one of the prior versions of Chaos Monkey to run an experiment that involves anything other than turning off an instance, you may not want to upgrade since you would lose that functionality.We also used this opportunity to introduce many small features such as automatic opt-out for canaries, cross-account terminations, and automatic disabling during an outage. Find the code on the Netflix github account and embrace the chaos!— Chaos Engineering Team at Netflix:Lorin Hochstein, Casey RosenthalOriginally published at techblog.netflix.com on October 19, 2016.",https://medium.com/netflix-techblog/netflix-chaos-monkey-upgraded-1d679429be5d?source=collection_home---4------22---------------------,0,netflix,"python,ruby",NULL,2016-10-19
To Be Continued,"by Hossein Taghavi, Ashok Chandrashekar, Linas Baltrunas, and Justin BasilicoOur objective in improving the Netflix recommendation system is to create a personalized experience that makes it easier for our members to find great content to enjoy. The ultimate goal of our recommendation system is to know the exact perfect show for the member and just start playing it when they open Netflix. While we still have a long way to achieve that goal, there are areas where we can reduce the gap significantly.When a member opens the Netflix website or app, she may be looking to discover a new movie or TV show that she never watched before, or, alternatively, she may want to continue watching a partially-watched movie or a TV show she has been binging on. If we can reasonably predict when a member is more likely to be in the continuation mode and which shows she is more likely to resume, it makes sense to place those shows in prominent places on the home page.While most recommendation work focuses on discovery, in this post, we focus on the continuation mode and explain how we used machine learning to improve the member experience for both modes. In particular, we focus on a row called “Continue Watching” (CW) that appears on the main page of the Netflix member homepage on most platforms. This row serves as an easy way to find shows that the member has recently (partially) watched and may want to resume. As you can imagine, a significant proportion of member streaming hours are spent on content played from this row.Previously, the Netflix app in some platforms displayed a row with recently watched shows (here we use the term show broadly to include all forms of video content on Netflix including movies and TV series) sorted by recency of last time each show was played. How the row was placed on the page was determined by some rules that depended on the device type. For example, the website only displayed a single continuation show on the top-left corner of the page. While these are reasonable baselines, we set out to unify the member experience of CW row across platforms and improve it along two dimensions:Intuitively, there are a number of activity patterns that might indicate a member’s likelihood to be in the continuation mode. For example, a member is perhaps likely to resume a show if she:On the other hand, a discovery session is more likely if a member:These hypotheses, along with the high fraction of streaming hours spent by members in continuation mode, motivated us to build machine learning models that can identify and harness these patterns to produce a more effective CW row.To build a recommendation model for the CW row, we first need to compute a collection of features that extract patterns of the behavior that could help the model predict when someone will resume a show. These may include features about the member, the shows in the CW row, the member’s past interactions with those shows, and some contextual information. We then use these features as inputs to build machine learning models. Through an iterative process of variable selection, model training, and cross validation, we can refine and select the most relevant set of features.While brainstorming for features, we considered many ideas for building the CW models, including:As mentioned above, we have two tasks related to organizing a member’s continue watching shows: ranking the shows within the CW row and placing the CW row appropriately on the member’s homepage.To rank the shows within the row, we trained a model that optimizes a ranking loss function. To train it, we used sessions where the member resumed a previously-watched show — i.e., continuation sessions — from a random set of members. Within each session, the model learns to differentiate amongst candidate shows for continuation and ranks them in the order of predicted likelihood of play. When building the model, we placed special importance on having the model place the show of play at first position.We performed an offline evaluation to understand how well the model ranks the shows in the CW row. Our baseline for comparison was the previous system, where the shows were simply sorted by recency of last time each show was played. This recency rank is a strong baseline (much better than random) and is also used as a feature in our new model. Comparing the model vs. recency ranking, we observed significant lift in various offline metrics. The figure below displays Precision@1 of the two schemes over time. One can see that the lift in performance is much greater than the daily variation.This model performed significantly better than recency-based ranking in an A/B test and better matched our expectations for member behavior. As an example, we learned that the members whose rows were ranked using the new model had fewer plays originating from the search page. This meant that many members had been resorting to searching for a recently-watched show because they could not easily locate it on the home page; a suboptimal experience that the model helped ameliorate.To place the CW row appropriately on a member’s homepage, we would like to estimate the likelihood of the member being in a continuation mode vs. a discovery mode. With that likelihood we could take different approaches. A simple approach would be to turn row placement into a binary decision problem where we consider only two candidate positions for the CW row: one position high on the page and another one lower down. By applying a threshold on the estimated likelihood of continuation, we can decide in which of these two positions to place the CW row. That threshold could be tuned to optimize some accuracy metrics. Another approach is to take the likelihood and then map it onto different positions, possibly based on the content at that location on the page. In any case, getting a good estimate of the continuation likelihood is critical for determining the row placement. In the following, we discuss two potential approaches for estimating the likelihood of the member operating in a continuation mode.A simple approach to estimating the likelihood of continuation vs. discovery is to reuse the scores predicted by the show-ranking model. More specifically, we could calibrate the scores of individual shows in order to estimate the probability P(play(s)=1) that each show s will be resumed in the given session. We can use these individual probabilities over all the shows in the CW row to obtain an overall probability of continuation; i.e., the probability that at least one show from the CW row will be resumed. For example, under a simple assumption of independence of different plays, we can write the probability that at least one show from the CW row will be played as:In this approach, we train a binary classifier to differentiate between continuation sessions as positive labels and sessions where the user played a show for the first time (discovery sessions) as negative labels. Potential features for this model could include member-level and contextual features, as well as the interactions of the member with the most recent shows in the viewing history.Comparing the two approaches, the first approach is simpler because it only requires having a single model as long as the probabilities are well calibrated. However, the second one is likely to provide a more accurate estimate of continuation because we can train a classifier specifically for it.In our experiments, we evaluated our estimates of continuation likelihood using classification metrics and achieved good offline metrics. However, a challenge that still remains is to find an optimal mapping for that estimated likelihood, i.e., to balance continuation and discovery. In this case, varying the placement creates a trade-off between two types of errors in our prediction: false positives (where we incorrectly predict that the member wants to resume a show from the CW row) and false negatives (where we incorrectly predict that the member wants to discover new content). These two types of errors have different impacts on the member. In particular, a false negative makes it harder for members to continue bingeing on a show. While experienced members can find the show by scrolling down the page or by using the search functionality, the additional friction can make it more difficult for people new to the service. On the other hand, a false positive leads to wasted screen real estate, which could have been used to display more relevant recommendation shows for discovery. Since the impacts of the two types of errors on the member experience are difficult to measure accurately offline, we A/B tested different placement mappings and were able to learn the appropriate value from online experiments leading to the highest member engagement.One of our hypotheses was that continuation behavior depends on context: time, location, device, etc. If that is the case, given proper features, the trained models should be able to detect those patterns and adapt the predicted probability of resuming shows based on the current context of a member. For example, members may have habits of watching a certain show around the same time of the day (for example, watching comedies at around 10 PM on weekdays). As an example of context awareness, the following screenshots demonstrate how the model uses contextual features to distinguish between the behavior of a member on different devices. In this example, the profile has just watched a few minutes of the show “Sid the Science Kid” on an iPhone and the show “Narcos” on the Netflix website. In response, the CW model immediately ranks “Sid the Science Kid” at the top position of the CW row on the iPhone, and puts “Narcos” at the first position on the website.Members expect the CW row to be responsive and change dynamically after they watch a show. Moreover, some of the features in the model are time and device dependent and can not be precomputed in advance, which is an approach we use for some of our recommendation systems. Therefore, we need to compute the CW row in real-time to make sure it is fresh when we get a request for a homepage at the start of a session. To keep it fresh, we also need to update it within a session after certain user interactions and immediately push that update to the client to update their homepage. Computing the row on-the-fly at our scale is challenging and requires careful engineering. For example, some features are more expensive to compute for the users with longer viewing history, but we need to have reasonable response times for all members because continuation is a very common scenario. We collaborated with several engineering teams to create a dynamic and scalable way for serving the row to address these challenges.Having a better Continue Watching row clearly makes it easier for our members to jump right back into the content they are enjoying while also getting out of the way when they want to discover something new. While we’ve taken a few steps towards improving this experience, there are still many areas for improvement. One challenge is that we seek to unify how we place this row with respect to the rest of the rows on the homepage, which are predominantly focused on discovery. This is challenging because different algorithms are designed to optimize for different actions, so we need a way to balance them. We also want to be thoughtful about pushing CW too much; we want people to “Binge Responsibly” and also explore new content. We also have details to dig into like how to determine if a show is actually finished by a user so we can remove it from the row. This can be complicated by scenarios such as if someone turned off their TV but not the playing device or fell asleep watching. We also keep an eye out for new ways to use the CW model in other aspects of the product.Can’t wait to see how the Netflix Recommendation saga continues? Join us in tackling these kinds of algorithmic challenges and help write the next episode.Originally published at techblog.netflix.com on October 12, 2016.",https://medium.com/netflix-techblog/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6?source=collection_home---4------23---------------------,0,netflix,"javascript,frontend,ruby",NULL,2016-10-12
"Zuul 2 : The Netflix Journey to Asynchronous, Non-Blocking Systems","We recently made a major architectural change to Zuul, our cloud gateway. Did anyone even notice!?Probably not… Zuul 2 does the same thing that its predecessor did — acting as the front door to Netflix’s server infrastructure, handling traffic from all Netflix users around the world. It also routes requests, supports developers’ testing and debugging, provides deep insight into our overall service health, protects Netflix from attacks, and channels traffic to other cloud regions when an AWS region is in trouble. The major architectural difference between Zuul 2 and the original is that Zuul 2 is running on an asynchronous and non-blocking framework, using Netty. After running in production for the last several months, the primary advantage (one that we expected when embarking on this work) is that it provides the capability for devices and web browsers to have persistent connections back to Netflix at Netflix scale. With more than 83 million members, each with multiple connected devices, this is a massive scale challenge. By having a persistent connection to our cloud infrastructure, we can enable lots of interesting product features and innovations, reduce overall device requests, improve device performance, and understand and debug the customer experience better.We also hoped the Zuul 2 would offer resiliency benefits and performance improvements, in terms of latencies, throughput, and costs. But as you will learn in this post, our aspirations have differed from the results.To understand why we built Zuul 2, you must first understand the architectural differences between asynchronous and non-blocking (“async”) systems vs. multithreaded, blocking (“blocking”) systems, both in theory and in practice.  Zuul 1 was built on the Servlet framework. Such systems are blocking and multithreaded, which means they process requests by using one thread per connection. I/O operations are done by choosing a worker thread from a thread pool to execute the I/O, and the request thread is blocked until the worker thread completes. The worker thread notifies the request thread when its work is complete. This works well with modern multi-core AWS instances handling 100’s of concurrent connections each. But when things go wrong, like backend latency increases or device retries due to errors, the count of active connections and threads increases. When this happens, nodes get into trouble and can go into a death spiral where backed up threads spike server loads and overwhelm the cluster. To offset these risks, we built in throttling mechanisms and libraries (e.g., Hystrix) to help keep our blocking systems stable during these events.Async systems operate differently, with generally one thread per CPU core handling all requests and responses. The lifecycle of the request and response is handled through events and callbacks. Because there is not a thread for each request, the cost of connections is cheap. This is the cost of a file descriptor, and the addition of a listener. Whereas the cost of a connection in the blocking model is a thread and with heavy memory and system overhead. There are some efficiency gains because data stays on the same CPU, making better use of CPU level caches and requiring fewer context switches. The fallout of backend latency and “retry storms” (customers and devices retrying requests when problems occur) is also less stressful on the system because connections and increased events in the queue are far less expensive than piling up threads.The advantages of async systems sound glorious, but the above benefits come at a cost to operations. Blocking systems are easy to grok and debug. A thread is always doing a single operation so the thread’s stack is an accurate snapshot of the progress of a request or spawned task; and a thread dump can be read to follow a request spanning multiple threads by following locks. An exception thrown just pops up the stack. A “catch-all” exception handler can cleanup everything that isn’t explicitly caught.  Async, by contrast, is callback based and driven by an event loop. The event loop’s stack trace is meaningless when trying to follow a request. It is difficult to follow a request as events and callbacks are processed, and the tools to help with debugging this are sorely lacking in this area. Edge cases, unhandled exceptions, and incorrectly handled state changes create dangling resources resulting in ByteBuf leaks, file descriptor leaks, lost responses, etc. These types of issues have proven to be quite difficult to debug because it is difficult to know which event wasn’t handled properly or cleaned up appropriately.Building Zuul 2 within Netflix’s infrastructure was more challenging than expected. Many services within the Netflix ecosystem were built with an assumption of blocking. Netflix’s core networking libraries are also built with blocking architectural assumptions; many libraries rely on thread local variables to build up and store context about a request. Thread local variables don’t work in an async non-blocking world where multiple requests are processed on the same thread. Consequently, much of the complexity of building Zuul 2 was in teasing out dark corners where thread local variables were being used. Other challenges involved converting blocking networking logic into non-blocking networking code, and finding blocking code deep inside libraries, fixing resource leaks, and converting core infrastructure to run asynchronously. There is no one-size-fits-all strategy for converting blocking network logic to async; they must be individually analyzed and refactored. The same applies to core Netflix libraries, where some code was modified and some had to be forked and refactored to work with async. The open source project Reactive-Audit was helpful by instrumenting our servers to discover cases where code blocks and libraries were blocking.We took an interesting approach to building Zuul 2. Because blocking systems can run code asynchronously, we started by first changing our Zuul Filters and filter chaining code to run asynchronously. Zuul Filters contain the specific logic that we create to do our gateway functions (routing, logging, reverse proxying, ddos prevention, etc). We refactored core Zuul, the base Zuul Filter classes, and our Zuul Filters using RxJava to allow them to run asynchronously. We now have two types of filters that are used together: async used for I/O operations, and a sync filter that run logical operations that don’t require I/O. Async Zuul Filters allowed us to execute the exact same filter logic in both a blocking system and a non-blocking system. This gave us the ability to work with one filter set so that we could develop gateway features for our partners while also developing the Netty-based architecture in a single codebase. With async Zuul Filters in place, building Zuul 2 was “just” a matter of making the rest of our Zuul infrastructure run asynchronously and non-blocking. The same Zuul Filters could just drop into both architectures.Hypotheses varied greatly on benefits of async architecture with our gateway. Some thought we would see an order of magnitude increase in efficiency due to the reduction of context switching and more efficient use of CPU caches and others expected that we’d see no efficiency gain at all. Opinions also varied on the complexity of the change and development effort.  So what did we gain by doing this architectural change? And was it worth it? This topic is hotly debated. The Cloud Gateway team pioneered the effort to create and test async-based services at Netflix. There was a lot of interest in understanding how microservices using async would operate at Netflix, and Zuul looked like an ideal service for seeing benefits.  While we did not see a significant efficiency benefit in migrating to async and non-blocking, we did achieve the goals of connection scaling. Zuul does benefit by greatly decreasing the cost of network connections which will enable push and bi-directional communication to and from devices. These features will enable more real-time user experience innovations and will reduce overall cloud costs by replacing “chatty” device protocols today (which account for a significant portion of API traffic) with push notifications. There also is some resiliency advantage in handling retry storms and latency from origin systems better than the blocking model. We are continuing to improve on this area; however it should be noted that the resiliency advantages have not been straightforward or without effort and tuning.  With the ability to drop Zuul’s core business logic into either blocking or async architectures, we have an interesting apples-to-apples comparison of blocking to async. So how do two systems doing the exact same real work, although in very different ways, compare in terms of features, performance and resiliency? After running Zuul 2 in production for the last several months, our evaluation is that the more CPU-bound a system is, the less of an efficiency gain we see.  We have several different Zuul clusters that front origin services like API, playback, website, and logging. Each origin service demands that different operations be handled by the corresponding Zuul cluster. The Zuul cluster that fronts our API service, for example, does the most on-box work of all our clusters, including metrics calculations, logging, and decrypting incoming payloads and compressing responses. We see no efficiency gain by swapping an async Zuul 2 for a blocking one for this cluster. From a capacity and CPU point of view they are essentially equivalent, which makes sense given how CPU-intensive the Zuul service fronting API is. They also tend to degrade at about the same throughput per node.  The Zuul cluster that fronts our Logging services has a different performance profile. Zuul is generally receiving logging and analytics messages from devices and is write-heavy, so requests are large, but responses are small and not encrypted by Zuul. As a result, Zuul is doing much less work for this cluster. While still CPU-bound, we see about a 25% increase in throughput corresponding with a 25% reduction in CPU utilization by running Netty-based Zuul. We thus observed that the less work a system actually does, the more efficiency we gain from async.  Overall, the value we get from this architectural change is high, with connection scaling being the primary benefit, but it does come at a cost. We have a system that is much more complex to debug, code, and test, and we are working within an ecosystem at Netflix that operates on an assumption of blocking systems. It is unlikely that the ecosystem will change anytime soon, so as we add and integrate more features to our gateway it is likely that we will need to continue to tease out thread local variables and other assumptions of blocking in client libraries and other supporting code. We will also need to rewrite blocking calls asynchronously. This is an engineering challenge unique to working with a well established platform and body of code that makes assumptions of blocking. Building and integrating Zuul 2 in a greenfield would have avoided some of these complexities, but we operate in an environment where these libraries and services are essential to the functionality of our gateway and operation within Netflix’s ecosystem. We are in the process of releasing Zuul 2 as open source. Once it is released, we’d love to hear from you about your experiences with it and hope you will share your contributions! We plan on adding new features such as http/2 and websocket support to Zuul 2 so that the community can also benefit from these innovations.— The Cloud Gateway Team (Mikey Cohen, Mike Smith, Susheel Aroskar, Arthur Gonigberg, Gayathri Varadarajan, and Sudheer Vinukonda)Originally published at techblog.netflix.com on September 21, 2016.",https://medium.com/netflix-techblog/zuul-2-the-netflix-journey-to-asynchronous-non-blocking-systems-45947377fb5c?source=collection_home---4------0---------------------,0,netflix,,NULL,2016-09-21
IMF: An Open Standard with Open Tools,"As Netflix expanded into a global entertainment platform, our supply chain needed an efficient way to vault our masters in the cloud that didn’t require a different version for every territory in which we have our service. A few years ago we discovered the Interoperable Master Format (IMF), a standard created by the Society of Motion Picture and Television Engineers (SMPTE). The IMF framework is based on the Digital Cinema standard of component based elements in a standard container with assets being mapped together via metadata instructions. By using this standard, Netflix is able to hold a single set of core assets and the unique elements needed to make those assets relevant in a local territory. So for a title like Narcos, where the video is largely the same in all territories, we can hold the Primary AV and the specific frames that are different for, say, the Japanese title sequence version. This reduces duplication of assets that are 95% the same and allows us to hold that 95% once and piece it to the 5% differences needed for a specific use case. The format also serves to minimize the risk of multiple versions being introduced into our vault, and allows us to keep better track of our assets, as they stay within one contained package, even when new elements are introduced. This allows us to avoid “versionitis” as outlined in this previous blog. We can leverage one set of master assets and utilize supplemental or additional master assets in IMF to make our localized language versions, as well as any transcoded versions, without needing to store anything more than master materials. Primary AV, supplemental AV, subtitles, non-English audio and other assets needed for global distribution can all live in an “uber” master that can be continually added to as needed rather than recreated. When a “virtual-version” is needed, the instructions simply need to be created, not the whole master. IMF provides maximum flexibility without having to actually create every permutation of a master.Netflix has a history of identifying shared problems within industries and seeking solutions via open source tools. Because many of our content partners have the same issues Netflix has with regard to global versions of their content, we saw IMF as a shared opportunity in the digital supply chain space. In order to support IMF interoperability and share the benefits of the format with the rest of the content community, we have invested in several open source IMF tools. One example of these tools is the IMF Transform Tool which gives users the ability to transcode from IMF to DPP (Digital Production Partnership). Realizing Netflix is only one recipient of assets from content owners, we wanted to create a solution that would allow them to enjoy the benefits of IMF and still create deliverables to existing outlets. Similarly, Netflix understands the EST business is still important to content owners, so we’re adding another open source transform tool that will go from IMF to an iTunes-compatible like package (when using Apple ProRes encoder). This will allow users to take a SMPTE compliant IMF and convert it to a package which can be used for TVOD delivery without incurring significant costs via proprietary tools. A final shared problem is editing those sets of instructions we mentioned earlier. There are many great tools in the marketplace that create IMF packages, and while they are fully featured and offer powerful solutions for creating IMFs, they can be overkill for making quick changes to a CPL (Content Play List). Things like adding metadata markers, EIDR numbers or other changes to the instructions for that IMF can all be done in our newly released OSS IMF CPL Editor. This leaves the fully functioned commercial software/hardware tools open in facilities for IMF creation and not tied up making small changes to metadata.The IMF Transform uses other open source technologies from Java, ffmpeg, bmxlib and x.264 in the framework. These tools and their source code can be found on GitHub at (https://github.com/DSRCorporation/imf-conversion).The IMF CPL Editor is cross platform and can be compiled on Mac, Windows and/or Linux operating systems. The tool will open a composition playlist (CPL) in a timeline and list all assets. The essence files will be supported in .mxf wrapped .wav, .ttml or .imsc files. The user can add, edit and delete audio, subtitle and metadata assets from the timeline. The edits can be saved back to the existing CPL or saved as a new CPL modifying the Packing List (PKL) and Asset Map as well. The source code and compiled tool will be open source and available at (https://github.com/IMFTool):We hope others will branch these open source efforts and make even more functions available to the growing community of IMF users. It would be great to see a transform function to other AS-11 formats, XDCAM 50 or other widely used broadcast “play-out” formats. In addition to the base package functionality that currently exists, Netflix will be adding supplemental package support to the IMF CPL Editor in October. We look forward to seeing what developers create. These solutions coupled with the Photon tool Netflix has already released create strong foundations to make having an efficient and comprehensive library in IMF an achievable goal for content owners seeking to exploit their assets in the global entertainment market.— by Chris Fetner and Brian KenworthyOriginally published at techblog.netflix.com on September 15, 2016.",https://medium.com/netflix-techblog/imf-an-open-standard-with-open-tools-d85a665162a6?source=collection_home---4------1---------------------,0,netflix,,NULL,2016-09-15
Netflix OSS Meetup Recap — September 2016,"Last week, we welcomed roughly 200 attendees to Netflix HQ in Los Gatos for Season 4, Episode 3 of our Netflix OSS Meetup. The meetup group was created in 2013 to discuss our various OSS projects amongst the broader community of OSS enthusiasts. This episode centered around security-focused OSS releases, and speakers included both Netflix creators of security OSS as well as community users and contributors.We started the night with an hour of networking, Mexican food, and drinks. As we kicked off the presentations, we discussed the history of security OSS at Netflix — we first released Security Monkey in 2014, and we’re closing in on our tenth security release, likely by the end of 2016. The slide below provides a comprehensive timeline of the security software we’ve released as Netflix OSS.Wes Miaw of Netflix began the presentations with a discussion of MSL (Message Security Layer), a modern security protocol that addresses a number of difficult security problems. Next was Patrick Kelley, also of Netflix, who gave the crowd an overview of Repoman, an upcoming OSS release that works to right-size permissions within Amazon Web Services environments.Next up were our external speakers. Vivian Ho and Ryan Lane of Lyft discussed their use of BLESS, an SSH Certificate Authority implemented as an AWS Lambda function. They’re using it in conjunction with their OSS kmsauth to provide engineers SSH access to AWS instances. Closing the presentations was Chris Dorros of OpenDNS/Cisco. Chris talked about his contribution to Lemur, the SSL/TLS certificate management system we open sourced last year. Chris has added functionality to support the DigiCert Certificate Authority. After the presentations, the crowd moved back to the cafeteria, where we’d set up demo stations for a variety of our security OSS releases.Thanks to everyone who attended — we’re planning the next meetup for early December 2016. Join our group for notifications. If you weren’t able to attend, we have both the slides and video available:Below is a schedule of upcoming presentations from members of the Netflix security team (through 2016). If you’d like to hear more talks from Netflix security, some of our past presentations are available on our YouTube channel.If you’re interested in solving interesting security problems while developing OSS that the rest of the world can use, we’d love to hear from you! Please see our jobs site for openings.— by Jason ChanOriginally published at techblog.netflix.com on September 14, 2016.",https://medium.com/netflix-techblog/netflix-oss-meetup-recap-september-2016-937355669a0c?source=collection_home---4------2---------------------,0,netflix,"sass,frontend,react",NULL,2016-09-14
Netflix Data Benchmark: Benchmarking Cloud Data Stores,"The Netflix member experience is offered to 83+ million global members, and delivered using thousands of microservices. These services are owned by multiple teams, each having their own build and release lifecycles, generating a variety of data that is stored in different types of data store systems. The Cloud Database Engineering (CDE) team manages those data store systems, so we run benchmarks to validate updates to these systems, perform capacity planning, and test our cloud instances with multiple workloads and under different failure scenarios. We were also interested in a tool that could evaluate and compare new data store systems as they appear in the market or in the open source domain, determine their performance characteristics and limitations, and gauge whether they could be used in production for relevant use cases. For these purposes, we wrote Netflix Data Benchmark (NDBench), a pluggable cloud-enabled benchmarking tool that can be used across any data store system. NDBench provides plugin support for the major data store systems that we use — Cassandra (Thrift and CQL), Dynomite (Redis), and Elasticsearch. It can also be extended to other client APIs.As Netflix runs thousands of microservices, we are not always aware of the traffic that bundled microservices may generate on our backend systems. Understanding the performance implications of new microservices on our backend systems was also a difficult task. We needed a framework that could assist us in determining the behavior of our data store systems under various workloads, maintenance operations and instance types. We wanted to be mindful of provisioning our clusters, scaling them either horizontally (by adding nodes) or vertically (by upgrading the instance types), and operating under different workloads and conditions, such as node failures, network partitions, etc.As new data store systems appear in the market, they tend to report performance numbers for the “sweet spot”, and are usually based on optimized hardware and benchmark configurations. Being a cloud-native database team, we want to make sure that our systems can provide high availability under multiple failure scenarios, and that we are utilizing our instance resources optimally. There are many other factors that affect the performance of a database deployed in the cloud, such as instance types, workload patterns, and types of deployments (island vs global). NDBench aids in simulating the performance benchmark by mimicking several production use cases.There were also some additional requirements; for example, as we upgrade our data store systems (such as Cassandra upgrades) we wanted to test the systems prior to deploying them in production. For systems that we develop in-house, such as Dynomite, we wanted to automate the functional test pipelines, understand the performance of Dynomite under various conditions, and under different storage engines. Hence, we wanted a workload generator that could be integrated into our pipelines prior to promoting an AWS AMI to a production-ready AMI.We looked into various benchmark tools as well as REST-based performance tools. While some tools covered a subset of our requirements, we were interested in a tool that could achieve the following:For these reasons, we created Netflix Data Benchmark (NDBench). We incorporated NDBench into the Netflix Open Source ecosystem by integrating it with components such as Archaius for configuration, Spectator for metrics, and Eureka for discovery service. However, we designed NDBench so that these libraries are injected, allowing the tool to be ported to other cloud environments, run locally, and at the same time satisfy our Netflix OSS ecosystem users.The following diagram shows the architecture of NDBench. The framework consists of three components:We currently provide the following client plugins — Datastax Java Driver (CQL), C* Astyanax (Thrift), Elasticsearch API, and Dyno (Jedis support). Additional plugins can be added, or a user can use dynamic scripts in Groovy to add new workloads. Each driver is just an implementation of the Driver plugin interface.NDBench-core is the core component of NDBench, where one can further tune workload settings.NDBench can be used from either the command line (using REST calls), or from a web-based user interface (UI).A screenshot of the NDBench Runner (Web UI) is shown in Figure 2. Through this UI, a user can select a cluster, connect a driver, modify settings, set a load testing pattern (random or sliding window), and finally run the load tests. Selecting an instance while a load test is running also enables the user to view live-updating statistics, such as read/write latencies, requests per second, cache hits vs. misses, and more.NDBench provides a variety of input parameters that are loaded dynamically and can dynamically change during the workload test. The following parameters can be configured on a per node basis:NDBench offers pluggable load tests. Currently it offers two modes — random traffic and sliding window traffic. The sliding window test is a more sophisticated test that can concurrently exercise data that is repetitive inside the window, thereby providing a combination of temporally local data and spatially local data. This test is important as we want to exercise both the caching layer provided by the data store system, as well as the disk’s IOPS (Input/Output Operations Per Second).Load can be generated individually for each node on the application side, or all nodes can generate reads and writes simultaneously. Moreover, NDBench provides the ability to use the “backfill” feature in order to start the workload with hot data. This helps in reducing the ramp up time of the benchmark.NDBench has been widely used inside Netflix. In the following sections, we talk about some use cases in which NDBench has proven to be a useful tool.A couple of months ago, we finished the Cassandra migration from version 2.0 to 2.1. Prior to starting the process, it was imperative for us to understand the performance gains that we would achieve, as well as the performance hit we would incur during the rolling upgrade of our Cassandra instances. Figures 3 and 4 below illustrate the p99 and p95 read latency differences using NDBench. In Fig. 3, we highlight the differences between Cassandra 2.0 (blue line) vs 2.1 (brown line).Last year, we also migrated all our Cassandra instances from the older Red Hat 5.10 OS to Ubuntu 14.04 (Trusty Tahr). We used NDBench to measure performance under the newer operating system. In Figure 4, we showcase the three phases of the migration process by using NDBench’s long-running benchmark capability. We used rolling terminations of the Cassandra instances to update the AMIs with the new OS, and NDBench to verify that there would be no client-side impact during the migration. NDBench also allowed us to validate that the performance of the new OS was better after the migration.NDBench is also part of our AMI certification process, which consists of integration tests and deployment validation. We designed pipelines in Spinnaker and integrated NDBench into them. The following figure shows the bakery-to-release lifecycle. We initially bake an AMI with Cassandra, create a Cassandra cluster, create an NDBench cluster, configure it, and run a performance test. We finally review the results, and make the decision on whether to promote an “Experimental” AMI to a “Candidate”. We use similar pipelines for Dynomite, testing out the replication functionalities with different client-side APIs. Passing the NDBench performance tests means that the AMI is ready to be used in the production environment. Similar pipelines are used across the board for other data store systems at Netflix.In the past, we’ve published benchmarks of Dynomite with Redis as a storage engine leveraging NDBench. In Fig. 6 we show some of the higher percentile latencies we derived from Dynomite leveraging NDBench.NDBench allows us to run infinite horizon tests to identify potential memory leaks from long running processes that we develop or use in-house. At the same time, in our integration tests we introduce failure conditions, change the underlying variables of our systems, introduce CPU intensive operations (like repair/reconciliation), and determine the optimal performance based on the application requirements. Finally, our sidecars such as Priam, Dynomite-manager and Raigad perform various activities, such as multi-threaded backups to object storage systems. We want to make sure, through integration tests, that the performance of our data store systems is not affected.For the last few years, NDBench has been a widely-used tool for functional, integration, and performance testing, as well as AMI validation. The ability to change the workload patterns during a test, support for different client APIs, and integration with our cloud deployments has greatly helped us in validating our data store systems. There are a number of improvements we would like to make to NDBench, both for increased usability and supporting additional features. Some of the features that we would like to work on include:NDBench has proven to be extremely useful for us on the Cloud Database Engineering team at Netflix, and we are happy to have the opportunity to share that value. Therefore, we are releasing NDBench as an open source project, and are looking forward to receiving feedback, ideas, and contributions from the open source community. You can find NDBench on Github at: https://github.com/Netflix/ndbenchIf you enjoy the challenges of building distributed systems and are interested in working with the Cloud Database Engineering team in solving next-generation data store problems, check out our job openings.— by Vinay Chella, Ioannis Papapanagiotou, and Kunal KundajeOriginally published at techblog.netflix.com on September 1, 2016.",https://medium.com/netflix-techblog/netflix-data-benchmark-benchmarking-cloud-data-stores-7266186ded11?source=collection_home---4------3---------------------,0,netflix,"python,sql,php,mysql,db,database,java,backend",NULL,2016-09-01
Engineering Trade-Offs and The Netflix API Re-Architecture,"Netflix’s engineering culture is predicated on Freedom & Responsibility, the idea that everyone (and every team) at Netflix is entrusted with a core responsibility. Within that framework they are free to operate with freedom to satisfy their mission. Accordingly, teams are generally responsible for all aspects of their systems, ranging from design, architecture, development, deployments, and operations. At the same time, it is inefficient to have all teams build everything that they need from scratch, given that there are often commonalities in the infrastructure needs of teams. We (like everyone else) value code reuse and consolidation where appropriate.Given these two ideas (Freedom & Responsibility and leveragability of code), how can an individual and/or team figure out what they should optimize for themselves and what they should inherit from a centralized team? These kinds of trade-offs are pervasive in making engineering decisions, and Netflix is no exception.The Netflix API is the service that handles the (sign-up, discovery and playback) traffic from all devices from all users. Over the last few years, the service has grown in a number of different dimensions: it’s grown in complexity, its request volume has increased, and Netflix’s subscriber base has grown as we expanded to most countries in the world. As the demands on the Netflix API continue to rise, the architecture that supports this massive responsibility is starting to approach its limits. As a result, we are working on a new architecture to position us well for the future (see a recent presentation at QCon for more details). This post explores the challenge of how, in the course of our re-architecture, we work to reconcile seemingly conflicting engineering principles: velocity and full ownership vs. maximum code reuse and consolidation.The Netflix API is the “front door” to the Netflix ecosystem of microservices. As requests come from devices, the API provides the logic of composing calls to all services that are required to construct a response. It gathers whatever information it needs from the backend services, in whatever order needed, formats and filters the data as necessary, and returns the response.So, at its core, the Netflix API is an orchestration service that exposes coarse grained APIs by composing fined grained functionality provided by the microservices.To make this happen, the API has at least four primary requirements: provide a flexible request protocol; map requests to one or more fine-grained APIs to backend microservices; provide a common resiliency abstraction to protect backend microservices; and create a context boundary (“buffer”) between device and backend teams.Today, the API service exposes three categories of coarse grained APIs: non-member (sign-up, billing, free trial, etc.), discovery (recommended shows and movies, search, etc.) and playback (decisions regarding the streaming experience, licensing to ensure users can view specific content, viewing history, heartbeats for user bookmarking, etc.).Consider an example from the playback category of APIs. Suppose a user clicks the “play” button for Stranger Things Episode 1 on their mobile phone. In order for playback to begin, the mobile phone sends a “play” request to the API. The API in turn calls several microservices under the hood. Some of these calls can be made in parallel, because they don’t depend on each other. Others have to be sequenced in a specific order. The API contains all the logic to sequence and parallelize the calls as necessary. The device, in turn, doesn’t need to know anything about the orchestration that goes on under the hood when the customer clicks “play”.Playback requests, with some exceptions, map only to playback backend services. There are many more discovery and non-member dependent services than playback services, but the separation is relatively clean, with only a few services needed both for playback and non-playback requests.This is not a new insight for us, and our organizational structure reflects this. Today, two teams, both the API and the Playback teams, contribute to the orchestration layer, with the Playback team focusing on Playback APIs. However, only the API team is responsible for the full operations of the API, including releases, 24/7 support, rollbacks, etc. While this is great for code reuse, it goes against our principle of teams owning and operating in production what they build.With this in mind, the goals to address in the new architecture are:As we look into the future, we are considering two options. In option 1 (see figure 2), the orchestration layer in the API will, for all playback requests, be a pass-through and simply send the requests on to the playback-specific orchestration layer. The playback orchestration layer would then play the role of orchestrating between all playback services. The one exception to a full pass-through model is the small set of shared services, where the orchestration layer in the API would enrich the request with whatever information the playback orchestration layer needs in order to service the request.Alternatively, we could simply split into two separate APIs (see figure 3).Both of the approaches actually solve the challenges we set out to solve: for each option, each team will own the release cycle as well as the production operations of their own orchestration layer — a step forward in our minds. This means that the choice between the two options comes down to other factors. Below we discuss some of our considerations.The developers who use our API (i.e., Netflix’s device teams) are top priority when designing, building and supporting the new API. They will program against our API daily, and it is important for our business that their developer experience and productivity is excellent. Two of the top concerns in this area are discovery and documentation: our partner teams will need to know how to interact with the API, what parameters to pass in and what they can expect back. Another goal is flexibility: due to the complex needs we have for 1000+ device types, our API must be extremely flexible. For instance, a device may want to request a different number of videos, and different properties about them, than another device would. All of this work will be important to both playback and non-playback APIs, so how is this related to the one vs. two APIs discussion? One API facilitates more uniformity in those areas: how requests are made and composed, how the API is documented, where and how teams find out about changes or additions to the API, API versioning, tools to optimize the developer experience, etc. If we go the route of two APIs, this is all still possible, but we will have to work harder across the two teams to achieve this.The two teams are very close and collaborate effectively on the API today. However, we are keenly aware that a decision to create two APIs, owned by two separate teams, can have profound implications. Our goals would, and should, be minimal divergence between the two APIs. Developer experience, as noted above, is one of the reasons. More broadly, we want to maximize the reuse of any components that are relevant to both APIs. This also includes any orchestration mechanisms, and any tools, mechanisms, and libraries related to scalability, reliability, and resiliency. The risk is that the two APIs could drift apart over time. What would that mean? For one, it could have organizational consequences (e.g., need for more staff). We could end up in a situation where we have valued ownership of components to a degree that we have abandoned component reuse. This is not a desirable outcome for us, and we would have to be very thoughtful about any divergence between the two APIs.Even in a world where we have a significant amount of code use, we recognize that the operational overhead will be higher. As noted above, the API is critical to the Netflix service functioning properly for customers. Up until now, only one of the teams has been tasked with making the system highly scalable and highly resilient, and carrying the operational burden. The team has spent years building up expertise and experience in system scale and resiliency. By creating two APIs, we would be distributing these tasks and responsibilities to both teams.If one puts the organizational considerations aside, two separate APIs is simply the cleaner architecture. In option 1, if the API acts largely as a pass-through, is it worth incurring the extra hop? Every playback request that would come into the API would simply be passed along to the playback orchestration layer without providing much functional value (besides the small set of functionality needed from the shared services). If the components that we build for discovery, insights, resiliency, orchestration, etc. can be reused in both APIs, the simplicity of having a clean separation between the two APIs is appealing. Moreover, as mentioned briefly above, option 1 also requires two teams to be involved for Playback API pushes that change the interaction model, while option 2 truly separates out the deployments.Where does all of this leave us? We realize that this decision will have long-lasting consequences. But in taking all of the above into consideration, we have also come to understand that there is no perfect solution. There is no right or wrong, only trade-offs. Our path forward is to make informed assumptions and then experiment and build based on them. In particular, we are experimenting with how much we can generalize the building blocks we have already built and are planning to build, so that they could be used in both APIs. If this proves fruitful, we will then build two APIs. Despite the challenges, we are optimistic about this path and excited about the future of our services. If you are interested in helping us tackle this and other equally interesting challenges, come join us! We are hiring for several different roles.— by Katharina Probst and Justin BeckerOriginally published at techblog.netflix.com on August 23, 2016.",https://medium.com/netflix-techblog/engineering-trade-offs-and-the-netflix-api-re-architecture-64f122b277dd?source=collection_home---4------5---------------------,0,netflix,"java,python",NULL,2016-08-23
