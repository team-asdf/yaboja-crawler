link,created_at,title,content
http://woowabros.github.io/woowabros/2018/09/18/introduce-small-home.html,2018-09-18,"새로운 오피스, 작은집","
안녕하세요
무더운 폭염과 장마가 지나고 난 후 벌써 선선한 바람이 부는 계절 어느덧 가을이 훌쩍 다가왔네요.
아침 저녁으로 일교차가 크네요, 여러분들 모두 감기 조심하세요~

오늘은 우아한형제들의 새로운 오피스 공간에 대해서 소개해dream~

‘사옥을 이동 했나요?’
아닙니다~
대부분 저희 우아한형제들의 사옥은 ‘몽촌토성역’ 에 있는 사옥을 떠올리시겠지만,
새로운 구성원들이 더욱 더 늘어남에 따라 새로운 오피스에 대한 니즈가 생기게 되었지요.
그리하여, 2018년 7월 16일 개발 조직과 일부 구성원이 ‘삼성생명 잠실빌딩’ 으로 새로 입주 했어요~!
잠실역 2호선 8번 출구 약 200m에 있기 때문에 출퇴근에 아주 용이하고, 사무실 통유리로 보이는 롯데타워가 한눈에..
(잠실역 8호선 9번 출구에서는 약 100m 거리라는건 안 비밀~)

기존에 몽촌토성역에 있는 우아한형제들이 입주해있는 사옥은 ‘큰집’이라고 하구요
이번에 저희가 새로 입주하게 된 곳은 ‘작은집’ 입니다~
저희 작은집의 인테리어는 우아한형제들의 공간디자인실과 디자인팀에서 직접 디자인과 설계를 맡아 진행 했습니다.
참고로 “큰집” 역시 입주 당시에 저희 공간디자인실과 디자인팀에서 공간 디자인을 진행 했었는데요,
2018 IF (Interior Architecture) Design Award의 Office/Work Space부문에서 혁신성을 인정 받아
명성있고 권위있는 디자인 어워드에서 상을 받았었다는 사실~레알~팩트~트루~

“우아한 오피스”
큰집에 이어 이번 작은집까지 디자인 설계를 직접 했다고 하니 너무 기대되지 않아요?

우아한형제들의 ‘큰집’ 디자인 당시 올림픽공원 건너편에 자리한 만큼
각 층당 혁신을 이룬 스포츠인 들을 컨셉으로 잡아 디자인 하였는데요, 못보셨다면 ▶랜선 집들이 클릭
작은집 역시 제한된 면적에서 어떻게 하면 효율적으로 모두에게
꼭 필요한 공간의 면적을 확보하고, 최대한 효율적인 공간을 만들 수 있을까를 고민 했다고 합니다.

또 우아한오피스의 가장 큰 매력인 코워킹 스페이스 공간을 더욱 신경 써 디자인 했고,
그래서 더 쉽고, 더 창의적이며, 더 깊은 커뮤니케이션을 '작은집'에서도 하고 있습니다.


'공간은 넓지 않지만 생각은 넓다'
“네, 저희는 자리에 혼자 앉아있는 것을 별로 좋아하지 않아요.
구성원이 한군데서 일하는 것을 지양하고, 계속해서 노트북을 들고 여기저기 왔다 갔다 하면서 일하는걸 지향해요.
기존의 회사들은 자기 자리에 앉아서 오랫동안 있으면 일을 열심히 한다고 생각 하지만,
저희는
‘저 사람 왜 대화를 하지 않을까?’
‘저 사람들은 왜 섞여서 일하지 않을까?’
이렇게 재밌는 공간들이 많은데 왜 다이나믹하게 움직이지 않지?
이런 생각들이 들죠.”
""우리의 공간은 넓지 않습니다. 그래서, 우리의 생각은 더 넓어질 수 있었습니다.""




끊임 없는 혁신 마인드를 계속해서 잊지 않기 위해 큰집과 동일한 ‘스포츠 혁신가’를 테마로 잡아
각 층별 개성 넘치는 컨셉을 더해 매력적인 공간으로 구성 했습니다.
오브라이언 투구
덕다이빙
롱스트로크
양1/양2/양3
본레스
………
위 명칭들은 뭘까요 ?
바로~ 작은집의 각 층별 회의실 명칭 입니다.
그리고, 실제로 해당 스포츠 종목에 혁신을 더한 기술 명칭들이자 회의실명 입니다.
아래 사진을 자세히 보시면 회의실 명 아래에 이세상 모든 맛있는 음식들을 찾는 쏠쏠한 재미까지 ㅋㅋㅋ
(가끔씩 점심 뭐 먹을지 고민할 때 저기서 찾으면 꿀잼)



뿐만 아니라, 앞에서 잠깐 말씀 드렸던 코워킹 스페이스 View도 보여드릴께요
아.. 짠내나는 투어를 다니는 프로그램에서 유민상씨가 했던 말이 생각 나요.
정말 절경이네요
정말 장관이고요~
정말 신이 주신 선물이네요.

(어느정도 길래?)







첫번째 사진과 두번째 사진 월드타워와 교통회관이 보이는 곳은 특히 저 역시도 아주 좋아하고
모든 구성원 분들도 좋아하는 빈백이 깔려있는 코워킹 스페이스에요!
빈백에 기대어 업무를 하기도 하고, 쉬고 싶을때에도 빈백에 누워 통유리 바깥으로 보이는 뷰가
정말 절경이구요, 장관이구요, 신이 주신 선물 같은 느낌!ㅋㅋㅋ
그리고 회의실이 아니더라도 어디에서든 구성원들과 편안히 앉아서 퀵 미팅을 진행할 수 있도록
코워킹 스페이스 곳곳에 퀵 미팅 데스크가 있다는 사실!
아직 끝이 아니죠! 각 층별 컨셉이 각각 다른 공간이므로 층별 사진들을 조금 더 보여 드릴께요











한가지.. 혹시 이 사진에 특별한 점을 발견 하신 분? 보시다시피 이 사진은 앞에서 보신 것 과 같이 코워킹 스페이스 인데요,
저희 작은집에 있는 여러 구역의 코워킹 스페이스 중 이 곳은 바로 높낮이 조절이 되는 “모션 스탠딩 데스크” 라는 점 !
(책상이 전부 다 높이가 다르죠?)
앉아서 일하다 보면 특히 허리가 뻐근하죠.. 저 역시도..
다른 구성원과 함께 코워킹 할 때 물론 눈 높이를 맞추고 친근하게 코워킹을 하기에도 정말 좋구요,
때로는 혼자 서서 일하고 싶을 때 다들 한번씩 있잖아요? 그리고 졸릴때도 좋아요 (소곤소곤)




이번엔 업무공간에 대해서 짤막히 소개 할까 해요.

이 곳 작은집은 특히 기획/개발 구성원들이 업무 하는 엄연한 R&D 센터이기에
업무공간 사진은 많이 보여드리기 어려운 점 양해 부탁 드립니다 ㅠ_ㅠ
잡담을 많이 나누는 것이 경쟁력이다.


두번째 사진은 저희가 우아한 테크캠프 2기를 진행 할 당시의 업무공간이에요~
(우아한 테크캠프가 궁금하시다면? 클릭)
혹시 다른 회사들과의 업무공간과 다른 점을 눈치 채셨나요?
…
바로 파티션이 없다는 거에요. 업무 공간의 책상에 파티션이 없기 때문에
저희는 바로 옆 동료와, 앞에 있는 동료와, 대각선에 있는 동료와 모두
고개만 돌려도 바로 이야기할 수가 있어서 더욱 더 빠른 의사소통과 업무 협업이 가능해요!


또 멀리에 있는 동료와도 바로 의견을 나누고 하고 싶은 얘기가 있다구요?
우리는 곳곳에 비치 되어 있는 이 스툴을 가지고 직접 동료 옆 자리로 가서 바로 얘기를 나눠요!
다음 공간은 구성원들에게 편안한 휴식을 주는 휴식 공간 입니다.
휴게실은 남/여 별도로 휴게실이 각각 있고, 캡슐방, 안마의자, 샤워시설로 구성 되어 있어요.
업무시간 혹은 점심시간에 피곤할 때 잠시 꿀잠을 잘 수 있어요~
(온돌방과 매트리스방으로 반반 나눠져 있는 건 안비밀)




이 곳은 저희 작은집에 방문하실 경우 제일 먼저 만나게 되는 얼굴이자 입구 입니다.
안으로 들어가볼까요 ?

이곳에서는 면접을 볼 수 있는 면접실과, 카페, 교육장, IT헬프데스크, 그리고 양평 같은 방 이 있는 곳이에요!
‘양평같은방?’ 궁금하죠? 가장 마지막에 보여 드릴께요!
먼저 문 안쪽으로 들어오게 되면 초록초록 벽이 산뜻하게 맞아주는데요,
그거 아세요?…..

심지어 전부 다 살아있는 생화 라는점…
게다가 펌프 방식으로 이용해 위에서 아래로 물을 흐르게 하여 24시간 내내 관리가 된다는 점!

굉장하죠. 아직 놀라긴 일러요, 다음으로 여러분을 맞이할 것은 바로 바로

바로 로봇 커피머신 ! (Feat.달콤커피)
이 로봇이 만드는 커피 맛은 의외로 기대보다 이상이라 아주 인기가 많구요,
심지어 어플로 주문 하기 때문에 자리에서 주문 하고, 잠시 후에 Pick up 번호 4자리를
입력하면 본인이 주문한 음료를 픽업 가능 하다는 점~~ 기다리지 않아도 된다구요~
더군다나 방문하신 손님은 FREE!!


우아한형제들에 면접을 보러 오신 분들을 위한 선수대기실 입니다.
면접실은 총 5개가 있고, 면접을 앞두고 긴장하고 계실 면접자분들을 위해
대기실에서 대기 후 편한 마음으로 면접을 볼 수 있도록 배려 하고 있습니다.
캔 아이 핼퓨?

그리고 면접실을 지나 한쪽편에는 임직원 분들의 자산 지급과 수리 등을 위한
IT 헬프데스크 공간이 마련 되어 있습니다.
다음, 만나볼 곳은 교육장과 회의실 공간인데요,
개방감을 준 회의실과 행사 또는 교육 컨퍼런스를 진행 하기에도 부족하지 않은 교육장 입니다.
사진으로 먼저 만나요 +_+





보시다시피 답답하지 않고 개방감을 만끽하며 기분 좋게 회의를 할 수 있습니다~



그리구 교육장은 구성원 누구나 교육 또는 회의를 위해 이용 가능하며,
때로는 큰 규모의 타운홀 미팅 과 컨퍼런스 진행에도 부족함이 없습니다.
김봉진 대표님과 김범준 CTO님께서 강연 세션을 진행할 당시의 사진입니다.
잊어버릴 뻔 했네요! 마지막으로 소개해드릴 ‘양평 같은 방’ 입니다.
여러분들 펜션 하면 어디가 생각 나세요? 양평? 가평?

우아한형제들에는 양평과 가평이 모두 있습니다. (ㅋㅋㅋㅋ)
큰집에는 ‘가평 같은 방’ 이 있고, 작은집에는 ‘양평 같은 방’ 이 있다는 사실…
뭐하는 방인지 궁금하죠? 눈으로 확인하시길~




엄훠놔~ 엄청나게 고급진 가정집? 아니쥬… 회의실이에요ㅋㅋㅋ
일반적인 회의실은 아니지만, 업무 목적의 팀 워크숍, 혹은 마라톤 회의,
동동동 모임 ( 동 아리 같은 동 아리 아닌 동 아리 같은 모임) 등의 활동을 하고자 할 때 사용 가능 한 회의실이에요.
‘양평 같은 방’은 어떤 건지 한번에 느낌 딱 왔쥬? :D
우아한형제들의 피플팀에 사전 신청 후 승인 받아 사용 가능한 공간 이라는 점!
이곳에서 회의를 한다면 엄청 설레일 것 같아요.. (사실 저도 아직 안해봤다는건 비밀)
이렇게 양평 같은 방을 끝으로 저희 우아한형제들이 새로 입주한 오피스 공간에 대해서 소개 해드렸는데요,
사실 처음엔 어디서부터 어떻게 소개를 해야 할지 굉장히 막막 했어요.

저 역시 여러분들께 소개해드리기 위해서 이곳 저곳 다시 돌아보며
새로 입주한 오피스 공간에 대해서 또 알게 되고,
배민다움에 대해 또 다시 많은걸 느낄 수 있는 시간이어서 너무 좋았습니다~
저는 ‘멋진하루’를 매일매일 이 곳에서 느끼고 있습니다.
여러분들도 느껴보시는 건 어떠세요?
▶채용 공고 보러가기

"
http://woowabros.github.io/tools/2018/08/16/jdbc-log-sql-projectinfo.html,2018-08-16,JDBC로 실행되는 SQL에 자동으로 프로젝트 정보 주석 남기기,"이 글은 Java기반에서 JDBC와 이를 기반으로 한 Persistence Framework를 이용해 SQL을 실행할 수 있는 개발자를 대상으로 모든 JDBC를 통해 실행되는 SQL 구문에 애플리케이션 정보를 주석으로 넣는 법을 설명합니다.
보통 Monolithic 아키텍처로 프로젝트를 진행하게 되면 모든 데이터를 하나의 데이터베이스에 다 넣는 방식으로 개발을 하게 됩니다.
서비스가 계속 Monolithic으로 유지 가능한 수준이라면 상관없지만 운 좋게도 폭발적으로 성장하여 버틸 수 없게 되면 마이크로서비스 아키텍처로 하나씩 분리를 하게 됩니다.
이때 한 번에 모든 데이터베이스를 각각의 마이크로서비스에서 한 번에 나눠서 가져가면 좋겠지만 공통 데이터베이스에서 완전히 벗어나기에는 상당히 오랜 시간이 걸립니다.
이 긴 고통의 시간 동안에 매우 많은 문제가 발생하게 됩니다.
여러 프로젝트에서 하나의 공통 DB에 쿼리를 날리다 보니 어느 팀의 누군가가 인덱스 안 타는 쿼리를 잘못 짜거나 혹은 대규모의 데이터를 읽어들이는 Batch 성 작업을 수행했을 때 공통 DB는 CPU 100%를 치게 되고 고객들은 하염없이 기다려야 하는 일이 매우 자주 발생하게 됩니다.
이때 빠르게 어떤 프로젝트의 쿼리가 문제를 일으켰는지 찾아내면 좋습니다만, 이게 그리 쉽지가 않습니다. IP 주소가 있더라도 그 IP의 서버에 가서 어느 팀의 무슨 프로젝트 서버인지를 확인하는 등의 복잡도가 추가됩니다.
그래서 모든 SQL의 맨 앞에 어느 팀의 무슨 프로젝트에서 온 요청인지를 주석으로 남겨두면 느린 쿼리 로그를 보자마자 조금이라도 빠르게 해당 팀을 찾아가 문제를 해결할 수 있지 않을까요?
사실 이에 관한 글을 몇 년 전에 쓴 적이 있습니다. JDBC SQL 구문에 클라이언트 정보 남기기 하지만 이 글은 Hibernate 그리고 MySQL JDBC 드라이버에 국한한 기법이었습니다.
배달의 민족은 최소 두 가지 이상의 데이터베이스를 사용하며 Persistence Framework도 Hibernate, jOOQ, QueryDSL, MyBatis, Spring JDBCTemplate 등 다양하게 사용 중입니다.
따라서 좀 더 보편적인 해결책이 필요합니다.
Java의 데이터베이스 접속과 실행에 관한 API들(JDBC)은 DataSource, Connection, Statement, PreparedStatement 등의 인터페이스로 잘 추상화돼 있습니다.
실행되는 SQL 구문이 결정되는 순간은 Connection.prepareStatement(""SQL 구문"") 이때거나, 혹은 Statement.execute(""SQL 구문""), Statement.executeQuery(""SQL 구문""), Statement.executeUpdate(""SQL 구문"") 이 정도입니다(사실 더 있지만…).
Java의 모든 Connection Pool은 DataSource 인터페이스를 구현하고 있습니다.
여기서 간단한 아이디어가 도출됩니다.
처음부터 모든 Proxy를 직접 구현해도 상관은 없겠으나 좀 더 쉽게 가는 방법을 찾아보았습니다.
Tomcat JDBC Connection Pool은 Tomcat과 함께 개발되고 있는 커넥션 풀로 HikariCP와 함께 요즘 가장 많이 사용되며(SpringBoot 1.x의 기본 커넥션 풀) 성능도 준수한 편입니다.
이 Connection Pool에는 JDBC Interceptor라는 개념이 있습니다. 커넥션풀에서 자체적으로  DataSourceProxy와 ConnectionProxy를 제공해주고 SQL 실행을 가로채서 slow query 로그를 남기는 등의 일을 할 수 있습니다.
문서를 보면 기본적으로 유용한 Interceptor 들을 몇 가지 제공해주고 있습니다.
저는 일을 간단히 끝내고자 Tomcat JDBC Connection Pool을 사용하고 SQL 구문을 가로채어 설정한 프로젝트 이름을 주석으로 맨 앞에 넣어주는 JDBC Interceptor 를 만들었습니다.
해당 소스코드는 woowabros/tomcat-jdbc-pool-sql-caller-info-comment라는 github 저장소에 공개해 두었습니다.
실제 코드는 파일 한 개이므로 사용하실 분들은 SqlCallerInfoCommentInterceptor.java 파일을 복사하여 자신의 프로젝트에 넣고 Tomcat JDBC Connection Pool을 만들어주시면 됩니다.
Spring Boot에서 YML로 설정한다면 다음과 같겠네요. baemin_in_woowabros 대신 자신이 넣고 싶은 정보를 넣습니다.
혹은 Java Code로 직접 설정한다면
위와 같이 설정하면 모든 SQL 구문은 맨 앞에 /* baemin_in_woowabros */ SELECT .... 형태로 주석이 붙은 상태로 전송됩니다.
혹시나 몰라 SQL Injection에 대비하여 영문자/숫자/밑줄 등만 가능하게 하였습니다. 하지만 이 부분은 원하는 대로 코드를 변경해서 한글을 넣게 하셔도 무방합니다.
tomcat connection pool의 JDBC Interceptor는 org.apache.tomcat.jdbc.pool.JdbcInterceptor를 상속해서 구현해야 합니다만, 또 귀찮으므로 최대한 많이 구현된 기본 구현체인 org.apache.tomcat.jdbc.pool.interceptor.StatementDecoratorInterceptor를 상속하였습니다.
StatementDecoratorInterceptor는 기본적으로 DataSource Proxy와 Connection Proxy까지는 돼 있기 때문에 Connection.prepareStatement 메서드와 일반 Statement 생성시 SqlChangeStatementProxy라는 객체를 생성해주고 그 안에서 Statement.execute 등...이 호출될 때 SQL을 가로채어 바꿔치기하게 하였습니다.
주석으로 프로젝트를 넣는 방법은 사용법이 너무나 간단하지만, 실제로 문제가 발생했을 때 상당한 도움을 줄 것으로 생각됩니다.
여러 애플리케이션이 사용하는 공통 DB가 있다면 Java를 사용하지 않는 프로젝트라도 이런 기능을 만들면 좋을 것 같고, 그게 안 되더라도 직접 주석으로 DBA가 알아보고 바로 연락할 수 있는 정도의 주석을 SQL 앞단에 남겨주는 습관을 지니는 것이 좋을 것 같습니다.
귀찮은 일을 줄이려고 Tomcat JDBC Connection Pool의 JdbcInterceptor를 사용하게 했더니 설정은 참 쉽지만, 코드를 이해하는 것은 오히려 더 어려워진 것 같습니다. 어디까지가 JdbcInterceptor가 해주는 것이고 어디부터가 직접 구현한 것인지 경계가 명확히 안 드러나 보여서 두 코드를 다 이해해야만 하게 되었습니다. 저도 막상 글을 쓰려고 코드를 다시 보다 보니 매우 헷갈립니다.
DataSource 부터 모두 직접 Proxy를 만들어보는 것도 좋은 공부가 될 것 같고, 오히려 코드가 더 간결해질 것 같습니다. 또한 특정 Connection Pool에 의존하지도 않고요.
저는 안 했지만 누군가는 하실 거라 믿으며… / 어서 빨리 DB 독립을 꿈꾸며 광복절 다음 날…
긴 글 읽어주셔서 고맙습니다.
"
http://woowabros.github.io/woowabros/2018/08/05/my_worry.html,2018-08-05,나를 술푸게 하는 고민들,"안녕하세요, 배민프론트서버개발팀의 개그 신동이자 귀염둥이 이신은입니다.
저는 작년 12월 신입 개발자 공채를 통해, 백엔드 개발자로 우아한형제들에 합류했습니다.
취업준비생에서 7개월 차 삐약삐약 병아리개발자가 된 지금까지, 지난 1년을 돌아보며 제가 했던 고민을 나누고자 합니다.
한글보다는 수식이나 JAVA 언어로 표현하는 것이 더 편한 수학전공 개발자이지만, 저의 의도가 왜곡되지 않고 이 글을 보시는 분들에게 잘 전달되기를 바라며 펜을, 아니 키보드를 두드립니다.
요즘 취업준비생들은 적게는 1~20개, 많게는 100여 개의 이력서를 제출합니다.
이력서를 제출한 모든 회사에 전력투구할 순 없을뿐더러 채용일정이 겹치는 경우가 왕왕 있기 때문에, 자신의 기준에 따라 회사들의 우선순위를 매기게 됩니다.
물론 서류, 인·적성/코딩테스트, 면접 등의 단계를 거치면서 이 우선순위가 뒤바뀌는 경우도 많죠.
우아한형제들의 입사를 확정했을 때 저는,
취업준비 시작부터 거의 마지막까지도 최우선으로 생각했던 회사에 최종합격한 상태였고,
합숙면접까지 거쳐서 힘겹게 올라간 회사의 임원면접을 앞둔 상태였습니다.
자랑하고 싶어서 밝히는 것이 아니라, 이러한 상황에서 한 치의 망설임도 없이(사실 약간 망설였…) 우아한형제들을 선택한 이유를 얘기하려고 합니다.
배달의민족 서비스는 알고 있었지만 한 번도 이용해보지 않았고, 4년 동안 (망령처럼) 연구실에만 틀어박혀 있던 대학원생에겐 너무나도 생소했던 우아한형제들이라는 회사 이름….
광고 쪽 일을 하던 선배에게 신입 개발자 공채 소식을 전해 들었을 때만 해도, ‘내가 고작 배달 앱 만들려고 머리카락까지 빠져가면서 연구했나?’  라는 생각이 들더군요(건방이 극에 달했…).
밑져야 본전이라는 생각으로, 회사 이름을 가볍게 검색해보다가 낯익은 이름을 발견했습니다.
꽤 오래전 어떤 경로에서였는지 기억나진 않지만, 김범준 CTO님의 개인 블로그를 방문한 적이 있었습니다.
나는 하나의 기계에서 어떻게 하면 최적화할 지를 고민하면서, 연속해서 읽어야 할 데이터는 X, X+1 주소에 저장하는 것이 아니라 memory bus bottleneck를 막기 위해 X, X+8에 저장해서 
읽어 내고 있고,
lock만 하더라도 spinlock을 쓸 지, readers-writer lock을 쓸 지, 그리고 lock을 거는 단위도 hash bucket head에 걸 때와 실제 hash bucket node에 걸 때를 구분해서 쓰고 있는데,
구글은 그런 류의 최적화가 아니라 몇 천대, 몇 만대의 서버를 분산 시스템으로 연결해서 서비스를 제공하고 있던 것이다.
그 순간 내가 하는 고민들이 너무 국지적인 최적화에 대한 고민이 아닐까 하는 생각과 내가 지금 열심히 쌓아 올린 지식들이라는 것이 상당히 많은 부분 쓸모없어질지도 모른다는 생각, 그리고 앞으로의 변화에 적절히 대응하지 않으면 도태되어 버릴 것 같다는 생각이 들었었다.
[출처: 김범준 블로그]
당시에 저도 대학원에서 통신 미들웨어 연구를 하면서 비슷한 고민을 하고 있었습니다.
‘내 연구가 시대의 흐름에 너무 뒤처지는 것은 아닌가?’,
‘내가 연구하던 프로토콜이 아닌 MQTT 및 CoAP가 IoT 표준, 이른바 대세 프로토콜이 되었다는데…. 앞으로 내가 해야 할 것은 무엇이지?’.
개인적으로 고민이 많은 편인데, 고민은 부정적인 것이 아니라 더 생산적이고 의미 있는 결과를 만들 수 있는 긍정적인 것이라고 생각합니다.
물론 좋은 결과를 만들기 위해서는, 고민하는 데 그치지 않고 해결하고자 하는 의지가 수반되어야 하죠.
따라서 지금 어떤 고민을 하는 건, 절대적으로 안 좋은 상황이 아니라 성장하고 있는 과정이라 믿고 있습니다.
CTO님의 개인 블로그 글을 다시 찾아보면서, ‘이렇게 자아 성찰을 게을리하지 않으시는 분(게다가 엄청나게 똑똑하신 분! 딸랑딸랑)이 수장으로 계시는 개발조직은 어떤 모습일까?’ 하는 호기심이 생겼습니다.
면접을 앞두고 찾아보았던 기술 블로그에 있는 서비스와 기술에 대한 고민 그리고 면접 자리에서 마주한 면접관님들과의 심도 있는 대화를 통해, 호기심은 어느새 확신으로 바뀌어있었습니다.
다른 회사의 많은 개발자 또한 우리와 비슷한 고민을 하며 다양한 방식으로 성장하고 있을 것으로 생각합니다.
하지만 혼자 숨어서 하는 고민이 아니라 드러내놓고 공유하며, 자신만의 방식을 찾아내고 회고하는 모습이 제겐 참 인상적이었습니다.
[내가 겪은 우아한형제들 인지도의 현주소]
회사에 대한 어르신들의 낮은 인지도, 신입은 무조건 큰 회사를 가야 한다는 생각 등으로 인한 주변의 우려들이 있었지만, 제 선택을 좌지우지할 만한 요인들은 아니었습니다.
입사한 지 7개월이 지난 지금, 그 선택에 아주 만족하고 있습니다 :) 아직 회사 뽕에 취해있어서 그런가?
우아한형제들에 입사하자마자 2개월 동안 신입 개발자 교육을 받은 후, 배민프론트서버개발팀에 배정받았습니다.
저는 분명 백엔드 개발자로 입사했는데, 프론트라는 단어를 듣고 직무가 바뀌었거나(프론트엔드를 교육해주셨던 코드스쿼드 크롱님이 Pick Me?) 행정 오류일 것으로 의심했습니다.
사실 팀 배정을 받기 몇 주 전에, 가벼운(줄 알았던) 식사 자리에서 CTO님께 ‘어떤 팀에서 어떤 일을 해보고 싶은지’  에 대한 질문을 받은 적이 있습니다.
신입 개발자 교육은 외부 전문기관에서 진행했기 때문에, 회사의 조직도도 모르고 각 팀의 R&R도 모르던 상태라 당당하게 평소 생각을 말씀드렸죠.
“많은 일을 하면서 빨리 성장하고 싶고, 배달의민족의 (엄청난) 트래픽을 경험해보고 싶습니다! 정적인 팀은 제게 맞지 않죠. 우후훗”.
이 트랩 대화가 팀 배정에 지대한 영향을 미치리라고는 미처 생각하지 못한 채, 해맑은 얼굴로 초밥을 참 맛있게 먹었더랬습니다.
그렇게…. 저는 그토록 꿈꾸던(?) 팀의 일원이 되었고, 팀장님과의 첫 미팅에서 ‘바빠서 못 챙겨주니 알아서 잘해라’ 는 막말 조언을 듣게 됩니다.
여기가 술을 퍼마시게 된 별 다섯 개의 뽀인트! 사실 말씀은 그렇게 하셨지만, 팀원들이 아마 팀장님만 빼고. ㅋㅋㅋ 많이 챙겨주셨습니다.
저를 왜 이 팀에 보내셨냐고 나중에 항의 여쭤봤더니, ‘그 팀에서 살아남을 신입이 신은님밖에 없을 것 같았다’ 며 어째서?!!! 왜?!!! 위로 아닌 위로를 해주셨던 CTO님.
배달의민족 앱을 실행하면, 처음 호출하게 되는 것이 바로 우리 팀에서 제공하는 API들입니다.
가장 앞단에 위치하다 보니 트래픽에 적극적으로 대응해야 하는 것은 필수 불가결한 사실이고,
앱에서 필요로 하는 API들을 제공하는 역할이다 보니 하나의 도메인을 깊게 파고들기보다는 다양한 도메인을 동시다발적으로 진행해야 하는 경우가 많습니다.

[우리 팀에서 제공하는 일부 API들 - 배너 및 카테고리 / 이런 것도 배달돼요?! / 주소]
처음에는 특정 도메인의 신규 로직을 개발하는 업무가 제게 주어졌지만, 시간이 지나면서 점차 운영 이슈와 장애 대응, 여러 도메인으로의 업무 확장이 진행되고 있습니다.
‘신입이라는 이유로 계속 단순 업무만 주고 중요한 업무에서는 배제하는 거 아닐까?’  하는 제 고민은 보기 좋게 빗나갔죠 :)
아직 인프라 지식이 부족하기 때문에 트래픽 이슈를 다루기도 너무 어렵고,
(멀티코어를 장착하고 싶은 싱글코어러이다 보니) A를 개발하다가 급하게 B를 처리해야 할 때 Context Switching이 신속 정확하게 이루어지지 않습니다.
이럴 땐 ‘정말 개발자를 그만둬야 하나?’, ‘난 역시 개발자와 맞지 않아’, ‘무슨 부귀영화를 누리겠다고 서비스 회사에 들어왔나?’ 싶은 고민과 좌절을 경험합니다.
그렇지만, 배포에 성공했을 때, 어려운 업무를 해냈을 때, 동료들이 기술에 대한 조언을 구할 때, 기획자에게 감사 인사를 받았을 때 등등 스스로가 대견하게 느껴질 정도의 성취감을 느끼는 순간도 많습니다.
이러한 성공과 실패, 좌절과 성취를 느끼는 과정들을 통해, 더욱더 성장할 수 있으리라는 흔들림 없는 믿음이 생겼습니다.
지난 7월 22일, 제2회 배민 치믈리에 자격시험이 성황리에 마무리되었습니다.
이를 홍보하기 위해서 앱 스플래시 화면에서 보여주던 치믈리에 포스터를 이제 내려달라는 업무 요청이 있었고, 스플래시 교체 건은 평소에도 제가 담당하던 익숙한 업무였죠.
스플래시 API는 모바일 기기의 정보를 받아 해상도에 맞는 이미지 URL을 반환하는 API입니다.
이번에는 스플래시 교체가 아닌 페이드아웃 요청이었지만, 교체할 이미지 URL 대신 빈 값을 넣어 반환하면 자연스럽게 스플래시가 페이드아웃 될 것이라는 가설을 세웠습니다.
가설에 맞게 코드를 수정한 후 제가 사용하는 아이폰에서 베타테스트를 해 보니,
매우 깔끔하게 메인화면에 진입되는 것을 보고 ‘역시 내 생각이 맞았어! 룰루랄라’  하는 자화자찬과 함께 해당 이슈의 상태를 배포 대기로 옮겼습니다.

[바로 그 치믈리에 포스터]
사실은 응답 데이터가 설계 목적과 달라지는 상황이라, iOS 및 안드로이드 개발자들과 적극적으로 업무를 공유하고 의견을 나눴어야 합니다.
하지만 협업 경험이 적고 소심한 성격 탓에 다른 팀에게 먼저 얘기를 꺼내기가 쉽지 않다는 이유로,
‘크게 바뀐 것도 아니고 사소한 기능인데 별일 없겠지!’ 하는 말도 안 되는 생각을 하게 됩니다.
또한, 업무에 익숙하지 않던 초기에는 간단한 로직 수정에도 동료들의 코드리뷰를 받고 여러 번의 테스트를 거쳤지만,
이제 업무가 조금씩 익숙해지고 바빠지면서 번거로운 단계는 알게 모르게, 은근슬쩍 생략하고 있었습니다.
각자의 업무는 스스로 해내는 것이 동료들을 편하게 해주는 것이라고 합리화하면서 말이죠.
이렇게 여러 단계의 안전장치를 무시해버린 채 운영에 배포되었고,
(제가 테스트해보지 않았던) 안드로이드 기기의 사용자들은 장장 6분간 (무려 컵라면을 2개나 만들 수 있는 시간) 배달의민족 앱에 접속할 수 없는 사태가 발생했습니다.
예상치 못한 사태에 가슴이 먹먹하고 팔다리가 저리는 멘붕에 빠져있는 동안, 팀원들과 앱 개발자들이 발 빠르게 대처해주셔서 장애 상황이 더 길어지는 것은 막을 수 있었습니다.
모든 장애를 미리 방지할 수 있으면 좋겠지만 사람이 하는 일이라 실수는 발생할 수 있다.
실수한 것에 대해 비난하지 않으니 너무 자책하지는 말되, 재발 방지를 위해 프로세스를 점검하고 문제점을 개선해야 한다.
같은 실수를 반복한다면 그것이 정말 부끄러워해야 할 일이다.
[장애 후 들었던 조언]
영화나 드라마를 보면, 경력 많은 주연급의 배우가 주인공이 아니고 몇 컷 등장하지도 않는 역할을 맡는 경우가 가끔 있습니다.
왜 이 역할을 수락했냐는 인터뷰어의 질문에 ‘시시한 역할은 없다. 시시한 배우만 있을 뿐’이라고 답하곤 합니다.
이는 비단 영화나 드라마계에만 해당하는 얘기가 아니라 우리 필드에도 적용되는 얘기입니다.
사소한 기능은 없고, 어떤 업무를 사소하다고 치부해버렸을 때 큰 문제가 되어 돌아올 수 있음을 느끼게 된 경험이었습니다.
지난 몇 년간 내버려 두었던 SNS를 입사하면서 다시 시작했습니다.
고민이 있을 때마다 형식도 없고 맥락도 없이 편하게 글을 남기면, 다음날 동료들이 다가와서 수다 타임을 제안합니다.
리빙 포인트! 커피가 마시고 싶을 땐, 고민이 있는 척을 한다.
제가 우아한형제들에 들어와서 가장 감사한 것은, 바로 이러한 동료들을 만난 것입니다.
나의 고민을 나눌 수 있는 동료, 나를 믿어주는 동료, 내가 성장할 수 있도록 도와주는 동료들이 있기에, 마음껏 고민할 수 있고 마음껏 도전할 수 있습니다.
지금 여러분의 고민은 무엇인가요?
"
http://woowabros.github.io/study/2018/08/01/linear_regression_qr.html,2018-08-01,파이썬으로 Linear Regression 해보기,"안녕하세요.
우아한형제들 데이터서비스팀에서 섹시미 막내를 맡고 있는 김세환입니다.
데이터를 보다보면 예측모델을 만들게 되는 경우가 많은데요, 
대부분의 경우 모델을 만들기 위한 함수들이 라이브러리 형태로 구현이 되어있다보니, 이런 라이브러리를 가져다 쓰는 것이 일반적입니다. 이 때문에 모델을 만드는 데 있어 그 내용은 놓치고 지나가기 쉽습니다.
이 포스트에서는 Linear Regression(선형회귀)를 파이썬으로 직접 풀어보고 내용을 한번 되짚어보며, 데이터를 통해 어떤 식으로 값을 예측을 할 수 있을지 간단하게 알아보려고 합니다.
다음과 같은 데이터가 있다고 해 봅시다.
각 배달건에 대해 배달거리와 배달시간을 기록해 둔 데이터인 듯 합니다.
이 데이터를 보면 딱히 계산을 하지 않더라도, 
“200m정도 떨어진 곳에서 배달시키려고 하는데, 몇 분정도 걸릴까?” 라는 질문에 
“음… 30-40분 정도 걸릴 것 같은데?” 라고 말할 수 있을 겁니다.
아마 100미터 거리가 20분, 150미터가 24분, 240미터가 32분… 과 같이 거리가 증가함에 따라 시간도 비슷한 속도로 증가하고 있는 패턴을 보이기 때문일겁니다.
데이터를 시각화 해보면 그런 패턴이 한 눈에 보입니다.
[파이썬 Matplotlib을 사용하면 쉽게 그래프를 그려볼 수 있다.]

명확하지는 않지만 어느정도 패턴이 보입니다. 
거리가 늘어남에 따라 시간도 비슷한 속도로 늘어나는 것을 볼 수 있습니다.

[대에충 빨간 선 느낌]
대에충 이런 느낌이면 50m거리에서는 13? 14분정도 걸린다고 예상할 수 있을 것 같습니다. 
200m면 30분정도면 배달이 될 것 같네요.
오늘 문제는 이런 선형패턴을 찾는 문제입니다. 
이런 패턴을 찾으면, 다음에 배달이 있을 때 어느정도 시간이 걸릴지 예측해보는데 도움이 될겁니다. (맞을지 틀릴지는 모르지만요…)
하지만 (대~충 그려보는 것 말고) 선형패턴을 찾으려면 어떻게 하는게 좋을까요?
위 예제에서 볼 수 있는 것 처럼 데이터의 패턴을 비슷~하게 따라가는 선을 그을 수는 있지만 모든 점 위를 지나가는 하나의 선은 그을 수는 없습니다. 
따라서 예측 값과 실제 값 사이에서 차이가 발생하게 됩니다.

[점이 선에서 멀수록 오차가 큰 것]
이 오차의 합을 최소한으로 줄이는 선을 찾는다면 예측을 가장 실제와 가깝게 하는 모델이라고도 할 수 있을 것 같습니다. 
(실제로는 데이터에 따라 더 여러가지 기준이 있을 수 있습니다.)
저희의 목표는 저 선과 점들의 거리의 합을 최소화 하는 것이기 때문에 
+, -가 있는 오차의 합보다는 오차 제곱의 합을 구하는 것이 더 바람직해보입니다.
이제 저희의 목표는 오차 제곱의 합이 최소화 되는 선을 찾는 것입니다. 
[Linear Least Squares]
사실 이런 선을 찾는 것은 배달거리 x와 배달시간 y 사이에 선형함수 $y = ax + b$가 있다고 생각하고 그 함수를 구하는 일입니다.
만약 그런 함수 $f$가 있다면, 다음과 같은 식들을 만들 수 있을 겁니다.
가진 배달데이터의 개수만큼 식을 가진 연립방정식이 나옵니다. 
연립방정식은 행렬로도 표현할 수 있습니다.
하지만 변수가 2개인데에 반해 식이 너무 많아서, $Ax = b$를 만족하는 $x$를 찾을 수는 없습니다. 
그래서 같음을 의미하는 $=$ 를 쓰지 못하고 $\cong$를 썼습니다. 
대신에 $Ax - b$를 최소화하는 $x$를 찾아야합니다. 위에서 그래프로 보았던 것 처럼요.
그럼 이제 이걸 풀어야 하는데… 이걸 어떻게 풀죠?
행렬 중 다음과 같은 특징을 가지는 행렬을 Orthogonal Matrix(직교행렬)이라고 부릅니다.
T는 transpose(전치)를 나타내는 기호로, 열과 행을 교환해서 새로운 행렬을 얻는 연산입니다. 
예를 들어 이런 식입니다.
대각선 방향으로 스윽 돌리기만 하는거에요.
행렬 $I$는 Indentity Matrix(단위행렬)로 대각선이 모두 1이고 나머지는 0인 행렬을 말합니다.
이 Orthogonal Matrix가 가지는 특징 중 하나가 바로
$Qv$의 크기(norm)는 $v$의 크기와 같다는 것인데요, $Q$를 임의의 백터에 곱해도 그 크기에는 영향을 주지 않는다는 것을 의미합니다.
자세한 설명은 생략합니다. 
이런 특성은 우리의 문제를 해결할 때 유용하게 쓰이니 기억해둡시다.
아까 위에서 식을 다음과 같이 만들었습니다.
그리고 $\left|Ax - b\right|$가 가장 작아지는 x를 찾는 것이 목표였죠.
만약 여기서, $A$가 Upper Triangular Matrix면 문제를 풀기 간단해집니다.
[Upper Triangular Matrix는 위 삼각형이 숫자, 아래 삼각형이 모두 0인 행렬을 말한다.]
이런 경우 식을 위(숫자가 있는 부분), 아래(전부 0인 부분) 두개로 나누어서 생각할 수 있습니다.
 
위 숫자가 있는 삼각형으로만 식을 만들었을 때 그 식을 만족하는 단 하나의 $x$를 구할 수 있습니다. 
그 밑에 전부 숫자가 $0$인 부분으로만 식을 만들었을 때는 어떤 $x$를 대입해도 $0$밖에 얻을 수 없습니다.
따라서 위를 $Rx = b_1$, 아래를 $0x = b_2$라고 표현했을 때, $Rx = b_1$을 만족하는 $x$를 구하면 우리는 답을 얻게됩니다.
오차의 크기는 $\left|Ax - b\right| = \left|b_2\right|^2_2$가 되겠죠.
A-1), A-2)에서 얘기한 성질을 통해, 문제를 더 간단하게 만들 수 있는 힌트를 얻었습니다.
[Q는 Orthogonal Matrix, R은 Upper Triangle Matrix]
를 만족하는 $Q$를 구하면, $Q^TA = R \rightarrow QQ^TA = QR \rightarrow A = QR$이 되고
로 나타낼 수 있게 됩니다.
이제 문제가 $Rx \cong Q^Tb$ 형태가 되었는데요, 이는 A-2)에서 봤던 것처럼 아주 풀기 쉬운 형태입니다. 
A-1)에서 본 것처럼, $Q^T$를 $b$에 곱해도 그 크기에는 영향을 주지 않기 때문에 A-2)처럼 풀 수 있습니다.
따라서, $A = QR$이 되는 $Q, R$을 찾기만 하면 문제는 아주 쉬워집니다.
$A$를 $QR$ QR코드 , 두개의 행렬의 곱으로 분해하는 방법을 QR decomposition(QR분해) 라고 합니다.
여기에는 몇가지 방법이 있는데, 오늘 소개해드리려고 하는건 Householder Transformation(하우스홀더변환)을 이용한 방법입니다.
행렬 중 하우스홀더행렬이라고 불리는 행렬이 있습니다. 
하우스홀더행렬은 길이가 1인 Unit Vector(단위벡터)$v$에 대해 다음과 같이 정의됩니다.
이렇게 만들어진 행렬 $H$는 임의의 벡터$x$에 곱했을 때, 벡터$v$에 직교하는 평면에 대해 벡터$x$를 반전시킵니다. 
따라서, $P = I - 2uu^T$일 때, 다음과 같은 그림을 그려볼 수 있습니다.

[$Px$는 $x$를 $u$와 직교하는 평면에 대해 반전시킨 결과다.]
이렇게 하우스홀더행렬은 벡터를 특정 평면에 대해 반전시킬 때 쓸 수 있습니다. 또, 하우스홀더행렬은 Orthogonal Matrix입니다.
그럼 만약 벡터$x$를 unit vector $y$의 방향으로 변환시키고 싶을 때는 어떻게 할 수 있을까요? 
벡터$x$를 unit vector $y$의 방향으로 변환시키면 $\left|x\right|y$가 될테니까, 벡터$x$와 벡터$\left|y\right|$를 양분하는 평면의 normal vector(법선벡터)를 구해 $v$라고 하고, 이 벡터로 하우스홀더행렬을 구해 $x$에 곱하면 됩니다.

[벡터$v$ 는 $x - \left|x\right|y$의 unit vector로 둘 수 있다.$]
[벡터$v$를 $x - \left|x\right|y$의 unit vector로 두면 $x$를 $\left|x\right|y$로 변환시킬 수 있다.]
여기서 만약 $y$가 첫번째 값이 $\pm 1$이고 나머지 값이 모두 $0$인 unit vector라면, $u = x - \left|x\right|y$라고 했을 때, $u$의 unit vector $v = \frac{u}{\left|u\right|}$라고 하고, 하우스홀더행렬 $Q_1 = I - 2vv^T$를 사용해 아래처럼
첫번째 열을 처음빼고 전부 0인 행렬로 만들 수 있습니다. (대박)
이걸 반복해서 하면 어떻게 될까요. 
그러니까 첫번째 줄은 1번째 빼고 다 0으로 만들었으니, 이번엔 첫번째 열, 첫번째 행을 제외한 나머지 부분에 대해서도 같은 계산을 해볼 수 있습니다.

[네모로 표시한 부분]
$Q_1$과 같은 방법으로 나머지 행렬에 대한 하우스홀더행렬 $\hat{Q_2}$를 구하면 $Q_1$보다 행렬의 크기가 작을테니, $Q_2$는
으로 정의해줍니다.
이런 $Q_1$ , $Q_2$를 $A$에 곱하면 다음과 같은 결과를 얻을 수 있습니다.
[Upper Trianglular Matrix를 얻었다!]
여기서 $Q_1$ , $Q_2$은 모두 orthogonal matrix이기 때문에, $\hat{Q} = Q_2 Q_1$ 또한 orthogonal matrix입니다. 
$Q = \hat{Q^T}$라고 할 때, $A$를 다음과 같이 표현할 수 있습니다.
[대박사건…]
이 과정을 파이썬으로 구현하면 다음과 같이 구현할 수 있습니다.
[하우스홀더 변환을 이용한 QR분해 구현. 출처]
드디어! $A = QR$의 형태를 얻었으니 이걸 이용해 원래 풀고 싶었던 문제인 $Ax \cong b$를 풀어 배달거리에 따른 배달시간 예측모델을 만들어볼 수 있습니다!
[파이썬 numpy를 사용해 간단하게 문제를 풀 수 있다.]
그리고 얻은 결과를 그래프로 그려볼 수 있습니다.

[slope: 0.092, intercept: 11.33]
[저 선 하나 그리려고 지금… ]
짠! 배달시간을 얼추 예상해볼 수 있는 모델이 생겼네요. 
$f(d) = 0.092d + 11.33$으로 거리 $d$에 따라 시간을 예상해볼 수 있겠습니다.
물론 실제 배달시간은 배달거리 이외에도 음식 종류, 날씨, 교통상황, 배달원이 누군지 등등, 여러 변수의 영향을 받게 되므로 실제로는 더 복잡한 방식으로 예측하고 있습니다. 하지만 이에 대해서도 유사한 방식으로 선형모델을 시도해 볼 수 있습니다.
이렇게 Linear Least Squares 문제를 파이썬으로 풀어보았습니다. 
감사합니다!
"
http://woowabros.github.io/woowabros/2018/07/02/woowahan_open_recruitment.html,2018-07-02,우아한 개발자 경력 공채,"우아한형제들/우아한신선들에서 개발자 경력 공채를 진행합니다.
많은 회사들과 마찬가지로, 우아한형제들에서도 개발자 분들을 열심히 채용하고 있습니다. 많은 좋은 분들이 함께 해 주고 계시지만, 사업과 서비스의 성장 속도가 더 빨라지고 있어서(2018년에도 성장 속도는 줄어들지 않고 더 빨라지고 있네요) 더 많은 분들이 필요한 상황입니다.
최근에는 우아한테크캠프 2기를 진행하면서, 이 과정이 끝나고 서로 원할 경우, 코딩테스트/서류전형/1차면접을 모두 생략하고 이후 채용 절차를 진행하는 형태로 신입 개발자 분들을 뽑는 시도를 하고 있는데요.
회사 내 개발자 분들과 얘기를 나누다 보니, 경력 개발자 분들도 공채라는 형태로 채용하는 것이 좋겠다는 생각을 하게 되었습니다. 한 달에도 몇 분씩 좋은 개발자 분들이 함께 해 주시는데, 이 분들이 조직에 빠르게 적응하고 성과를 내기 위해서는, 회사의 기존 시스템에 대한 교육과 함께 서로 편하게 얘기할 수 있는 네트워크가 있으면 좋겠다는 것이죠.
이것은 작년에 공채로 채용한 신입개발자 분들을 보면서 많이 느낀 부분입니다. 아무래도 공채로 채용되다보니, 1) 회사에서 필요로 하는 프로그래밍 지식도 같이 교육받는 기회가 있고, 2) 회사와 서비스에 대한 이해를 높일 수 있는 여러 프로그램도 제공되었으며, 3) 또 서로가 잘 알다보니 업무를 풀어 나가는 데 있어서 훨씬 더 빠르게 적응하고 좋은 성과를 내는 모습을 살펴 볼 수 있었습니다. 그래서 경력 개발자 분들도 공채 형태로 채용하려는 생각을 하게 되었습니다.
[우아한형제들 신입개발자 모집 영상]
위에서 공채 형태의 경력직 개발자 채용을 생각하게 된 이유에 대해서 말씀 드렸는데요. 공채로 진행하는 또 한 가지 이유를 말씀 드린다면, 우아한형제들에서 개발자를 뽑고 있다는 사실을 알리기 위함입니다. 많은 분들이 우아한형제들이 개발자를 뽑는다는 사실을 알고는 있지만, 당장 지원할 것이 아니라면 그냥 스쳐 지나가는 생각이 되어 버리고 맙니다.
아주 적극적으로 이직을 고민하는 분들이 아니라면, 누군가의 소개를 통해 지원한 것이 아니라면, 지금 다니는 회사와 하는 일 외에 특별히 외부 회사 동향에 신경을 쓴 분이 아니라면, 우아한형제들이 개발자를 적극적으로 뽑는다는 사실 자체를 모르실 수 있다고 생각했습니다.
그래서 이번 경력 개발자 공채 시기에 맞춰서, 아래 사진에서 보시는 것과 같이 지하철 역과 버스 정류장 및 버스에 우아한형제들 개발자 모집을 알리는 활동을 병행하고 있습니다.




그리고 이번에 우아한형제들의 개발 조직 관련해서 또 한 가지 큰 변화가 있는데요. 서비스를 만드는 기획/디자인/개발 조직이 몽촌토성역 옆 사무실에서 나와서, 잠실역 근처로 새로운 사무실을 얻어서 이전한다는 것입니다. 아무래도 개발 조직의 경우, 여러 대의 모니터를 이용하다보니 책상과 같은 근무 환경이 좀 더 넓게 확보될 필요도 있었고, 우아한형제들이 워낙 빠르게 성장하다보니 현재 이용하는 사무실도 올해가 가기 전에 공간이 모자라서 새롭게 사무실을 얻게 되었습니다.
그래서 이번에는 새로운 사무실 이전에 맞춰서, 분양 광고 컨셉을 패러디하여 우아한형제들 개발자 대모집을 알리는 채용 공고 영상을 제작하여 보았습니다. 아래 영상을 보고 재미있거나 맘에 드신다면, 주변의 개발자 분들도 한 번 보고 웃으시라고(그리고 지원을 생각하시라고 :-) 공유 부탁 드립니다.
[우아한형제들 경력개발자 모집 영상]
이번 공채는 우아한형제들/우아한신선들에서 배달의민족과 배민찬 서비스를 개발하실 분을 채용하는 과정입니다. 모집 분야는 다음과 같습니다.
위 직무 중 서버프로그래머, 웹프론트엔드 프로그래머, 데이터엔지니어는 우아한신선들에서도 채용을 진행하며, 공채 지원 시에 두 회사 동시 지원 또는 한 회사 지원을 선택하시면 됩니다. 경력직 공채의 규모가 궁금하실 수 있는데, 이번 경력 개발자 공채를 통해 채용하고자 하는 목표 인원은 수십명 수준입니다. 위에서도 잠깐 공유 드렸다시피 영상 제작, 옥외 광고를 비롯하여 우아한 개발자 경력 공채 진행을 알리고 진행하기 위해 정말 많은 사람들이 고생하고 있는데요. 이 분들의 수고를 헛되이하지 않기 위해서라도 많은 분들이 지원했으면 하는 바람이 있습니다.
이번 우아한개발자 경력 공채의 서류 접수 기간은 7월 23일부터 8월 5일까지입니다. 자세한 내용은 경력 개발자 공채 지원 페이지를 통해서 확인하실 수 있고, 우아한닷컴의 인재채용-경력개발자모집 링크를 통해서도 확인하실 수 있습니다.
우아한형제들/우아한신선들이 왜 개발자를 많이 채용하려고 하고, 개발자 분들이 오시면 어떤 일을 할 수 있다고 생각하고 있으며, 개인 입장에서는 여기서 무엇을 얻을 수 있을 지에 대한 생각은 “우아한형제들의 Developer Relations”라는 글에서 설명을 드린 바 있는데요. 이번에 지원을 고민하시는 분들이라면, 공채 지원 페이지만 보지 마시고, 바로 앞에서 말씀 드린 글을 꼭 읽어 보시면 좋겠습니다.
“우아한형제들의 Developer Relations” 글에서 이런 말씀을 드린 적이 있습니다.
좋은 회사는 현재로서 완성된 좋은 모습을 갖추어서가 아니라, 어떤 것이 좋은 것인지, 더 좋은 것인지를 지속적으로 고민하고 실행에 옮길 수 있는, ‘좋음’의 모습이 현재 진행형인 회사라고 생각합니다. 우아한형제들이 매출이 가장 높은 회사여서가 아니라, 복지 혜택이 가장 좋아서가 아니라, 일을 더 잘 하기 위한 고민을 같이 나누고 변화시켜 나갈 수 있기에, 이 글을 보시는 많은 좋은 개발자 분들과 같이 일하고 싶다는 말씀 전하고 싶습니다.
혁신이라는 것은 뭔가 멋진 것을 얘기하는 것이 아니라, 현재의 불편한 점을 꾸준히 개선하는 것을 반복할 때 이룰 수 있다고 믿습니다. 지금은 무모해보여도, 아주 작게나마 변화를 위한 다양한 시도를 할 때 이룰 수 있다고 믿습니다. 아래 영상은 2022년에 로봇이 가져올 수 있는 변화를 컨셉 영상으로 만들어 본 것입니다. 이 영상은 2022년보다 가까운 미래가 될 수도, 2022년보다 먼 미래가 될 수도 있습니다. 이러한 미래가 오기 위해서는, 같은 미래를 꿈꾸는 사람들이 필요하고, 단지 꿈꾸는 것에서 머무르는 것이 아니라 그 꿈을 이루기 위한 시도와 변화를 같이 만들어 갈 분들이 이런 경력 개발자 공채에 많이 관심 갖고 지원해 주시면 좋겠습니다.
[배달의민족이 꿈꾸는 배달로봇 Life]
"
http://woowabros.github.io/security/2018/06/29/aws-network-mirror.html,2018-06-29,AWS에서 서버(EC2) 패킷 미러링 하기,"Public cloud의 문제점중 하나는, 가시성 확보가 어렵다 이다. 보안을 위해서 여러 Layer의 가시성 확보가 필요한대, 특히 Network 트래픽 데이터에 대해서는 필수이다. on-premise 환경에서는 TAP hardware 장비등을 이용하여 가시성 확보가 가능 했지만, Public cloud에서는 hardware를 사용할 수 없기때문에 다른 방법을 사용해야 한다. 리눅스 서버의 경우 iptables의 TEE module을 사용하여 간편하게 네트워크 트래픽을 미러링 할 수 있는데, 이 방법에 대해 기술하고자 한다.
Public cloud의 보안을 하다보면 큰 문제에 부딪히는데, 바로 가시성 확보가 어렵다는 부분이다. 특히 네트워크 트래픽 데이터에 대한 검증은 필수 요소이나 L3/L4 수준의 가시성만 제공한다. 하지만 정확하고 정밀한 보안을 하려면 L7수준 까지의 확보가 필요하고, 아쉽게도 모든 Public cloud에서는 Network mirroring기능을 제공하지 않는다. 그렇다고 상용 솔루션을 구매해서 agent를 설치하자니 비용적으로나 운영측면에서 장애 요소가 부담 되는데, 이를 해결 할 수 있는 방법을 iptables에서 찾아보았다.



(Unix iptables man page에서 발췌)


Unix의 tee 명령은 특정 프로그램 실행 결과를 입력으로 받아서, 파일이나 스트링으로 출력 할 수 있다. 이것을 iptables에 접목한 것으로 iptables의 TEE module은 Network Interface card(NIC)에서 발생한 트래픽을 다른쪽(Server/NIC)으로 전달하고자 할때 사용할 수 있는데, 흡사 Network switch 또는 TAP 장비와 같이 mirroring이 가능하다.



테스트용 구성은, 외부망의 Laptop과 EC2(118.118.77.30)간 통신 트래픽을 IP주소 118.118.84.105 EC2에게 Mirroring 하도록 해서, 105번 인스턴스에서 laptop과 30번 인스턴스간 통신 내역을 확인 할 수 있도록 했다.
1.1 118.118.77.30 EC2로 ssh 접속.
1.2 sudo iptables -t mangle -I POSTROUTING -j TEE –gateway 118.118.84.105 명령어 실행으로 내부 Private IP를 외부 Public IP로 변환 mirroring 정책 추가.
1.3 sudo iptables -t mangle -I PREROUTING -j TEE –gateway 18.118.84.105 명령어 실행으로 외부 Public IP를 내부의 사실IP로 변환 mirroring 정책 추가.
1.4 sudo /etc/init.d/iptables start 명령 실행으로 iptables 활성화.
1.5 sudo iptables -t mangle -L 명령 실행으로 1.2와 1.3의 정책이 잘 들어 갔는지 확인.



(정책이 잘 들어갔다면, 이렇게 확인 되어야 한다)


1.6 재부팅시 iptables 정책이 초기화 되는데, 이를 방지 하려면 sudo /etc/init.d/iptables save 명렁 실행.
원하는 protocol, port를 필터링 하려면 URL 참조(http://www.packetinside.com/2012/08/iptables.html)
2.1 AWS의 EC2 콘솔 접속.
2.2 promiscuous mode 활성화를 위해, Mirroring 받을 EC2 선택 후 Action → Networking → Change Source/Dest. Check → Disable 선택.
기본적으로 EC2는 자기 자신을 향하는 트래픽이 아니면, 받지 않도록 되어 있다.




(Disable 버튼을 클릭하자)


3.1 mirroring 트래픽을 수신하는 EC2 접속.
3.2 sudo tcpdump -nni eth0 -A src 118.118.77.30 명령어로, 30번 EC2에서 들어오는 트래픽의 데이터만 볼 수 있도록 tcpdump 실행.
3.3 laptop에서 30번 EC2의 Public IP주소로 ping이나, ssh로 접속해서 외부의 웹사이트에 접속 해보기.
3.4 mirroring 트래픽을 수신하는 EC2에서, 30번 EC2의 트래픽이 tcpdump로 보인다면 성공! 만약, 안된다면 promiscuous mode 설정이 올바로 되었는지 확인.




(laptop에서 30번 EC2로 ping 보내는것을 105번 EC2에서 tcpdump로 확인 할 수 있다. )


mirroring 받은 트래픽을 활용 할 수 있는 방법은 많이 있다. suricata나 Bro와 같은 opensource IDS를 설치해서, VPC 내부 트래픽을 모니터링 하거나 요즘 문제가 되고 있는 AWS의 네트워크 품질에 대해서 packet들의 시퀸스 번호를 트래킹 한다면, 어느정도 loss가 발생하는지 등의 품질 측정도 가능하다.


끝!


"
http://woowabros.github.io/experience/2018/06/26/bros-cicd.html,2018-06-26,라이더스 개발팀 모바일에서 CI/CD 도입,"이 글은 CI/CD를 안드로이드에 도입하게 되면서 정리한 내용입니다.
  구축 및 운영하고자 하시는 분에게 경험을 공유하고자 합니다.
안녕하세요 라이더스 개발팀 장인수 입니다.
우선 라이더스 개발팀이 하는 일을 소개 합니다. 저희 라이더스 개발팀은 배달되지 않는 음식점의
음식을 민트색 헬멧을 쓴 라이더 분들이 오토바이를 이용하여 음식을 픽업 후 고객님에게 배달하는 일정
과정들을 원활하고 효율적으로 운영이 될 수 있도록 개발하는 팀 입니다.
그중 모바일(app)은 라이더스 분들이 주문이 들어오면 주문을 확인 후 픽업 -> 배달까지 필요한 부분을 제공하고 빠르게 이용 할 수 있도록 개발하고 있습니다.
라이더스는 B2B앱으로써 일반사용자를 위한 앱은 아니고 오직 라이어스 분들만을 위한
앱이라고 할 수 있습니다.
Build , Test를 실시하는 프로세스를 말하며 이러한 통합 프로세스를 상시로 실시해 주는것을 CI라고 합니다.
짧은 주기로 소프트웨어를 개발하는 소프트웨어 공학적 접근의 하나로, 소프트웨어가 언제든지 신뢰 가능한 수준으로 
출시될 수 있도록 보증하기 위한 것이다. 소프트웨어를 더 빠르게, 더 주기적으로 빌드하고 테스트하고 출시하는 
것을 목표로 한다. 
이러한 접근은 더 많은 증분 업데이트를 업무 애플리케이션에 적용할 수 있게 함으로써 변경사항의 배포에 대한 비용, 
시간, 위험을 줄일 수 있게 한다.
짦은 주기로 개발중인 소프트웨어를 배포하고 그 과정을 자동화 하겠다는 뜻이다.

수동으로 개발자의 손을 통해서 배포가 이루어지다 보니 Human Error 의 발생의 소지가 있고 앱은 서버의 배포와 달리 한번 잘못 배포가 되어지면 다시 배포하는 과정의 어려움이 작지 않아서 최대한 자동화를
이루고자 하는 마음에서 도입하게 되었습니다.
CI/CD를 하기위해서는 여러가지 선택이 있습니다. Travis CI, Circle CI, BITRISE, Jenkins 등이 있지만, 아래 같은 장점을 가진 Jenkins 를 선택하게 되었습니다.
그리고 도커를 선택한 이유는 젠킨스 서버를 띄우기 위해서는 여러가지의 서버 설정등과 설치등이 필요한데
이 모든 일련의 과정을 Dokerfile에 작성을 하고 손쉽게 띄울수 있는 도커를 선택에서 작업시간 및
운영에 오는 리소스를 줄이고자 선택하였습니다.
아래 그림과 같이 도입하려고 했습니다.

저희는 Slack 등을 사용하고 있어서 apk 를 타겟 시스템 별로 아래 그림과 같이 배포하도록 만들었습니다.
Slack에서는 slack bot api등을 지원을 해줘서 어렵지 않도록 Slack bot을 만들고 사용할 수 있습니다.
여기서는 자세한 설명은 생략하고 위의 https://api.slack.com 로 가시면 자세한 사항을 만들어 볼 수 있습니다.
저희는 Slack bot을 만들고 그것을 사용해서 아래 그림과 같이 배포하여서 사용하였습니다.

Docker 는 docker hub 라는 repository가 있습니다. 개발자들은 다들 알고 계신 github와 비슷하다고 보시면 됩니다.
간단하게 jenkins official dockerfile 을 이용하고 약간의 추가 사항을 통해서 어렵지 않도록 Jenkins를 이용하기 위한 docker image 를 만들 수 있습니다.
https://hub.docker.com 에 접속해서 jenkins를 검색하면 아래 그림과 같이 나옵니다.

jenkins Official 이미지는 Android SDK 등이 포함 되지 않았으므로
git repository 를 clone 해서 수정해서 사용하도록 합니다.
공식 dockerfile 은 아래와 같고
dockerfile link
공식 dockerfile 에서 안드로이드에서 사용하기 위해서
Gradle , OpenJDK, Android SDK를 추가하도록 하겠습니다.
위와 같이 하면은 Jenkins가 띄워지고 사용하실 수 있습니다.
아직 한땀, 한땀 으로 공들여서 배포하시는 분들은

CI / CD 를 도입해서 아래와 같이 공지해주는 것에서 해방 되시기를 바랍니다.

"
http://woowabros.github.io/experience/2018/06/11/frrom_woowahna_techcamp.html,2018-06-11,우아한테크캠프를 통해 내가 얻은 것들,"안녕하세요! 작년 12월에 우아한형제들에 입사한 신입 개발자 배민프론트개발팀의 박예준입니다!
딱 작년 이맘때에 즈음 우아한테크캠프 지원서를 작성하고 있었는데, 이렇게 기술 블로그에 글을 남기려니 기분이 이상하네요.
2018우아한테크캠프를 앞두고 있습니다. 
앞서 우아한테크캠프 참가자에서 우아한개발자가 되기까지 를 작성하신 전한나님과 함께 작년 우아한테크캠프 참가자로서 우아한개발자가 되기까지의 경험을 담은 글을 작성하면 우아한테크캠프를 준비하시는 분, 우아한개발자가 되기를 희망하시는 분들께 조금이나마 도움이 되지 않을까 하여 시리즈물을 기획하게 되었습니다.
제 전공은 기술경영학입니다. 경영계열 학문으로 커리큘럼에 프로그래밍 포함되어있지 않지만, 제가 S/W 분야에 관심을 갖고 독학을 시작하게 된 계기를 만들어주었습니다.
독학으로 웹 개발 공부를 진행하는 것은 쉽지 않았습니다. 
생활코딩, gitbook, codecademy 등 수많은 채널들을 통해 독학을 도전했지만, 완주에 실패하기 일쑤였습니다.
연속되는 작심삼일에 지칠 때 즈음 ‘멋쟁이 사자처럼’ 이라는 비전공자 대상 개발 교육 동아리에 참여하게 되었습니다.
멋쟁이 사자처럼에서 진행된 팀 프로젝트를 통해 하나의 서비스를 기획하고 개발하는 플로우를 처음으로 완주해보게 되었습니다. 저희 팀의 아이디어를 개발해나가는 과정에서 그동안 느껴보지 못했던 몰입감과 짜릿함을 느껴 볼 수 있었습니다.
이 경험을 계기로 저에게 개발이란 막연히 배우고 싶었던 것에서 재미있는 것, 내가 잘 하고 싶은 것으로 다가왔습니다.
개발이 점점 재밌어지면서 전공인 경영계열 직무를 선택하는 길과 개발을 심도있게 공부하여 개발자의 삶을 사는 것을 고민하던 중에 2017우아한테크캠프에 참여하게 되었습니다.
제가 우아한테크캠프를 통해 얻고자 했던 것은 아래 두 가지였습니다.
결론부터 말씀드리자면, 제가 얻고자 했던 것보다 더 많은 것을 얻을 수 있었습니다.
우아한테크캠프의 수업은 일방적인 지식전달의 형태가 아닌, 미션을 바탕으로 한 피드백으로 이루어졌습니다.
미션이 주어지면 이를 기간 내에 풀어오거나 답을 찾아오는 게 목표가 아니라 문제를 해결하기 위한 방안을 스스로 고민해보고 자신의 접근법을 설명할 수 있도록 하는 것이 목표였습니다.
(프론트엔드 트랙의 경우에는) 아침마다 gist 로 제출한 과제물의 피드백을 나누는 시간을 가졌었습니다. 이 시간을 통해 제 해결방법의 모자란 점이 무엇이었는지, 어떤 것을 더 공부하면 좋을지 피드백을 받을 수 있었습니다.
이러한 피드백 중심 커리큘럼은 문제 접근과정에서의 방향성을 스스로 살펴보는 좋은 습관을 가지게 해주었습니다.
우아한테크캠프는 총 2개월의 기간 중 1개월의 강의와 1개월의 팀 프로젝트로 진행되었습니다. 길지 않은 기간이기 때문에 강의 진도가 꽤나 빠르게 진행되었습니다. (제 역량이 모자라서 더 짧다고 느껴진 것 같습니다..) 
강의 진도를 맞추기만 하기엔 모자란 점이 너무 많아 거의 매일 남아서 공부하고 갔습니다. 스스로의 모자람을 채워나간다는 만족감과 성장의 뿌듯함이 큰 모티베이션이 되었던 것 같습니다.
팀 프로젝트에 들어오고 나서는 이 몰입감은 더욱 깊어졌습니다. 거의 매일을 9 to 10 의 일정을 소화하는데도 개발을 하는 순간순간들이 너무 즐거워서 지치는 줄  몰랐습니다.
프로젝트 막바지 때 즈음에는 어떤 버그를 잡지 못해서 전전긍긍하다 잠에 들었는데, 잠결에 해결 방법이 생각나서 이걸 해결하고 다시 잠에 든 경험도 있었습니다 ㅎㅎ
이렇게 즐겁게 몰입해봤던 경험은 개발자로서의 진로에 확신을 갖게 해주었습니다.
제가 위에서 언급한 우아한테크캠프를 통해 얻고자 했던 리스트에선 협업과 관련한 내용이 없습니다. 사실 지원 당시에는 제 개발능력 향상에만 초점을 맞추다 보니 협업능력의 성장 부분을 간과했던 것 같습니다.
프로젝트를 진행하면서 가장 많이 고민했던 부분은 협업과 관련한 것들이었습니다. 개발이란 것은 혼자 하는 것이 아니기에 서로 의견을 나누고 함께 문제를 해결하는 과정을 거쳐야만 했습니다. 교육을 담당했던 코드스쿼드의 마스터들이 항상 강조했던 협업의 중요성을 그제서야 진짜로 이해할 수 있었습니다.
잘 싸워야 한다.
교육과 프로젝트를 진행하면서 협업과 관련하여 가장 크게 느낀 점은, 잘 싸워야 한다는 것이었습니다.
우아한테크캠프 교육을 담당했던 코드스쿼드의 마스터분들은 항상 싸우는 걸 피하지 말라고 하셨습니다.
저는 의견 다툼을 가능한 피하려는 경향이 강했었는데, 불필요한 갈등을 만들지 않기 위한 나름의 희생(?)이라고 생각했습니다. 하지만 이 생각은 프로젝트를 진행하면서 바뀌게 되었습니다.
논리적이고 타당한 근거가 있다면 충분히 상대방을 설득시킬 수 있는데, 이전에는 제 의견에 근거가 모자라서 상대방을 설득시키지 못했던 것이었습니다.  그리고 그것을 희생이라고 스스로 합리화했었습니다.
의견 다툼은 결국 좋은 결과물을 만들어내기 위한 좋은 방법을 찾아가는 과정이었습니다. 의견 다툼을 피한다는 것은 좋은 결과물을 만들 방법에 대해 논의할 수 있는 좋은 기회를 날려버리게 된다는 것을 알게 되었습니다.
우아한테크캠프 프로그램을 통해 개발에서 협업이 얼마나 중요한 부분인가를 느낄 수 있었고, 좋은 협업이란 무엇인가 고민하기 시작하게 되었습니다.
보다 자세한 우아한테크캠프 이야기를 보고 싶으시다면
이렇게 제가 성장하고 몰입할 수 있었던 것은 우아한테크캠프였기 때문에 가능했다고 생각합니다.
이런 환경이 제공되었었기에 제 성장에 집중할 수 있었습니다.
그리고 전 개인 역량의 성장은 물론, 진로의 확신까지 얻을 수 있었습니다.
성장에 대한 욕심이 있으신 분들이라면, 우아한테크캠프를 통해 정말 많은 것을 얻고 성장해 나가실 수 있을 거라고 확신합니다.
"
http://woowabros.github.io/experience/2018/06/07/vue-story-of-baminchan.html,2018-06-07,배민찬은 Vue를 어떻게 사용하나요?,"이 글은 제이쿼리, PHP 기반의 쇼핑몰 서비스에 Vue를 도입한 사례를 정리한 내용입니다.
서비스에 Vue 도입을 고민 중이신 분들을 위해 경험기를 공유합니다.
배민찬은 푸드 커머스 사업에 혁신을 만들어가는 스타트업 서비스다.
우리 개발팀은 솔루션 기반의 커스터마이징된 쇼핑몰을 시작으로 올해 3년 이상 레거시 코드와 분투 중이다.
팀내 프론트엔드 개발자로서 리엑트, 앵귤러, 노드 같은 트렌디한 키워드가 떠올랐고, 하루라도 빨리 효율적인 기술 스택으로 갈아타고 싶었다.
제한된 개발 리소스에 빅뱅 방식의 개선은 비현실적이라고 판단. 조금씩 시도해 보고 빠르게 결과를 검증해 나가는 방법이 필요했다.
점진적인(Progressive) 자바스크립트 프레임워크
“점진적인”을 슬로건으로 내세운 Vue 프레임웍은 이러한 고민에 대한 일말의 해결책으로 보였다.
곧장 기술 조사와 안정성 검토 후 팀내 소프트 랜딩을 위한 전파 교육을 진행했고, 기존의 기술 부채를 조금씩 개선해 나가기 시작했다.
한 페이지만 CDN으로 다운받아 사용해 보는 것을 시작으로 NPM 다운로드를 통해 좀 더 확대 적용하였다. 
결국 .vue라는 단일파일컴포넌트(Single File Component)까지 들여왔고 하나의 파일에서 마크업, 스크립트, 스타일을 관리하는 방법으로 확대했다. 
이것은 Vue가 주장하는 “점진적인” 방법을 활용하는 것이었고 운영중인 서비스에 안정적으로 기술을 도입하는 좋은 방법이었다.
레거시의 가장 큰 문제는 코드의 중복!
하나의 코드로 다양한 디바이스에 동작하는 반응형 웹과는 달리, 배민찬은 디바이스별로 파일을 작성해야 하는 일반적인 웹이다.


[모바일과 데스크탑]

모바일 페이지 개발을 마치고 비슷한 데스크탑 페이지를 만드는 것은 무척이나 지루한 일이다.
디바이스 크기별로 작성하는 마크업과 스크립트는 중복이 많았고 소프트웨어 공학 관점에서도 전혀 드라이(DRY)하지 않은 코드로 보였다.
Vue의 컴포넌트 조합은 이러한 문제를 꽤나 효율적으로 해결할 수 있는 도구인데 가이드라인에 따르면 두 가지 방법이 있다.
믹스인과 확장(extends)
공통의 부모 컴포넌트를 만들고 이를 자식 컴포넌트가 상속하는 믹스인 방식보다는, 기존 컴포넌트를 확장해서 유사한 컴포넌트를 만들 수 있는 extends 옵션이 더 적합해 보였다. 
모바일 컴포넌트를 먼저 만들고 이를 확장한 데스크탑 컴포넌트를 만들기 때문이다.
코드로 보면, 먼저 모바일 컴포넌트를 이용해서 화면을 만든다.
이러한 모바일 컴포넌트는 extends 옵션을 이용하면 단숨에 데스크탑 컴포넌트를 정의하는 데 재사용 될 수 있다.
라이프사이클 훅과 메소드는 일정한 규칙에 따라 오버라이딩 되기 때문에 두 컴포넌트는 같은 코드를 공유하면서 비슷하게 동작한다.
이러한 코드 재사용은 개발 속도를 비교적 빠르게 앞당겼고 무엇보다 신나는 일이었다! 
컴포넌트 병합 옵션에 대한 몇 개 특성만 이해하면, 다중 뷰를 위한 효율적인 컴포넌트 설계는 그렇게 어려운 일이 아니라고 생각한다. 
둘 간의 차이를 잘 살펴보고 사용하기 바란다.
(병합 옵션에 대한 자세한 사항은 UI 컴포넌트 확장 참고)
이미 Vue 기술을 도입한 깃랩은 블로그(번역)에서 언급한 것처럼 웬만하면 Vuex를 사용한다.
하지만 배민찬에서는 다소 부담되는 상황이다.
백엔드 개발자 위주의 팀에서 프론트엔드 기술(예를 들어 Flux) 이해에 대한 요구는 개별로 다르기 때문이다.
최대한 심플하게 뷰를 사용하고 싶었다.

[웬만하면 단순하게]
기술 도입에 앞서 먼저 동료들이 어떻게 Vue를 사용하는지 유심히 관찰했다. 
SFC 개발환경까지 갖추었지만, 예상과 다르게 하나의 루트 컴포넌트로만 화면을 만들고 있었다.
제이쿼리 기반의 화면 개발에 지친 우리는 v-bind를 필두로 한 다양한 Vue 디렉티브 사용에 더 흥미를 느꼈던 것이다.
돔(DOM) 조작으로 화면을 직접 제어하는 것은 복잡한 코드로 이어지기 쉽다.
반면, Vue 디렉티브는 데이터 기반의 사고를 유도하기 때문에 화면 로직과 맞닿아 있지는 않다. 
데이터만 잘 다루면 화면은 Vue가 알아서 제어해 준다.
UI 기반의 사고에서 데이터 기반의 사고 전환
이것은 데이터를 다루는 백엔드 개발자가 Vue를 바라보는 매력 포인트라고 생각한다.
그럼에도 불구하고 화면이 복잡해지면 컴포넌트로 쪼개야 하고 상태관리 솔루션을 도입하고 싶은 유혹이 생긴다.
아직은 충분히 Vue의 기본 기능에 익숙해져야 하는 단계라고 생각했고 대안이 필요했다.
복잡한 페이지가 아닌 이상, 위의 두 가지 규칙만으로도 화면을 구성하는데는 충분했다.
나중에는 2단계로 분리한 컴포넌트(예를 들어 페이지네이션)를 공통 파일로 분리한 뒤 적재적소에 끼워 넣어 재활용할 수 있었다.


[페이지네이션 컴포넌트를 재활용할 수 있다]
데이터 중심의 화면 개발임에도 불구하고 돔을 직접 제어해야 하는 경우는 불가피했다.
swiper 나 sticky-kit 같은 제이쿼리 기반의 플러그인이 그러한 경우다.
이것을 Vue로 직접 구현하기보다는 어떻게든 Vue에 녹여내는 게 더 효율적인 방법인데……
Vue는 돔 접근을 위해 커스텀 디렉티브를 만들라고 안내한다.
슬라이드에 사용하는 swiper 디렉티브로 래핑한 예제를 보면 써드파티 라이브러리를 어떻게 사용하는지 알 수 있다.
디렉티브 훅 함수(여기서는 inserted)를 적절히 이용해서 서드파티 라이브러리리가 돔에 접근하도록 도와줄 수 있다.
이것을 컴포넌트에서 사용하려면 뷰 생성 객체에 directive 키로 전달하면 된다.

한편 값을 변경하는 유틸리티성 함수는 Vue의 필터로 정의한다.
예를 들어 숫자 형식을 출력하는 필터를 다음과 같이 만들 수 있다.
이것을 사용하려면 뷰 생성 객체의 filters 키로 추가해야 한다.
디렉티브와 필터 모두 필요할 때마다 컴포넌트에 코드를 주입하여 재사용할 수 있다.
뷰 스캐폴딩을 자동으로 만들어주는 vue-cli는 기본적으로 SPA를 위한 프로젝트를 생성한다.
그러나 배민찬 서비스는 각 페이지별로 자바스크립트를 로딩하는 고전적인 MPA 구조라서 다른 방법이 필요하다.
처음에는 웹팩의 엔트리를 이렇게 수동으로 작성했다.
화면별로 폴더를 나누고 (/mobile/home, /desktop/home) 이에 따라 자바스크립트 파일을 분류해서 생성한다. 
화면을 추가할 때마다 엔트리 포인트 추가를 위한 웹팩 설정파일을 수정해야 하는 상황이다.
자동화가 필요한 시점!
규칙을 정했다.  화면별로 유일한 엔트리 포인트가 있는데 전부 같은 파일명(app.js)으로 만들고,
이러한 약속하에 엔트리 포인트를 자동 생성하는 코드를 추가했다.
getEntries() 함수로 각 화면별 스크립트가 있는 최상위 폴더명과 번들 파일명의 꼬리표(prefix)를 전달한다.
그 결과 모든 폴더의 app.js 경로를 찾아 번들명을 키로 하는 엔트리 객체를 만들어 낼 수 있다.
이렇게 자바스크립트 엔트리 포인트를 자동화함으로써 웹팩 수정 없이 자바스크립트를 추가할 수 있다.
프로그레시브한 Vue는 앵귤러, 리엑트에 비해 사전작업이 거의 없다. 
CDN 주소를 스크립트 태그에 로딩한 뒤 그냥 쓰면 된다. 익숙한 제이쿼리처럼 말이다. 
이런 모습이 운영 중인 서비스에 Vue를 사용하는데 비교적 가볍게 느껴졌다.

[Vue 홈페이지]
뷰의 컴포넌트 확장 방법은 데스크탑과 모바일 페이지를 따로 개발해야 할 때 매우 효율적이다. 
모바일 퍼스트라고 하지만 여전히 데스크탑을 무시할 수 없고 커머스라면 더욱 그렇다. 
한정된 리소스로 두 개의 플랫폼을 개발해야 하는 상황이라면 컴포넌트 재사용은 꽤 효율적인 솔루션이다.
화면 중심의 개발 방법에서 데이터 중심의 사고로 전환할 수 있다.
까다로운 화면 제어를 Vue에게 맡겨버리고 데이터 위주로 사고하면 UI 개발의 스트레스는 줄어들고 생산성은 향상된다.
다른 UI 프레임웍과 달리 Vue SFC는 마크업과 스타일을 한 파일에 정의할수 있으므로, 컴포넌트를 잘만 설계한다면 퍼블리셔와의 협업도 기대해 볼 수 있다.
배민찬에는 아직 할 일이 많다. 레거시를 탐험하고 멋진 코드로 개선하고 싶다면, 그리고 그런 경험이 절실하다면 우리 회사에 지원해 보는 것은 어떨까?
// 꼼꼼하게 리뷰해 주신 기술블로그 파워 커미터 종립님께 감사드립니다 🙏
"
http://woowabros.github.io/experience/2018/05/28/billingjul.html,2018-05-28,장애와 관련된 XtraBackup 적용기,"안녕하세요. 우아한형제들에서 빌링시스템을 개발하고 있는 이주현입니다.
입사한 이래로 2년 가까이 일하며 정말 다양한 문제를 마주하고 해결하며 소중한 경험을 쌓고 있습니다. 그중 얼마 전 MariaDB 백업 방식으로 적용한 XtraBackup에 대하여 이야기해보려고 합니다. 
그전에 잠시 2016년으로 돌아가 보겠습니다.
개발자가 언제 죽는다고 생각하나?  총알이 심장을 관통했을 때? 아니야..
불치병에 걸렸을 때? 아니지! 맹독 버섯스프를 마셨을때? 아니다!!
그건 바로 메인 DB를 날렸을 때다.
12월 27일 오후 3시
결제시스템 모듈을 개발하던 중 저의 실수로 빌링 데이터베이스의 주요 테이블 9개가 DROP 되는 사고가 발생했습니다. 모니터링 시스템에 빨간불이 들어오고 각종 장애알림이 빗발치기 시작했습니다. 식은땀이 흐르고 머릿속이 새하얘지며 아무것도 생각나지 않습니다. 
빨리 복구를 해야겠다는 생각은 가득한데 부끄럽게도 데이터베이스 시스템에 별다른 지식이 없던 저로서는 눈 앞이 캄캄해졌죠.

결국 이 장애로 배달의민족 결제가 잠시 중단되었습니다. 그나마 다행인 건 애플리케이션에서 임시 데이터베이스를 바라보도록 수정하여 장애 시간이 길지는 않았다는 점입니다. ㅠㅠ
우선 급한대로 Full Backup 데이터부터 복구 하기 시작했습니다.
당시 장애 상황을 재현한 데이터로 실제와는 많은 차이가 있을 수 있습니다.
빌링 데이터베이스는 매일 새벽 6시 mysqldump를 사용해 전체 데이터를 백업하고 있었습니다. 테이블별로 데이터를 SQL형식으로 생성한 뒤 압축한 형태였기 때문에 DROP 된 테이블을 쉽게 복구할 수 있었습니다.
많은 분들께서 아시겠지만 gzip으로 압축된 sql내용의 파일은 아래와 같은 명령어로 DB에 실행할 수 있습니다.
결제 데이터가 많이 쌓여있던 상황이라 시간이 조금 걸리기는 했지만 빌링 시스템 Open ~ 새벽 6시까지의 데이터는 복구할 수 있었습니다.

사실 중요한 점은 새벽 6시 ~ 15시까지의 데이터를 어떻게 복구하냐 였습니다.
어렴풋 MariaDB 서버에서 보았던 binary log가 도움이 되지 않을까 지푸라기라도 잡는 심정으로 구글링을 해보았습니다. 그 결과는 다행히 ‘가능하다’였습니다. binary log는 데이터 수정과 관련된 모든 정보가 담겨 있는 파일인데 크게 두 가지 중요한 목적이 있다고 합니다.
2진 형식으로 기록된 binary log를 텍스트 파일로 복구하는 데는 mysqlbinlog 유틸을 이용합니다. mysqlbinlog에는 다양한 옵션이 존재합니다. 그 중에서도 --(start|stop)-position, --(start|stop)-datetime은 데이터 복구시 아주 유용합니다. 특정 position이나 시간에 대한 데이터를 뽑아낼 수 있기때문입니다.
$ mysqlbinlog --start-datetime=.. | mysql -u root .. 와 같은 명령어로 복구된 이벤트 내용을 mysql에 직접 실행할 수 있습니다. 하지만 테이블 전반적인 내용들이 담겨있기 때문에 9개 테이블에 대한 필터링 작업이 필요하여 restore.log로 저장 후 수정 작업을 진행했습니다.
주의할 점은 --stop-datetime에 대한 시간을 잘못 지정하여 ‘DROP TABLE..’쿼리가 다시 실행되면 안됩니다.

이렇게 데이터가 모두 복구되었습니다.
부끄럽지만 binary log존재의 필요성이나 사용법 등을 처음 알게 된 계기가 되었습니다. 복구 방법에 대해 확신이 없는 상태에서 끝까지 믿고 맡겨주신 팀원들에게 아직도 고맙습니다.
위의 이야기는 하루에도 몇 번씩 회자(놀림)되고 있으며 제가 이 회사를 퇴사하는 순간까지 아니 퇴사 후에도 길이길이 남을 것 같습니다.
빌링 데이터베이스 대규모 개편 작업에 있었습니다. 신규 DB서버를 구매하고 파티셔닝도 진행하는 큰 규모의 작업이었습니다.
저도 모르는 사이 저희 팀 공식 DBA가 되어있던 저는 기존 데이터베이스의 데이터를 신규 서버로 이관하는 작업을 맡았습니다. 진행 방식은 위에서 언급한 장애 복구 방법과 비슷했습니다. 테스트도 할 겸 테이블 백업 데이터를 신규 서버에 INSERT 하기 시작했습니다.
그런데! 퇴근 시간이 지나도 끝날 기미가 보이지 않습니다. 주요 결제 테이블 한 개만 복구하는데도 엄청난 시간이 필요했습니다 (core가 32개인데 왜 사용하지를 못하니..). 그 동안 배달의민족 주문수가 급증하며 데이터가 많이 축적되었고 더이상 mysqldump를 통한 백업, 복구를 할 수 없다고 판단했습니다.  비슷한 장애가 발생했을 때 몇 십 시간씩 데이터 복구에 시간을 낭비 할 수는 없기 때문입니다.
그래서 이번 기회에 새로운 백업 방식을 알아보았습니다.
XtraBackup은 Percona에서 개발된 오픈소스로 백업 도구입니다. mysqldump가 테이블 생성, 데이터 쿼리에 대한 SQL 생성문을 갖는 논리적 백업이라면 XtraBackup은 엔진 데이터를 그대로 복사하는 물리적 백업 방식입니다.
MySQL 엔터프라이즈 라이센스에 포함된 백업 도구의 기능을 모두 제공할 뿐만 아니라 더 유용한 기능들도 제공합니다.
XtraBackup의 백업 방식은 크게 전체 백업, 증분 백업, 개별(db, table) 백업, 압축(qpress) 백업, Encrypted 백업이 있습니다. 또한 stream을 지원하기 때문에 파이프(|)를 통하여 다른 프로그램의 표준 입력으로 리다이렉션이 가능합니다.
이 내용에서는 전체 백업 + stream을 이용한 백업과 복구 방법에 대하여 공유해보려고 합니다.
Centos 6.x에 MariaDB는 10.2.x를 설치했습니다. 현재 최신버젼인 XtraBackup 2.4을 기준으로 합니다.
XtraBackup은 xtrabackup, innobackupex두 가지 유틸을 지원합니다. 백업을 해준다는 점에서는 같지만 각각 기능과 사용할 수 있는 옵션값에 차이점이 존재했습니다.
XtraBackup에서 innobackupex는 next major부터 삭제된다고 했으나 아직까지도 유지되고 있습니다. 관련 도서나 커뮤니티에서 대부분 innobackupex기준으로 설명하며 다양한 편의 기능이 포함되어 있기 때문에 innobackupex를 사용하여 설명 드리겠습니다.
빌링 데이터베이스는 장애 시 복구 과정을 단순하게 하기 위해 증분 백업을 사용하지 않고 전체 백업을 하고 있습니다. 말 그대로 운영중인 데이터베이스를 통짜로 복사합니다.
백업하는 동안에 table lock을 없애려면 --no-lock옵션을 추가해야 합니다. 하지만 InnoDB table이 아닌 상황에서는 경우에 따라서 일관성 없는 백업 결과가 나올 수 있으므로 사전에 확인이 필요합니다.
참고: cmdoption-innobackupex-no-lock
위와 같은 명령어를 실행하면 /home/backup/xtrabackup/yyyy-MM-dd_HH-mm-ss경로에 테이블, 리두로그, XtraBackup관련 데이터들이 함께 백업된 것을 확인할 수 있습니다.
XtraBackup은 qpress를 이용한 compress 백업을 지원합니다. 하지만 압축 효율성과 속도에 아쉬움이 있다면 stream + pigz를 사용할 수 있습니다.
tar를 해제할 때는 -i 옵션을 추가하셔야 합니다. eg) $ tar -xizf backup.tar.gz 관련문서
백업된 데이터를 그대로 사용하면 좋겠지만 항상 생각대로 되는 게 없습니다.. 백업이 특정 시점에 완벽하게 이루어지면 좋겠지만 데이터 크기와 서버의 성능에 따라 백업하는 시간도 수십 분 ~ 수 시간 걸릴 수 있습니다. 이때 INSERT, UPDATE, DELETE쿼리가 유입된다면 백업된 데이터와 일관성이 없어지게 됩니다.
복원 준비 단계는 백업 중 수행 된 트랜잭션 로그파일(xtrabackup_logfile)을 적용하여 데이터를 일관성 있게 만들어줍니다.
준비 단계는 백업한 뒤 즉시 실행할 필요가 없습니다. 데이터 복구 전 실행하셔도 됩니다.
복원 원리는 간단합니다. 준비된 백업 파일을 MariaDB의 datadir로 옮겨주면 끝입니다. 이와 관련된 옵션을 innobackupex에서 지원해줍니다.
그전에 주의할 점이 있다면 mysql 서비스를 종료하고 datadir에 내용이 남아있다면 다른 폴더에 백업을 한 뒤 비워줘야 합니다.
위와 같이--copy-back명령어를 사용하면 백업된 내용을 원본 디렉토리(/var/lib/mysql)에 이동시켜줍니다.
추가적으로 데이터 파일의 권한을 MariaDB 서비스 계정으로 수정이 필요할 수 있습니다.
지난번과 같이 오후 3시에 관리자의 실수로 데이터베이스를 삭제했고 새벽 6시에 진행한 전체 백업 덕분에 데이터를 어느 정도 복구했다고 가정합니다. 이제 우리는 새벽 6시 ~ 오후 3시 데이터를 binary log를 통해 복구해야 합니다.
그런데 binary log파일은 어떤 걸 사용해야 하고 백업을 종료지점은 어떻게 알아낼 수 있을까요? 그것은 바로.. 전체 백업 후 데이터를 준비하는 과정(--apply-log)에서 생성되는 xtrabackup_binlog_info파일에 있습니다. 해당 파일에는 백업에 사용된 binary log과 position값이 적혀있습니다.

mysql-bin.000004파일의 551 position에서 오후 3시까지의 데이터를 복구하면 됩니다. 이 정보는 데이터 복구할 때뿐만 아니라 Slave서버를 추가적으로 구성하는데도 유용하겠죠.
위에서 말씀드렸지만 테이블이 정확히 DROP 된 시간을 알아내는 건 중요합니다. 그렇지 않으면 같은 내용의 장애 쿼리(DROP..)가 다시 실행될 수 있습니다. 반드시 mysqlbinlog 결과 값을 새로운 파일로 리다이렉션 하시고 데이터를 확인하시기 바랍니다.
이렇게 복원이 모두 완료되었습니다.
데이터 사이즈가 크지 않다면 mysqldump를 사용하는 게 간단하며 복원 시에도 신경 써줘야 할 포인트가 적습니다 하지만 데이터 사이즈가 수십 ~ 수백 GB에 이르면 이야기가 달라집니다. 실제 빌링 데이터베이스에 적용한 결과 아래와 같은 차이를 얻어낼 수 있었습니다.

이 글에서 직접 다루지는 않았지만 증분 백업, 테이블별 백업 등 여러분 들에게 좋은 선택이 될 수 있는 기능들을 제공하고 있으니 XtraBackup 공식 메뉴얼을 둘러보시는것도 좋을것 같습니다.
데이터베이스의 서비스, 관리용 계정 분리하기. 쿼리 날리기 전 한번 더 확인하기. rm -rf막기 등 장애를 관리할 수 있는 포인트는 많이 있습니다.
아직까지도 전체 백업이나 binary log가 존재하지 않았으면 어떻게 됐을지 상상이 안됩니다.
혹시나 이 글을 보시는 다른 개발자분들도 장애 시 전체 복구 포인트가 존재하는지 점검하는 계기가 되었으면 좋겠습니다.
회사가 빠르게 성장하고 있습니다. 데이터가 급격하게 늘어나니 개발에 있어서도 고민하고 해결해야 할 부분들이 많이 생깁니다. 이런 일을 제가 언제 또 맡아서 해볼 수 있을까요.
성장할 수 있도록 좋은 서비스와 환경을 만들어주신.. 그리고 저를 믿고 맡겨주신 회사와 팀원들에게 너무 고맙습니다.
(이력서 한 줄 추가욧!)
mysqldump 소개
mysqlbinlog
xtrabackup innobackupex
도서: DBA를 위한 MySQL 운영 기술
도서: Real MySQL
"
http://woowabros.github.io/experience/2018/05/26/woowahan-juso.html,2018-05-26,주소검색서버(woowahan-juso) 개발기(上),"안녕하세요 라이더스개발팀 정세빈 입니다 :)
BROS(Baemin Riders Operating System)에 주소검색을 위한 주소검색서버 개발 및 배포 경험을 공유 드리려고 합니다.
BROS는 배민라이더스의 주문 건을 배달하기 위한 라이더 운영 시스템입니다.
라이더 분들이 배달할 때 픽업지 혹은 배달지의 위치가 굉장히 중요합니다.
해당 위치의 변경 및 확인을 위한 주소 검색서버가 필요했고, 여러 솔루션 중 직접 DB에 데이터를 쌓고 서버를 배포하는 것을 택하게 되었습니다.

[주소 검색 및 변경 화면]
위 화면처럼 주소 검색을 위한 서버가 (여러가지 이유로) 필요했었습니다. 그래서 저희는 SpringBoot Batch, Amazon CloudSearch, JPA등을 이용하여 woowahan-juso라는 주소검색 서버를 배포하였습니다.
사내에도 잘 모르시는 분들을 위해 공유 겸, 해당 서버를 구성하는 것들에 대한 개발 경험을 나눠 볼려고 합니다.
는 아닙니다.
주소 검색서버(woowahan-juso) 개발에 쓰인 서버 스펙 중, 이번 글에서는 SpringBoot Batch를 이용하여 주소DB를 재구축한 부분에 대해서 포스팅하려고 합니다.
그리고 SpringBoot Batch의 설정 및 사용방법 등 기술적 부분보다는 주소 데이터를 어떤식으로 다뤘는지, 왜 그렇게 했는지 등 경험을 중점적으로 다뤄 볼 예정입니다.
(기술적인 부분은 역시 Reference Doc이죠)
유심히(?) 보신 분들은 봤겠지만, 이번 글의 타이틀에 (上)을 붙였습니다. (下)에서는 재구축한 주소 DB를 이용하여 Amazon CloudSearch 서비스를 사용한 부분을 포스팅할 예정입니다. (Feat. 빅픽쳐)
왜 이미 있는 주소 서비스를 사용하지 않고 DB를 새로 작업한건지, 왜 일반적인 batch 프로세스를 개발하지 않고 spring batch를 이용했는지에 대한 자체 Q&A를 진행해봤습니다.
www.juso.go.kr 에 가면 주소검색 솔루션과 주소검색 API 그리고 txt형식의 주소 데이터를 제공해줍니다. 그중에서 몇 가지 이유로 txt형식의 주소데이터를 택해 새로운 주소DB를 구축했습니다.
주소검색 API는 데이터를 정부에서 관리하고, 주소검색 솔루션은 주소 업데이트를 일정 기간 이상 하지 않으면 서비스가 중단되는 특징이 있습니다.
BROS의 주소 현황과 배민의 주소 현황의 싱크를 유지해야하기 때문에, 늘 최신데이터를 업데이트 해야 하는 주소 서비스를 이용할 수 없었습니다.
상시 운영상황에서의 테스트를 위한 주소가 존재하는데, 해당 주소를 DB차원에서 관리하려면 불가피하게 DB를 재구축해야 했습니다.
만약 서비스의 운영환경에서 주소검색이 안되면 서비스의 에러상황으로 이어지게 됩니다. 하지만 www.juso.go.kr 에서 제공하는 서비스는 SLA에 대해서 보장해주지 않습니다. 이벤트 트래픽 혹은 갑작스런 장애상황 등에 SLA가 보장되지 않는 서비스가 껴있는건 서버 운영측면에서 리스크를 안고 가게 됩니다.
그래서 트래픽 대응, 장애대응 등 직접적으로 운영, 관리할 수 있는 주소 서버가 필요했습니다.
Spring Batch의 특징은 굉장히 많지만 그 중에서도 주소 데이터를 넣는 Batch Job을 실행할 때 얻을 수 있던 장점에 대해 써보겠습니다.
주소 txt파일을 읽어 주소데이터를 넣을 때 예상치 못한 에러로 Batch Job이 중 될 수도 있습니다.
파일 라인중 특정 컬럼이 비어있어서 NPE가 날 수도 있으며, 예상과 달리 Unique특성이 깨지는 데이터가 있어서 SQL예외가 발생할 수도 있습니다.
여러 데이터가 들어있는 주소txt파일은 전체로 보면 2천만이 넘는 row를 가지고 있습니다. 이런 많은 데이터를 전체적으로 valid체크하는 것도 어려움이 있고 valid하지 않다고 해서 필요없는 데이터라고 판단할 수도 없습니다. 또 실패했다고 처음부터 다시 job을 실행하기엔 데이터양이 많아 비효율적인 작업이 됩니다.
Spring Batch를 통해 Application을 실행하면 BATCH_JOB_EXECUTION_CONTEXT, BATCH_STEP_EXECUTION_CONTEXT 라는 테이블이 생성되고, 이 테이블에는 특정 Job or Step의 실행을 지속할 수 있게하는 데이터가 업데이트 됩니다. 이를 통해 중간에 batch job이 중단되더라도 문제파악 후 실패시점부터 다시 실행할 수 있었고, 더불어 위의 문제점들을 해결 할 수 있었습니다.
Spring Batch에서는 여러 형태의 데이터들(DB, 플랫파일)을 하나의 인터페이스(ItemReader, ItemWriter)로 Step을 구현할 수 있게 제공해줍니다. 그럼으로서 여러 형태의 데이터에 대한 직접적인 대응을 줄일 수 있었고, 교육 비용도 줄일 수 있었습니다.
이 외에도 Batch Process를 처리할 때 Spring Batch의 기능, 특징으로 얻을 수 있는 장점이 많기 때문에 선택했습니다.
Spring Batch에 대해서 모르시는 분들께 해당 글 이해를 위한 작업 실행구조를 간단히 설명하고 시작하겠습니다
(이미 아시는 분들은 넘어가셔도 좋지만, 피드백은 언제나 감사합니다 :)

JobRepository: 현재 실행 중인 프로세스의 meta data를 저장합니다.
JobLauncher: Client로부터 요청을 받아 Job을 실행하는 객체입니다.
Job: Step들의 컨테이너, Step들의 단계 혹은 재시작 가능성과 같은 모든 단계에 대한 전역 속성을 구성합니다.
Step: 실제 batch처리를 정의, 제어하는 정보가 들어있는 도메인 객체입니다.
ItemReader: 한 Step안에서 FlatFile, XML, DB 등 여러 input에서 Item을 읽어 들입니다.
ItemProcessor: ItemReader로부터 읽어들인 Item을 DB에 Write하기 전에 필요한 로직을 처리를합니다.
ItemWriter: ItemReader로부터 읽어 들인 Item을 Insert, Update 처리합니다.
Spring Batch는 크게 위의 구조를 통해 동작합니다. 더 자세하게는 위 구조를 동작하기 위한 여러가지 설정과 DB, Object 등이 있지만, 이번 포스팅에서 다루기엔 너무 많고 어렵기 때문에 링크만 공유드립니다.(회피)

각 데이터 별(주소, 지번, 부가, 도로명) 주소데이터 txt파일을 읽는 ItemReader, 주소DB에서 시도, 시군구 등 데이터를 읽는 ItemReader 구현합니다.
ItemReader로 읽은 Item(도메인객체)들을 새로운 주소DB에 Insert할 ItemWriter를 구현합니다.
ItemReader와 ItemWriter, 필요에 따라서는 ItemProcessor를 추가하여 데이터별 Step을 구현합니다.
각 Step들을 플로우에 맞게 실행할 Job을 생성하고
해당 Job name을 parameter로 JobLauncher에게 실행요청을 합니다.
위에서 간단하게 설명한 실행 구조들을 실제로 어떻게 구현했고 사용했는지에 대해 코드와 함께 설명드리려고 합니다.
코드는 Java이고 실제 프로젝트의 코드가 아닌 포스팅용으로 수정한 코드임을 알려드립니다.
우선 txt파일을 read해서 도메인 객체를 리턴할 ItemReader를 구현해야 합니다.
우선 FlatFileItemReader를 이용하여 FlatFile(.txt)을 읽는 ItemReader Bean을 생성했습니다.
플랫파일을 라인단위로 읽은 후, LineTokenizer와 FileSetMapper를 이용하여 read한 각 라인을 도메인객체로 리턴받게 구현하였습니다.
앞 단계에서 ItemReader를 이용해 txt파일로부터 Item을 읽은 후, 원하는 DB Table에 저장할 ItemWriter를 구현해야 합니다.
JdbcBatchItemWriter를 이용하여 Jdbc모듈 형식으로 mysql에 접근하였습니다. 그 중 NamedParameterJdbcTemplate을 이용하여 read한 Item (도메인객체)을 namedParameter로 가져와 insert할 value를 셋팅했습니다.
앞서 구현한 ItemReader와 ItemWriter를 통해 원하는 데이터를 read, write할 Step을 구현해야 합니다.
각 데이터에 맞는 도메인 객체와, txt파일을 Resource객체로 변환 후 set했습니다.
step에는 chunk size를 지정해 줄 수 있습니다. chunk size 만큼 ItemReader와 ItemWriter가 동작하여, 원하는 단위로 트랜잭션 커밋을 할 수 있습니다.
주소 txt파일을 읽어 저장한 주소DB에는 약 800만개의 지번정보와 약 35만개의 도로명정보, 기타 부가정보들이 있습니다.
해당 파일에는 시도, 시군구, 행정동에 대한 정보를 따로 제공해주지 않아 전체 주소 정보로 부터 추출해야 합니다.
앞서 구현한 것들과 달리, 이 Step은 DB로 부터 read와 write를 합니다.
read, write과정이 앞 단계보다 간단해서 generic type을 이용하여 각 Step들을 구현했습니다.
각 데이터에 맞는 read sql, rowMapper를 parameter로 받아 JdbcCursorItemReader를 사용하였고, JPA Entity 객체를 통해 write하는 JpaItemWriter를 사용하였습니다.
이제 구현한 Step들을 Job단위로 묶어 원하는 순서대로 실행할 Job을 만들어야 합니다.
Sido, Sigungu, DongOfAdmin Step들은 4.에서 구현한 createStep method를 통해 만든 Step입니다.
jusoStep을 먼저 실행하여 주소txt 파일로부터 DB에 데이터를 쌓고, 그 후에 시도, 시군구 등의 데이터를 다시 주소DB로 부터 추출해야 합니다.
그래서 Job을 생성하고 start, next로 Step을 실행하였고, Job이 Step의 순서를 보장해 주어서 원하는 동작을 할 수 있었습니다.
Application 실행 시 -Dspring.batch.job.names=jobName 으로 원하는 jobName을 넣어주면 JobLauncher가 parameter를 받아 해당 job을 실행하고, 원하는 주소 데이터들을 주소DB에 쌓을 수 있습니다.
현재 운영중인 서버는 위 구현 과정을 통해 Job을 만들고 실행하여 원하는 주소 DB를 만들고 잘 사용중입니다. (다행이다)
주소 검색서버의 목표는 www.juso.go.kr 메인의 검색 기능을 구현하는 것이었습니다. 해당 기능을 구현하기 위해 Amazon CloudSearch 서비스를 선택하였고, 그 부분을 다음 포스팅에 적어 볼 예정입니다.
이렇게 개발했던 경험을 블로그로 포스팅한 건 이번이 처음입니다.
역시 글을 쓰는 건 (정말)힘들고 (너무)어려웠지만 덕분에 다시 Spring Batch에 대해서 더 깊게 공부할 시간을 가져서 좋았습니다.
더불어 이 주소서버 개발을 할 때 처음으로 Spring Batch를 공부하며 진행했었는데, 이렇게 doc문서를 더 자세히 보면서 이전 코드를 보니 코드리뷰도 된 것 같아서 더 좋은 경험이 되었습니다.
글을 마무리 지으려니 어떻게 지어야 할지 잘 모르겠네요.
그래서 급 마무리 인사드립니다.
이렇게 누추한 글에 귀한 분들께서 읽어주셔서 감사합니다
다음 포스팅에서 Amazon CloudSearch와 함께 돌아오겠습니다.

[cause you are my girl~]
"
http://woowabros.github.io/experience/2018/05/23/google-io.html,2018-05-23,Google I/O 2018 에 다녀오다!,"안녕하세요. 배민서비스개발실 배민프론트개발팀에서 산업기능요원으로 iOS 개발을 하고 있는 강경완입니다.
우아한형제들에서는 iOS 개발을 하고 있지만, GDG (Google Developer Groups) Korea Android 커뮤니티의 오거나이저로 활동하고 있어서 Google I/O 2018에 참여했습니다.
먼 나라 미국까지 날아가서 2박 3일동안 전 세계의 개발자들을 만나며 무엇을 했는지를 짧게 공유해드리고자 합니다.
저는 일단 산업기능요원 신분이기 때문에 출국이 자유롭지 않습니다. 그래서 국외여행 허가를 받아야 합니다.

5월의 캘리포니아는 날씨가 매우 이상합니다. 샌프란시스코 쪽은 바람이 많이 불어서 춥고, 구글 I/O가 열리는 마운틴 뷰 쪽은 낮에는 강렬한 태양의 뜨거움과 태양이 없는 그늘에서는 추위를 느낄 수 있습니다. 그래서 개발자 반팔 티셔츠와 입고 벗기 편하게 후드집업을 챙겨가면 좋습니다.

개발자 티와 후드집업으로 세계는 통한다
Google I/O는 2박 3일 일정으로 진행되고, 메인 홀인 Amphitheatre과 Stage 7개에서 서로 다른 주제의 세션이 있습니다. 세션 외에도 각기 다른 주제로 이루어진 부스인 Sandbox가 A부터 I까지 총 9개의 부스가 마련되어 있습니다. 이 외에도 코드랩부스, Office Hour Time 등이 준비되어 있습니다.
Sandbox로는 Android, Android Things, Wear OS, AI/ML, Google Cloud, Googld Assistant, Design, AR/VR 등이 있었습니다.
Android 부스에는 KTX, Android Jetpack, Android P 등에 대한 전시와 함께 해당 분야 팀의 구글러가 옆에서 직접 설명해주거나 질문을 받아주었습니다.
AI/ML 부스에서는 Tensorflow를 이용한 게임, Tensorflow Lite에 대한 간단한 데모 (Local에서 사물 판단하기)와 TPU v1, v2, v3 전시, 이미지 분석을 통한 자율 주행차 등을 전시해두었었습니다.
Google Cloud 부스에서는 Firebase Test Lab에 대한 소개로 실제 Deck을 전시해두고 UITest가 어떻게 진행되는지를 전시해 두었습니다. 또한 Firebase에 추가된 ML킷에 대한 내용도 있었습니다.
Google Assistant 부스에서는 Action on Smart Displays 같이 직접 Assistant를 체험할 수 있도록 해두었습니다.
AR/VR 부스에서는 구글의 Daydream을 이용해서 직접 체험해볼 수 있었습니다.
이렇게 큰 개발자 컨퍼런스의 매력은 역시 전 세계의 많은 개발자들이 한 자리에 모이는 것입니다.  물론 한국에서도 판교만 가도 개발자 같을 것 같으신 분들이 많이 돌아다니시고, 샌프란시스코 역시 마찬가지긴 하지만, 여기는 정말 10명 중 9명은 개발자이며 심지어 다양한 나라에서 날아오신 분들입니다.
작년 I/O17를 다녀온 뒤 I/O에 대한 경험이 주니어 개발자에게 어떤 성장의 거름이 되었는지에 대해 발표를 한 적이 있습니다. 영어가 부족해서 하고 싶은 말이 많았지만 많은 대화를 나누지 못한 것에 대한 회고를 했었는데, 올해 I/O18 에서는 글로벌 친구들과 커뮤니케이션을 많이 하려고 노력했습니다.
주로 안드로이드 개발자를 만나면 패턴이 비슷했는데…
여담이지만 작년 I/O에서 Kotlin이 Android의 공식 지원언어가 되었고, Java와 Kotlin을 병기하여 나오던 예제코드들이 올해는 100% Kotlin 코드로 세션에서 발표가 되었었습니다. Kotlin 쓰고있냐고 안 물어보는 안드로이드 개발자를 만나지 못했어요
다른 에피소드로는 개발 문화적인 차이도 있었습니다. 가령 예를들면 Flutter… 아프리카 지역의 학생 개발자 그룹에서 I/O에 참가하게 되었다고 소개한 대학생 두 명을 만났습니다. Flutter에 빠져있다고 하더군요.
지금도 트위터에서는 native, pwa, react, flutter 등에 대한 discussion이 일어나고 있는 것 같습니다. (Dart 언어이기도 하고.. Flutter는 안써봐서 잘 모르겠습니다.)
장애인들을 위해 Accessibility를 어떻게 더 잘 구현할 수 있을지에 대한 세션이 눈에 띄었습니다.
[Session] What’s new in Android accessibility
[Session] Accessibility for AR and VR
[Session] What’s new in web accessibility
[Session] An accessible process for inclusive design
각 플랫폼 별로 1개씩 있었는데 AR, VR에 대한 Accessibility 세션은 자리가 없을 정도로 인기가 많았습니다. 작년 말에 배달의민족 앱도 accessibility 중 하나인  VoiceOver 지원에 대한 리뷰가 올라와서 대응을 한 적이 있었기 때문에 관심이 가는 주제였습니다. 동영상 자막으로 효과음 또는 그냥 흘러 지나갈 수 있는 대사가 아닌 배경 소리도 자막 처리를 하는 것이 청각장애인을 위한 것이었다는 것에 신선한 충격을 받았었는데, AR, VR 플랫폼도 accessibility 대응을 해야한 다는 생각을 전혀 못하고 있던 저에게 이 또한 신선한 충격이었습니다.
Don't design for yourself or your situation  사용자 경험에 대해 다시 생각해보는 시간이 되었습니다.
얼마 전 Tensorflow Summit에서 Javascript 버전의 Tensorflow JS을 발표하더니 이제는 Firebase 위에서 사용할 수 있는 모바일 버전의 ML Kit 을 발표했습니다.
텍스트인식, 얼굴검출, 랜드마크 인식, 바코드 스캐닝, 라벨링 등 모바일 사용 환경에서 자주 쓸만한 머신러닝 모델들을 제공합니다.
Firebase 클라우드 환경에 데이터를 전송해도 되고, 모바일 환경에서 네트워크 없이 사용할 수도 있습니다.

iOS와 Android 모두 지원하는 ML Kit
ML Kit 외에도 Tensorflow Lite 에 대한 시연을 샌드박스에서 볼 수 있었는데, Tensorflow 보다 가벼운 Tensorflow Lite를 라즈베리파이 같은 작은 기기에서 카메라 모듈을 이용해 라벨링을 하는 것을 볼 수 있었습니다.
생각보다 잘 돌아갔던게, 선글라스와, 그냥 안경 두 가지 모델을 모두 넣었는데 구분해내더군요.

이런 Tensorflow Lite에서 만든 모델을 ML kit에 추가해서 쓸 수도 있다고 합니다. 이젠 정말 모든 플랫폼 개발자가 머신러닝에 대해 공부하거나 준비해야될 때가 되지 않았나 싶었습니다.
해외 컨퍼런스에 참여한다는 것은 사실 많은 것을 투자해야만 합니다. 비행기 삯과 컨퍼런스 티켓을 포함한 돈과 시간, 연차, 그리고 장기휴가이기 때문에 팀원들과 일정 조율 등등등.. 하지만 많은 것을 투자한 만큼 개인에게 있어 더 가치있고 큰 경험을 얻어갈 수 있는 기회였다고 생각합니다. 단순히 구글에서 발표하는 신기술을 누구보다 빠르게 세션을 통해 알게 되는 것 뿐만 아니라 전체적인 부분이랄까요? 음식점에서 행복을 느낄 때 평가하는 요소가 맛이 전부는 아니잖아요? 내년에는 올해보다 더 성장한 개발자가 되어 다시 가보고 싶습니다!!
"
http://woowabros.github.io/experience/2018/05/19/build-app-by-react-native.html,2018-05-19,라이더스앱(iOS) 개발기,"안녕하세요 라이더스개발팀 박민 입니다.
배민라이더스의 라이더분들이 배달을 수행하기 위해 사용되는 애플리케이션 ‘라이더스’ 개발기를 적어보려 합니다.
먼저 BROS(Baemin Riders Operating System)가 있습니다. (네! Geo-fence를 신선하고 유익하게 설명해주신 민철님의 글에서도 등장합니다.)
라이더스는 BROS에서 발생하는 배달을 라이더가 수행할 수 있도록 배차 요청을 비롯해 픽업, 전달 등의 배달업무를 처리하는 애플리케이션입니다.
플랫폼의 이름과 같은 Android 버전에서의 ‘BROS’에서 직관적인 이름의 ‘라이더스’로 변경되었습니다.
늘어나는 주문을 처리하기 위해 우리는 많은 라이더분이 필요합니다. 그래서 모집공고를 통해 많은 예비라이더 분을 뵙게 되는데,
10명 중 2명은 아이폰을 사용하시기에 OS의 문턱에 발걸음을 돌리셨다고 합니다.(물론 업무용으로 따로 Android기기를 사용하시는 분도 계십니다.)
20%의 수치는 현재 아이폰 사용률, 특히 젊은 층의 사용률을 미루어 보았을 때 앞으로 더욱 높아질 수 있다고 생각했습니다.
그래서 우리는 앞으로 더 많은 예비라이더 분들을 놓치지 않기 위해 iOS 버전의 라이더스가 필요했습니다.
iOS 버전 라이더스의 필요성은 충분히 공감했지만 저는 iOS 애플리케이션을 만들어 본 적이 한 번도 없었고, 하물며 우아한형제들과 일하게 되며 맥북을 처음 사용했습니다.(#새맥북#좋은개발환경) 그런 제게 Object-C, Swift는 낯설기만 했고 벽은 높아만 보였습니다.
네이티브로 만드는 것이 닿지 않을 듯 멀게만 느껴지고, React Native가 힙해 보여서 사용하게 된 것은 아니었습니다.
선택에는 위 2가지 이유가 있었고, 우선 React Native로 개발 가능 여부를 빠르게 검토하고, 안 되겠구나싶으면 Object-C, Swift를 배워야겠다! 하는 마음으로 시작했습니다.
누가 시켜서 한건 절대로 아니에요. 다만 @재일님께서 해보라고 ‘권유’는 하셨습니다. ~(^^~)
자바스크립트가 네이티브에서 동작하기 위해서는 네이티브 코드가 필요합니다.
버튼을 예로 들어보겠습니다.
먼저, 자바스크립트에서 버튼을 구성하고 네이티브에서의 버튼 기능을 수행할 컨트롤 과 bridge로 통신하면 됩니다.


잠깐만요? 네.. 네이티브의 벽은 높지만 구현 해야 합니다.

물론 React Native는 네이티브 경험 없이도 애플리케이션을 만들 수 있는 매력적인 플랫폼입니다. 그리고 화면 구성을 위한 정말 다양한 컴포넌트가 (Button,RefreshControl,WebView 등) 이미 제공되기 때문에 네이티브 개발을 줄여주고, 빠른 애플리케이션 개발을 도와줍니다.하지만 간단한 애플리케이션인 라이더스에서도 React Native가 제공하는 컴포넌트 이외의 화면이 필요했습니다.(간단하게는 지도화면부터, 애플리케이션을 환경(live, beta)별로 관리하거나, build를 할 때 필요한 네이티브의 설정들을 제공하는 네이티브 모듈들이 필요했습니다.)그리고 만약 저처럼 제공하지 않는 컴포넌트로 화면을 구성하기 위해서는 보통 2가지 방법이 있습니다.
React Native는 네이티브 개발의 필요성을 줄여주지만, 여전히 네이티브 개발이 필요합니다. 그래서 네이티브 경험 없이 애플리케이션을 개발하는 것은 힘들게 느껴질 수 있겠지만, 오히려 제게는 네이티브를 통한 간편한 확장성이 React Native를 쉽게 선택할 수 있게 하는 장점이라고 생각합니다.(이가 없으면 잇몸으로!)
React Native로 개발하며 흥미로웠던 부분은 네이티브와는 다른 React Native의 개발과정이었습니다.
예를 들어 네이티브의 경우에는 테스트 디바이스에 애플리케이션을 설치하고 확인하고, 다른 해상도의 테스트 디바이스에 설치하고.. ‘엇 어긋나네..?’ 수정하고, 다시 설치하고, 이전 기기에 또 설치하고.. 확인하고.. x100
React Native는 로컬의 노드 서버에서 published JavaScript 코드를 수행합니다. 그렇기 때문에 에뮬레이터를 비롯한 실제 디바이스에서의 변경사항 확인은 애플리케이션의 컴파일 없이 새로 고침으로 충분합니다.

조금만 자세하게 설명해보겠습니다
혹시 네이티브 영역의 코드가 변경되야한다면! 네.. 다시 패키징하는 방법으로 배포해야합니다.
앞서 말씀드린 것처럼 네이티브의 지식도 필요합니다. 그래서 iOS 경험이 없던 저는 Object-C도 함께 학습해야 했지만 제 생각에는 React에 대한 학습이 좋은 애플리케이션을 만드는데 더 많은 영향을 미치는 것 같았습니다. 서비스에 따라 네이티브의 영향이 큰 프로젝트도 많겠지만, 라이더스의 경우에는 빈번한 화면 render와 화면 전환이 중요한 부분이었고, (지도를 제외한 모든 화면이 React로 구성되었습니다.) 특히나 사용자에게는 네이티브로 개발된 Android 버전의 라이더스BROS가 비교 대상이었기 때문에 그만큼의 퍼포먼스가 요구되었습니다.
React는 setState() 메서드를 통해 컴포넌트를 표현할 때 사용하는 state를 업데이트할 수 있습니다.
setState() 메서드가 수행되면 컴포넌트에 Dirty로 마크되고, 다음 이벤트 루프 때 Dirty 컴포넌트를 다시 render하게 되는데, 이때 Dirty 컴포넌트의 하위 컴포넌트까지 render의 대상이 됩니다.

컴포넌트를 표현하기 위한 state의 잦은 변경은 자칫 불필요하게 빈번한 화면 render를 수행하게 할 수 있습니다. 이를 효율적으로 처리하기 위해서는 적절한 shouldComponentUpdate를 활용해야 합니다. shouldComponentUpdate는 render가 수행되기 전에 render 필요 여부를 return 하는 React 생명주기의 일부입니다.
물론 React에서는 shouldComponentUpdate를 이미 구현한 pureComponent도 제공을 합니다. 하지만 이 컴포넌트는 변경된 props와 state를 shallow level에서 비교한 결과로 render 여부를 판단하기 때문에 의도하지 않은 render가 발생할 수 있습니다.
만약 직접 shouldComponentUpdate를 구현한다면 주의할 점이 있습니다.
컴포넌트가 커지면 자연스럽게 사용되는 props와 state가 다양해집니다. 그렇다 보면 shouldComponentUpdate에서의 정확한 render 여부를 반환하기 위해 props와 state의 deep-compare 유혹에 빠지게 될 때가 있습니다.하지만 shouldComponentUpdate에서 deep-compare는 권장되는 행동이 아닙니다. we don’t recommend deep equality checks
요약해보면, shouldComponentUpdate에서 render 하지 않겠다고 판단하는 데 소비하는 시간이 render 하는 시간보다 길어질 것을 우려하기 때문입니다. 제 경험으로는 효율적인 render를 위해서 무엇보다 각 컴포넌트를 목적에 따라 작게 나누고, 컴포넌트는 필요한 props와 state만 다루는 것이 결국 render가 수행되어야 하는 시점을 쉽게 판단할 수 있도록 해주었습니다.
다음은 라이더스를 만들며 사용한 React Native의 API와 유용한 라이브러리를 소개합니다.
많은 이야기를 하고 싶은 마음과 절제하려는 열 손가락 사이에서 두서없는 글이 탄생하게 되었습니다. ㅠ_ㅠ
처음 개발 시작할 때 함께한 우려와 머뭇거림이 아직 프로젝트 곳곳에 고스란히 묻어있지만 이미 React Native로 개발된 iOS 버전의 라이더스는 릴리즈 되었고, Android 버전도 준비 중입니다.

[React Native를 검색할 때 tutorial과 앞다투는 검색어]
위 사진 속 우려의 검색어가 시작할 때 저를 주춤하게 한 가장 큰 걸림돌이었습니다. 만약 React Native를 시작하려 하시는 분이 계신다면, (추천하기에는 저는 아직 많이 배워가는 단계지만) 그저 애플리케이션을 개발하는 방법이 하나 더 생겼다는 마음으로 가볍게 마주하면 어떨까 싶습니다. 서툰 글을 읽어주셔서 감사합니다.
 그리고 도움 주신 동료 iOS 개발자분들께 많은 감사드립니다.
참고 사이트
"
http://woowabros.github.io/experience/2018/05/08/billing-performance_test_experience.html,2018-05-08,"결제 시스템 성능, 부하, 스트레스 테스트","안녕하세요. 우아한형제들에서 결제시스템을 개발하고 있는 권용근입니다. 입사한 지 4개월 만에, 드디어 우아한형제들 기술 블로그에 글을 남기게 되어 감회가 새롭습니다.
저는 최근 결제 시스템의 개비를 진행하며 경험한 성능, 부하, 스트레스 테스트 경험을 작성해보려고 합니다.
입사하고 보니 저에게는 결제 API 단순화, 결제 시스템 데이터베이스 분리 및 파티션 도입, 비동기 결제 시스템 개발 이라는 굵직굵직한 작업들이 기다리고 있었습니다.
Java, Spring Framework, ORM 등의 기술 지식은 그간 해온 게 있기 때문에 (구글링이 있기 때문에) 파악하는데 어렵지 않았지만, 이미 구축되어 있는 시스템을 손대는 것은 쉬운 일이 아니었습니다.
“거대 규모 프로젝트에서 내가 수정한 코드가 어떤 사이드 이펙트를 발생시킬지..”
이래서 존재하는 것이 바로 테스트 코드 !! 다행히 테스트 코드를 지향하는 프로젝트였기에 작성된 테스트의 보호 아래 신나게 개발을 할 수 있었습니다.

결국 테스트를 모두 통과시키며, 개발이 완료되었습니다!
그리고 이때부터 본격적인 미지의 영역에 대한 공포가 시작되었습니다.

내가 만든 이 시스템은 실제 장비에서
전혀 감이 안 잡혔습니다. 이것은 흔히 말하는 localhost:8080 레벨에서는 확인할 수 없는 문제였습니다.
위의 수많은 공포에 대하여, ‘내가 예상하는 데로’, ‘내가 정의해놓은 데로, 작성해놓은 데로 잘 동작하겠지’ 라고 믿을 수는 없었습니다. 호되게 당한 적도 있기도 하고, 실제 사용자의 돈이 움직이는 결제 시스템에서 전혀 보이지 않는 불안요소를 가지고 가기에는 너무나 두렵고 무서웠습니다.
그래서 테스트 코드가 두려움을 해소해줬듯, 이번에도 두려움을 해소해줄 성능 테스트 환경을 만들게 되었습니다.
결제 시스템은 사용자의 주문 비용을 각종 결제수단(PG사, 포인트, 쿠폰) 등의 시스템과 통신하여 지불처리하는 시스템입니다. 많은 부분을 생략하고 간단하게만 표현하면, 하나의 결제는 결제수단 시스템이란 외부 인터페이스를 거치게 됩니다.

그래서 저는 결제수단 시스템을 Mock 처리해야 했습니다. 온전히 테스트 대상 시스템의 성능을 측정하기 위해서 외부 시스템은 항상 기대한 결과만을 반환하는 환경이 필요하기 때문입니다.
1. 객체 Mocking
객체 Mocking은 테스트 코드를 작성할 때 가장 많이 사용하는 방식이라 친숙할 것 입니다. 그러나 로직에 대해 검증을 하는 테스트와 달리, 성능 테스트는 어플리케이션 동작과 자원의 사용을 모두 보아야만 하는 테스트입니다.
객체 Mocking 은 해당 객체의 행위 뒤로 들어가야 할 동작들을 무시해버리게 됩니다. 예를 들어 Spring Profile 을 사용하여 RestOpertation 을 객체를 Mock 처리하였을 때
등등 성능 테스트에서 중요한 관점인 Thread 사용, 리소스 사용을 전부 무시하게 됩니다.
외부 인터페이스를 Mocking 하는 것처럼 보이지만, 내부 인터페이스도 Mocking 해버리는 객체 Mocking 은 성능 테스트에서 피해야 합니다.
2. 같은 어플리케이션에 Dummy Controller 생성
이 방식도 아주 간혹 테스트 코드를 작성할 때 사용하는 방식입니다. 이 방식이 1번 방식과 다른 것은 실제로 요청을 보내고 받으며 자원을 사용한다는 것 입니다.
그러나 Dummy Controller 의 로직은 테스트 시스템의 자원과 리소스를 같이 사용해버리게 됩니다. 테스트 대상 시스템이 더 늘어나 버리는 신뢰성이 굉장히 떨어지는 의미없는 성능 테스트를 하게 됩니다. 테스트를 위한 요소는 대상 시스템에 절대로 영향을 미쳐서는 안 됩니다.
그래서 우리는 테스트 대상 시스템과 완벽히 분리된 Mock Server 를 띄워야 합니다.
외부 인터페이스 Mock이 갖추어야 할 조건을 아래와 같이 정의했습니다.
제가 만든 것은 가짜 PG사인 Gazua PAY 입니다. 요청 인터페이스는 기대한 결과와 퍼포먼스로 응답을 하도록 하는 값을 받도록 하였습니다.

(개발 당시에는 코인 시장이 엄청 핫할 때 였습니다. 승인 결과의 message는 차트의 상승을 표현했는데 아무도 눈치채지 못했습니다.)
저는 Spring 쟁이라 Spring Boot 로 아주 간단히 모든 요청에 기대한 결과, 퍼포먼스를 내는 Mock Application을 만들었습니다.
이제 Mock Application 을 배포해야 합니다. 이럴 때 정말 유용했던 것은 AWS Elastic Beanstalk 입니다. (우아한형제들은 AWS 사용을 적극 지원해주기 때문에 마음껏 쓸 수 있었습니다.)

(Elastic Beanstalk 홍보 영상 중..)
Elastic Beanstalk 은 애플리케이션을 업로드하기만 하면 용량 프로비저닝, 로드 밸런싱, Auto Scaling, 애플리케이션 상태 모니터링에 대한 배포 정보를 자동으로 처리해줍니다.
배포를 위한 스크립트를 작성하거나, 서버 설정을 해줄 필요 없이 클릭만으로 간단하게 하나의 환경을 만들 수 있고, Mock Server 에 병목이 생겨도 클릭만으로 Scale Out 하여 병목을 해소할 수 있습니다.
이로써 병목이 되지 않는, 기대한 결과와 퍼포먼스를 반환하는 Mock Server 가 완성되었습니다.

nGrinder 는 성능 측정 목적으로 개발된 오픈소스 프로젝트로

등의 기능을 제공합니다.
성능 측정 도구로 nGrinder 가 가장 좋았던 것은 groovy 스크립트로 테스트 시나리오를 작성할 수 있다는 것 입니다.

groovy는 gradle, jenkins file, spock 등에서 자주 다루었던 친숙한 언어였기에 내가 원하는 테스트 시나리오를 쉽고 자유롭게 작성할 수 있었습니다.

pinpoint 는 Java로 작성된 대규모 분산 시스템용 APM 도구입니다.
사내에서 사용하고 있는 모니터링툴이기도 하며, Transaction 의 추적을 제공하는 APM 중 하나입니다.

단일 Transaction의 Stack Trace 를 기록하여 직접적인 병목이나 문제를 빠르게 추적할 수 있고,

Transaction 이 DOT 로 그려지는 응답시간/요청시간 그래프 Transaction View 는 테스트의 상태를 실시간으로 확인하여, 가장 빠르게 이상을 감지하도록 도와줍니다.
Transaction View 는 패턴에 따른 어플리케이션 상태 예측에도 큰 몫을 하였습니다. A 구간의 병목을 보였을 때 보이는 패턴, 외부 인터페이스가 병목을 보였을 때 등등 예상 패턴을 통해 더 빠른 조치가 가능하기도 했습니다.
실제로 테스트를 하며 수많은 이상 패턴들이 탄생하기도 하였습니다.

(기영이 패턴)

(L타워 패턴)
그리고 노력 끝에 얻어진..

(백설기 패턴)
pinpoint 로 어플리케이션의 전반적인 상황을 파악할 수 있었지만, pinpoint 의 Trace 기능으로 모든 패키지와 클래스를 탐색 하는 것은 너무 과하며, Thread 간의 경합 으로 발생되는 예기치 않은 현상들을 탐지하기는 어렵습니다.
이럴 때 우리는 Thread Dump 를 분석해야 합니다.

저는 JVM 의 내장 명령 도구인 jstack 을 사용하여 쉽게 Thread Dump 를 획득할 수 있었습니다.
이제 Tread Dump 를 분석하여 병목의 원인을 파악할 수 있습니다.

(Tread Dump 를 보기 편하게 가공)
우리가 만드는 시스템은 결국 하드웨어 위에서 동작하게 되고, 시스템의 리소스 자원 사용은 Scale Up, Scale Out, Scale Down 의 중요한 지표가 됩니다.
그러므로 우리는 테스트를 통해 이 시스템은 리소스 자원을 최대한으로 사용하고 있다 라는 결론으로 도달해야 합니다.
리소스 자원을 실시간으로 모니터링하기 위해 dstat 을 사용하였습니다.
dstat 은 vmstat, iostat, ifstat, netstat 정보 등을 결합한 내용을 보여주고, 실시간성 통계를 제공해주어 성능 테스트 중 모니터링하기에 매우 적합했습니다.

dstat 하나의 명령어로 대부분의 리소스를 모니터링할 수 있었습니다.
dstat으로 모니터링 가능한 자원 : aio, cpu, cpu24, disk, disk24, disk24old, epoch, fs, int, int24, io, ipc, load, lock,
        mem, net, page, page24, proc, raw, socket, swap, swapold, sys, tcp, time, udp, unix,
        vm
kingpoint 라는 도구를 아시나요? 제가 만든 것이라 당연히 모를 것 입니다. 비동기 어플리케이션의 완벽한 모니터링이 아직은 어렵기도 하며, 어플리케이션의 특성을 반영한 모니터링을 하기에도 쉽지 않았습니다.
정말 보아야 하는 것이 있는데 그것을 지원하는 도구가 없다면, 테스트를 위한 요소가 실제 어플리케이션의 성능에 절대 영향을 주지 않는다는 것을 꼭 지키는 선에서 만들어보는 것도 나쁘지 않을 것 같습니다.

비동기 대한 모니터링을 위해 요청과 완료 시점에 특정 key 값으로 통계를 전송하도록 하여, 분석한 지표를 chart.js 로 그려주는 간단한 모니터링 툴 입니다. 저는 이로인해 많은 두려움을 해소할 수 있었습니다.

그래서 저는 아래와 같은 테스트들을 진행했습니다.
Mock Server 를 올리고, 위의 도구들을 사용하여 수백 번의 테스트를 해본 것 같습니다. 점차 원하는 패턴, 안정적인 수치와 지표를 찾을 수 있었습니다.
라는 미지의 영역을 개척할 수 있었습니다.
지금까지 결제 시스템 개비를 진행하며 경험했던 성능, 부하, 스트레스 테스트 환경 구축 및 진행에 대한 내용이습니다.
테스트를 했다고해서 이상적으로 동작하는 어플리케이션을 만든 것은 아닐 것 입니다. 많은 상황을 예방할 수 있겠지만, 언제나 전혀 예상치 못했던 상황들이 생깁니다.
그러나 적어도 확인한 것, 확보한 지표 를 기반으로 장래의 부하, 장애를 최소한의 비용과 시간으로 합리적으로 대응할 수 있을 것 입니다.
보았어야 했는데 보지 않았던 것, 더 쉽게 볼 수 있었는데 어렵게 보았던 것 등 많은 시행 착오가 있었지만, 환경을 만들고 테스트를 하면서 많은 자신감을 얻을 수 있었습니다.

그래도 배포할 땐..

긴 글 읽어주셔서 감사합니다.
"
http://woowabros.github.io/woowabros/2018/05/01/from_woowahan_techcamp_to_woowahan_developer_hanna.html,2018-05-01,우아한테크캠프 참가자에서 우아한개발자가 되기까지,"2017 우아한테크캠프 참가자에서 우아한형제들에 입사하기까지의 경험을 공유합니다.
안녕하세요, 2017년 12월 22일 입사 한 따끈따끈한 신입 개발자 배민플랫폼개발실 주문중계팀 전한나입니다.
2017 우아한테크캠프를 준비하면서 기술 블로그를 읽고 참고했던게 벌써 1년의 시간이 흘렀습니다.
2018 우아한테크캠프 모집기간입니다.
제 경험이 우아한테크캠프를 준비하시는 분들에게 조금이나마 도움이 되고자 블로그를 작성합니다.
저는 비전공자로, 우아한형제들과 인연이 깊은 코드스쿼드라는 교육기관에서 iOS 개발자의 꿈을 키웠습니다.
코드스쿼드가 타 교육기관과 다른점은 기술적 조언들은 물론이고, 협업을 강조하는 교육을 진행한다는 점입니다.
저는 이전까지 누군가와 함께 일을 하거나, 의견을 조율하는 등의 경험이 없었습니다.
처음 동료와 협업을 해보고, 내가 아닌 다른사람과 함께 결과를 만들어 내는 과정이 쉽지 않다는 것을 알게되었습니다.
팀원들과 감정적인 다툼도 있었고 힘든 과정이었지만 다른사람을 잘 설득시키고 잘 설득 당하는 방법을 터득할 수 있는 첫번째 단계였습니다.
이 경험은 우아한테크캠프에 참여하면서 팀원들과 팀워크를 다지고, 의견을 조율하는데 큰 도움이 되었습니다.
스스로 부족한 점이 많다고 생각하고 있었기 때문에 우아한테크캠프를 통해서 성장하고 싶은 욕심을 가지고 지원하였습니다.
성장하고 싶은 욕심이 어필이 되었는지 우아한테크캠프에 합격소식을 받을 수 있었습니다.
아직도 합격 메일을 확인했을 때의 그 짜릿함과 두근거림이 잊혀지지 않습니다.
[감격의 합격메일]
2017 우아한테크캠프는 iOS와 웹프론트엔드 두개의 트랙으로 진행되었습니다.
트랙별로 12명이 참가하였고, iOS 2명 + 웹프론트엔드 2명 총 4명이 한 조로 프로젝트를 진행하였습니다.
7월은 각 트랙별 교육을 진행하였고, 8월은 자유주제로 프로젝트를 진행하였습니다.
코드스쿼드에서 했던 프로젝트에서는 서버가 필요했었기 때문에 iOS개발보다는 Node.js로 서버를 구축하는데 더 많은 역할을 맡았습니다.
백엔드에 흥미가 생기면서 iOS개발과 백엔드개발 사이에서 진로를 고민하던 시기에 iOS를 더욱 깊에 공부해 보면서 진로를 결정하자는 생각으로 우아한테크캠프에 참가하였습니다.
그렇기 때문에 iOS개발에 몰입하고, 최대한 우아한테크캠프 교육과정을 기반으로 프로젝트를 진행하고 싶었습니다.
이 생각에 팀원들이 공감해 주고, 우아한테크캠프를 위해 회사에서 배달의민족 beta API를 제공해 주어서 프론트엔드에 집중할 수 있는 결과물을 만들 수 있었습니다.
클라이언트에 집중하면서 사용자경험에 대한 많은 고민을 할 수 있었고, iOS개발자로 성장하는 발판이 되었습니다.
팀원의 성장을 위해서, 그리고 나의 성장을 위해서 솔직한 피드백을 하자.
팀원들 모두 프로젝트에 욕심이 있었기 때문에 프로젝트 과정에서 팀원들간의 의견 충돌 생길 수 밖에 없었습니다.
프로젝트 첫 주에는 감정이 상하는 말들이 오가기도 하였고, 누군가는 항상 져주는 사람이 생기기도 했습니다.
우아한테크캠프 마스터님들께서 항상 협업에 대해서 강조하시면서 솔직한 피드백이 중요하다고 말씀하셨습니다.
마스터님들의 조언대로 매주 금요일 회고 시간을 적극 활용하여 팀원들과 팀에 대한 생각, 서로에 대한 생각, 기술적인 생각 등 팀과 프로젝트가 더 좋은 방향으로 나아가기 위한 생각들을 솔직하게 나누었습니다.
솔직한 회고는 프로젝트 진행 과정에서 팀원들간의 감정적인 싸움없이 논리적으로 서로를 설득할 수 있게 된 계기가 되었습니다.
또한 매주 받은 피드백을 수용하는 방법도 배우면서 매주 성장하고 성숙해져가는 팀원들의 모습을 볼 수 있었습니다.
이렇게 적극 활용한 솔직한 회고를 배움으로써 협업에 대해 고민하고 성장 할 수 있는 두번째 단계였습니다.
우아한테크캠프 지향점 by.코드스쿼드
[2017 우아한테크캠프!]
[우아한형제들 공채 준비 스터디 기획서]
우아한테크캠프가 끝나고 취업준비를 하는 캠프동기들과 함께 취업스터디를 진행했습니다.
막연히 취업을 위한 스터디를 기획하던 중에 우아한형제들 공채를 진행한다는 소식을 듣게되었습니다.
캠프에서의 좋은 경험들은 스터디원들에게 막연한 취업에서 우아한형제들의 개발자가 되고 싶다는 구체적인 목표를 갖게 해주었고, 스터디의 목표를 우아한형제들 공채 합격으로 바꾸게 되었습니다.
스터디에서 가장 도움이 되었던 점은 동기들과의 모의면접이었습니다.
혼자서 준비했다면 나의 장점과 단점, 보완해야할 부분과 어필해야 할 부분에 대해서 객관적으로 바라볼 수 없었을 것입니다.
동기들과의 모의면접을 통해서 나를 객관적으로 바라봐주고 보완해야 할 점을 함께 고민하면서 동기들에게 많은 도움을 받았습니다.
(물론 그대로 평탄하게 면접이 진행되진 않았…ㅠㅠ)
[더 감격스러운 합격메일]
우아한테크캠프에서의 좋은 기억들은 공채 준비과정에서 큰 동기부여가 되었고, 결국 좋은 결과를 얻을 수 있었습니다.

[2018년 우아한신입개발자 & 우아한테크캠프 모집]
2017 우아한테크캠프 참가자에서 우아한형제들의 구성원이 된 만큼 2018 우아한테크캠프가 누구보다 기대됩니다.
제가 우아한테크캠프에서 많은 것들을 경험하고 성장한 것 처럼, 이번 캠프에 참가하시는 분들도 이루고자 하는 것을 캠프를 통해 이루시기를 바라며 좋은 경험이 되었으면 좋겠습니다.
2018 우아한테크캠프 지원하러가기!
"
http://woowabros.github.io/woowabros/2018/04/22/do_you_wanna_be_a_woowa_developer.html,2018-04-22,우아한개발자가 되기 위한 우아한테크캠프,"우아한(Woowa) 개발자가 되고 싶은 이들을 위한 우아한테크캠프 2기를 소개합니다.
우아한테크캠프는 대학생들의 여름 방학 기간에 맞춰서 2017년에 진행했던 교육형 인턴 과정입니다. 한 달은 정해진 커리큘럼에 따라 개발 관련 지식을 익히고, 그 다음 달은 조를 짜서 프로젝트를 진행하면서 실제 서비스를 만들면서 필요한 개발 지식을 익히는 과정을 수행하는 프로그램입니다.
작년에 진행한 우아한테크캠프 관련 글들은 아래 링크에서 볼 수 있습니다.
올해의 우아한테크캠프는 작년 프로그램을 진행하면서 얻은 피드백을 바탕으로 그 내용과 형식이 수정되었는데요. 가장 크게 달라진 점은 교육 커리큘럼의 변화, 그리고 우아한형제들의 신입 개발자 공채 과정으로 활용된다는 것입니다.
작년에는 웹프론트엔드와 iOS 프로그래밍을 중심으로 커리큘럼이 작성되었는데, 참가하신 분들의 피드백을 들어보면 백엔드와 프론트엔드를 모두 배울 수 있으면 좋겠다는 내용이 많았습니다. 좀 더 경력이 쌓이면 본인의 전문 분야를 택할 수 있겠지만, 신입 개발자 또는 주니어 개발자 입장에서는 전체적인 서비스를 구성하는 흐름과 연동 방식을 두루 경험할 수 있는 커리큘럼이면 좋겠다는 의견이 있었습니다.
이번 우아한테크캠프는 서버 백엔드 4주, 웹프론트엔드 2주 교육 과정, 그리고 3주 동안의 프로젝트 수행 과정으로 이루어져 있어서, 하나의 작은 서비스를 이루는 전체 데이터의 흐름과 시스템의 구조를 파악할 수 있는 것을 목표로 진행합니다.
웹프론트엔드는 작년 우아한테크캠프에도 참가하셨던 윤지수님이 맡아서 진행해 주실 예정이고, 서버 백엔드는 자바지기라는 필명으로 유명한 박재성님이 맡아서 진행하실 예정입니다. 윤지수님의 교육 관련해서는 웹 프론트엔드 개발자, 어떻게 준비해야 할까?라는 글을 읽어 보시고, 박재성님의 교육 관련해서는 패스트캠퍼스의 박재성님 강의 소개를 읽어 보시길 권해 드립니다.
작년에는 우아한테크캠프와 우아한형제들의 신입 개발자 공채는 완전히 분리되어 있었습니다. 우아한테크캠프 신청자/참가자 분들이 이후 신입 개발자를 뽑을 때 우대사항이나 그런 것이 없냐고 여러 번 문의를 주셨는데, 두 가지 과정은 완전히 분리되어 진행된다고 말씀 드리고 진행했습니다.
그런데 커리큘럼을 수정했던 것과 비슷하게, 저희가 우아한테크캠프를 처음 만들 때 생각했던 부분과 실제로 캠프에 참석하는 분들의 생각은 조금 다른 부분이 있었습니다. 커리큘럼의 경우는 백엔드 강화에 대한 의견을 꽤 많은 분들이 얘기해 주신 반면, 이것에 관한 의견은 반반으로 나뉘어졌는데요.
이미 다니던 직장을 그만두고 우아한테크캠프에 참가하는 분들 또는 바로 취업을 해야 하는 대학교 졸업반의 경우, 우아한테크캠프 과정에 정말 참가하고 싶어도 본인이 실제로 취업할 수 있는 회사의 인턴을 하면서 보내는 것이 더 현실적인 선택일 수밖에 없다는 의견을 주었습니다.
작년에 우아한테크캠프 과정을 만들면서 일부러 대학생으로 제한을 두지 않은 것은, 이미 개발일을 하고 있는데 정말 더 잘 하기 위해서 제대로 개발을 배워 보고 싶은 분들, 그리고 학벌이나 전공과 관계없이 본인이 관심을 갖고 잘 해 보고 싶은 분들에게 기회를 주고 싶었던 마음이 있었는데요. 우아한테크캠프가 취업이라는 과정과 연결 고리가 전혀 없다보니, 오히려 그 분들 입장에서는 인턴을 잘 수행하면 취업 시 우대점을 주는 프로그램을 택할 수밖에 없다는 점이 안타까웠습니다.
이와 더불어, 우아한형제들에서 작년 말에 신입 개발자를 뽑으면서 느낀 고충도 있었는데요. 우아한테크캠프를 뽑는 것과 비슷한 노력과 과정을 한 번 더 반복해야 한다는 것이었습니다. 일의 횟수가 문제가 아니라, 우아한테크캠프 참가자들을 선발하기 위한 문제와 신입개발자를 선발하기 위한 문제는 어떻게 별도로 구성을 해야 하는지, 그리고 면접 과정은 어떻게 차이를 둘 것인지 등을 명확히 정리하는 것이 쉽지 않은 문제였습니다.
캠프 참가자, 캠프에 참가하고 싶었는데 못했던 분들, 그리고 신입 개발자 공채 과정에 참여했던 분들과 내부에서 그 과정을 진행했던 분들의 의견을 모은 결과, 올해 진행하는 신입 개발자 공채는 우아한테크캠프를 통해서 진행하는 것으로 결정하였습니다.

[2018년 우아한신입개발자 & 우아한테크캠프 모집]
올해의 우아한테크캠프는 우아한형제들의 신입개발자 채용 과정이기도 하지만, 본질적으로는 작년에 말씀 드린 것처럼 IT 업계에 뛰어드는/뛰어든 많은 개발자 분들이 꼭 참가하고 싶은 캠프가 되고자 합니다.
우아한형제들이라는 회사에 취업을 하고 싶은 분들도 (당연히) 환영하고, 당장 우아한형제들에 꼭 취업할 생각은 아니지만 개발을 잘 하기 위해서는 무엇이 필요하고 어떤 것을 배워야 하는지 궁금하고 경험해 보고 싶은 분들도 환영합니다.
2017 우아한테크캠프에 참가했던 두 분의 후기, “넌 강해졌다, 돌격해!“와 “우아한테크캠프: 좋은 개발자가 되고 싶다면“를 읽어 보시면 어떤 캠프인지 좀 더 잘 아실 수 있으리라 생각합니다.
작년과 마찬가지로 학력에 대한 제한은 전혀 없습니다. 이미 2-3년의 경력을 갖고 있는 분들이 지원하시는 것도 아무런 문제가 없습니다. 다만 7/2부터 8/31까지 우아한테크캠프에 전념할 수 있어야 하고, 9월 또는 12월 말에 취업이 가능한 상태여야 합니다.
많은 회사들이 개발자를 뽑고 있습니다. 여러분들과 여러분의 주변 분들(특히 부모님들)은 삼성전자, SK텔레콤, 현대자동차, 현대카드와 같은 굴지의 대기업 또는 네이버, 카카오같이 인터넷 업계에서 가장 큰 회사를 선호하는 분들이 많이 있을 겁니다.
신입 또는 주니어 개발자에게 좋은 회사란 어떤 회사일까요? 이 질문에도 여러 가지 답이 있겠지만, 본인이 하는 일과 본인이 속한 조직의 분위기에서 모두 배울 수 있는 점이 많은 회사가 좋은 회사라는 답변에는 딱히 반론은 없겠지요.
우아한형제들은 이 블로그를 통해 여러 번 얘기했지만, 조직과 본인이 같이 성장할 수 있어야 그 성장이 건강하게 오래 지속될 수 있다고 생각합니다. 최근에 이 블로그에 게재된 우아한형제들의 Devloper Relations라는 글을 읽어 보시면, 우아한형제들이 어떤 고민을 하고 있고 개발자 분들과 어떻게 해결해 나가는지를 보실 수 있습니다. 그 외에도 조직과 개인의 성장 관련해서는 다음과 같은 글들을 보시길 권합니다.
여러분이 우아한테크캠프에 참여하고 우아한형제들에 입사하게 되면, 신입개발자로서 여러분이 경험할 수 있는 최고의 교육 과정을 제공받을 수 있습니다. 우아한테크캠프만 참석하더라도, 그 두 달 간의 경험이 이후 여러분이 구직 활동을 할 때 큰 경험과 자산이 되어 줄 것입니다. (작년 캠프 참석자들은 우아한형제들을 비롯하여, 삼성전자, 넥슨, 티맥스 등 다양한 기업에 취업한 것으로 알고 있습니다.)
우아한형제들에 입사하고 싶은 신입/주니어 개발자들이라면 우아한테크캠프가 유일한 선택이 될 것입니다.`
우아한테크캠프가 진행되는 동안, 여러분들에게는 150만원의 인턴 비용이 지급되는데요. 월 수백만원을 지불하더라도 들을 수 없는 교육 과정 및 좋은 동료들과의 프로젝트 경험을, 오히려 돈을 받으면서 가질 수 있는 기회는 우아한테크캠프밖에 없다고 생각합니다. 꼭 우아한형제들에 입사하지 않더라도, 이 캠프를 통해서 만난 사람들이 또 하나의 좋은 인적 네트워크가 되어 줄 것이라 생각합니다.
이번 캠프가 우아한형제들에서 원하는 신입 개발자들을 채용하는 수단으로써만이 아니라, 우아한테크캠프의 좋은 경험이 직간접적으로 널리 퍼져서 우리 나라의 많은 개발 조직이 새롭게 이 업계에 진입하는 신입/주니어 개발자들이 성장하는데 관심을 가지길 원합니다. 어떻게 하면 경력같은 신입을 뽑을지 고민하는 것이 아니라, 신입을 어떻게 하면 경력 개발자처럼 키울 수 있을지 고민하고 실행하기를 원합니다.
‘우아한개발자가 되기 위한 우아한테크캠프’라는 이 글의 제목은 그래서 두 가지 의미를 가지고 있습니다. 우아한형제들에 입사하고 싶은 개발자를 위한 캠프라는 뜻 외에도, 꼭 이 회사에 입사하지 않고 어디서 어떤 일을 하더라도 개발을 잘 하고 싶은 사람들을 위한 캠프라는 뜻을 가지고 있습니다.
Q ) 우아한테크캠프 모집 대상자는?
A ) 3년차 이하의 경력을 가진 분들 중, 7/2~8/31까지 우아한테크캠프에 참석 가능하고, 9월 중 또는 12월 말에 취업이 가능한 분이면 됩니다.
Q ) 우아한테크캠프 운영 시간 및 장소는?
A ) 월요일 1시~6시, 화~금 9시~6시. 우아한형제들 사무실에서 진행됩니다. 우아한테크캠프를 위한 교육장 및 사무공간도 세팅되어 있습니다. :-)
Q ) 우아한테크캠프 참가자에 지급되는 금액이 있나요? 
A ) 월 150만을 지급합니다.
Q ) 우아한테크캠프 진행 시 필요한 장비는 지원되나요? 
A ) 장비는 회사에서 대여하여 캠프 시작시에 지급합니다.
Q ) 코딩 테스트는 어떤 언어로, 어떻게 진행되나요? 
A ) 프로그래밍 언어는 지원하시는 분들이 편한 것을 이용하시면 됩니다. 온라인으로 접속하시면 문제가 주어지고, 해당 웹사이트 내에서 프로그래밍하고 테스트하실 수 있습니다.
Q ) 우아한테크캠프 참가자에 대한 입사 특전이 있나요?  
A ) 2018년도 우아한형제들 신입 개발자 공채는 우아한테크캠프를 통해 진행됩니다. 캠프에 참석한 분들 중 신입 개발자를 선발할 예정이며, 선발 규모는 정해져 있지 않습니다. (캠프 참가자 전원을 선발할 수도 있습니다. :-)
마지막으로 우아한형제들의 개발자 모집 영상으로 이 글을 마무리할까 합니다.
이 영상에 소개된 내용이 우아한형제들이 다니기 좋은 회사임을 보여 드리는 것이라면, 그보다 더 중요한 것은 일하기 좋은 회사인가 하는 것일테데요. 그 부분은 우아한테크캠프를 통해서 느껴 보시는 건 어떨까요? :-)
여러분의 많은 지원 부탁 드립니다.
[우아한형제들 개발자 모집 영상]
우아한형제들 또는 이번 우아한테크캠프에 대해 궁금한 부분들은 tech_hr@woowahan.com 으로 편하게 문의해 주시면 됩니다. 최대한 성실하게 답변 드리도록 하겠습니다.
"
http://woowabros.github.io/experience/2018/04/17/linux-maxuserprocess-openfiles.html,2018-04-17,"Java, max user processes, open files","안녕하세요? 우아한 형제들에서 결제/정산 시스템을 개발하고 있는 이동욱입니다.
올해 사내 블로그 포스팅 주제로 Linux의 open files, max user processes 설정에 대해 정리하게 되었습니다.
계기는 단순했습니다.
팀에서 서버 작업하던 중 쓰레드와 관련해서 문제가 발생했는데요.
제가 진행하던 일이 아니라서 옆에서 해결하는 과정을 지켜봤습니다.
부끄럽게도 전혀 모르는 내용이 오고 갔습니다.
복기가 필요하단 생각에 정리를 진행 하던 중, 이왕 하는김에 회사 블로그에 올리면 좀 더 자세히 공부하지 않을까 하는 마음에 선택하게 되었습니다.

(퀄리티는 에피타이저, 마음만은 메인 디쉬로 가겠습니다!)
여기에서 사용된 코드는 실제 회사에서 사용한 코드는 아니며, 포스팅을 위해 최대한 유사하게 만들어진 별도의 샘플 코드임을 먼저 말씀드립니다.
Linux에는 OS 레벨에서의 제한 설정이 있습니다.
보통 이를 ulimit (user limit) 이란 명령어로 확인하는데요.
2가지 옵션으로 대부분 확인합니다.
톰캣을 이용해서 서버 운영 도중, 다음과 같이 OutOfMemoryError가 발생했다고 가정하겠습니다.

더이상 쓰레드를 생성할 수 없다는 에러인데요.
뭐가 문제였는지 하나씩 확인해보겠습니다.
맨 처음 서버를 할당 받은 초기 상태 그대로라 ulimit -a는 아래와 같습니다.

화면을 보시면 open files 와 max user processes의 값이 1024로 동일하게 잡혀있습니다.
쓰레드 생성에 문제가 발생한거라 max user processes가 문제인것 같지만, 확신할 수 없으니 테스트 환경을 구축해서 실험해보겠습니다.
테스트용 서버는 AWS EC2의 t2.micro입니다.
t2.micro로 생성후 ulimit -a로 확인해보겠습니다.

t2.micro는 기본 설정이 open files가 1024, max user processes가 3902로 잡혀있다는 것을 알 수 있습니다.
자 그럼 간단하게 추측할 수 있는 것이, 현재 설정에서 1024 <= 동시에 생성가능한 쓰레드수 <= 3902라면 max user processes가 부족해서 발생한 문제임을 알 수 있겠죠?
이를 확인하기 위해 간단한 스프링부트 프로젝트를 생성하겠습니다.
코드는 간단합니다. /4000으로 HTTP 요청이 오면 비동기로 4천개의 쓰레드를 동시에 생성하고, 20분간 유지합니다.

프로젝트를 테스트용 EC2에 배포하고 curl과  tail -f nohup으로 확인해보겠습니다.

사진속을 보시면 3855번째에서 unable to create new native thread 에러 메세지가 발생했습니다.
즉, open file 제한인 1024개를 초과해서 쓰레드가 생성된 것입니다!
max user processes만큼만 쓰레드가 생성된것을 확인할 수 있습니다.
Linux에서는 프로세스와 쓰레드를 동일하게 봅니다.
두번째로 위에 있던 open files 값은 어떤 값을 가리키는지 알아보겠습니다.
open files에 관해 검색을 해보면 이 값이 프로세스가 가질 수 있는 소켓 포함 파일 개수를 나타낸다는 것을 알 수 있습니다.
자 그럼 소켓을 open files 값 보다 많이 만들면 어떻게 되는지 테스트 해보겠습니다.
테스트 방법은 간단합니다.
API 요청을 받아, 20분간 대기시켜줄 서버로 ec2를 한대 더 생성합니다.
테스트에 사용한 코드는 다음과 같습니다.
요청을 보낼 메소드

요청을 받을 메소드

자 그리고 동시에 1100개의 요청을 보낼수 있도록 간단한 비동기 요청 스크립트를 만듭니다.

(실제 운영환경에선 Ngrinder등을 통해서 하겠지만, 여기선 간단한 테스트이니 스크립트로 대체합니다.)
준비가 다 되었으니 한번 테스트를 진행해보겠습니다!
…..
테스트를 진행하자마자 바로 에러가 발생했습니다.
로그를 확인해보니

단순 쓰레드 생성과 달리, EC2의 서버 메모리가 먼저 부족해져서 EC2 사양을 높여서 다시 실험하겠습니다.

(t2.micro는 메모리가 1GB인지라, t2.large (8GB) 로 변경합니다.)
업데이트된 EC2의 ulimit은 아래와 같습니다.


AWS EC2의 사양을 올릴수록 max user processes가 적절한 값으로 증가하는 것을 확인할 수 있습니다.
이를 통해 알 수 있는 것은, AWS EC2를 사용하실 경우 max user processes AWS 내부에서 인스턴스 사양에 맞게 적절한 값을 세팅해주니, 굳이 저희가 손댈필요는 없다는 것입니다.
자 그럼 다시 한번 테스트를 해보겠습니다.
테스트용 서버에 프로젝트를 배포하고, 해당 프로젝트가 생성한 open file count를 아래 명령어로 확인합니다.

PID를 찾고,

생성된 open file 리스트를 확인할 수 있습니다.
준비가 다 되었으니, 1100개 요청을 보내는 스크립트를 실행해봅니다!

엇?
결과가 뭔가 이상합니다.
분명 open files의 값은 1024개로 되어있는데 1330개가 열려있다고 나오다니요.
이상하다는 생각에 스크립트를 몇번 더 실행합니다.
(한번 실행때마다 1100개의 요청이 간다고 생각하시면 됩니다.)

2번을 추가로 더 실행해서 3300개의 요청이 갔음에도 에러없이 처리되고 있습니다.
언제 터지는지 확인하기 위해 추가로 더 요청해봅니다 (총 4400개가 갑니다.)

드디어 Open File 에러가 발생했습니다!
보시면 4097개까지만 열린채로 에러가 발생한 것을 알 수 있는데요.
이전 요청이 3532개를 생성했으니 추가로 1100개를 요청하면 4600개 이상이 생성되어야하는데 왜 4097개까지만 생성된 것인지 궁금합니다.
혹시나 하는 마음에 ulimit -aH로 soft가 아닌, hard옵션을 확인해봅니다.

오?
마침 딱 4096개 입니다!
프로세스별 open file은 soft 값이 아닌 hard 값까지 생성가능한게 아닐까? 라는 추측이 됩니다.
검증하기 위해 hard의 open files를 5120개로 증가시킵니다.
(soft 옵션은 그대로 1024개 입니다.)

자 그리고 다시 요청을 진행해보겠습니다.
이번에 4096개가 넘는 요청이 가능해진다면 soft가 아닌, hard 값까지 생성 가능하다는걸 알수있겠죠?

예상대로 4000개가 넘는 API 요청 (4635)이 가능해졌습니다!
실제로 ulimit -a로 확인할 수 있는 soft 값으로 소켓 생성이 제한되지는 않는다는 것을 알 수 있습니다.
결국 소켓 생성 제한은 hard 옵션에 따라간다 라는 이야기가 됩니다….

이 글을 마무리하려던중, 깜짝 놀랄 이야기를 들었습니다.
파이썬은 안그러는데요?
제보를 해주신 배민찬의 모 선임님과 함께 확인을 해봤습니다.
soft limit으로 1024, hard limit으로 4096인걸 확인하고, 파이썬 스크립트로 file을 임의로 열어봅니다.

여기서 3개를 빼야하는 이유는 stdin, stdout, stderr의 표준 입/출력이 포함됐기 때문입니다.
1021개에서 1개를 더 추가하니 바로 Too many open files Error가 발생합니다!
헉? soft 옵션이 파이썬에서 잘 적용된걸까요?
진짜 그런지 한번 더 확인해봅니다.
open files soft 값을 2000으로 증가시킨후 다시 1997개가 넘는 file을 open 해봅니다.

여기서도 마찬가지로 1997개 이상 file open시에 바로 Too many open files Error가 발생합니다.
왜 파이썬은 soft옵션까지만 file이 오픈되고, Java에선 hard 옵션까지 file이 오픈되는건지 이상했습니다.
이상하단 생각에 strace로 JVM 로그를 확인해보니!

이렇게 setrlimit으로 limit을 업데이트하는 로그가 찍혀있습니다!
왜 이런 로그가 발생했는지 오라클 Java 옵션을 찾아봤습니다.
문서에는 MaxFDLimit 라는 옵션이 있었는데요, 뭔가 file limit과 관련돼 보입니다.

이 옵션이 뭔지 찾아보니 openjdk 코드에서 이 옵션이 true일 경우 setrlimit 으로 limit을 증가시키는 것을 확인할 수 있습니다.

그리고 설치된 Java의 MaxFDLimit 기본값이 true임을 확인할 수 있습니다.

즉, 리눅스 OS 환경에서는 JDK 실행시 자동으로 limit 사이즈를 증가시켜준다는 것을 알 수 있습니다.
위 2개의 실험으로 얻은 결론입니다.
참고로 Tomcat은 8 버전부터 기본 Connector 방식을 NIO로 사용합니다.
(7 버전까지는 BIO)
그러다보니 maxConnections은 10,000, maxThreads는 200이 기본값입니다.
(BIO에서는 둘의 값이 동일해야 합니다)
이번 테스트에서 사용되는 connection 수가 1만을 넘지 않기 때문에 기본옵션으로 진행했습니다.
보통 soft limit과 hard limit을 다르게 설정하진 않습니다.
둘의 값을 동일하게 적용하는데요.
서버의 open files, max user processes등 옵션을 permanent (영구) 적용하기 위해선 /etc/security/limits.conf 을 수정하면 됩니다.

맨 앞의 *가 있는 자리는 사용자 계정을 나타냅니다.
ec2-user로 지정하시면 ec2-user 계정에서만 옵션이 적용되는데요.
이미지처럼 *로 지정하시면 모든 사용자 계정에 옵션이 적용됩니다.
이렇게 적용후, 다시 접속해서 확인해보시면!

옵션이 잘 적용되어있음을 확인할 수 있습니다.
제가 준비한 내용은 여기까지입니다.
좋은 기회로 그 동안 막연하게 알고 있던 max user processes, open file를 제대로 정리해볼 수 있었습니다.
부족함이 많은 글임에도 끝까지 읽어주셔서 감사합니다.
다음에도 이와 같이 기본적인 내용이지만, 프로젝트에서 도움을 받는 일이 발생한다면 잘 정리해서 공유드리겠습니다.
그럼, 다음에 또 뵙겠습니다.
감사합니다!

(사용된 모든 짤은 레진코믹스의 레바툰입니다.)
"
http://woowabros.github.io/woowabros/2018/04/15/developer-relations.html,2018-04-15,우아한형제들의 Developer Relations,"투자가를 대상으로 한 IR(Investor Relations) 이야기가 아닌,  개발자를 대상으로 한 DR(Developer Relations) 이야기.
Developer Relations라는 말은 다소 생소합니다.
IR(Investor Relations)은 많이 들어보셨을 텐데요. IR은 투자자들을 대상으로 기업 설명 및 홍보 활동을 하여 투자 유치를 원활하게 하는 활동을 의미합니다.
우아한형제들은 IR 만큼이나, DR(Developer Relations) 활동을 중요하게 생각합니다. 개발자들을 대상으로 기업을 설명하고 홍보하여 좋은 개발자 분들이 같이 할 수 있는 기회가 많아지기를 원합니다.
IR과 DR 모두 대상자만 다를 뿐, 회사의 매력을 전달하는 것이 핵심입니다. 중요한 것은 내가 하고 싶은 얘기를 전달하는 것이 아니라, 상대가 궁금한 것을 전달해야 한다는 것입니다.
최근에 여러 회사가 개발자 채용을 적극적으로 진행한다는 기사를 보았습니다. 월요일 1시 출근, 주35시간 근무, 연봉 5천만원 이상, 무제한 도서비 지원 등의 문구로 지하철 역에 광고가 나오기도 했더군요. 개인 입장에서 처우와 근무 조건/환경은 분명 중요한 요소라고 생각합니다. 우아한형제들도 작년에 이런 구인 동영상을 페이스북 페이지에 게재하기도 했고요. (이 자리를 빌어 멋진 동영상을 만들어준 영상디자인팀 분들께 감사 인사 드립니다. 잘 만들어진 동영상이니 꼭 한 번 봐 주세요 :-)
개발자를 채용하는 회사들이 많아지면서, 개발자에 대한 처우가 개선되는 것은 고무적인 일입니다. 그러면 이런 상황에서 개발자는 어떤 기준으로 회사를 택해야 할까요? 회사는 어떤 부분을 개발자 분들에게 어필해야 할까요? 다시 말해 DR은 어떤 이야기를 전달해야 할까요? 이 부분을 생각해 보았습니다.
모든 직군이 마찬가지지만, 개발자 분들은 특히 성장에 대한 욕구가 크다고 생각합니다. 어떤 처우를 받고, 어떤 근무 조건이냐도 중요하지만, 자신이 성장할 수 있는 환경인지를 무척 중요하게 여깁니다.
문제는 이러한 환경인지 아닌지를, 개발자 입장에서 입사 전에 어떻게 알 수 있을 것이며, 회사 입장에서 어떻게 알릴 수 있냐는 것입니다. 모든 회사는 다 조직과 개인이 같이 성장하는 회사라고 얘기하니까요.
제가 20년 가까이 일을 하면서 느낀 점은, 회사의 성장이 크지 않은 상황에서 개인이 성장하기는 힘들다는 것입니다. 이 말은 회사를 위해서 개인이 희생해야 한다는 것이 아니라, 개인의 성장 동력으로써 회사의 환경이 뒷받침되어야 한다는 것을 의미합니다.

[우아한형제들 실적 추이]
위 그림을 보시면, 우아한형제들의 2017년 매출은 전년 대비 92% 증가했고, 영업이익은 768% 증가했습니다. 2010년 6월 처음 서비스를 제공하고 7년차에서 8년차로 넘어가는 시점인데도 매출이 2배 가까이 성장했다는 것은 엄청난 증가폭입니다.
배달의민족 서비스는 매출이 모두 온라인 서비스에서 나오기 때문에, 매출이 늘었다는 것은 서비스의 트래픽이 그만큼 늘었다는 것을 의미합니다. 제가 2015년 8월에 입사했는데, 전월의(2018년 3월) In-App Purchase 수를 그 때와 비교하면 무려 7.5배로 늘어났습니다. 주문 시도 기준으로 하루에 100만 건이 넘는 수치를 보인 날도 있고, 저녁 시간같은 peak time에는 시간당 10만 건이 넘는 수치를 보이기도 합니다.
이러한 서비스 변화는 개발자들에게 도전 과제로 주어지게 됩니다. 1) 선착순 이벤트를 하면 1.89초 만에 마감되는 서비스, 2) PG사와 새롭게 연동했더니 PG사와의 전용선 bandwidth 제약에 걸리고, 그것을 해결했더니 PG사 서버가 다운되는 서비스, 3) 다른 서비스 플랫폼을 이용하여 프로모션을 했더니 해당 서비스가 죽어서 치도스라는 신조어를 만들어 낸 서비스. 이런 서비스가 문제 없이 돌아가게끔 하기 위해, 계속 새로운 시스템을 고민하고 개선해 나가야 합니다.

[버스 정류장에 설치된 블랙 후라이드데이 포스터]
사업과 서비스가 빠르게 성장하면서 개발자들이 기술적으로 해결해야 하는 과제가 계속 주어지는 회사에서는, 개인에게도 성장의 기회가 많이 주어질 수밖에 없습니다. 여기서 중요한 것은 이미 몇 년째 100만명, 1,000만명을 감당하고 있는 서비스 회사보다는, 이전에 1만명에서 10만명으로, 다시 100만명으로, 그리고 1,000만명을 담당하는 서비스로 성장하는 곳에 있는 것이 훨씬 더 개인의 성장에도 도움이 된다는 것입니다. 남들의 경험을 듣는 것보다, 본인이 직접 경험하는 것이야말로 온전히 자기 것이 될 수 있기 때문입니다.
우리 나라의 여러 서비스 중에서 Read 성격의 요청이 아니라 한 시간에 10만 건 이상의 실제 주문을 처리하는 서비스가 얼마나 될까요? 이미 몇 년째 서비스의 트래픽이 비슷하거나 완만하게 유지되는 것이 아니라, 계속해서 2배 가까이 성장하면서도 저런 규모의 주문을 처리해야 하는 시스템이 얼마나 될까요? 이미 완성되어 운영되는 것이 아니라, 계속 기존 시스템을 개선하는 과제의 필요성이 절실히 존재하고 실제로 추진하고 작은 성공을 만들어 가는 곳이 얼마나 될까요?
본인이 아무리 노력하더라도 회사의 사업/서비스가 본질적으로 필요로 하는 기술 과제가 없는 경우, 개인이 발전하는 것은 무척 힘들 겁니다. PT(Personal Training)를 받아 본 분들은 아실텐데요. 옆에서 누군가 계속 내가 운동을 하게끔 만드는 환경과, 혼자 의지만 갖고 운동하는 것과는 큰 차이가 있을 수밖에 없습니다. 정말 끈질긴 노력을 통해 뭔가를 만들었다고 해도, 피드백을 받지 못하는 상황에서는 제대로 한 것인지 알기 어렵습니다. 자신만을 위한 Over Engineering 과제로 남을 확률이 높습니다. 그래서 회사가 얼마나 성장하고 있는가 하는 것이, 개발자 입장에서 회사를 택할 때 무척 중요한 요소라고 생각합니다.
어느 서비스나 장애가 발생하는 것은 심각한 상황입니다. 개발팀은 개발팀대로, 고객 응대를 하는 부서는 그 부서대로 심한 스트레스를 받게 되고, 장애가 해소된 이후에도 여러 가지 힘든 업무를 수행해야 합니다.
개발자라면 잘 아시겠지만, 장애는 항상 날 수 있습니다. 장애를 줄이는 방법은 크게 보면 두 가지입니다. 하나는 비용을 들여서 시스템을 증설하여 확률을 줄이는 것이고, 또 하나는 근본적으로 특정 시스템에서 장애가 나더라도 전체 시스템에 문제가 없는 failure resilient한 시스템 구조를 만들고 구축하는 것입니다.
그런데, 우리나라의 많은 회사에서는 두 가지 방법 모두 실행하기가 여의치 않습니다. 장애가 나는 것은 모든 회사가 싫어하지만, 그 상황이 지나가고 장애 복구 및 이후 대처 방안에 비용 증가가 수반될 경우 많은 회사들이 그 방안을 쉽게 채택하지 않습니다. 전체 시스템을 개선하는 방안은 더욱 실행하기 힘듭니다. 결국 인력이 투입되어야 하는데, 새로운 사업 요구 사항과 장애를 줄이는 두 가지 선택 중에서는 대부분 전자가 채택되기 마련이죠.
많은 개발자들과 만나서 얘기해 보면, 구글, 페이스북, 넷플릭스 등이 failure resilient한 시스템을 구축하고 개선해 나가는 것에 대한 선망이 있습니다. 그들은 왜 그런(개발자들을 시스템 구조를 개선하는 일에 투입하는) 선택을 했을까요? 그건 그들의 사업에 있어서 그러한 투자가 훨씬 이득이 되기 때문입니다.
구글, 페이스북, 넷플릭스는 장애로 인한 1%의 매출 하락을 방지하기 위해, 몇 백 명의 개발자들을 투입하는 것이 이득입니다. 한국에서는 사업과 서비스의 규모가 그 정도로 되는 회사가 정말 드물죠. 네이버의 검색 서비스와 카카오의 메신저 서비스 정도일 것 같네요. 이런 면에서 배민(배달의민족)은 그 규모는 앞서 말한 서비스들보다 작으면서도, 기술에 대한 요구 수준은 굉장히 높은 특이한 서비스입니다.

[서비스 장애가 난 어느 주말의 트위터 풍경]
위 스크린샷은 배민 서비스에 장애가 난 어느 주말 저녁에 올라온 트위터 글입니다. 다른 서비스라면, 특정 시간에 장애가 생기면 한 두 시간 뒤에 다시 시도하면 됩니다. 그런데, 배민은 먹는 서비스이다보니 사용자들은 내가 원하는 그 시간에 실행이 안 되는 것에 굉장히 민감해 합니다. 배민이 요기요나 배달통보다 훨씬 높은 점유율을 갖고 있기에 배민 서비스가 다운되면 요기요나 배달통도 다운되는 경우가 많다보니, 일부 사용자 분들은 흡사 전기나 수도가 끊긴 것과 비슷한 반응을 보일 때도 있습니다.
일반 사용자 분들이 아닌, 배민에 광고비를 내시는 매장의 사장님들을 생각하면 훨씬 더 심각합니다. 배민의 핵심 비즈니스 모델이 한 달에 정해진 금액을 내는 광고 모델이다보니 장애가 일어난 시간에 대해 보상을 하고 있습니다. 문제는 주말 저녁의 경우 peak time이기 때문에, 단순히 한 달 720시간중 1시간에 대한 보상이 아니라, 사장님들 입장에서 그 날 준비한 장사에 여러 의미로 지장을 준 것에 대한 의미로 더 크게 보상을 하는 것으로 결정한 경우가 꽤 있다는 것입니다. (사장님들 입장에서는 이 보상도 탐탁치 않게 여기시지만요)
배달의민족은 이러한 서비스의 특징 때문에, 시스템을 안정적으로 운영하기 위한 비용 지출과, failure resilient한 시스템으로 개선해 나가는 프로젝트 수행을 적극적으로 진행하고 있습니다. 우아한형제들이라는 회사가 기술을 중요하게 생각한다고 포장하고 싶어서가 아니라, 장애에 내성이 강한 시스템을 만드는 것이 실제 회사 차원에서 ROI가 나오는, 본질적으로 중요한 과제이기 때문입니다.
위에서 회사의 성장과 개인의 성장에서도 했던 얘기지만, 회사 입장에서 본질적으로 중요하다고 생각하는 과제가 개발자 관점에서도 의미가 있는 과제일 때, 그것이 지속될 수 있고 성과를 만들어낼 수 있습니다. 회사의 메시지는 말이나 글로 읽는 것이 아니라, 그 회사가 시간과 비용을 어디에 쓰는가를 보면 알 수 있다고 생각합니다. 이런 관점에서 우아한형제들은 개발자 처우 수준을 높이고 시스템 비용을 아끼지 않고, 실질적인 시스템 개선을 위한 프로젝트를 수행하고 있다는 점을 개발자 분들이 알아 봐 주시면 좋겠습니다.
외부 개발자 분들과 얘기하다 보면, 배민이 잘하는 것은 알겠지만 한계가 있을 것 같다는 얘기를 듣는 경우가 있습니다. 음식이라는 하나의 도메인에서 배민이라는 서비스만 제공해서 얻을 수 있는 매출에 한계가 있을 것 같다는 의견입니다.
잘 아시다시피, 배민이라는 서비스는 이미 배달을 해 주던 업소들의 메뉴를 주문할 수 있는 서비스인데요. 작년 11월부터는 배민라이더스 서비스가 배민과 통합되어, (서울 지역에서는) 배민의 메인 화면에서 그동안 배달 음식점에서 보기 힘들었던 다양한 메뉴들이 노출되고 판매되고 있습니다.

예전에는 배달을 직접 수행해야지만 배달 서비스를 제공할 수 있었으나, 배민라이더스를 이용해서 배달을 이용하여 매출을 증대시킨 많은 사장님들이 계속해서 배민라이더스 서비스에 문의를 주시고 매장이 늘어가고 있습니다. 모든 식사가 배달로 바뀌진 않겠지만, 원래 배달을 하지 않던 매장들이 빠르게 배달 서비스를 제공하면서 늘어나는 시장이 지금까지 만들어 왔던 시장보다 훨씬 더 클 것으로 예상하고 있습니다.
이러한 사업기회의 확장은 단순히 매출이 늘어나는 것만을 의미하지 않습니다. 배민라이더스는 저희 회사와 계약 관계에 있는 라이더 분이 주문을 받으면 음식점으로 이동해서 음식을 수령한 후 고객님들에게 배달하는 서비스인데요. 가만히 생각해 보시면 이 서비스는 우버와 동작이 똑같습니다. 고객이 우버를 부르면(주문이 발생하면), 우버 기사는 고객에게 이동하여 태우고(라이더는 식당으로 이동하여 음식을 수령하고), 목적지로 이동한 후 승객을 내려주는(주문한 고객 위치로 이동한 후 음식을 전달하는) 동작과 거의 같습니다. 규모는 우버에 비할 바 못하겠지만, 실시간으로 움직이는 에이전트에 대한 스케줄링과 최적화 관점에서는 우버와 동일한 기술적 과제가 존재함을 의미합니다.
배민라이더스 외에도 많은 분들이 알고 있는 서비스로, 배민찬 서비스가 있습니다. 배민찬 서비스는 배민프레시라는 이름으로 제공이 되다가, 작년부터 반찬이라는 카테고리에 집중하여 제공하고 있는 Food Commerce 서비스입니다. 반찬으로 집중한 후 2017년 한 해만 4배가 넘는 성장세를 보이고 있습니다.
배민찬에서 일하는 개발자 분들이 경험을 공유해 준 글들로는 다음과 같은 글들이 있습니다.
배민라이더스와 배민찬 외에도 배민상회라는 서비스도 제공하고 있는데요. 이 서비스는 사장님들을 위한 물품 구매 사이트입니다. 음식을 배달 제공할 때는, 숟가락, 젓가락, 비닐봉지서부터 여러 가지 물품이 필요한데, 바로 그런 물품들을 판매하는 사이트입니다. 사장님들 입장에서는 다른 곳보다 저렴하게 구입할 수 있고, 배달의민족 특유의 위트가 가미된 제품들로 인해 고객들이 좋아한다면, 배민상회 서비스를 이용하시지 않을 이유가 없겠죠. 이 서비스도 최근 1년 동안 30배 넘게 성장하는 모습을 보여 주고 있습니다. 아래는 배민상회의 스크린샷입니다.

[배민상회 스크린샷]
지금까지 소개해 드린 사업은, 이미 현재 제공하고 있어서 더 이상 숨길 이유도 없고 편하게 말씀 드릴 수 있는 것이었다면, 어떤 사업들은 이런 공간에서 오픈하고 말씀 드릴 수 없는 것들도 많이 있습니다. 그 중에서 배달로봇 관련된 것은 이미 신문기사를 통해 부분적으로 공개되었는데요. 로봇이 처음부터 끝까지 모든 작업을 다 할 수 있진 않더라도, 앞으로 다가올 자율주행차와 로봇을 결합하면 현재 배달이라는 작업을 수행하는데 들어가는 비용을 획기적으로 줄일 수 있을 거라고 생각하고, 꾸준히 계속 추진하려는 계획을 갖고 있습니다.

오랜만에 회사에 대해 말씀 드리다보니, 글이 무척 길어졌네요. DR(Developer Relations)도 한꺼번에 하려고 하면 안 되고, 꾸준히 계속해야 한다는 것을 새삼 다시 느끼게 됩니다. :-)
다시 찬찬히 읽어보니, 어찌보면 위에 적은 글들은, “우리 사업 잘 하고 있어요”라는 자화자찬인것 같아 부끄럽기만 합니다. 제가 정말 전하고 싶었던 말은, 우아한형제들이라는 회사가 일을 더 잘 하기 위한 고민을 끊임없이 계속한다는 것입니다. 사업과 서비스가 성장하면서 고민할 수밖에 없도록 요구받고 있고, 고민하고 일해야지만 해결할 수 있는 과제들이 충분히 많이 놓여 있습니다.
이것은 일 자체에 대해서만이 아니라, 일을 잘 하기 위한 메타적인 일에도 적용이 됩니다. 우리가 일하는 환경을 더 낫게 만들려면 어떻게 해야 할 지 고민하다가, 서비스를 만드는 기획/디자인/개발 조직은 몽촌토성역이 아닌 잠실역에 새로운 공간을 얻어서 이동하는 것으로 결정한 것처럼 말이죠. 공간과 일하는 방식 모든 것을 고민의 대상으로 삼고 있습니다.
좋은 회사는 현재로서 완성된 좋은 모습을 갖추어서가 아니라, 어떤 것이 좋은 것인지, 더 좋은 것인지를 지속적으로 고민하고 실행에 옮길 수 있는, ‘좋음’의 모습이 현재 진행형인 회사라고 생각합니다.

우아한형제들이 매출이 가장 높은 회사여서가 아니라, 복지 혜택이 가장 좋아서가 아니라, 일을 더 잘 하기 위한 고민을 같이 나누고 변화시켜 나갈 수 있기에, 이 글을 보시는 많은 좋은 개발자 분들과 같이 일하고 싶다는 말씀 전하고 싶습니다. :-)
채용공고는 우아한형제들 홈페이지를 참고하시면 되고, 개발자 채용 관련해서 궁금하신 사안은 tech_hr@woowahan.com 으로 메일 보내 주시면 답변 드릴 수 있도록 하겠습니다.
감사합니다.
"
http://woowabros.github.io/experience/2018/03/31/hello-geofence.html,2018-03-31,"Hello, Geo-fence!","안녕하세요 라이더스개발팀 박민철 입니다.
BROS(Baemin Riders Operating System)에 Geo-fence를 도입한 경험을 적어보겠습니다.
(BROS는 라이더를 배차하고 음식을 픽업하여 고객에게 전달하는 서비스를 지원하는 배달 플랫폼 입니다.)
‘geo-fencing’ 으로 불리기도 하는데, 지도위에 가상의 도형들의 영역을 말합니다.

[이런식으로 그리면 혼나요]
기존에는 지점(관제센터)의 행정동코드 기반으로 배달 가능 여부를 판단하였습니다.
고객의 행정동코드와 지점의 배달 가능한 행정동코드를 매핑하여, 배달 가능한 지점이 존재하면 주문을 생성할 수 있었습니다.
여기에서 1가지 같은 2개의 이슈가 발생합니다.

[경계선 주변 배달불가능]
첫 번째는 고객이 배달 가능 지역에 가까워도 특정 지점의 행정동코드가 없으면, 배달할 수 없는 문제입니다. 경계선 근처에 거주하는 고객에게 서비스를 제공하지 못한 안타까움이 있습니다.


[움직일 기미가 안 보이는 라이더님]
두 번째는 배달 가능 지역내 일부 영역을 제외할 수 없는 문제입니다.
이 프로젝트가 진행되어야 하는 이유의 많은 지분을 차지하고 있습니다. 
공원이나 오토바이가 진입하기 어려운 지역, 오랜 시간을 소요해야 하는 지역(타*팰리스는 보안 때문에 출입시간 소요가 많습니다.)을 피크타임에 유연하게 처리할 수 있도록 해야 합니다.
먼저 Geo-fence를 사용하기 위해 네이버 지도 API 문서를 잘 읽었습니다.
그다음 BROS에서 지점별로 접수영역과 차단영역을 그리고 저장할 수 있도록 기능을 개발했습니다. 
그리고 행정동코드기반 로직에 접수영역과 차단영역을 추가로 판단하여 배달 가능 여부를 전달하도록 수정하였습니다.

[접수영역과 차단영역을 그려봅니다]
약간의 문제가 있습니다.
기존 시스템에서는 지점들의 배달 가능한 행정동코드를 On/Off로 관리하고 있었습니다.
관리자가 접수영역과 차단영역을 적절하게 그릴 수 있도록 행정동의 정보를 Geo-fence로 같이 볼 수 있어야 했습니다.
그래서 통계청 통계지리정보서비스에서 제공하는 행정동 경계 파일을 기반으로 작성된 geojson 파일을 사용하여 
행정동 영역을 저장하였습니다. ( github에 공유해주신 개발자님 감사합니다. )
하지만, 통계지리정보서비스에서 제공하는 코드는 7자리 행정구역분류코드이고, 저희가 사용하는 코드는 10자리 행정표준코드관리시스템코드이기 때문에 변환이 필요했습니다. ( 아 … 할많하않 )
통계분류 포털 홈페이지의 행정구역분류 엑셀 데이터를 다운받아 매핑을 할 수 있었습니다.
(이 삽질작업은 오픈 프로젝트인 우아한주소에 저장되어 있습니다.)

[행정동과 같이 접수영역과 차단영역을 그려봅니다]
마지막으로 로직을 수정합니다.
기존 로직에 행정동코드 포함 여부에 추가로 고객의 위치가 접수영역 안에 있는지 혹시 차단영역 안에 있는지 판단하는 기능을 추가하였습니다. 
이렇게 간단하진 않지만, 대략 이런 느낌입니다.





바쁘신 분들은 뒤로가기를 눌러주세요.
Geo-fence에서 생성한 도형 안에 점(Point)이 존재하는 여부를 JTS(평면 기하학을 위한 기능을 제공하는 오픈소스 Java 라이브러리)를 사용했습니다.
Point.class와 Polygon.class은 Geometry.class를 상속받아 사용하고 있었으며, 적절해 보이는 within이라는 함수를 사용했습니다.
한번 테스트를 해봅니다.

[테스트통과는 언제나 기분이 좋아요]
궁금증이 하나 생깁니다. 영역의 경계선 위에 점(Point)이 있으면 어떨까요? 제대로 동작할까요?

점(Point)이 영역의 경계선에 있으면, 안에 있다고 판단하지 않습니다. 그렇군요.
문서를 읽어봅시다. (영어라서 문서를 먼저 안 읽…)


문서를 보면 contains, covers, coveredBy, equals … 그럴듯한 함수들이 있습니다.
앞서 사용한 within 함수를 살펴보면 DE-9IM 단어와, [T * F * * F * * * ] 표기가 있습니다.
잘 모르겠습니다. 그래서 검색을 해봤습니다.
DE-9IM?

[DE-9IM : 두 도형의 관계를 행렬로 표현해 봅시다]
DE-9IM은 두 도형의 내부, 경계, 외부의 총 9가지의 관계를 표현하며, 교집합에서 발생하는 도형의 Dimension(차원)을 행렬로 나타냅니다. 
그림에서 보면 II(가장 왼쪽 위)는 교집합의 도형이 2차원이기 때문에 2가 나오고, IB는 교집합이 직선이므로 1이란 값이 도출됩니다.
A simplified version of dim(x) values are obtained mapping the values {0,1,2} to T (true),
so using the boolean domain {T,F}.
[Dimension이 0이라고 false가 아닙니다!]
그리고 DE-9IM을 boolean으로 변환할 때 점, 선, 면은 모두 true이며 존재하지 않는 도형은 false가 됩니다.
그럼 다시, 앞서 작성한 두 가지의 테스트코드를 다시 살펴봅시다.
테스트를 통과하지 못한 이유
앞에 두 가지의 경우에 대해 테스트코드를 작성했습니다.
문서에 보면 within을 포함한 covers, contains … 함수들의 DE-9IM 규칙을 설명한 내용이 있습니다. 

within의 규칙은 [T * F * * F * * *]입니다. 그래서 1번 조건은 만족하지만, 2번 조건을 만족하지 않습니다.
그래서 테스트코드에서 within의 값은 false로 반환됨을 알 수 있습니다.
2번 조건을 만족하기 위해선 wihtin 함수가 아니라 coveredBy 함수를 사용해야 합니다. 
contains는 within의 도형의 순서가 바뀐 것이며, covers와 coveredBy도 유사한 관계를 알았습니다.
역시 아는 만큼 보입니다!





수포자들은 뒤로가기를 눌러주세요
DE-9IM을 사용하여 두 도형의 관계를 표현하는 방식을 알았습니다.
그런데 말입니다.
두 도형의 교집합을 판단하는 건 어디까지나 저의 뇌피셜로 판단을 했는데요, 
한 점(Point)이 다른 도형 안에 있다는 것을 어떻게 컴퓨터가 판단하는지 궁금해졌습니다. 그래서 한번 찾아봤습니다.
보통 가장 많이 사용하는 방법은 Ray Casting Algorithm입니다.

[알고리즘이라서 그런지 0부터 시작(소오름)]
알고리즘은 대략 이렇습니다. 그림을 보면 알 수 있듯이, 점을 지나는 임의의 직선을 하나 쭉! 그립니다. 
그러면, 그 직선과 도형이 만나는 교점이 생깁니다. 이 교점을 기준으로 7개의 선이 만들어집니다. 
그 점이 홀수 번째 선 위에 있으면 도형 안에 포함, 짝수 번째 선 위에 있으면 도형 밖에 있다고 판단하면 됩니다. 
( 이렇게 심플한 방법이! )
그럼 만만한 문제를 한번 풀어봅시다.
수학적 사고를 언어로 작성합니다.

[수학문제 같이 생긴 문제를 풀어봅시다]
임의의 점(4, 8)이 삼각형 안에 있는지 밖에 있는지 확인하시오. (3점)  
문제를 해결하는 순서는 다음과 같습니다.
먼저 좌표들을 순회하면서 직선의 방정식을 구합니다.
직선의 방정식
직선의 방정식은 y = ax + b ( a는 기울기, b는 y절편 ) 입니다.
두 점이 있는 경우 기울기( y증가량 / x증가량 )를 구하고, y절편은 두 점 중 한 점과 기울기를 대입하여 값을 구합니다.
3개의 직선 방정식을 구했습니다. 이제 직선들과 점(4, 8)을 지나는 파란 직선(y = 8)과의 교점을 구합니다.
y = angle * x + yIntercept 직선에 y = 8을 대입하여 교점(?, 8)들의 x좌표를 구합니다.
교점
교점을 모두 구하면, 다음과 같이 3개의 교점(빨간색점)의 x값을 찾을 수 있습니다. 

[예외인 녀석이 있는 것 같다]
그림에서 보면 가장 왼쪽에 있는 교점(빨간색점)은 삼각형의 변 위의 점이 아니기 때문에 예외를 해줘야 합니다.
두 점의 x 값을 범위로 지정하여 교점이 그 사이에 있는지 판단하고, 삼각형과 만나는 교점만을 리스트에 추가합니다.
몇 번째 선 위에 있는가?
마지막으로, 점(4, 8)이 교점들 사이에 생긴 선 중 몇 번째 속하는지 판단합니다.

감동적인 마무리
수고하셨습니다. 드디어 끝났어요! ( 짝짝짝 )
이렇게 간단(?)하게 작성할 수 있었습니다. 하지만 현실은 그렇게 간단하지 않죠.
알고리즘이 매력적으로 간단하지만, 항상 잘 작동하는 것은 아닙니다.
Unfortunately, this method won’t work if the point is on the edge of the polygon.

이런 경우는 임의의 점이 도형 안에 포함되어 있지만, 알고리즘에 의하면 짝수 번째의 선 위에 있어 밖에 있다고 잘못된 값을 반환하게 됩니다.
물론 현실에서 사용하는 도형들은 유한한 점으로 표현하기 때문에 충분히 예외를 처리할 수 있습니다.
포스팅을 간단하게 시작했지만, 내용이 산으로 가기도 하고, 기술과 멀어지고, 많이 길어지기도 했습니다. 
이런 글도 있어야 다양한 곳에서 인사이트를 얻을 수 있다는 핑계로 작성을 했습니다. 
개인적으로, 오래전부터 지도 API를 사용하면서 폴리곤 같은 도형 안에 포함 여부를 판단하는 로직에 궁금증이 있었습니다. 
우연한 기회에 잊고 있던 작은 호기심을 깊게 찾아볼 수 있던 좋은 경험이었습니다.
긴 글 읽어주셔서 감사합니다.
#문서를잘읽습니다 #영어공부필수 #일을이렇게열심히
참조문서
"
http://woowabros.github.io/woowabros/2018/03/25/newbie-education.html,2018-03-25,신입개발자 교육 후기,"작년 12월 말에 입사한 후 1월부터 9주 동안 진행된 신입 개발자 교육을 마쳤습니다!
교육이 어떤 식으로 진행되었는지, 무엇을 배웠는지 짧게나마 공유해드리고자 합니다.
크게 위 과정으로 진행되었는데, 모든 과정은 페어 프로그래밍(짝 프로그래밍)으로 진행되었습니다.
교육받기 전까지 혼자 결정하고 혼자 코딩하던 저에겐 적응하기 힘든 방식이었지만, 교육 후반쯤 되었을 때는 너무 적응을 한 나머지 혼자 결정하기가 무서워졌었죠..
그리고 교육은 대부분 실습으로 진행되었는데, Github으로 코드를 관리하며, 강사님들이 코드 리뷰를 통해 코드가 더 깔끔해질 수 있는 방법들을 말해주는 방식으로 진행되었습니다.
먼저 저는 백엔드로 지원을 했습니다. 그 전에도 js(Javascript)를 다뤄봤었지만, 
함수만 수십 개 만들어 구조란 찾아볼 수 없는 모습, 잘 돌아가는지 확인하기 위해 새로 고침만 수십번 누르다가 이내 질려버리고 말았었죠. (내가 못 짠 건 생각도 안 하고…)
하지만 교육을 받으며, js에도 클래스가 있고, 테스트 코드도 만들 수 있다는 걸 알았습니다. 그러면서 점점 구조가 잡히고, 새로 고침 대신 테스트코드를 돌리며 js의 맛을 조금 알게 되었습니다.
뿐만 아니라, Promise를 통한 비동기 처리, Event delegation을 통한 효과적인 이벤트 적용 방법, es5와 es6의 차이 등 백엔드를 공부하면서는 접하기 힘들었던 부분들을 알게 되었습니다.
이 시간 덕분에 프론트와 조금이나마 친해질 수 있었고, 프론트 개발하시는 분들을 이해할 수 있게 되었습니다.
백엔드 언어는 자바지기님이 지키는 Java로 진행되었습니다.
Java를 그래도 좀 안다고 생각했지만, 개발하면서 한 번도 들어본 적이 없는 Optional과 Stream을 만나고… 그건 어디까지나 제 착각이었다는 것을 뼈저리게 느꼈습니다…
또, 최대한 TDD를 따르려고 노력하며, HTTP, Spring 프레임워크, JPA 등을 배웠습니다. 테스트를 정말 열심히 만들었지만, 구조가 바뀌면서 망가지는 테스트들을 보며 정말 가슴 아팠습니다.
가장 기억에 남는 자바지기님의 한마디는 “비즈니스 로직은 상태 값을 가지는 객체가 하게 해라.” 라는 말인데, 평소 Spring Boot 기반으로 개발을 하면서 습관적으로 Service 계층에서 많은 비즈니스 로직을 처리하던 부분을 모델이 직접 처리하게 만들고 나서 보니 한층 코드가 깔끔해진 것을 느꼈습니다.
그리고 개발하면서 구글링을 하다 종종 뵀던 자바지기님이 이렇게 옆집 삼촌 같은 분인지 알게 되었던 귀중한 시간이었습니다.
팀 프로젝트를 하면서 배웠던 모든 내용을 바탕으로 Trello 와 비슷한 서비스를 만들었는데, 각자의 직군으로 나눠서 개발하지 않고, 백엔드와 프론트를 모두 같이 만들었습니다.
혼자 프로젝트를 진행할 때와는 달리 직접 구현하지 않은 기능들이 생겼는데, 어떻게 만들었는지는 모르지만, 동료들을 믿고 내가 맡은 기능에 충실해야 한다는 것을 깨달았습니다.
또, 다른 사람과 의견대립이나 합의가 필요할 때, 빠르고 더 좋은 결과를 내기 위해 투표도 해보고, 팀장을 뽑아서 결정권을 위임하기도 해보았습니다.
교육을 받다 보니 다른 사람과 같이 일하는 방법을 배우는 것이 목적처럼 느껴졌는데, 제 생각이 맞다면 이 교육은 꽤나 성공적이었다고 생각합니다. 매일 붙어서 같이 코드를 짜면서 내 의견을 설득도 해보고, 설득당할 때도 있다 보니, 지식도 지식이지만, 서로 존중하면서 대화하는 방법을 더 많이 배울 수 있었습니다.
조금 신기했던 부분은, 교육을 받으면서 수많은 커뮤니케이션 과정에서 한 번쯤은 감정이 격해질 만도 한데, 동기들끼리 한 번도 싸우거나 멀어진 적이 없었습니다. 다들 한 인성 하던 분들인 듯… (저 포함)
아무튼 아주 만족스럽고 좋은 교육이었습니다. 현업에서도 교육받은 내용을 잘 적용할 수 있도록 노력하겠습니다. 읽어주셔서 감사합니다!
ps. 리눅스의 신 honux님, IOS 마스터 JK 님도 감사합니다!
"
http://woowabros.github.io/security/2018/03/20/isms-certification.html,2018-03-20,Hybrid-Cloud 환경에서 보안인증 준비 하기,"지난 2월 오석윤님이 포스팅해주신  [AWS에서 네트워크 공격 자동차단하기]의 Summary에서 내용을 일부 가져오고자 합니다.
“Public Cloud 환경에서의 서비스 운영은 Legacy 환경의 IDC 서비스 운영과는 너무도 다르기 때문에 보안이 쉽지 않다.”.



특히나 국내 실정에 맞추어 만들어진 ISMS 인증 표준 또한 Cloud 서비스에 맞추기 위해 변화하고 있지만 아직은 IDC 환경의 서비스에서 준비하기가 조금 더 원활하도록 초점이 맞추어져 있다고 생각이 됩니다.
따라서 이번에 ISMS 인증심사를 준비하며 겪었던 몇 가지 이슈사항들과 어떻게 해결하였는지 그 과정을 정리하고자 합니다.
하지만 공개되어 있는 공간이므로 실제 심사 결과의 취약점은 다루지 않고 심사를 준비하며 겪었던 내용을 바탕으로 포스팅해보겠습니다.
솔직히 기술적인 부분이 많이 들어가지 않은 글을 기술 블로그에 쓰려고 하니 걱정이 앞섭니다.
[기술이 없는 기술블로그를 작성해보자]
우선 보안인증에서 첫걸음으로 가장 중요한 부분은 인증 범위의 선정이라고 볼 수 있습니다. ISMS 인증의 경우 관리체계 수립이 자산을 기준으로 진행됩니다. “배달의민족 서비스”에 대한 ISMS 인증을 받고자 한다면 인증 범위에는 배달의민족 서비스를 위한 자산(서버, DB, 네트워크장비, 개발자의 PC 등)들이 모두 포함되게 됩니다. 그리고 인증 범위에 포함된 자산들이 AWS와 IDC에 나누어져 운영되고 있다면 AWS와 IDC 모두가 인증 범위에 포함되어 인증 대상이 되어야 합니다.
사무실도 예외로 할 수 없습니다. 개발을 하고 기획을 하는 모든 업무가 배달의민족 서비스에 반영되고, 개발자와 기획자가 업무를 하며 이용하는 백오피스 시스템도 배달의민족 서비스의 한 부분이니까요.
따라서 사무실 보안에 대해서도 보안대책을 세워야 합니다.
[CCTV같은 물리적 보안도 포함해서 말이죠]
여기까지 인증 범위 선정하기가 정리되었습니다. 하지만 문제는 여기서부터 발생합니다. 일반적으로 많이 사용하는 IDC와 Cloud 서비스를 제공하는 AWS는 다른 부분이 너무 많았습니다. 물리적으로 존재하는 방화벽이 아닌 SecurityGroup… 서버가 하루가 다르게 생겼다 없어졌다 하는 EC2… 등등 AWS의 이런 편리한 기능들은 서비스와 관리를 하기에는 굉장히 유용하지만 자산을 바탕으로 정보보호 관리체계를 수립해야 하는 ISMS 인증 준비단계에서 큰 고민거리가 되었습니다.
회사에서 정보보호관리체계를 세우기 위해서는 우선 보호하고자 하는 자산의 식별과 식별된 자산의 위험평가 그리고 위험평가를 위한 취약점 진단이 진행되어야 합니다.
하지만 Cloud 서비스에서는 자산의 식별부터 막히게 됩니다. Auto-Scaling 등과 같은 Cloud 서비스의 특성으로 인해 식별되어야 하는 자산이 하루가 다르게 생겼다 없어졌다를 반복해버립니다.
또한 식별이 되었다 한들.. 이렇게 변화무쌍한 서버들에 대해서 취약점 진단과 위험평가는 어떻게 진행해야 할까요. 이 부분이 중요한 이유 중 하나는 ISMS 인증심사 진행 시 심사비용 수수료 결정에 가장 큰 영향을 미칩니다. 자산 수에 따라 심사일수를 정하게 되고, 정해진 심사일수에 따른 수수료가 결정이 되죠.




두 번째로는 개인 정보를 전송할 때에는 암호화 통신이 되어야 합니다. 하지만 서비스를 AWS와 IDC에서 함께 사용하는 환경인 경우, 정적인 통신 대상과 동적인 통신 대상이 있고 시스템의 환경 설정을 할 수 없도록 되어있는 Public Cloud의 관리형 서비스 환경 등… 이렇게 제한되어 있는 상황에서 어떤 보호대책을 수립해야 효율적인 서비스를 유지하면서 컴플라이언스를 만족할 수 있을까요.
[서울에도 Region이 있다. 이건 비밀인데 어디에 있냐면…]
세 번째로 AWS은 사용되는 IP 주소 목록이 Public 하게 공개되어 있습니다. 그에 따른 보안 위협은 끊임없이 들어오는 단순 스캔 공격만 봐도 알 수 있습니다. 보안 위협을 방어하기 위한 IPS, 웹방화벽 등과 같은 보안장비의 적용이 필요합니다. 하지만 일반적으로 AWS에서는 IDC 환경처럼 물리적 보안장비를 직접 구축하여 운영할 수가 없습니다. Region이 어디 있는지 모르고, 어디 있는지 알더라도 들어갈 수 없으니까요. 공격자가 전산센터의 위치가 어디 있는지 모르고 들어갈 수 없지만 사용자도 같이 모르고 들어갈 수가 없으니 안전하다니… 대체 어떻게 해야 할까요.
[심심하면 들어오는 SSH BruteForce 공격]
AWS의 급변하는 인스턴스에 맞춰 자산을 식별하려면 자산 식별의 기준이 필요했습니다.
첫 번째로 자산 목록을 결정할 특정 기준일을 정했습니다. 해당 특정 기준일을 시점으로 최근 한 달간 변화를 보고 최소로 줄어든 인스턴스 개수를 자산목록에 기입하여 식별하였습니다. 예를 들어 하나의 Baemin-server라는 호스트네임을 가진 인스턴스가 Auto-Scaling으로 인해 한 달간 최대로 늘어난 수가 Baemin-server-1~20이고, 최소가 Baemin-server-1~4로 나타났다면 자산 식별 수는 4개로 결정하고, 식별된 자산 목록에는 Baemin-server-1,2,3,4로 4개의 server가 들어갑니다.
다음 식별된 자산을 기준으로 취약점 진단을 진행해야 하는데, 동일한 설정을 가진 시스템이라는 전제하에 1대씩 샘플링으로 진행을 합니다. 일반적으로 취약점 진단은 체크리스트를 기반으로 스크립트를 통한 수동 점검을 진행합니다.
하지만 AWS에는 애플리케이션의 취약성 또는 모범 사례와 비교한 차이점을 자동으로 평가하는 Amazon Inspector(이하 Inspector)라는 취약점 자동 점검 툴이 있습니다.
운영 중인 서비스에 직접 툴을 적용할 경우 리소스 과부하가 발생하지 않으리라는 보장이 없기 때문에 운영 인스턴스들을 복사하여 별도의 VPC를 생성한 뒤 툴을 적용한 후 취약점 점검을 진행하였습니다.
하지만 Inspector를 통한 점검 결과로 위험평가를 진행하기에는 기존의 위험평가 Concerns List와는 다른 점이 많았습니다.
그래서 기존의 항목을 기반으로 Inspector를 이용하여 점검할 수 있는 항목을 Concerns List에 녹여냈습니다.
[서버 취약점 점검 평가 항목]
[위험평가 Concern List 일부인데 잘 안보이는게 맞습니다. 가렸으니까요.]
그 다음은 당연히 발견된 취약점에 대해 위험조치계획을 수립하고 위험관리를 수행하면 됩니다.
실제 우아한형제들의 시스템 간 네트워크 구성이 어떻게 되어있는지는 언급하지 못하므로 검토했던 내용 중 2가지에 대해 간략하게 기술해보겠습니다. 우선 AWS-IDC 간 개인정보가 평문으로 전송되지 않도록 하는 것이 핵심이었습니다. 개인정보의 암호화 통신이 키워드이니까요.
첫 번째로 AWS에서 제공하는 Direct Connect 서비스가 있습니다. 용어 그대로 IDC환경의 온프레미스 IT 자원과 AWS 클라우드 자원을 전용 회선으로 연결하는 서비스입니다. 전용선을 구축할 경우 암호화통신으로 볼 수 있기 때문에 개인정보 암호화통신 이슈를 해결할 수 있습니다.
두 번째로 IDC의 서비스와 분리하여 배달의민족 서비스가 AWS 내부에서만 통신 가능하도록 구성하는 방법이 있습니다. 완전한 Cloud 서비스 환경으로 구성하는 아주아주 간단한(?) 방법입니다. IDC가 아닌 Cloud 내부에서만 통신을 하게 된다면 이슈가 줄어들게 됩니다.
여러 가지 방법들 중 회사의 환경과 상황에 맞게 구축 진행하면 해결할 수 있는 문제였습니다.
[이러지 않았으면 좋겠습니다.]
현재도 그렇고 앞으로도 가장 고민이 많은 부분입니다. 보안장비를 구축함에 있어서 석윤님이 포스팅해주신 “자동화”가 핵심이라고 생각합니다.
공개되어 있는 공간인 만큼 특정 제품을 언급하지는 못하지만, Public Cloud 환경의 특성상 선택폭이 상대적으로 크지가 않습니다.
하지만 현재 AWS에 적용이 가능한 보안 제품들이 많이 나오고 있으며, appliance 형태의 보안장비가 아닌 SaaS 형태의 보안장비는 AWS에 인스턴스를 생성하여 구축, 운영할 수 있습니다.
현재 회사에서 꼭 필요한 것이 무엇인지, 또 잘 활용할 수 있는 보안장비가 무엇인지 파악하는 것이 중요하다고 생각합니다.
[이번 고비가 지나면 다음 고비가 온다. 정말로. 꼭 옵니다.]
현재 많은 기업들이 비용 면에서 효율적인 Public Cloud 서비스를 이용하고 있습니다. 하지만 아직 국내 컴플라이언스가 Public Cloud 환경과 맞지 않는 부분이 많은 것으로 보입니다.
(우아한형제들의 공식적인 입장이 아닙니다. 우아한형제들은 국내 보안규정을 준수하고 있습니다.)
컴플라이언스를 충족하면서 정보보호관리체계를 구축하고 운영하려면 아직 많은 고민거리들이 존재합니다.
등등… 많은 고민들이 생겨나고 해결되고 또 다시 생겨나게 될 겁니다.




물론 위에서 언급했던 방안들이 모두 정답이라고는 할 수 없습니다.
하지만 급변하는 IT 환경 속에서도 우리는 답을 찾을 것입니다. 언제나 늘 그랬듯이
"
http://woowabros.github.io/swift/2018/03/18/swift-debugging-with-SIL.html,2018-03-18,SIL(Swift Intermediate Language)을 통한 Swift debugging,"몇일 전 그렇게 멀지않은 방이동, 경복궁 건물 6층 배민찬 개발팀에서,
제너릭을 가지는 구조체에 반환값이 있는 readonly 프로퍼티를 선언한 스위프트 개발자 A는 구조체를 확장하며 지난 선언을 잊고 프로퍼티를 중복해서 선언한 후, Xcode에 숨겨진 무서운 비밀을 만나는데…
Xcode 사용자 분들은  An internal error occured. Source editor functionality is limited. Attempting to restore...라는 에러와 함께 코드 autocompletion동작이 동작하지 않거나 코드 하일라이트가 사라지는 등의 문제를 여러번 겪으셨을 겁니다. 이전에는 단순히 Xcode의 버그라고 생각해, 재부팅이나 clean등으로 해당 문제를 해결하려고 했습니다. 하지만 항상 Xcode의 버그만 이런 문제를 발생시키는건 아닌것 같습니다. Xcode는 소스들의 인덱싱 작업을 하며 swift빌드 단계의 특정(AST->SIL) 단계를 이용하여 코드 하일라이트나 autocompletion을 수행하고 있다는 추측을 하고 있습니다. 이 글을 쓰게 된 계기는 바로 SIL코드 생성 단계가 실패하여 발생한 문제였습니다. 이를 파악하는 도중 알게된 SIL에 대한 지식과 사용가능한 몇 가지 툴을 공유하고자 합니다.
코드를 작성 중 어떤 특정 변수를 실수로 중복해 선언한 상태로 중복 선언이라는 에러 메시지 없이 xcode의 데몬이 멈추고 build 시 sil 생성중 에러를 반환하는 경우를 만나게 되었습니다.
이전에 비슷한 경우를 만났던 경우 의심되는 코드를 완전히 지우고 다시 설계하고 작성해서 넘어갔었는데, 몇 번 비슷한 상황을 겪은 터라 이번에는 확실히 알고 넘어가기로 결심했습니다. 문제를 더 간단한 코드로 재현하여, 명확하게 문제를 파악해서, 제대로된 에러를 반환하고 그럼으로써 문제의 원인을 파악하여 해결책을 찾아보기로 결심했습니다.
구조는 좀 더 복잡했지만, 재현과 분석을 원활하게 하기 위해 원 코드를 단순화한 코드로 바꿔 진행해보겠습니다.
여기 A와 B, 두 구조체가 있습니다.
struct A
struct B
A와 B는 각각 Element라는 제너릭을 선언하고 있습니다. 이것은 어떤 값이라도 선언할 수 있도록 아무런 constraint를 선언하고 있지 않습니다.
A는 b 함수를 선언하고 반환 값은 B<Element>입니다. A의 Element와 b 함수가 반환하는 B 클래스의 Element는 동일합니다. A.Element == B.Element입니다.
그런데 b함수가 인자를 받지 않는 것이 눈에 띕니다. 이것이 거슬렸던 저는 함수를 프로퍼티로 바꾸기로 결심합니다.
그러면서 한 가지 실수를 저지르게 되었죠.
위의 코드는 제가 범했던 실수를 명확하게 드러내주고 있습니다.
A라는 구조체를 확장하고 b함수를 프로퍼티로 변경하는 과정에서 b는 readonly 프로퍼티이니 A의 확장에서 b를 선언하는게 좋겠다고 판단하고는 확장에서 선언을 한 후에 A 구조체 선언부에서 b 프로퍼티 부분을 제거하는 것을 잊어버렸습니다. 그리고 그 순간부터 Xcode는 지속적으로 에러와 복구를 반복해서 수행했고, 저는 Xcode가 또 땡깡을 부리는구나 판단하고는, clean, 재부팅등을 시도했습니다. 변한것은 없었고, 뭔가 문제가 있으니 빌드를 해볼까 싶어 빌드를 시도했습니다. 그런데 에러가 등장하기는 했습니다만, 소스 코드상에 특정 부분을 지적하는 에러가 아닌, 개발할 때 제일 꼴보기 싫은 에러인 Segmentation fault: 11에러를 아래와 같이 만나게 되었습니다.
swiftc ./test.swift
0  swift                    0x000000010ca7236a PrintStackTraceSignalHandler(void*) + 42
1  swift                    0x000000010ca717a6 SignalHandler(int) + 662
2  libsystem_platform.dylib 0x00007fff50a80f5a _sigtramp + 26
3  libsystem_platform.dylib 0x0000000000000008 _sigtramp + 2941776072
4  swift                    0x0000000109cc4c9b swift::Lowering::SILGenFunction::SILGenFunction(swift::Lowering::SILGenModule&, swift::SILFunction&) + 203
5  swift                    0x0000000109c36d45 swift::Lowering::SILGenModule::emitFunction(swift::FuncDecl)::$_1::operator()(swift::SILFunction) const + 261
6  swift                    0x0000000109c362c9 swift::Lowering::SILGenModule::emitFunction(swift::FuncDecl*) + 761
7  swift                    0x0000000109d1f176 swift::Lowering::SILGenModule::visitExtensionDecl(swift::ExtensionDecl*) + 422
8  swift                    0x0000000109c3c91b swift::Lowering::SILGenModule::emitSourceFile(swift::SourceFile*, unsigned int) + 1115
9  swift                    0x0000000109c3e2a9 swift::SILModule::constructSIL(swift::ModuleDecl, swift::SILOptions&, swift::FileUnit, llvm::Optional, bool) + 841
10 swift                    0x00000001093ced06 performCompile(swift::CompilerInstance&, swift::CompilerInvocation&, llvm::ArrayRef<char const>, int&, swift::FrontendObserver, swift::UnifiedStatsReporter*) + 12966
11 swift                    0x00000001093ca1f4 swift::performFrontend(llvm::ArrayRef<char const>, char const, void, swift::FrontendObserver) + 7716
12 swift                    0x000000010937ee78 main + 12248
13 libdyld.dylib            0x00007fff507ff115 start + 1
14 libdyld.dylib            0x000000000000000f start + 2944405243
Stack dump:
보통 이런 에러를 만나면 좌절했겠지만, 요즘 스위프트의 빌드 과정에 대해 공부하고 있던 차 stack dump의 1. While emitting SIL for getter for b at ./test.swift:13:6이 눈에 들어왔습니다. 이건 스위프트 빌드 단계 중 두 번째인  Swift Intermediate Language를 생성하는 단계에서 test.swift 파일의 13번째 라인 6번째 컬럼에 있는 b를 위한 getter SIL을 생성하는 도중 에러가 발생했다는 의미입니다.
여기서 잠깐 SIL? AST? 이런 용어를 아마 처음 들으시는 분들도 계시리라 생각합니다. 이것은 스위프트의 빌드 단계를 지칭하는 이름이기도 하고 그 단계에서 생성하는 코드의 형태를 지칭하는 단어이기도 합니다. 본격적인 분석에 들어가기 전에 스위프트의 빌드는 어떻게 이루어지는지 살펴보겠습니다.
####1. LLVM
여기서는 주제와 좀 떨어져 있으니 LLVM의 빌드 구조만 간략하게 소개합니다. 상세한 내용은 llvm.org의 내용을 참조해주세요.
Frontend->LLVM Optimizer->LLVM Backend
####2. 스위프트의 빌드 단계
스위프트의 코드 작성과 실행 가능한 바이너리가 만들어지기 까지는 아래와 같이 몇 단계를 거치게 됩니다.
Swift Code -> Swift AST -> Raw Swift IL -> Canonical Swift IL -> LLVM IR -> Assembly -> Executable
참고: swift/SIL.rst
다음 챕터부터 SIL의 코드를 보게 될텐데 그 전에 대강 SIL이 이렇구나 하는 정도로 훑어보겠습니다.
다음의 간단한 스위프트 파일을 SIL로 빌드해보겠습니다.
스위프트 파일에서 SIL 파일을 생성하기 위해서는 스위프트 컴파일러인 swiftc에 -emit-silgen 혹은 -emit-sil 옵션으로 컴파일합니다. 이 옵션들은 AST로부터 SIL을 생성하게 되는데, -emit-silgen은 raw SIL을 -emit-sil은 canonical SIL을 생성하게 됩니다. swift-demangle툴은 생성한 SIL의 알아보기 힘든 이름들을 정리해서 읽기 쉽게 변경해 줍니다.
Person의 n변수의 getter부분을 살펴보겠습니다.
$0등의 숫자와 주석의. user: %.., id: %..등이 눈에 띕니다. 이것은 SIL에서 사용하는 레지스터를 표현하는 것이기도 하고 해당 레지스터나 선언이 어떤 레지스터와 의존관계를 이루고 있는지 알 수 있게 해주는 레이블로서의 표현이기도 합니다. 아래에서 이 소스 코드에서 사용하고 있는 SIL의 키워드에 대해 설명해보겠습니다.
hidden - Person 구조체를 default로 선언했기 때문에 n 프로퍼티는 같은 모듈에서만 사용할 수 있다는 뜻입니다. 스위프트의 코드 스코프를 알려주는 키워드입니다.
[transparent] - inline 함수라는 의미입니다.
@test.Person.n.getter : Swift.String : $@convention(method)  (@guaranteed Person) -> @owned String
// %0                                             // users: %2, %1
bb0(%0 : $Person):
debug_value %0 : $Person, let, name ""self"", argno 1 // id: %1
%2 = struct_extract %0 : $Person, #Person.n // users: %3, %6
%3 = struct_extract %2 : $String, #String._core // user: %4
%4 = struct_extract %3 : $_StringCore, #_StringCore._owner // user: %5
retain_value %4 : $Optional          // id: %5
return %2 : $String                             // id: %6
} // end sil function ‘test.Person.n.getter : Swift.String’
​
대충 보면 도저히 읽을 수 없을 것 같은 코드들을 SIL.rst를 참조하며 읽어보았습니다.
이렇게 SIL의 극히 일부분을 살펴봤습니다만, 문서를 참조하며 한땀한땀 차분히 읽다보면, 다른 부분을 읽을 때도 의미를 잘 파악하실 수 있으리라 생각합니다.
자 드디어 다음 단락부터 문제의 분석에 들어갑니다.
우선 분석에 앞서 더 간단한 코드를 통해 invalidate redeclaration 에러를 발생시킬 수 있을지 알아보겠습니다.
A의 b를 프로퍼티가 아닌 초반에 만들었던 함수로 만들어 확장 시 해당 함수를 재선언 해보겠습니다.
$ swiftc ./test.swift
제대로 에러를 발생합니다.
흠, 혹시 제너릭 선언과 프로퍼티 선언 간의 복합적인 문제일까요? 그래서 몇 가지 조건을 실험해보기로 했습니다.
실험 1 A와 B 둘 다 제너릭이 없고 b가 프로퍼티인 경우
실험 2 A와 B 둘 다 제너릭이 없고 b가 함수인 경우
실험 3 A만 제너릭을 선언하고 b가 프로퍼티인 경우
실험 4 A만 제너릭을 선언하고 b가 함수인 경우
실험 5 B만 제너릭을 선언하고 b가 프로퍼티인 경우
실험 6 B만 제너릭을 선언하고 b가 함수인 경우
실험 7 A와 B 둘 다 제너릭이 있고 b가 프로퍼티인 경우
실험 8 A와 B 둘 다 제너릭이 있고, 제너릭의 타입이 동일하며  b가 함수인 경우
실험 9 A와 B 둘 다 제너릭이 있고, 제너릭의 타입이 다르며 b가 함수인 경우
A.Element != B.Element
실험 10 A와 B 둘 다 제너릭이거나 A만 제너릭이며, 확장 시 where 절을 통해 Element의 타입을 명시할 경우
결과 빌드가 정상적으로 실행
결과를 보면 sil단계에서 에러를 발생하는 경우는 다음과 같습니다.
(* 이런 경우를 복잡한 코드에서 만나게 되는 경우 정말 손도 발도 쓰기 힘들다는 게 가장 문제입니다.)
결국 b가 어떤 타입이냐는 중요하지 않습니다. A가 제너릭을 가질 것, 제너릭이 Any타입(암묵적이든 명시적이든) 일 것, 확장 시 이미 선언한 프로퍼티를 중복해서 선언할 것으로 문제를 발생시키는 경우를 좁힐 수 있습니다.
자 swift의 빌드 구조가 과연 어떻게 되기에 이런 문제가 발생하는지 한 번 알아보겠습니다.
먼저 빌드를 성공시킬 수 있는 정도로 코드를 단순화해서 sil 코드를 만들어 보겠습니다. class는 현재 단계에서 볼 필요가 없는 코드를 많이 생성하기 때문에 좀 더 간단한 결과를 볼 수 있는 struct로 바꿨습니다.
$ swiftc -emit-silgen test.swift > test.silgen
우리가 class, struct 등으로 묶어서 그리고 있는 구조가 sil에서는 해체되고 각 선언들만이 남게 됩니다. 이것을 mangled format이라고 합니다. 다만 네이밍에 AVACyx…등의 알아보기 힘든 형태로 되어있어 툴을 사용해 좀 더 알아보기 쉽게 바꿔보겠습니다.
이름들이 이제 알아보기 쉬운 형태로 변환되었습니다. 주석으로 각 선언이 어떤 역할을 수행하는지도 알아보기 쉽게 되어 있습니다.
이번에는 다음과 같이 확장의 Element타입을 명시적으로 Int로 선언해보겠습니다.
여기까지는 sil 코드가 이전과 별 차이는 없습니다.
그렇다면 A.Element == Int 일 경우에만 A의 확장 내용을 사용할 수 있도록 where절로 Element 제너릭을 Int형으로 명시적으로 선언하고 value1 프로퍼티를 추가해보겠습니다.
이번에는 SIL 코드에 value1의 getter 선언이 추가되었습니다.
code 1
where constraint를 빼면 어떨까요?
code 2
code 1에서는 value1.getter가 A의 확장 함수임을 명시하고 있지만, code 2의 경우에는 해당 부분이 빠져있습니다.
문제가 있던 코드 상태로 다시 돌아가 보겠습니다.
여기서 sil코드를 출력해서 보고싶지만, 당연하게도 에러가 발생하는 상황으로 인해 더 방법이 없습니다. 그렇다면, 좀 더 자세하게 sil 코드가 생성되는 과정을 디버깅할 수 있는 옵션을 추가해서 출력을 해 보겠습니다.
debug constrainted output
사실 여기서 뭘 자세히 조사해야 문제를 알 수 있을지 잘 모르겠습니다. 일단 해본 것으로 만족하고 다른 방법을 모색해보겠습니다.
그렇다면, 아래와 같이 A.s: String readonly 프로퍼티를 선언하고 A where Element == Int와 같이 확장을 하며 동일한 프로퍼티를 선언하면 어떨까요?
에서 생성한 SIL 코드에서 var s 끼리 비교해보면 아래와 같습니다.
where Element == Int
where이 있는 경우 메소드 선언부에서 가 빠지며, `@(extension in test)`가 이 getter는 test에서 A의 확장 함수라는 것을 알려주는 것으로 보입니다. 이 경우 struct A의 s와는 구현은 같지만, 완전히 다른 함수로 선언됨을 알 수 있습니다. 이런 연유로 중복선언처럼 보이지만, 문제 없이 빌드가 됩니다.
여기까지 와서 실험할 때 제가 빠트린 경우의 수가 생각이 나서 두 가지를 추가해봤습니다.
여기까지는 결국 Element의 타입이 어디에도 명시되지 않았기 때문에, 문제가 발생했다고 볼 수 있습니다. 그런데 이건 처음의 구조와는 좀 다르기 때문에, 새로 알게 된 옵션을 이용하여 아래와 같이 코드를 다시 구성해 결과를 살펴봤습니다.
이 경우 아래와 같이 에러를 표시합니다.
A 확장에서 var b: B 변수의 getter에 대한 SIL코드를 생성하는 도중 에러가 났다는 사실은 확실합니다.
중반부에서 A를 확장하며 Element타입을 명시하면 SIL 코드 생성에서 에러가 발생하지 않는다는 것을 실험에서 알 수가 있었습니다. 이제 같은 상황에 Element타입을 Int로 명시해보겠습니다. 그리고 extension A.b의 getter를 생성하는 부분을 살펴보겠습니다.
// A.b.getter에서 제너릭이 A로 선언되어있는데 이 부분은 스위프트코드에서 무슨 이름으로 선언하던지 제너릭 선언 순서대로 A, B, …로 바뀌게 됩니다.
제너릭이 두 개인 경우
A<Element, Value>로 선언해도 SIL에서는
// A<A, B>.b.getter 로 바뀌게 됩니다.
getter구문이 생성된 것을 보면 , 이 코드를 SIL코드로 변환하는 부분에서 문제가 생긴다는 추정이 가능합니다.
자 여기까지 오느라 힘드셨죠. 고지가 얼마 안남았습니다(물론 저는 남이 이런말을 하면 믿지 않습니다). 그래도 우리 조금만 더 힘내서 하나만 더 테스트해보겠습니다.
이번에는 protocol P를 만들고 변수 b에 대한 선언을 명시해서 A가 P를 상속하도록 해보겠습니다. 코드는 sil terminated 되는 그 코드입니다.
$ swiftc test.swift
어라 좀 다르네요? 첫 번째는 그렇다 치고 두 번째에 타입 B인 b 변수가 하나 이상 있다는 에러를 볼 수 있습니다. 여기까지 SIL문법을 제대로 모르지만, 제 작은 회색 뇌세포를 총동원해서 추론해보면, 구조에 대한 선언 없이 확장을 통해 A 타입에 어떤 프로퍼티를 추가하는 경우, 그리고 A의 제너릭 타입을 명시하지 않는 경우 SIL이 getter 구문을 생성할 때 참고할 만한 어떤 자료가 전혀 없기 때문에, sil 생성시 terminated가 발생하게 된다고 생각할 수 있습니다. 그리고 protocol로 먼저 구조를 만들게 되면 SIL 구문 체크시 참고할 데이터가 있기 때문에, 좀 더 명확한 에러를 반환해주는 것 같습니다. 혹은 확장시 A의 제너릭 타입(Swift에서 제공하거나 혹은 개발자 명확하게 선언한 어떤 타입인 경우)을 명시해 주는 경우에는 참조 가능한 구조가 생성되어 별도의 SIL함수 제공되니 빌드가 정상적으로 이루어진다고 추론해보겠습니다. 하지만 추론은 추론이고 더구나 근거가 매우 미약합니다.
저는  명확한 답을 얻어보고자 스위프트 사용자 포럼에 문의를 해놓은 상태입니다. 좋은 답이 오게되면 이 글의 해당 부분을 개선할 수 있을거 같습니다. 
  추가 답이 달렸습니다. 스위프트의 버그가 틀림없으니 버그 리포트를 하라는군요. ㅎㅎ 신나게 버그 리포트를 달고오겠습니다.
지금까지 SIL코드와 툴, 여러가지 변수를 통해 문제를 파악해보려 했습니다.
결국은 실수에서 벌어진 일입니다. 스위프트는 Protocol Oriented Programming 추구한다는 것이 힌트가 될 수 있겠죠.
객체 혹은 타입을 작성하기 전에 protocol로 구조를 먼저 만들고 해당 protocol을 상속하는  객체 혹은, 타입을 만듭시다. 이것은 여러 장점이 있겠지만, 이 글에서 발생한 문제에 대해서도 에러가 명확해진다는 큰 장점이 있습니다. Xcode도 뭔지 모를 에러를 만들지 않겠지요.
초반에 스위프트의 확장을 사용할 때, 저는 단순하게 상속의 오버라이딩과 비슷할 것이라 단정하고 코드를 구현하다 버그를 만들어낸 적이 있습니다. 확장은 객체의 상속과는 사실상 아니 당연하지만 매우 다릅니다.
예를 들어 아래와 같은 코드를 보겠습니다. 이 코드에서 저는 A를 확장하며 A.Element == Int인 상황에서는 프로퍼티 s1이 “extension A” 문자열을 반환하리라 기대했습니다.
code a
하지만 기대를 배신하고
A1
A2
를 출력합니다.
A를 확장하며 또 A.Element == Int인 경우 “extension A”를 반환하도록 했음에도 A2를 출력합니다.
이번에는 코드를 한 줄 추가해보겠습니다.
code b
A1
extension A
의 출력 결과를 볼 수 있습니다.
이런 경우를 스위프트 코드만으로는 애매모호한 부분이 있지만, SIL코드를 살펴보게 되면 이유가 명확해집니다. 여기에서는 swift-demangle을 쓰게 되면 실제 좌표가 사라지기 때문에, 이번에는 좀 읽기는 어렵겠지만, 가리키는 곳이 명확하게 드러나게 SIL코드를 생성하겠습니다.
$ swiftc -emit-gen test.swift
sil a
위의 구조를 찬찬히 보면 확장한 A에서 반환하는 s는 _T04test1AVAASiRszlE1sSSfg가 됩니다만 print부분에서는 사용하고 있지 않습니다. _T04test1AV2s1SSfg주소는 원 선언의 s의 getter 주소입니다.
sil b**
이번에는 명확하게 a2를 print 할 때, A.s1.getter 선언부인 _T04test1AVAASiRszlE2s1SSfg를 가져오고 있습니다.
SIL코드를 보지 않았다면, 이런 문제는 머리속에 계속 물음표로만 남을 수 밖에 없었을 것입니다.
이렇듯 스위프트를 다루며 명확하지 않은 에러나 현상을 만날 때, SIL에 어느 정도 익숙해져 있다면, 상당한 도움이 될 수 있습니다.
WWDC가면 이런 거 잘 다루는 애플 개발자를 만날 수 있다는 전설이 있습니다. WWDC 2018 가고싶다.
여기까지 길고 재미없는 글을 읽어주신 분들에게 감사를 표하며 행복과 행운이 있기를 바랍니다. 앞에서도 이야기했지만, 지적과 지식은 언제나 환영합니다.
참조
그리고, 가지가지 오타와 문법 오류, 교정을 봐주신 종립님 고맙습니다.
"
http://woowabros.github.io/swift/2018/03/18/translation-SIL-for-the-moment-before-entry.html,2018-03-18,"[번역] SIL(Swift Intermediate Language), 일단 시작해보기까지","원문 Swift中間言語の、ひとまず入り口手前まで
이 글은 훌륭한 Swift, iOS 개발자이자 교육자인 Tomohiro Kumagai씨가 qiita에 올리신 글을 허가를 얻어 배포합니다.
*(주) 내용은 최대한 정확하게 번역하려 했지만, 문장은 초벌 번역에서 겨우 벗어난 수준입니다. 시간 날 때마다 개선할 예정입니다.
올해(2016년 글임)의 WWDC를 방문했을 때, Swift Lab에서 Sean Callanan씨가 Swift의 작은 동작을 탐색하는 방법으로 SIL을 활용하고 있는 모습을 보여주었습니다. 뭔가 있을 때 저도 그 흉내를 내어 SIL을 살펴볼 기회가 늘었습니다.
SIL은 Swift Intermediate Language의 약칭으로, Swift 중간 언어입니다.
그때부터 Swift에서 불가사의한 동작을 맞닥뜨렸을 때마다 SIL을 보고 있는데, 코드를 읽을 수 있다는 것만으로도 큰 도움이 되고 있습니다. 하지만 읽지 못하는 부분도 많아서, 좀 더 잘 읽게 되면 SIL을 통해 여러 가지를 알게 될 수 있을 것 같습니다.
그런 생각으로 이번 기사를 쓰게 되었습니다.
알듯 말듯한 SIL에 대해, 어느 정도 읽을 수 있는 정도를 목표로 SIL에 대해 조사하기로 했습니다. SIL의 구문과 명령 체계 분석까지 가능했다면 좋았겠지만, 생각보다 깊은 세계였고 저의 기초가 부족한 데다가 시간도 부족하여 깊이 들어가지는 못했습니다.
어중간한 내용 같습니다만, SIL에 흥미를 불러일으키는 계기가 되기를 바랍니다.
SIL이란 Swift Intermediate Language의 약자로, Swift의 소스 코드를 실행 가능한 바이너리 코드로 바꿀 때, 다른 컴파일러(LLVM)가 취급하기 쉬운 중간표현(Intermediate Representation)으로 변환한 코드를 말합니다. (* LLVM IR과는 다릅니다.)
Swift의 컴파일러는 LLVM을 통해서 바이너리 코드를 생성하는데, 바이너리 코드를 생성하는 과정에서 LLVM을 위한 중간표현인 LLVM IR을 생성하게 되고, SIL은 Swift 코드와 LLVM IR과의 중간에 위치합니다.
SIL의 목적은 프로그래머가 입력한 Swift 소스 코드와 LLVM IR과의 표현의 차이를 메꾸는 것입니다. 그리고 LLVM IR이 다루기 힘든 Swift 소스 코드 레벨에서의 정적 분석도 범위에 들어갑니다.
SIL은 정적 단일 배정 방식(Static single assignment form)에 가까운 언어인듯합니다.
상수만을 허용하는 방식으로 보입니다. 그렇다면, Swift의  var를 사용했을 때 let으로 표현이 바뀌는 걸까? 라고 추측했지만, 스위프트와는 별개로 SIL 언어가 그렇게 설계되어 있다고 합니다.
SIL이 Swift 소스 코드를 바이너리 코드로 빌드하는 과정 중에 어느 단계인지 알아 두면 파악하기 쉬울 것 같아, Swift 컴파일러의 처리의 흐름에 대해 조사해 보기로 했습니다.
Swift 소스 코드로 바이너리 코드를 생성할 때, 컴파일러는 다음의 과정을 따라갑니다.
구문분석(AST) → 의미분석 → 모듈 임포트 → SIL 생성 → SIL 정규화 → SIL 최적화 → LLVM IR 생성 → …
Swift 소스 코드를 파서로 처리해서, 타입 정보가 없는 추상 구문 트리(Abstract Syntax Tree, AST)를 생성합니다. 소스 코드의 문법(예약어, 타입 체크 등등이 아닌 순수한 구문 분석입니다. 예를 들면 명사 목적어 동사의 순서가 맞는지 같은.)에 맞지 않는 부분이 배제됩니다.
덧붙여 파서는 재귀 하향 파서(Recursive Descent Parser)으로 구현되어 있고, 그 코드는  lib/Parse에 있습니다.
구문분석으로 생성한 AST는 의미 분석기를 통과하며, 타입 추론 등을 수행해서, 타입 정보를 포함한 완전한 형태의 AST로 변환됩니다. 이 단계에서, 소스 코드에서 의미상 맞지 않는 부분이 배제됩니다.
추가로 이 규칙은 ocs/TypeChecker.rst에 언급되어 있으며, 코드는 lib/Sema에 있습니다.
AST가 완성되면, 다음으로 Clang Importer에 의해 Clang module이 포함되어, 여기에서 Objective-C나 C api가 Swift의 규칙에 맞는 형태로 사용할 수 있습니다.
다만, 앞의 의미분석의 단계에서, 외부 모듈에 규정되어 있는 메소드 등을 토대로 해서 타입 체크를 하는 것으로 보입니다. 그러니 이 단계에서 처음으로 모듈이 관여하는 것도 아닙니다. 이 부분은 잘 이해하지 못하고 있습니다.
이 코드는 lib/ClangImporter에 있습니다.
여기부터, 이 글의 테마인 SIL을 생성하는 최초의 단계가 될 것 같습니다. SIL 생성기에 의해 AST로부터 Raw SIL이 생성됩니다. SIL에서는 Swift의 var 변수는, 엄격한 정적 단일 배정 방식이 아닌, 변경이 가능한 메모리 영역으로서 표현되는 것 같습니다.
덧붙여 구현은 lib/SILGen에 있는 것 같습니다.
SIL을 작성했다고 완료된 것은 아니고, 계속해서 정당성의 검증을 행하고, 마지막으로 Canonical SIL이 생성되는 것 같습니다. 이에 의해, 말하자면 “변수에 값을 넣지 않은 채 사용하려고 한다”와 같은 작은 실수를 보정해서, Swift 코드로서 엄격함이 보증되는 것 같습니다.
덧붙여 구현은 lib/SILOptimizer/Mandatory에서 수행하는 것 같습니다.
그리고, 정규화된 Canonical SIL을 사용해서, SIL 코드를 최적화가 이루어지는 것 같습니다. 구체적으로는 예를 들어 ARC나 가상 메소드의 호출, 제네릭 언저리의 최적화를 도모하는 것 같습니다.
덧붙여서, 이 부근의 구현은 lib/SILOptimizer/Analysis, lib/SILOptimizer/ARC, lib/SILOptimizer/LoopTransforms, lib/SILOptimizer/Transforms에서 수행하는것 같습니다.
이렇게 해서 조정이 마무리된 Swift 코드가 IR 생성기를 통해서 LLVM IR로 변환되어, LLVM의 세계로의 다리가 놓인 것 같습니다.
덧붙여서 구현은 lib/IRGen에서 수행하는 것 같습니다.
또한, 이런 처리 과정은 어느 정도 단계별로 살펴볼 수 있어, Swift 컴파일러 swiftc에 다음의 옵션을 넣는 것으로 지정한 단계까지 처리한 결과를 텍스트로 확인할 수 있습니다.
그렇다면 시험 삼아 다음의 Swift 코드의 처리 과정을 출력해보겠습니다.
code1
예를 들어 구문분석한 결과를 보고 싶을 때는, 이 코드를 test.swift라고 하는 이름으로 저장하고 다음과 같이 swiftc 명령어를 실행해서 확인할 수 있습니다.
먼저 -dump-parse 옵션을 추가해, 타입 정보가 없는 AST를 출력하면, 다음과 같이 됩니다.
모르는 키워드가 잔뜩 나와서 읽기가 상당히 주저됩니다만, 그래도 진득하게 살펴보면 뭔가 본 기억이 있는 단어가 여기저기 보이는 것을 알 수 있습니다. 일단 구조마다 괄호로 둘러 쌓여있고, 자식 요소는 괄호 안에 인덴트를 포함하고 있는 모양입니다.
Swift 소스 코드를 문법의 규칙에 따라 의미가 있는 부분만을 잘라 내어, 거기서부터 문법적으로 어떤 역할에 해당하는지를 예를 들어 var_decl 같은 식별자에 태그를 붙여, 그 외의 필요한듯한 정보와 함께 괄호로 묶고 있다고 생각하시면 어느 정도 읽을 수 있게 될 거라 생각합니다.
관련해서 이 단계에서는 문법상의 오류만 검출되는 것 같습니다.
문법의 오류중에는 예를 들어 private와 public을 동시에 지정했다던가 하는 것도 포함합니다만, 이건 분명 의미를 분석했다기보다는, 무엇의 뒤에 무엇이 와야 하는지에 대한 규칙에 맞지 않는 것이 검출된다는 느낌입니다.
이때, 함수를 호출하는 곳에서는 (unresolved_decl_ref_expr type=’' name=arc4random_uniform specialized=no)라고 표기하고 있습니다.
세세한 의미까지는 모르겠습니다만, 이 단계에서는 아직 미해결(unresolved)인 상태, 함수명은 있지만, 애초에 존재하는지도 포함하여 “알지 못한다”라는 것을 알 수 있습니다. 실제로 아예 적당한 함수명을 지정해도, 이 단계에서는 에러를 검출하지 않습니다.
이 단계에서 출력에 타입 정보가 등장한다고 해도, 어디까지나 기호에 지나지 않는 모양입니다.
예를 들어, 변수 선언에서 타입을 지정한 경우는 그 타입 정보가 (type_ident (component id=’UInt32’ bind=none)) 라고 나타납니다만, 이것은 어디까지나 “타입 식별자가 있어야 하는 장소에 UInt32라고 쓰여 있다” 정도에 지나지 않습니다. 게다가, 타입 추론을 돕는 타입 명시라고 한다면, 여기에서는 as NSString 입니다만, 이 단계에서는 불필요한 데다, 애초에 정보로서 포함하지 않는 것 같습니다.
계속해서 -dump-ast 옵션을 지정해서, 타입 정보까지 포함해서 완성하는 AST까지 진행해보겠습니다.
이 단계에서 갑자기 코드가 많아져서, 저도 모르게 보는 것을 그만두고 싶어집니다만, 구조상으로는 앞의 -dump-parse의 경우와 크게 달라지지는 않았기 때문에, 그렇게 생각하며 기분을 차분하게 한다면 계속 살펴볼 수 있을 것입니다.
어째서 여기까지 많아진 이유는, 바로 전의 단계에서 <null type>으로 되어 있던 type의 위치에 구체적인 타입 정보가 포함되었기 때문에, 소스 코드의 해당 지점에 관련한 정보가 추가된 것이 길어진 요인입니다. 이에 의해, 얼핏 봐서는 알아보기 힘들게 되었지만, 반대로 어디가 어떻게 해석되었는지가 확실히 쉬워졌습니다.
이 단계에서, 작성하지 않는 타입이 추론되어서, 앞의 단계에서는 <null type>으로 지정되어 있던 type이 모두 타입이 명확하게 정해져 있습니다. 함수나 메소드등에 대해서도, 그것이 어떤 타입의 인수를 받는지 어떤 타입을 반환하는지를 고려해서 타입 정보가 지정되어 있습니다. 아울러서, 그 함수가 어떤 namespace에 속해 있는지에 대해서도 decl에서 확인이 가능합니다
함수나 타입의 정보가 조사되기 때문에, 함수가 정의되어 있는지, 대입이나 인수에 넘기는 등에서 제대로 적절한 타입을 사용하고 있는지 등이 체크됩니다. 여기에 부정합이 발생할 때는 에러로 통지됩니다.
덧붙여서 이 단계에서는, AST의 어느 부분이, 어느 소스 파일의 어디에 해당하는지나, 함수 등이 어떤 namespace에 정의되어있는지 등의 정보가 location과 range로 나타나고 있습니다.
그 정보가 너무 길어서 코드의 양이 지나치게 길어진 것처럼 보이지만, 오히려 파일의 이름과 행 번호로 목적하는 것을 찾을 수 있기 때문에 찾기 쉽게 되어 있습니다. 물론, 코드 양이 많은 만큼, 전체 구조를 파악한다는 것에서는 가독성이 떨어지기 때문에, 구조만 보고 싶다면 -dump-parse를 사용하는 것이 좋을지도 모릅니다.
이 단계에서 상수 let에 값을 2회 대입하는 문제도 검출합니다.
타입 체크가 거기까지 판단에 관여하는 것도 신기하지만, “값을 넣을 수 있는 상황” 같은 인식하는 방법은 하고 있는 것 같은 느낌이어서, 혹시 그에 관계되어 완전한 AST를 만들 수 없었다고 하는 결론에 도달하지도 모릅니다.
다음으로 SIL로의 변환을 살펴보겠습니다. 여기부터 착 하고 인상이 변하는 건, 드디어 Swift Intermediate Language의 세계에 들어갔기 때문에, 지금까지의 Swift 소스 코드와는 다른 관점에서 코드가 만들어집니다. 여기부터, 어쩐지 읽을 수가 없게 되었습니다.
코드의 양도 2435줄(역주: -emit-silgen 옵션으로 raw sil을 생성해보니 191줄이 나오니 뭔지 하는 생각이 듭니다.)로 크게 늘어, 여기에 모든 코드를 발췌하는 것은 어렵기 때문에, 중간을 모두 생략해서 기록하고 있습니다.
어찌 됐건, 먼저의 구문이 바뀐 느낌입니다만, Swift 언어와 Swift Intermediate Language는 다른 언어이기 때문에, 새로운 언어를 하나 더 기억하는 기분으로 바라보는 편이 대하기 쉬울 것 같습니다. 무엇보다 양측 모두가 Swift의 세계를 이루고 있다는 것에는 변함이 없기에, Swift에 대한 지식이 깊어진다면 조금씩 더 나아질 것 같습니다.
다만, 뭔가 (머리가) 잘 돌아가지 않으니 이걸 조금이라도 더 읽을 수 있게 하는 것이 저 자신의 이번 목표입니다.
여기서는 단순한 Swift의 타입 정보만이 아닌 그것을 어떤 방식으로 메모리에 배치하는 지나, 메모리 관리의 조작이라던가 하는 세밀한 부분도 코드에 표현하고 있는 인상입니다.
또, 이 Swift Intermediate Language의 단계에서도, 원래의 소스 코드의 어느 부분이 작성된 부분인지가 loc에 포함되어 있기 때문에, 원래의 코드의 어느 부분에 대응하고 있는지 알 수 있을 것 같습니다. 다만, 상당히 코드가 자세하게 기재되어 있는 것인지, Swift의 엄밀한 동작을 알고 있지 않으면 소스 코드의 장소가 기록되어 있다고 해도, “어째서 거기에 그 intermediate code가 있을까?” 같은 느낌이 될 거 같습니다.
거꾸로 말하면, 자세한 동작을 읽어 해석할 수 없을 때 SIL에 의지하면 보이지 않던 작은 동작을 알 수 있을 것 같습니다.
그런데, 생각하지 않았습니다만, 타입 체크를 완료한 AST로부터 Raw SIL을 만드는 단계에서 새로 발생한 에러는 뭐가 있을까요. 확실히 무언가 있었던 기분이 듭니다만, 잊어버렸습니다.
구체적으로 어째서 에러가 되었는지 안다면, 무엇을 하고 있는지를 구체적으로 상상하기 쉬울 터이니, 혹시 뭔가 알고 계신 분이 있다면 알려주시기 바랍니다.
하나 더, 정규화가 완료된 SIL이 있습니다. 비록 2종류가 있다고 해도 “어느 쪽도 SIL 언어이고, 정규화로 그다지 변하지 않겠지”라고 생각하고 있었습니다만, 자 하고 살펴보면, 코드의 양이 4188줄까지 늘어난 모양입니다.
무엇보다 스코프 같은 것이 형성되어 있거나, 가상 테이블 같은 것이 정의되어 있거나, 상상 이상으로 늘어나 있습니다. 무엇보다, 임포트한 기능에 관련된 정보도 포함된 것 같이 보여서, 생각한 것보다 많은 것을 하고 있는 것 같습니다.
덧붙여서 이 코드가 Raw SIL인지 Canonical SIL 인지는, 가장 최초의 줄에 있는 sil_stage를 확인하면 알 수 있습니다.
이 단계에서 일어나고 있는 일에 대해서는 Guaranteed Optimization and Diagnostic Passes에 있습니다만, 지식이 부족해 무엇을 하고 있는지까지는 이해할 수 없었습니다.
어쨌거나, 함수의 인라인화 같은 것도 이 단계에서 일어나는 것 같으니, 원래의 Raw SIL에 나름의 다양한 가공이 일어나고 있는 인상입니다.
그 외에도, 이 단계에서 변수 let이나 var를 초기화하지 않은 채로 사용한 경우를 에러로 검출하는 것 같습니다.
예를 들어 128을 초과하는 정수 리터럴을 Int8로 캐스팅할 때에 오버 플로우를 검출하는 것도 이 단계로 들어가면서부터 입니다.
이것으로 일단 막연하지만 SIL의 입구 부근까지 살펴볼 수 있었기에, 가능하면 이 대로 SIL 문법구조나 명령 셋 같은 것도 살펴보고 싶었지만, 안타깝게도 시간이 부족했습니다.
명령 셋도 꽤 수가 많은 것 같아서, 일단 SIL을 정말 알아야 할 필요가 있어야 진행이 수월할 거 같습니다. 그렇게 천천히 알아가보도록 하겠습니다.
"
http://woowabros.github.io/experience/2018/03/13/k8s-test.html,2018-03-13,쿠버네티스를 이용해 테스팅 환경 구현해보기,"실제로 서비스에 도입해보기 전에 쿠버네티스를 유용하게 사용해 볼 수 있는 방법 중에 하나가 아닐까.
이 글은 앞으로 우리가 관리하는 서비스(배민찬)의 아키텍처가 컨테이너 기반의 마이크로 서비스를 지향할 것으로 결정한 후 이를 위해 우선 아키텍처를 테스팅 환경으로 구현하여 실제로 서비스에 도입하기 전에 충분한 기간을 가지고 사용경험을 쌓는 것을 목적으로 쿠버네티스로 테스팅 환경을 구축하면서 겪었던 여러 상황들을 정리해본 글이다. 쿠버네티스의 구조와 그 리소스(Pod, Service, Deployment, ETC…)등은 그것만으로도 책 한 권은 가벼운 주제라 생각되니 그에 대한 설명을 할 수는 없지만 전체적인 테스팅 환경의 구조와 작업을 진행하면서 막혔던 부분, 주의해야 했던 부분을 짤막하게 언급하는 식으로 작성하였다.
일단 쿠버네티스 리소스를 올리려면 클러스터를 만들어야 한다. AWS에서 클러스터를 구축을 하는 데에는 kops를 사용하였다. 공식 사이트의 가이드에도 AWS에서 클러스터를 올리는 방법 중 하나로 설명이 되어있으며 비교적 손쉽게 클러스터를 생성할 수 있기 때문이다. kops가 요구하는 AWS 권한을 가진 계정으로 awscli를 사용할 수 있는 상황이라면  가이드를 따라서 손쉽게 클러스터를 만들어낼 수 있다. 생성할 클러스터의 구조는 다음과 같다.

시작은 작은 규모로 t2.medium 마스터 1대, 미니언 2대로 구성하도록 하였다. 마스터 노드의 수는 Raft 알고리즘의 특성상 홀수로 유지하는 것이 좋은데 두 개의 마스터노드는 한 개만 못한 결과를 초래할 수 있기 때문이다(둘 중 하나만 다운돼도 둘 다 다운되는 효과가 나오는 기적). 마스터, 미니언 노드는 private 서브넷에 생성되며 private 서브넷에 있는 노드(EC2 인스턴스)에 접속하려면 Bastion 노드를 통해야 한다. 위의 구조를 가지는 클러스터를 생성하는 명령어는 다음과 같다. 자세한 파라미터 목록은 kops create cluster 문서를 참조하자.
클러스터를 만드는 것은 가이드대로 하면 간단한 일이지만 클러스터를 생성하면 클러스터에 사용되는 VPC 가 생성되는데, 다른 VPC와의 피어링 등의 이슈가 있어 VPC와 서브넷의 CIDR 값을 수동으로 지정해야 한다면 kops update cluster --yes 명령어를 사용하여 최종적으로 클러스터를 생성하기 전에 kops edit cluster 명령어를 사용하면 에디터가 열리고 VPC와 Subnet 의 CIDR을 수정할 수 있다. 이미 클러스터를 생성했는데 이를 변경해야 한다면 차라리 kops delete cluster 명령어로 클러스터 자체를 삭제하고 새로 만드는 게 더 빠르다.
수정 내역을 저장한 후 kops update cluster --yes 명령어를 사용하여 클러스터를 생성했다면 이제 Pod(이하 팟)에 사용할 도커 이미지를 준비한다.
클러스터 환경이 준비되었다면 Pod에 사용될 도커 이미지를 만들어야 한다.  테스팅 환경을 구성하기 위해선 4개의 컨테이너가 필요했는데, 이 컨테이너들은 다음과 같은 관계를 지닌다.

필자는 로컬에서 도커 이미지를 빌드하여 ECR 에 푸쉬한 후 클러스터에서 사용하였다. kops를 통해 AWS상에 클러스터를 만들었다면 동일한 계정의 ECR에 올린 도커 이미지는 별도의 imagePullSecret 이 없어도 가져다 쓸 수 있다.
이미지가 준비되었다면 이제 쿠버네티스 리소스를 만든다. 다음과 같은 구조로 구현하였다. 참고로 그림에 있는 팟들은 전부 Deployment 리소스가 관리하고 있다고 생각하면 된다.

테스트를 구성하는 팟에는 한 개의 컨테이너만 배포되어있지만 팟에는 여러 개의 컨테이너를 한 번에 배포하여 사용할 수 있다. 나중에 젠킨스를 통한 빌드 작업에서 하나의 팟에 여러 컨테이너를 실행하여 사용하는 것을 볼 수 있을 것이다.
필자가 구현할 테스팅 환경은 https 프로토콜을 사용하기 위해 로드밸런서가 생성하는 ELB가 AWS ACM 인증서를 사용하도록 해야 했는데, 이것은 로드밸런서 설정에 다음 어노테이션을 추가하여 해결하였다. 해당 어노테이션을 추가하면 AWS 콘솔에서 ACM이 ELB에 사용되는 것을 확인할 수 있을 것이다.
테스팅 환경을 구성할 쿠버네티스 리소스의 yaml 설정 파일들이 파일이 준비되었다면 kubectrl create -f 명령어를 통해 쿠버네티스 리소스들을 클러스터에 모두 생성하고 테스트를 해 본다. 감사하게도 QA 팀 분들이 테스트를 해주셔서 테스팅 환경이 구현되었음을 확인할 수 있었다.
이제 쿠버네티스 클러스터에 테스팅 환경이 올라갔다. 이제 이 테스팅 환경에서 테스트를 진행할 수 있을 것 같은데, 현재는 하나의 테스팅 환경을 올렸을 뿐이다. 그리고 현재는 테스팅 환경을 하나 만드는 것만 해도 다음과 같은 절차가 필요하다.
만약 이것을 자동화하지 않으면 클러스터에 테스팅 환경을 매번 배포하는 것부터가 큰일이니 젠킨스를 사용하여 자동화를 해야 할 것 같다. 이 과정을 젠킨스에서 처리하면 다음과 같은 절차를 거치게 될 것이다.

그런데 막상 배포 자동화를 하려고 하니 문제가 있었다. 여러 명이 각자의 테스팅 환경을 사용할 수 있도록 각 테스팅 환경은 alpha-cluster01~05-www.testdomain.com와 같은 서로 다른 도메인을 사용해야 했는데, 이를 위해서는 쿠버네티스 리소스와 함께 nginx 의 설정 파일, Java의 환경변수, PHP 설정 파일을 변경해야 했고 이를 젠킨스에서 처리하도록 하는 것은 매우 비효율적이었기 때문이다.
Helm은 쿠버네티스 패키지 매니저인데, helm 사용하면 클러스터에 Tiller라는 팟이 설치되고, 이 팟을 통해 Helm 패키지(이하 차트) 내부에 정의한 쿠버네티스 리소스들을 클러스터에 올릴 수 있다. 흥미로운 점은 Helm에서 템플릿 기능을 지원한다는 것이다. 이 템플릿 기능을 사용하여 도메인에 따라 설정을 변경해 줘야 하는 작업을 해결할 수 있었다.
{{ .Release.Name }}  은 helm을 사용하여 차트를 배포할 때 --name 파라미터로 지정한 값으로 치환되어 들어갈 것이다. (지정하지 않으면 도커 컨테이너처럼 임의의 자동으로 생성된 이름이 할당된다.) 그럼 이제 도메인의 변경이 필요한 파일들을  도커 이미지에 넣지 않고 ConfigMap(이하 컨피그맵)에 넣은 후에, 차트 배포 시 치환된 컨피그맵을 팟의 컨테이너에 파일로 마운트 하거나 환경변수로 설정한다
이때 --name 으로 넘기는 파라미터는 생성된 테스팅 환경에 접속할 때 사용할 도메인의 접두사 (ex: alpha-cluster01~05)를 사용하는데 이는 alpha-cluster01-www.testdomain.com와 같은 도메인 설정을 동적으로 하고 차트를 통해 배포된 테스팅 환경에서 생성된 ClusterIP 서비스를 각 테스팅 환경끼리 구분하기 위해서 사용된다.
그리고 helm에서는 차트의 의존성을 관리할 수 있는데, 이 기능을 사용하면 테스팅 환경에 필요한 각 서비스별로 차트를 일일이 배포하지 않고, 이들을 포함하는 하나의 임의의 패키지를 만들어 한번에 배포할 수 있다(사실 차트를 동일한 이름으로 중복해서 배포할 수 없으므로 테스팅 환경을 구성하는 서비스들의 차트들이 동일한 도메인을 사용하려면 하나의 차트로써 배포될 필요도 있었다). helm 차트로 구성한 테스팅 환경의 구조는 다음과 같다.

이제 helm install 명령어로 alpha 차트를 배포할 때 --name 에 설정한 도메인 접두사가 각 서비스의 컨피그맵에 치환되어 들어갈 것이다.  차트가 준비되었으면 helm install 명령어를 통해 클러스터에 차트를 배포하는 것이 가능하다. helm install 명령어에서는 템플릿에서 사용할 값을 파라미터로 넘겨줄 수도 있는데, 이 기능은  잠시 후에 jenkins 파이프라인 예시에서 helm 차트를 배포할 때 파이프라인을 통해 빌드된 도커 이미지의 태그를 제공하는 용도로 사용하였다.
helm 차트를 만듦으로써 테스팅 환경의 배포는 helm install 명령어 하나로 처리할 수 있게 되었고 도메인 변경에 따른 문제도 해결되었다. 그럼 이제 배포 자동화를 위해 젠킨스 파이프 라인을 구성해보도록 하겠다.
helm을 이용하면 helm 차트 저장소에 등록되어 있는 차트들을 내려받아 사용할 수 있는데 이중에는 젠킨스 차트도 있으며 이 차트는 이미 쿠버네티스 클러스터 관련 설정이 이미 되어있어 매우 유용하다. (쿠버네티스 젠킨스 플러그인이 이미 설치되어있고 해당 젠킨스 차트가 올라가있는 클러스터에 관한 설정이 이미 되어있는 상태), 그리고 설치되면서 자동으로 PersistentVolume(이하 PV) 리소스를 생성하여 젠킨스 관련 데이터를  영속적으로 저장하기 때문에 노드 자체가 날아가는 상황에서도 별도의 설정 없이 젠킨스의 데이터를 유지할 수 있는 장점이 있다.

참고로 본인은 이 패키지의 존재를 모르고 오피셜 젠킨스 이미지를 사용하던 중 노드가 다운되는 일이 벌어졌는데 PV을 사용하지 않아 거의 다 완성한 파이프라인을 날려먹는 참사를 당하게 되었다. 별도로 PV를 마운트 하거나 hostPath를 마운트 한 경로에 데이터를 저장하지 않으면 팟의 삭제와 함께 모든 데이터가 날아가니 팟에서 만들어지는 소중한 데이터는 꼭 영속적인 저장소에 저장하도록 하자.
그리고 젠킨스의 쿠버네티스 플러그인은 젠킨스 슬레이브 팟을 클러스터에 배포하여 빌드를 지원하는데, 흥미롭게도 이 슬레이브 팟에는 여러 컨테이너를 추가할 수 있어 이를 빌드 과정에서 사용할 수 있다. 그렇다면 굳이 마스터 젠킨스에 빌드를 위한 기능을 설치하지 않고 빌드 전용 컨테이너를 만들어 빌드 작업 자체의 휴대성을 높일 수도 있을 것이다. 이는 php, composer, npm 등 빌드에 필요한 게 많았던 레거시 웹 서비스 빌드에 특히 유용했다.

그리고 젠킨스 슬레이브 팟에서 kubectl, helm 컨테이너를 사용 시에는 별도의 설정 없이 클러스터를 사용할 수 있어 kubectl의 config 파일을 설정할 필요도 없어진다. 이제 이를 사용하는 파이프라인을 작성해보자.
빌드를 실행했는데 빌드가 계속 대기 중이고 시작되지 않는다면 젠킨스 설정에서 Executor 설정을 확인하자. 필자의 경우 젠킨스 차트를 통해 설치 했을 때 기본값이 0으로 저장되어 있어 영문도 모르고 한동안 기약 없는 대기를 타야 했다.
이것으로 배포 과정이 자동화되었다. 이제 차트가 배포되었을 때 생성된 Router 로드밸런서의 ELB에 Route53을 통해 도메인을 연결해주기만 하면 테스팅 환경을 사용할 수 있게 되었다. 그럼 이제 클러스터가 기존의 5개의 테스팅 환경(기존에 테스팅 환경을 5개까지 사용했으므로)을 감당해낼 수 있는지 확인해보자.
안타깝게도 다수의 테스팅 환경을 배포하는 과정에서 미니언 노드 하나가 다운되었는데, 원인을 알아본 결과 팟의 리소스 사용량에 제한을 두지 않아서 이런 문제가 발생한 것으로 보인다. 특히 CPU 보다 메모리 사용량을 제한하지 않을 경우 치명적인데 최악의 경우 OOM(Out of Memory) 이 발생하여 노드가 재시작될 수도 있다. 팟의 리소스 제한을 설정하면 노드에 충분한 리소스가 남아있지 않을 경우 팟이 아예 배포되지 않으므로 노드의 OOM을 방지할 수 있을 것이다. 일단 팟의 리소스 제한을 설정하기 위해 어떤 팟이 얼마나 리소스를 사용하는지 모니터링을 해보자
Heapster를 설치하면 kubectl top (node|pod) 명령어를 사용하여 각 노드와 팟의 CPU, 메모리 사용량을 모니터링할 수 있다. Heapster만 설치해서는 명령어를 친 순간의 리소스 사용량만 모니터링할 수 있지만  지금은 이걸로 충분한 것 같다(influxDB, Grafana 플러그인을 사용하면 좀 더 상세한 모니터링이 가능하다). 그럼 명령어를 한번 실행해 보자.
kubectl top pod 명령어로 팟의 리소스를 모니터 할 때는 팟이 클러스터에 올라가고 시간이 좀 지나야 한다. 금방 올린 팟의 리소스 사용량이 안 뜬다면 잠시 후에 명령어를 다시 실행해 보자.
모니터링 결과 java로 만든 msa1과 Api-gateway 팟의 메모리 사용량이 상당함을 알 수 있었다. t2.medium 타입의 EC2 인스턴스를 두 미니언 노드의 가용 메모리는 8Gi인데 자바를 사용하는 팟 두 개 만으로도 1Gi가 넘는 메모리를 사용하는 셈이다. 배포에 사용되는 젠킨스 팟은 한술 더 뜨는데, 두개의 팟이 거의 2Gi 의 메모리를 사용하고 있는 상황이다. 이런 상황에서 메모리 사용량을 제한하지 않았으니 팟은 약간의 노드에 약간의 리소스만 사용 가능해도 배포되었을 것이며 시간이 지남에 따라 제한 없이 사용 메모리를 계속 늘려감으로써 노드의 OOM을 초래하게 되는 것도 무리가 아니었다. 안정적으로 다수의 테스팅 환경을 위해 팟에 리소스 제한을 설정하자.
쿠버네티스에선 팟에서는 사용할 리소스를 제한할 수 있는데, 만약 제한하지 않으면 팟은 메모리를 제한 없이 사용하게 된다. 팟이 사용할 리소스 설정에는 request, limit 두 가지가 있는데 request는 해당 팟이 실행되기 위해 요구하는 리소스 사용량으로 쿠버네티스는 노드에 request 에 설정한 값 이상의 리소스가 남아있을 경우에만 팟을 노드에 배포한다. limit는 팟이 최대로 사용할 수 있는 리소스의 값이다.

Deployment에서 관리하는 팟의 리소스 제한을 걸어준다. 리소스 사용량을 모니터링을 하면서 서비스가 유지되는 한도 내에서 최대한 줄여보았다. 다른 팟들의 리소스 사용량도 설정했다면. 이제 배포에 사용되는 젠킨스 슬레이브 팟의 리소스도 제한해야 하는데 마찬가지로 다음과 같이 컨테이너별로 리소스 사용량을 제한할 수 있다.
이제 노드가 OOM으로 다운되는 문제가 방지되는지 다시 테스팅 환경을 5개 이상 배포해보니 클러스터가 매우 느려지고 마지막에 배포하려는 테스팅 환경이 배포되지 않는 상황이 발생하긴 하지만 노드 자체가 다운되는 현상은 발생하지 않았다. 이제 나름 안정을 찾게 된 듯하다. 그런데 리소스 사용량을 설정하다 보니 젠킨스 슬레이브 팟의 메모리 사용량 워낙 많음(1Gi 정도를 제공하지 않으면 제대로 동작하지 않았다)을 알게 되었는데, 만약 두 개의 노드에 테스팅 환경을 위한 팟이 골고루 배포된다면 어느 노드에도 젠킨스 슬레이브 팟이 배포되지 않을 가능성이 있지 않을까?

사용할 수 있는 총 자원은 충분하지만 팟이 어떻게 배포되느냐에 따라 자원 소모가 큰 팟이 배포가 되지 못할 수 있다. 그렇다면 배포를 할 때 젠킨스 슬레이브 팟의 배포를 위한 자원을 확보하려면 두 개의 노드에 어떤 식으로 팟이 배포될지 제어를 할 필요가 있다.
팟에 NodeAffinity를 설정하면 팟이 배포될 때 어떤 노드를 선호할지 노드에 붙은 라벨을 사용하여 설정할 수 있는데, 노드에 라벨을 붙이는 방법은 다음과 같다.
kubectl label nodes <node-name> <label-key>=<label-value>
이미 붙은 라벨을 수정하려면  –overwrite 옵션을 사용해야 한다.
배포를 위한 젠킨스 슬레이브 팟에도 NodeAffinity를 적용하고 싶었으나 안타깝게도 지금은 nodeAffinity 설정을 지원하지 않는 것 같다. 하지만 다행스럽게도 nodeSelector는 지원하는데, nodeSelector 역시 노드에 붙은 라벨을 이용하여 팟의 배포를 제어할 수 있지만 nodeAffinity 처럼 선호하는 노드를 지정하는 것이 아니라 해당 라벨을 가진 노드에만 팟의 배포가 가능하도록 하는 방식이다. 즉 nodeSelector에 지정한 라벨을 가진 노드가 없을 때는 팟이 배포되지 못하니 주의하자.
이제 테스팅 환경용 팟은 alpha-pod-affinity=service 라벨이 붙은 노드에 우선적으로 배포된 후 더 이상 배포하기 힘들 때에나 라벨이 alpha-pod-affinity=deploy 인  노드에 팟을 배포할 것이다. 그리고 젠킨스 슬레이브 팟은 alpha-pod-affinity=deploy 라벨이 붙은 노드에만 배포될 것이다.
클러스터가 안정되니 이번엔 15분을 넘어가는 배포 속도가 거슬리기 시작했다. 만약 이렇게 느린 배포 과정에서 마지막 도커 이미지 빌드에 사용되는 Git 브랜치 명이라도 잘못 입력하여 빌드가 실패한다면 썩 유쾌하진 않을 것이다. 젠킨스 빌드 로그를 보면 빌드에 걸리는 속도의 주범은 gradle, composer, npm 등을 사용하여 라이브러리 의존성을 처리하는 작업인 것으로 보이는데 이런 작업들은 일반적인 상황이라면 실행 시 캐시가 남아 다음 실행 시의 실행시간이 단축되지만 현재 빌드 환경에서는 언제나 새로운 젠킨스 슬레이브 팟이 배포되기 때문에 캐시가 남지 않는 것이 엄청나게 느린 속도 빌드 속도의 주범이다. 그렇다면 helm의 jenkins 패키지처럼 PV를 마운트 하여 캐시를 영속적으로 저장한다면 빌드 속도를 개선시킬 수 있지 않을까? 일단 AWS 에서 사용할 EBS를 만들고 해당 id를 이용하여 클러스터에서 사용할 PV를 만들도록 하자.

PV 와 PVC 리소스를 만들었다면 파이프라인에 persistent volume 을 설정해준다.
이미 생성된 EBS의 ID 를 사용할 경우 배포될 팟과 EBS의 availability zone 은 동일해야 한다. 그렇지 않으면 팟이 배포되지 않는다. 빌드에 사용되는 EBS 는 모두 ip-10-yy-yy-yy.ap-northeast-2.compute.internal 노드와 동일한 availability zone 에 생성해두었다.
EBS를 마운트 하여 사용하니 자바 서비스의 빌드 속도가 눈에 띄게 개선되었다(MSA #1 서비스의 경우는 8분 → 2분, ApiGateway 5분 → 2분으로 많이 단축되었지만 Front Web은 8분→5분 정도로 살짝 단축됨). 그리고 클러스터 상에 자원이 많을 경우 파이프라인의 parallel 문을 사용하여 java로 된 MSA #1, ApiGateway 서비스와 Front Web 서비스를 빌드하는 각각의 젠킨스 슬레이브 팟을 클러스터에 동시에 배포하여 빌드를 병렬로 진행하는 파이프라인을 추가하여 배포에 걸리는 시간을 더 단축할 수 있었다.
이제 마지막으로 수동으로 해줘야 하는 작업이 남아있는데, 그것은 바로 테스팅 환경이 하나 배포될 때마다 생성된 ELB에 Route53 의 도메인을 연결하는 작업이다. 사실 도메인 연결은 귀찮긴 하지만 별것 아니지만 새로 생성된 ELB의 시큐리티 그룹을 설정해주는 것이 더 문제다. 테스팅 환경이 배포될 때마다 새로 생성된 ELB의 시큐리티 그룹을 다시 설정해주지 않으면 테스팅 환경이 외부에 노출될 수도 있을 것이다. 이제 이 문제만 해결한다면 배포는 완전 자동화를 달성할 수 있을 것이다. 이 문제는 Ingress 리소스를 사용하여 ELB를 한 개만 고정으로 사용하고 Ingress 에서 받은 요청을 호스트 값에 따라 알맞는 테스팅 환경으로 각각 포워딩하면 해결할 수 있을 것이다. Ingress 는 apache 의 virtual host 와 같은 역할을 한다고 생각하면 될 것이다. 그런데 Ingress 리소스는 쿠버네티스에서 기본으로 지원되는 것은 아니라서 사용하려면 우선 ingress addon 을 설치해줘야 한다. Ingress를 addon을 설치하면 kube-ingress 네임스페이스에  ingress-nginx 라는 로드밸런서를 만드는데, 이 로드밸런서 역시 AWS ACM인증서를 사용하기 위해서 ACM 설정을 위한 annotation을 추가해준다. (ACM인증서 사용을 위한 annotation은 이 글의 윗부분에서 찾을 수 있다) 이제 ELB는 이 로드밸런서에서만 사용할 것이다.

이제 ingress 리소스를 설정하여 클러스터에 추가하자
이제 ELB 는 Ingress 에 연결된 1개만 남았고  테스팅 환경을 배포하면 Ingress 를 통해 접속할 수 있어 Route53과 Security Group 을 테스팅 환경을 배포할 때마다 수정할 필요가 없어지게 되었으니 이제 배포가 자동화되었다고 할 수 있을 것 같다.
이제 테스팅 환경을 배포하는 것은 자동화 하였는데, 만약 수정한 서비스는 한 개인데 테스팅 환경에 필요한 모든 리소스를 재배포해야 한다면 효율적이지 않을 것이다. 원하는 하나의 서비스만 교체하는 파이프라인을 만들어 보자. 테스트를 구성하는 서비스는 Deployment를 통해 팟을 관리하니 kubectl set image 명령어를 사용하면 지정한 Deployment에서 사용하는 팟의 도커 이미지가 수정할 수 있는데, Deployment 리소스는 사용하는 도커 이미지가 변경될 경우 팟을 변경된 이미지를 사용하도록 재배포하므로 이를 이용하여 원하는 서비스만 갱신하는 파이프라인을 만들 수 있다.
이제 원하는 서비스만 빌드하여 배포하는 것이 가능해졌으니 좀 더 효율적으로 테스팅 환경을 사용하는 것이 가능할 것이다.
쿠버네티스를 사용하며 경험을 쌓아보자는 생각으로 시작한 작업이었지만 실제로 테스팅 환경을 구축하면서 느낀 점은 테스팅 환경을 구성하는 데에 있어서도 편리하고 유용했다는 점이다. 쿠버네티스의 리소스들은 클러스터가  AWS든 GCE 든 상관없이 클러스터 환경이 구성되어 있다면 얼마든지 올려서 사용할 수 있으며 AWS를 다루는데 익숙하지 않은 본인의 입장에서 도커 이미지를 빌드하고 쿠버네티스 리소스를 생성하는 것으로 손쉽게 다른 서비스들을 테스팅 환경에 추가할 수 있다는 점에서 상당히 매력적이었다. 그리고 정말 잘 구성한다면 테스팅 환경을 유지하는데 드는 예산도 절약할 수 있는 가능성을 보여준다. 사용하는 AWS 인스턴스 수가 줄어드는 건 소소한 덤이라고 볼 수 있겠다.
정리를 해놓고 보니 쿠버네티스 자체보다는 배포 자동화에 대한 내용이 더 많아진 것 같다. 필자는 젠킨스의 쿠버네티스 플러그인을 사용하여 파이프라인을 작성하였지만 굳이 이 플러스인 사용 없이도 얼마든지 배포를 자동화시킬 수는 있을 것이다. 하지만 젠킨스 슬레이브 팟을 클러스터에 배포하고 팟에 포함된 컨테이너를 통해 빌드를 하는 것은 나름 신선한 경험이었고 빌드를 위한 컨테이너를 별도로 만들어서 빌드 작업에 사용하는 것도 의외의 편리함을 제공하여 한번 사용해 볼 만한 가치는 있다는 느낌이 들었다.  만약 쿠버네티스에 관심이 있고 사용해보고 싶지만 실제로 적용하는데 부담이 된다면 테스팅 환경과 그 배포 환경을 쿠버네티스를 이용하여 구성해보는 것부터 비교적 가볍게 시작해보는 건 어떨까?
"
http://woowabros.github.io/study/2018/03/05/sdp-sap.html,2018-03-05,안정된 의존관계 원칙과 안정된 추상화 원칙에 대하여,"Robert C. Martin의 Agile Software Development - Principles, Patterns, and Practices 에서 SDP, SAP 를 정리해보았습니다.
이 글은 기본적으로는 Java와 Spring Framework 기반(혹은 이와 유사한 계층형 방식)으로 개발하시는 개발자분들을 대상으로 합니다.
개발을 하면서 어떨 때는 interface를 만들고 어떨때는 안 만드는지 혹시 규칙이 있는 것인 아닌지 궁금할 때가 있습니다.
특히 Controller 패키지, Service 패키지, Repository 패키지의 기본 계층으로 개발을 할 때 많이 제기되는 문제이면서 항상 마지막엔 “정답은 취향따라”로 결론지으며 끝나는 논의 중에 “Service는 interface를 뽑아내고 구현해야하는지에 대한 고민”이 있습니다.
그러다가 Robert C. Martin 의 Agile Software Development - Principles, Patterns, and Practices 에 “20장 패키지 설계의 원칙”에 나온 내용을 읽다보니 아 이부분을 통해서 interface 사용 여부를 결정하는 것도 가능하겠구나 라는 생각이들어서, 이를 제 나름대로 이해하고 정리하며, 개인 의견을 덧 붙인 것입니다. 책의 내용 요약과 제 개인 의견이 혼재되어 있으니 주의해서 봐주세요.
해당 장에서 두가지 부분 Stable Dependencies Principle(안정된 의존관계 원칙)과 Stable Abstraction Principle(안정된 추상화 원칙)에 대한 정리가 이 글의 핵심입니다. Controller-Service-Repository 레이어는 비교 예제일 뿐 이 원칙들이 유일하게 적용되는 대상은 아닙니다.
먼저 패키지에서 안정성이 의미하는 바를 알아보고 안정적인 패키지는 어떻게 추상적이어야 하는지 살펴보겠습니다.
“20장 패키지 설계의 원칙”을 보면 Stable Depencies Principle은 간단히 “의존은 안정적인 쪽으로 향해야 한다.”라는 것을 뜻합니다.
소프트웨어 설계는 정적일 수 없습니다. 유지보수를 하려면 변화는 필연적입니다. 안정된 의존 관계 원칙을 통해 쉽게 변할 수 있는 패키지를 만들 수 있습니다. 이 패키지는 변화할 것을 예상하고 있습니다. 이렇게 쉽게 바뀔 것이라고 예상되는 패키지들은 바뀌기 어려운 패키지의 의존 대상이 되어서는 안됩니다.
쉽게 바뀔 수 있는 모듈에 무언가가 의존하기 시작하면 금세 이 모듈은 변경하기 어렵게 됩니다. 클래스 다이어그램을 그릴 때 불안정한 것을 위로 안정적인 것을 아래로 두는 습관을 가지면 그 방향이 바뀌었을 때 문제를 알기 쉽습니다.

소프트웨어 패키지를 변경하기 힘들도록 만드는 한 가지 확실한 방법은 바로 다른 많은 소프트웨어 패키지가 그 패키지에 의존하게 만드는 것입니다. 의존해 오는 패키지들이 많은 패키지는 변경한 내용이 의존하는 모두를 만족시키려면 매우 많은 일이 필요하므로 매우 안정적이라고 말합니다.
안정성은 패키지에 의존하는 수와 패키지가 의존하는 수를 통해 측정할 수 있습니다.
불안정성 = 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수 / (이 패키지에 의존하는 외부 클래스의 수 + 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수)
불안정성이 1 이면 이 패키지에 의존하는 다른 패키지가 없다는 의미 입니다. 이는 최고로 책임을 지지 않고 의존적이며 불안정합니다.
불안정성이 0 이면 이 패키지는 책임을 지며 독립적이며 안정적입니다. 이 패키지에 의존하는 다른 요소가 많기 때문에 함부로 변경하기 어려우며 다른 것에 의존하지 않으므로 자신의 의존성에 의해 변경될 가능성도 적습니다.
안정된 의존관계 원칙에 따르면 어떤 패키지의 불안정성 측정값은 그 패키지가 의존하는 다른 패키지들의 불안정성 값들보다 반드시 커야 합니다(즉, 의존 관계의 방향으로 불안정성 측정값이 줄어들어야 합니다).
자 이제 생각해 봅시다.
Controller 는 일반적으로 Service와 Repository에 의존합니다. 하지만 일반적으로 Controller 코드를 사용하는 다른 코드는 존재하지 않습니다.
Controller가 10개가 있고 이들 모두 Service, Repository에 의존하지만 어느 것도 Controller에 의존하지 않는다고 했을 때
Controller의 불안정성 = 10 / (0 + 10) = 1 이 됩니다. 최고로 불안정하게 됩니다.
이제 Service를 보면 Repository에 의존하는 Service가 10개가 있고, 이 Service들에 20개의 Controller들이 의존한다고 하면
Service의 불안정성 = 10 / (20 + 10) = 0.333 이 됩니다. 약간 안정적입니다.
이제 Repository를 보면 Repository 자체는 우리가 직접 만든 다른 계층에 의존하지 않고(0) 이 Repository에 의존하는 Service들과 Controller들이 20개가 있다면
Repository의 불안정성 = 0 / (20 + 0) = 0 이 됩니다. 최고로 안정적이게 됩니다.
즉, 의존간계의 방향으로 Controller (1) -> Service (0.333) -> Repository (0) 불안정성 값이 줄어들게 됩니다.

이것을 보면 안정된 의존 관계원칙으로 봤을 때 Repository는 절대로 Service나 Controller에 의존하면 안 됨을 알 수 있습니다. 마찬가지로 Service도 Controller에 있는 코드를 호출해서는 안됩니다.
그렇다면 이런 안정적인 설계가 필요한 코드는 설계의 유연성이 떨어지게 됩니다. 안정적이면서도 설계의 유연성을 높일 수 있는 방법은 무엇일까요?
바로 추상 클래스(abstract class)입니다.
패키지는 자신이 안정적인 만큼 추상적이기도 해야 한다.
안정된 추상화 원칙은 안정적인 패키지는 그 안정성 때문에 확장이 불가능하지 않도록 추상적이기도 해야하며, 거꾸로 이 원칙에 따르면 불안정한 패키지는 구체적이어야 하는데, 그 불안정성이 그 패키지 안의 구체적인 코드가 쉽게 변경될 수 있도록 허용하기 때문입니다.
따라서 어떤 패키지가 안정적이라면 확장할 수 있도록 추상 클래스들로 구성되어야 하며, 확장이 가능한 안정적인 패키지는 유연하며 따라서 설계를 지나치게 제약하지 않아야 합니다.
안정된 의존 관계 원칙은 의존관계의 방향이 안정성의 증가 방향과 같아야 한다고 말하고, 안정된 추상화 원칙은 안정성이란 추상성을 내포한다고 말하기 때문에 따라서 의존 관계는 추상성의 방향으로 흘러야 합니다.
이를 간단히 말하면
추상성 = 패키지 안에 들어있는 추상 클래스 수(하나 이상의 순수한 인터페이스를 가지고 있으며 인스턴스화 할 수 없는 클래스) / 패키지 안에 들어있는 클래스 수
추상성은 0부터 1로 나올 수 있으며 0은 패키지에 추상 클래스가 하나도 없다는 뜻이고 1은 패키지에 추상 클래스 밖에 없다는 뜻입니다.

추상성과 안정성의 그래프를 위와 같이 그려볼 수 있습니다.
모든 클래스가 “(0,1) 안정적, 추상적” 위치나 혹은 “(1,0) 불안정, 구체적” 위치에 올 수는 없습니다. 여기서 가강 좋은 것은 위 그래프에서 “주계열” 부분에 위치하도록 코드를 만드는 것입니다.
그에 앞서 “쓸모없는 지역”과 “고통의 지역”은 무조건 배제하는 것이 좋습니다.
(0,0) 안정적이고 구체적인 영역에 패키지가 있다고 해보면 추상적이지 않기 때문에 확장하기 어려운데 안정적이라서 변경하기도 함듭니다. 잘 설계된 패키지는 (0, 0) 위치에 오기 어렵습니다. 이 부분을 “고통의 지역”이라고 부릅니다.
하지만 고통의 지역에 있을 수 밖에 없는 경우도 있습니다. 데이터 베이스 스키마는 매우 구체적인데 이에 의존하는 코드도 무척이나 많은 안정적인 것입니다. 데이터베이스 스키마 변경이 고통스러운 이유중의 하나입니다.
그 외에 String 관련 클래스들이 있습니다. String 관련 클래스들은 매우 안정적으로 다른 수많은 코드가 의존하고 있는데 매우 구체적인 코드입니다. 하지만 문자열 코드는 변경의 가능성이 매우 적기 때문에 별로 해가 되지 않습니다.
여기서 봤을 때 정말 어쩔 수 없는 경우와, 변경 가능성이 0에 가까운 문자열, 유틸리티성 코드가 아니면 안정적이면서 구체적으로 만드는 행위는 피해야 함을 알 수 있습니다.
(1, 1) 지역은 매우 추상적이면서도 불안정합니다. 불안정하다는 것은 이에 의존하는 다른 코드가 없음을 나타냅니다. 그런데 추상적이라는 것은 인터페이스를 만들고 구현했다고 보면 됩니다.
바로 Controller를 만들 때 인터페이스를 만들고 이를 구현하면 바로 이 부분 (1,1) 지역에 들어가게 됩니다. 이름 그대로 불안정한 Controller를 추상적으로 만드는 것은 매우 “쓸모 없는 행위”임을 알 수 있습니다.
이와 비슷한 것으로 Spring Batch의 Tasklet 같은 것이 있겠습니다. Tasklet 인터페이스 그 자체는 Sprig Framework가 Tasklet을 인식하기 위한 인터페이스이지 Tasklet 코드를 위한 인터페이스는 아닙니다. Tasklet은 Spring에 의해서만 호출되지 개발자가 직접 호출하지 않습니다. 즉 매우 불안정한 것인데, Tasklet에 별도의 인터페이스를 또 만들어서 구현하게 하는 것은 쓸모 없는 행위 임을 알 수 있습니다.
여기서 봤을 때 우리가 직접 사용하는 코드가 아닌 프레임워크에 의해 호출되는 코드는 프레임워크 규약을 따르기 위한 인터페이스를 제외하고는 별도의 인터페이스를 따로 빼서 구현할 필요가 없음을 알 수 있습니다.
우리는 고통의 지역과 쓸모 없는 지역은 항상 피하려고 노력해야 합니다. 물론 절대로 피할 수 없는 경우도 있습니다.
가장 바람직한 패키지는 주계열의 양끝 (0, 1) 안정적이고 추상적인 위치와 (1, 0)불안정하고 구체적인 위치이지만 대부분의 코드는 그 외 어딘가에 존재합니다. 그 중에서도 주계열이라고 표시된 지역에 있는 패키지는 “안정성에 비해 너무 추상적”이지도 않고, “추상성에 비해 너무 불안정적”이지도 않습니다. 즉, 쓸모 없지도 않고 특별히 고통스럽지도 않습니다. 자신이 추상적인 정도만큼 의존의 대상이 되며, 자신이 구체적인 정도만큼 다른 패키지에 의존합니다. 패키지를 주계열쪽에 위치하게 설계하는 것이 좋습니다.
주계열로 부터의 거리는 다음과 같이 측정합니다.
거리 = |추상성  + 불안정성 - 1|
이 결과는 절대값이므로 항상 0~1사이만 나오며, 거리가 0인 패키지는 주계열 바로 위에 있음을 의미합니다. 1은 가장 멀리 떨어져 있음을 뜻합니다.
이 주계열로 부터의 거리를 측정함으로써 예외적인 패키지들을 판변해 내고 이들을 조사해 보는 것이 좋습니다.
여기부터는 순전히 제 개인 의견만 기술합니다.
위에서 말한 Controller를 계산해보면 위에서 Controller의 불안정성을 1(불안정적)이라고 했고, Controller에 대해 전혀 인터페이스를 만들지 않을 것이므로 추상성은 0이 됩니다. 따라서,
Controller의 주계열로부터의 거리 = |Controller의 추상성 0 + Controller의 불안정성 1 - 1| = 0
0은 주계열 바로 위를 뜻하므로 좋습니다.
다시 위에서 말한 Repository를 계산해보면 위에서 Repository의 불안정성을 0(안정적)이라 했고, Repository는 항상 인터페이스를 구현하도록 했으므로 추상성은 1이 됩니다. 따라서,
Repository의 주계열로부터의 거리 = |Repository의 추상성 1 + Repository의 불안정성 0 - 1| = 0
마찬가지로 0은 주계열 바로 위를 뜻하므로 좋습니다.
그렇다면 Service는 어떨까요? 바로 상황에 따라 조사를 해봐야 한다는 의미가 됩니다.
해당 서비스의 안정성과 추상성을 측정해보면서 주계열에 가까운지를 지속적으로 살펴보면 됩니다. 하지만 몇가지 예를 살펴보도록 하겠습니다.
대체로 우리가 웹서비스를 만들 때 Email 발송은 매우 많이 하게 됩니다. 최초의 시작은 SMTP 서버에 의존하겠지만 나중에는 DB에 메일 내용을 넣고 배치작업으로 메일을 보내거나 아니면 MQ를 통해 메일 내용을 전달하고 다른 프로그램에서 실제 메일 발송을 하게 하는 경우도 있는등 변동성이 매우 큰 것이 Email 발송입니다.
EmailService의 불안정성 = 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수(SMTP 의존 1) / (이 패키지에 의존하는 외부 클래스의 수 100여개의 이메일 호출 + 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수 SMTP 의존 1) = 1 / (100 + 1) = 0.009
불안정성 0.009라는 것은 정말 매우 안정적임을 의미합니다.
매우 안정적인데 추상성이 0이라면
주계열로부터의 거리 = |추상성 0 + 불안정성 0.009 - 1| = |-0.991| = 0.991
주계열로부터의 거리 0.991는 거의 1에 가깝다는 뜻으로 매우 멉니다. 문제가 있지요.
따라서 추상성을 1로 높이는 것이 좋습니다. 즉, 인터페이스를 구현해야 합니다.
EmailService를 인터페이스로 구현했다면 추상성이 1이 되어
|1 + 0.009 - 1| = 0.009
주계열로부터의 거리 0.009는 주계열에 매우 가깝다는 의미이므로 매우 좋습니다.
Repository 5개를 호출해 그 결과를 조합해서 다른 결과를 도출하고, Controller 한 개에서만 사용되는 Service가 있다고 할 때 이를 일단 BusinessService라고 해보지요.
BusinessService의 불안정성 = 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수 Repository 의존 5 / (이 패키지에 의존하는 외부 클래스의 수 1개의 컨트롤러가 호출 + 패키지 외부 클래스에 의존하는 패키지 내부 클래스의 수 Repository 의존 5) = 5 / (1 + 5) = 0.83
불안정성 0.83은 매우 불안정함을 뜻합니다.
이 매우 불안정한 클래스를 추상적으로 만드는 것은 계산해보지 않아도 크게 의미가 없어 보입니다.
하지만 상황은 변하게 마련입니다. 시간이 지남에 따라 안정성이 변하고 그에 따라 필요한 추상성도 변할 수 있습니다.
그리고 원칙적으로 이 계산은 클래스 단위 보다는 패키지 단위입니다.
어떤 패키지 군에 속하느냐에 따라 어떤 Service는 추상적이어야 할 수도 있고 구체적이어야 할 수도 있습니다.
이 책에서는 추상 클래스란 하나 이상의 순수한 인터페이스를 인스턴스화 할 수 없는 가진 클래스라고 합니다.
하지만 저는 여기에 또다른 한가지 규칙이 더 내포돼 있다고 생각합니다.
“인터페이스의 메소드 시그너쳐에 구현에 대한 정보가 없어야 한다” 라는 규칙입니다.
다시 EmailService로 돌아가서 아무리 EmailService 인터페이스를 만들고 그 구현을 EmailSerivceSmtpImpl 클래스로 만들었다해도 메소드 시그너쳐가 다음과 같다면 무의미합니다.
위 코드를 보면 인터페이스 메소드 시그너처에 String smtpHost, String username, String password가 포함돼 있습니다. 이 파라미터들은 Email 발송을 SMTP로 구현한다고 가정할 때만 의미 있는 값들입니다.
인터페이스에서 그 구현이 SMTP 서버를 통해서 이메일을 발송해야 한다고 제약하고 있는 것입니다.
실제 이메일을 보내는 측에서는 그 구현이 SMTP이건 MQ로 다른 EMail 전송 서버에 쏴주건 DB에 메일 발송 관련 정보를 저장하건 상관이 없습니다. 호출자측에서 중요한 것은 그 뒤에 있는 발송자 이메일, 수신자 이메일, 제목, 내용 이것 뿐입니다.
따라서 Email을 발송하는게 목적인 인터페이스의 메소드 시그너쳐는 아래와 같이 구현의 상세에 대한 정보를 가지고 있어서는 안 됩니다.
일단 두가지로 구분해 볼 수 있어보입니다.
Email 발송, Push, SMS 발송, Logging 등등 인프라성 코드는 거의 불안정성이 0에 가깝습니다(안정적). 호출자는 매우 많은데 그 자신이 의존하는 것은 별로 많지 않은 경우가 많습니다. 그러면서도 그 구현체는 시스템의 성장에 따라 바뀌기 쉽습니다(Email의 경우 SMTP → DB → MQ). 안정성이 높으면 변경 대응을 위해 추상적이어야 합니다.
따라서 인프라성 Service는 거의 무조건 interface를 구현해야 하며, 위에서 제가 말한 추상성의 의미 - 인터페이스의 메소드 시그너처에 구현에 구현의 상세를 포함하지 말 것 -을 지켜야 합니다.
Repository Layer도 마찬가지 입니다. 저는 과거에 “세상에 DB를 바꿀일이 뭐가 있다고 Repository를 인터페이스로 만들어?”라고 생각했었습니다. 하지만 지속적으로 성장하는 서비스를 맡아하면서 여러번 DB를 다른 DB 시스템 혹은 NoSQL 등으로 변경합니다. 그 뿐만 아니라 Persistence Framework도 바뀔 수 있습니다. 저는 얼마전까지 iBatis를 사용하던 프로젝트들을 JPA, QueryDSL, jOOQ 기반으로 변경하는 작업을 하기도 하였습니다. 이렇게 안정성이 높으면서 그 변경도 잦을 수 있는 Repository는 철저하게 추상적으로 만들기를 권합니다.
비즈니스 로직 서비스는 상황을 지켜봐야 할 것으로 생각됩니다. 처음부터 무작정 인터페이스를 만들면 복잡도만 높고 개발 효율은 떨어질 수 있습니다(쓸모없는 지역).
비즈니스의 성장에 따라 고통스러운 순간이 오거나 혹은 측정에 의해 인터페이스로 분리하는 작업을 해도 될 것 같습니다.
밥 아저씨(Robert C. Martin)는 다음과 같이 이 장을 마무리하십니다.
“측정값은 신이 아니다. 그것은 단지 임의로 만든 어떤 기준에 따라 측정해본 값일 뿐이다. 이 장에서 선택한 기준이 특정한 애플리케이션에만 적합하고, 다른 것에는 적합하지 않은 일도 분명히 일어날 수 있다.”
또한 이 글은 간략화를 위해 Controller, Service, Repository 계층을 빗대어 예로 들었지만, 코드 계층에 이 세가지만 있는 것은 결코 아닙니다. 기본적으로는 interface를 만드느냐 안마드느냐는  Controller/Service/Repository 계층화 개발과는 별개의 문제입니다.
또한 interface 구현의 필요성을 여기 나온 수식으로만 판단해서는 안 됩니다. 의존성 역전 원칙 등에 따라서 다각도로 고민이 필요합니다.
그리고 이 책의 내용은 기본적으로 패키지를 기준으로 하지만 저는 클래스 단위와 패키지 단위를 혼재해서 사용하였습니다. 책의 내용 전체를 정리한 것이 아닙니다. 빠진 부분도 있으니 책을 읽어주세요.
긴 글 읽어주셔서 감사합니다.
"
http://woowabros.github.io/study/2018/03/01/spock-test.html,2018-03-01,Spock으로 테스트코드를 짜보자,"Spock으로 테스트코드를 작성한 경험을 공유합니다.
안녕하세요! 우아한형제들 배달의민족/배민라이더스 주문시스템 팀 정용준입니다.
여러분은 어떻게 테스트 코드를 작성하고 계신가요? 일정 지키기도 힘든데 무슨 테스트코드냐 하시는 분들도 계신가요?
저도 아직 테스트코드를 작성하는 것이 습관화되어 있지 않고 익숙하지 않습니다. TDD는 먼 나라 이야기이고 무엇을 어떻게 테스트해야 하는지를 정하기조차 어렵기도 하지만 조금씩 빈도를 늘려가고 있습니다.
저는 JUnit 기반의 테스트코드를 작성해왔습니다.
그러던 중 사내에서 Spock의 사용빈도가 높아지면서 좋은 점을 어필해주시는 주변 동료들 덕분에 Spock을 알게 되었는데요.
그동안 테스트코드를 작성하기도 쉽지 않을뿐더러 새로운 배움에 대한 부담감에 (일정도 빡빡하고 하다 보니) Spock에 도전해 볼 마음이 들지 않았습니다. 그런데 막상 테스트를 작성하다 보니 생각보다 간단했고 아직 Spock을 잘 모르시는 분과 함께 경험을 나누면 좋겠다고 생각이 들었습니다.
저도 이제 막 시작하는 단계이다 보니 부족한 점이 많습니다.
틀린 부분은 가감 없이 지적해주시고, 빈약한 내용은 열심히 공부해서 꼭 다음에 풍성히 채워오겠습니다.
먼저 Spock을 사용하기 위해서 아래 두 의존성을 추가해야 합니다. (Gradle 기준)
이제, Spock으로 테스트를 작성할 수가 있습니다.

Spock은 Groovy를 사용합니다. 사실 Groovy를 잘 알지 못한 점도 하나의 허들이었습니다.
이번 기회에 Groovy도 같이 공부할 수 있으니 좋네요 :)
(초록막대가 나오길 두근두근..) 

딸랑, 3줄 작성한 건데. (좀 당황스러웠습니다. ㅎㅎ)
저는 JUnit 테스트를 작성할 때 //given //when //then을 잊지 않기 위해 테스트 템플릿을 만들어 사용하고 있습니다.
이번에도 습관적으로 주석으로 명시하고 테스트를 돌려봤습니다. (나름 잘했다고 생각하고 있었는데..)
코드 블록을 선언하지 않은 것이 테스트가 깨지는 원인이었습니다.
Spock에서는 given, when, then과 같은 코드 블록을 block이라 부릅니다.
block은 테스트 메소드 (feature method) 내 최소한 하나는 있어야 하고요!
JUnit에서는 있어도 그만 없어도 그만이었는데 Spock에서는 필수입니다. (개인적으로는 더 좋네요!)

[원본 출처] https://code.google.com/archive/p/spock/wikis/SpockBasics

코드블록을 선언하고 나니 잘 수행이 되네요 :)
사실, where block을 처음 보고나서 Spock을 써봐야겠다고 생각했습니다.
테스트를 하다보면 다양한 케이스를 검증해야 할 때가 많으시죠?

where block을 사용하면 간단하게 해결할 수 있습니다.
처음 보는 생소한 기능이지만 코드를 따로 설명해 드리지 않아도 이해하는데 조금도 어려움이 없으실 거에요.
만약, JUnit 기반의 테스트코드를 작성했다면 어땠을까요? 

의미없는 중복된 코드가 여기저기 널부러져 있었을 것 같습니다. 
여러분은 이런 경우 어떻게 테스트코드를 작성하시나요?
그리고 테스트가 실패되는 경우 JUnit은 제일 처음 실패한 케이스만 알 수 있다면,
Spock은 실패한 모든 테스트 케이스와 그 내용을 더 상세히 알려줍니다.

0보다 작은 음수가 들어왔을 때 ‘예외’가 발생하는지를 테스트 해보겠습니다.

Spock에서 예외는 thrown() 메서드로 검증할 수 있습니다.
thrown() 메서드는 발생한 예외를 확인할 수 있을 뿐만 아니라 객체를 반환하기 때문에 예외에 따른 메시지도 검증을 할 수 있습니다.
그리고 테스트코드를 작성한 흐름에 따라 예외를 확인할 수 있으니, 처음 코드를 본 사람이 더 쉽게 이해할 수 있을 것 같습니다 :)

Spock에서 Mock 테스트도 어렵지 않습니다.
가짜 객체의 반환 값은 ‘»‘으로 설정할 수 있고 예외를 발생시키고 싶다면 아래와 같이 하시면 됩니다.
Spock으로 테스트코드를 작성하는 것 생각보다 어렵지 않은 것 같습니다.
아직은 작은 단위의 기능을 검증하는 정도로 적용하고 있는데요,
기존에 JUnit으로 작성한 테스트코드를 Spock으로 구현해봐야겠습니다.
다음번에는 좀 더 깊이 있는 내용으로 찾아뵙겠습니다.
"
http://woowabros.github.io/study/2018/02/26/mysql-char-comparison.html,2018-02-26,MySQL에서 'a' = 'a '가 true로 평가된다?,"DB 알못의 어떤 리서치
안녕하세요 기계인간 이종립입니다. FC플랫폼개발팀에서 배민찬 백엔드를 개발하고 있습니다.
DB알못인 저는 업무 중에 우연히 MySQL에서 'a' = 'a '의 결과가 1로 나오는 이상한 현상을 발견하게 되었습니다.
특수한 조건에서만 발생하는 버그일까요?
또는 버그가 아닌데 내가 잘못 생각하고 있는 것인지, 다른 DB에서도 같은 일이 일어나는지 궁금해졌습니다.
그래서 다른 DB도 조사해보기로 했습니다.
차이점을 발견할 수 있다면 알아둘 만한 정보가 될 거라는 생각도 들었고요.
종합해보니 다음과 같은 결과를 얻을 수 있었습니다.
제가 조사한 결과 중에서는 PostgreSQL 과 SQLite 만 'a'와 'a '를 다른 값으로 평가하는군요.
여러 DB를 확인했더니 그렇게 돌아가더라 하고 끝나면 안 되겠죠?
레퍼런스 문서를 찾아보기로 했습니다.
MySQL 5.6 Reference Manual을 확인해보니 버그가 아닙니다.

이럴 수가……. DB알못은 그저 혼란스러울 뿐입니다.
혼란스럽던 차에 마침 개발실 서가에 가보니 “SQL 전문가 가이드”라는 책이 꽂혀 있습니다.
표지에 “국가공인 2013 Edition” 이라 인쇄되어 있으니 일단 국가를 믿고 참고해 보도록 합니다.
한참 책을 뒤지다 보니 159쪽에서 CHAR를 비교하는 일에 대한 내용을 발견할 수 있었습니다.
CHAR에서는 문자열을 비교할 때 공백(BLANK)을 채워서 비교하는 방법을 사용한다. 공백 채우기 비교에서는 우선 짧은 쪽의 끝에 공백을 추가하여 2개의 데이터가 같은 길이가 되도록 한다. 그리고 앞에서부터 한 문자씩 비교한다. 그렇기 때문에 끝의 공백만 다른 문자열은 같다고 판단된다. 그에 반해 VARCHAR 유형에서는 맨 처음부터 한 문자씩 비교하고 공백도 하나의 문자로 취급하므로 끝의 공백이 다르면 다른 문자로 판단한다.
오호 드디어 알겠습니다.
비교하려는 두 문자열의 길이가 다른 경우, 짧은 쪽에 공백을 이어붙여 길이를 똑같이 만든 다음 비교하기 때문에 발생하는 일이었습니다.
이런 식이라면 'a'와 'a  '를 비교해도 'a '와 'a '를 비교하는 것과 똑같을 수밖에 없겠네요.
그런데 왜 이렇게 하는 것일까요? 그냥 길이가 다르면 FALSE 라고 하면 안 되는 이유라도 있는 것일까요?
MySQL 때문에 시작한 일이었으니 일단 MySQL 레퍼런스 문서를 좀 더 찾아봤습니다.
다음 문장이 눈에 띄는군요.
When CHAR values are stored, they are right-padded with spaces to the specified length.
When CHAR values are retrieved, trailing spaces are removed unless the PAD_CHAR_TO_FULL_LENGTH SQL mode is enabled.
그리고 여기에서도 같은 문제를 언급하는 예제가 있습니다.

다음과 같은 테이블도 있네요.

CHAR(4) 컬럼에 'ab'를 저장하면 'ab__'처럼 된다는 말이군요. (이제부터는 가독성을 위해 공백 대신 _ 를 쓰겠습니다.)
CHAR는 VARCHAR 처럼 가변 길이가 아니기 때문에, 길이를 맞추기 위해 컬럼 정의에 따라 우측에 공백이 추가되어 보관됩니다.
그렇다면 CHAR(4)에 저장한 'ab'와 CHAR(6)에 저장한 'ab'를 정확히 비교하려면 두 가지 방법이 있을 것입니다.
만약 이런 방법을 사용하지 않는다면 'ab__'와 'ab____'를 비교하는 셈이 됩니다.
즉, 저장하기 전엔 'ab'로 똑같았던 값을 저장된 값을 가져와 비교하면 무조건 FALSE가 나오는 황당한 일이 벌어집니다.
좀 이상하게 느껴지긴 해도 짧은 쪽에 PAD를 추가해 'ab____'로 바꾼 다음 비교하는 것이 납득이 갑니다.
이렇게 하지 않으면 길이가 다른 모든 타입의 컬럼에 저장된 문자열을 비교하는 것이 불가능할 것입니다.
FALSE만 나오게 되겠죠.
짧은 쪽에 맞추기 위해 긴 쪽의 문자를 지우면 알고리즘이 지저분할 테니, 공백 PAD를 붙이는 심플한 방법 쪽을 선택했을 거라는 생각도 듭니다.
CHAR 문자열을 저장하는 방식 때문에 PAD를 사용한 비교가 도입된 것이라는 추측이 드는군요.
그러나 멋대로 결론 내리기 전에 먼저 문자열 저장과 비교에 대한 표준을 찾아보는 것이 순서일 것 같습니다.
SQL 표준을 명시하는 SQL-92에서 space로 검색해서 하나하나 찾다 보니 234쪽에서 다음과 같은 문단을 찾을 수 있었습니다.
데이터를 저장할 때의 일반 규칙 중 하나입니다.
이게 한 문장이라니……. 침착하게 읽고 정리해보니 다음과 같은 내용이었습니다.
만약 문자열이 정의된 길이보다 짧을 경우 오른쪽을 space로 채운다는 말이로군요. 이 방식이 표준이 맞네요.
그렇다면 이번에는 비교에 대한 표준을 찾아볼 차례인 것 같습니다.
위에서 조사했을 때에는 Oracle, MySQL, SQL Server 와 PostgreSQL, SQLite의 결과가 모두 같지는 않았습니다.
문자열 비교에 대해 표준을 지키지 않는 DB가 있다는 말일까요?
역시 SQL-92에서 comparison으로 일일이 찾아보니 208쪽에서 찾아냈습니다.
찬찬히 읽어보니 다음과 같은 정보를 알 수 있었습니다.
아하. 국가공인 책에 나왔던 'a' = 'a '를 'a ' = 'a '로 만들어 비교하는 방식이 표준이 맞는군요.
그리고 effectively라는 표현은 아마도 비교 알고리즘의 효율성이 아니라 각기 길이가 다른 여러 타입을 비교하는 관점에서의 효율을 말하는 것 같습니다.
만약 알고리즘상의 효율이라면 PADDING을 하지 않고 그냥 왼쪽 글자부터 비교했을 테니까요.
그렇다면 이제 PostgreSQL 과 SQLite 에서 'a' = 'a ' 가 FALSE로 평가되도록 구현한 이유가 궁금해지는군요.
그리고 PostgreSQL이 CHAR를 저장하는 방식도 뭔가 표준과 다를 수 있다는 생각이 듭니다.
그런데 아무래도 저 혼자서만 궁금증을 느낀 것은 아닌 모양입니다.
운이 좋았는지 Albe Laurenz라는 분이 2013년 01월 17일에
PostgreSQL 개발자들에게 보낸 이메일을 발견할 수 있었습니다.
편지의 내용을 요약하자면 다음과 같습니다. (이메일도 편지니까 편지라고 하고 싶습니다.)
오 제 궁금증과 일치하는 질문이네요.
그렇다면 이 편지에 대한 PostgreSQL 측의 답장도 읽어봐야 할 것 같습니다.
답장을 한 사람은 컴퓨터 과학자이자 PostgreSQL 개발팀의 멤버인 Tom Lane입니다.
Tom Lane의 편지의 내용을 요약하자면 다음과 같습니다.
영어 실력이 딸려서 더 어렵게 느껴집니다. 그러나 적어도 표준이나 아니냐의 이분법적인 관점으로 접근하는 것이 곤란하다는 생각이 드네요.
그리고 시스템의 측면에서 문자를 바라보는 시각도 제시해주는군요.
아마도 이런 생각이 PostgreSQL의 비교 방식에도 영향을 준 모양입니다.
그런데 아직 저장 방식과 PAD / NO PAD의 관계에 대해 아직 뚜렷하게 이해하지 못한 것 같은 느낌이 듭니다.
그래서 일단 다음과 같이 간단한 테이블을 MySQL과 PostgreSQL에 하나씩 만들고 레코드 하나를 입력했습니다.
참고로 vc2와 c2는 'test__'로 같은 입력값이며, 오른쪽 공백이 두 개입니다.
그리고 다음과 같이 테스트를 수행해 보았습니다.

테스트 1.1의 결과를 보면 MySQL과 PostgreSQL의 결과가 다릅니다.
또한, 테스트1.2의 결과를 보면
한편 테스트 1.3의 결과를 보면 VARCHAR는 입력값을 그대로 저장하고 있습니다.
VARCHAR와 CHAR를 비교해보면 의미 있는 결과를 얻을 수 있을 것 같네요.
이 테스트 결과를 보니 Tom Lane의 말이 좀 더 와닿는군요.
물론 PostgreSQL은 저장할 때 오른쪽 공백을 없애고, MySQL은 저장할 때 오른쪽 공백을 추가했겠지만, 결과는 똑같습니다.
다음 테스트는 VARCHAR와 타입이 정의되지 않은 CHAR의 비교입니다.

테스트 3.2를 보면
테스트 3.1과 3.3은 위에서 수행했던 테스트를 다시 한 것입니다.
테스트 3.2와 3.4가 공백을 이어붙이는 경우인데, MySQL에서도 이어 붙이는 경우에 대해서는 PADDING을 하지 않는군요.
어쩌다보니 MySQL과 PostgreSQL의 비교로 끝이 났습니다.
그러나 어느쪽이 더 표준을 잘 지키고 어느 쪽이 더 바람직하다는 결론은 아닙니다.
다만, 개인적으로는 PostgreSQL이 문자열 비교에 대해 제가 갖고 있던 상식과 일치하는 느낌이라 종전보다 강한 호감을 갖게 되었습니다.
개념의 혼동을 최소화할 수 있는 선택을 선호하기 때문입니다.
사실 이 글은 제가 2016년 6월에 우아한형제들 사내 위키에 쓰다 만 글을 토대로 새로 작성한 것입니다.
원본은 각 DB 비교하는 부분까지 있었으니 분량이 3배 이상 늘었네요.
모든 테스트는 2018년 2월 25일에 다시 수행하였으며, 2016년의 DB2 테스트 결과는 삭제하였습니다.
예전에 DB2를 테스트했던 웹 사이트에서 DB2를 선택하는 옵션이 사라졌기 때문입니다.
대신, 2016년에 “다음에 알아보기로” 했었던 PostgreSQL의 이야기를 조사할 수 있어서 매우 기쁘고 재미있었습니다.
긴 글 읽어주셔서 감사합니다.
의견과 격려를 주신 최광훈님, 최윤석님, 김정환님께도 감사를 드립니다.
그리고 마지막으로, 이렇게 어려운 DB를 관리해주시는 세상의 모든 DBA님들께 존경과 감사를 드립니다.
EOB
"
http://woowabros.github.io/security/2018/02/23/aws-auto-security1.html,2018-02-23,AWS에서 네트워크 공격 자동차단 하기,"최근 Public cloud 시장이 활성화 되면서 많은 기업들이 Cloud 도입을 시작하고 있으며, 한국의 경우 대표적으로 Amazon Web Service(AWS)를 사용하고 있다. 그리고 한국의 Seoul region(ap-northeast2)은 2017년 12월 기준으로 AWS Global 매출 TOP 5에 포함된 만큼 폭팔적으로 사용량이 늘어나고 있다. 그러나 Legacy환경의 IDC과 너무도 다른 환경에서의 보안은 쉽지 않기때문에 본 글에서는 우아한형제들의 AWS에서 기술적인 보안 방법을 공유 하는것을 목적으로 하고, 시리즈로 연재 하고자 한다.
AWS에서 네트워크 보안은 몇가지 어려움이 있는데, 그 중에서 SecurityGroup은 White-list 기반 정책만 가능하기때문에 특정 EC2로 네트워크 공격이 들어오는 경우 Black-list로 차단 할 수가 없다. 차단 하려면 윈도우 방화벽이나, iptables를 이용해야 하는데 그렇게 하려면 scale-in, out이 빈번하게 일어나는 환경에서 매번 ec2 한대씩 터미널로 접속을 해야하는 엄청난 수고를 해야한다.그래서 VPC의 subnet에 있는 NACL을 이용을 권장하며, NACL은 subnet 단위로 L4 기반 차단을 할 수가 있다.
그.러.나 GuardDuty에서 탐지되는 수백개의 이벤트를 하나씩 확인해서, 공격 대상이 되고 있는 ec2들이 사용하는 subnet을 확인하고, NACL을 입력해주는것은 수백마리의 벌레들을 한마리씩 잡는것과 같다. 그정도로 많은 이벤트가 발생할까? 라는 생각을 할 수도 있는데, AWS는 AWS에서 사용되는 IP주소 목록을 Public하게 공개 해뒀고, 그 목록 안에서 사용자들에게 할당/회수를 반복적으로 하기때문에 IP주소 목록을 기반으로 많은 공격들이 들어온다.



(그나마 몇개 잡으면 다른 것들이 벌때처럼 밀려들어온다..)

나는 고민끝에 GuardDuty에서 탐지되는 네트워크 공격들의 대상 정보를 자동으로 Pasring 하고, 대상 정보를 바탕으로 NACL에 차단 Rule을 자동으로 추가 및 차단 로그를 남기고, 지정된 시간이 경과하면 자동으로 차단 해제하는 자동화된 초동 대응용 모델을 AWS API들을 이용하여 디자인하였다.






GuardDuty에서 탐지 이벤트 발생을 CloudWatch Event의 trigger 조건으로 설정해서, 이벤트가 발생하면 탐지 데이터를 특정 Lambda로 넘겨주고 Parsing 과정을 거친 다음 NACL에 차단 Rule을 추가하는 순서로 진행된다. 때문에 탐지된 데이터를 적절히 Parsing 해야, NACL에 차단 Rule을 추가/삭제를 용이 하게 할 수 있기때문에 아래와 같이, 먼저 샘플용 탐지 데이터들을 생성하여 참고하는것이 좋다.
탐지 이벤트 발생과 동시에, CloudWatch Event에서 Lambda를 호출하게 되는데 Parsing 및 NACL 차단 Rule 추가시 주의해야할 사항은 아래와 같다.
GuardDuty에서 탐지 이벤트가 발생되면, Lambda를 실행 하도록 trigger 설정을 CloudWatch Event에서 해줘야 하며, 모든 공격에 반응하기 보다는 serverity가 5이상의 수준에만 반응 하도록 설정 해주는것이 워크로드 증가를 줄일수 있기 때문에 권장한다.



차단된 로그들은 s3에 저장하고, 저장된 로그들을 위의 그림처럼 ElasticSearch로 가져와서 차단 추이 분석이나, 다른 AWS Account별로 공유하여 IP Black-list를 생성 할 수 있다.
나는 ElasticSearch를 이용하여 차단 로그들을 관리하고 NACL을 조절함으로써 자동차단 도입전 대비 공격 시도를 약 80% 줄였다.
Public cloud는 flexibility and scalability가 최고의 장점이다. 그런데, 보안하는 입장에서는 신경 써야할것이 엄청나게 많다. 예전에는 IDC환경에서 장비들 박아놓고 관리만 하면 되었지만, 지금은 늘어났다 줄어났다도 부족해서 여기갔다 저기갔다 하는 IT자산들에 맞춰서 움직이고, 구성을 변경하고, 보안 범위를 늘였다 줄였다 해야한다.이런 환경에서 사람이 수동으로 작업 한다는것은 미친짓이다. 때문에 이런 환경에 빠르게 대응 하려면 자동화는 필수이다. Public cloud로 이전을 고려하고 있는 회사의 보안 관리자라면 이러한 Security orchestration and automation를 반드시 고민해야 할것이다.
"
http://woowabros.github.io/experience/2018/02/22/usability-testing-in-baeminchan.html,2018-02-22,내 안의 가짜를 부수고 진짜 사용자를 만나는 방법,"9월 11일, 공식적으로 배민프레시는 배민찬으로 브랜드명을 바꿨습니다. 신선 커머스에서 반찬 플랫폼으로 변화를 시작합니다.
..그리고 서비스를 만드는 사람으로서 많은 고민이 시작됐습니다.
다른 동료들과 배민찬, 서비스에 대해 깊은 이야기를 나누고 싶었습니다.
그러나 업무 중 남는 시간을 활용해 깊이 있는 이야기를 해야 하니 아쉬움이 많았습니다. 그리고 다른 분들도 같은 고민을 하고 있다는 확신이 들었습니다. 차라리 정기적으로 서비스에 대해 얘기를 하는 시간을 가졌으면 좋겠다는 생각에 주변 기획자, 개발자, 디자이너를 모아 수다 모임을 시작하게 됐습니다.
모임 시작 후, 서비스에 대해 많은 얘기를 하면서 느낀 부분은 다음과 같았습니다.
모임 내에서도 저를 포함해 배민찬 서비스를 맡게 되면서 배민찬을 알게 된 분들이 많았기 때문에 배민찬을 사용하는 것에 대해 막연히 좋을 것 같다는 느낌은 있지만 반대로 공감하기 어려운 부분도 있었습니다. 서비스로 시작한 수다는 자연스럽게 사용자로 흘러갔고, 진짜 배민찬 사용자를 이해하기 위해 무엇을 할 수 있을지에 대한 토론을 하게 됐습니다. 그 결과 사용자에 대해 어떻게 더 깊게 알 수 있는지를 먼저 알아야 된다는 공감대가 만들어졌습니다.
그렇게 UX 스터디를 시작하게 됐습니다.
제목이 끌려 (사용자를) 생각하게 하지 마!1 란 책을 보게 됐습니다. 얇고 내용도 실용적인 거 같아서 스터디 할 책으로 선정했습니다. 수다 모임은 챕터를 하나 읽고 매주 한 번씩 점심시간에 모여 토론하는 스터디로 방향을 바꿨고 그래서 ‘수다 모임’에 ‘UX 스터디’를 붙여 UX 스터디 같은 수다 모임이 됐습니다.
줄여서 ‘사생하’라고 불렀습니다
사실 아직도 UX가 뭔지에 대해 정의해보라면 막연합니다. 하지만 스터디 전에 UI가 안 좋아 보이거나 사용하기 불편한 걸 UX가 좋지 않다고 말하던 것보다는 좀 더 깊은 이해가 생겨났다고 생각합니다.
제가 이해한 UX란, 이해관계자 입장에서 사용자에게 가치를 전달하는 과정과 사용자 입장에서 서비스를 사용하면서 느끼는 것이 복합된 형태라고 생각합니다.
UX = 사용자에게 가치를 전달하는 과정 + 사용자가 사용하면서 느끼는 것
그래서 대부분의 UX 방법론은 서비스에 대해 구체화하는 방법과 서비스를 사용하는 사용자에 대해 알아가는 방법 그리고 그렇게 만들어진 서비스를 평가 및 개선하는 방법을 모두 아우르고 있습니다. UX는 서비스 전체를 포괄하는 면도 있어서 한마디로 정의하기가 어렵다고 생각합니다.
따라서 UX가 좋다는 말은 “사용자를 깊게 이해하고 다른 서비스와 차별되는 특별한 가치를 잘 전달하는 것”이라 이야기할 수 있지 않을까 생각해봅니다.
처음에는 이게 정말 책에서 소개하는 것처럼 꼭 필요한 것일까 하는 의문을 가졌습니다. 하지만 스터디를 해오면서 공감 가는 부분이 정말 많았으므로 사용성 평가 데모 영상을 같이 감상해봤습니다. 24분짜리 사용성 평가 데모 영상2이고, 내용은 사용자를 한 분 초대해 렌터카 웹사이트를 사용해보게끔 하는 겁니다. 이 영상을 보면서 느낀 점은 한마디로 답답하다였습니다. 웹사이트는 왜 저렇게 불편하게 만들었으며, 사용자는 왜 저렇게 잘 사용 못 하는지 답답했습니다. 만드는 사람과 사용하는 사람의 관점 차이를 생각할 수 있는 기회가 되었습니다.
책에서 사용성 평가에 대해 계속 강조하는 부분은 쉽고, 누구나 할 수 있고, 심지어 셀프 평가도 가능하다는 점이었습니다.
슬슬 스터디 하면서 알게 된 것을 기반으로 뭔가를 해보고 싶었기 때문에 한번 실습해보자는 의견이 모였고 책에 나온 가이드를 따라 준비를 시작했습니다.
사용성 평가를 시작하기 전에 서비스에서 어떤 걸 평가하고 싶은지 먼저 정해야 합니다. 책에서 나온 가이드는 이렇습니다:
서비스에서 가장 중요한 기능을 몇 가지 추린 후에, 그 기능이 포함된 시나리오를 만들고, 이를 사용성 평가할 때 사용성 평가 참가자에게 과제로 내드리는 겁니다.
10분 정도 시간을 내서 배민찬에서 가장 중요한 기능에 대해 각자 다섯 가지씩 적어보기로 했습니다. 그리고 전부 취합해서 세 가지를 뽑아보니 이렇게 나왔습니다:
아무래도 커머스 서비스이다 보니 어느 정도는 예상된 결과였다고 생각되지만, 한편으로는 이렇게 합을 차근차근 맞춰본 게 계속 스터디를 진행하는 동기 부여도 되지 않았을까 생각합니다.
아무래도 주문하는 과정을 가장 중요하다고 생각했기 때문에 모든 시나리오에 주문하는 과정까지 포함시켰습니다. 그리고 상품 탐색 및 추천 기능에 대해서는 사용자가 마주할 만한 실제 상황을 가정해 시나리오를 작성 했습니다. 만들었던 시나리오는 다음과 같습니다:
지금 생각해보면 시나리오에 아쉬운 부분이 있지만 일단 해보는 데 의의를 두자는 생각으로 두려움을 쫓으면서 계속 진행을 했습니다. 시나리오에서 가장 중요한 부분은 사용성 평가에 참여한 참가자 입장에서 공감이 되는가 같습니다. 그래서 처음에는 시나리오를 지정해드렸지만, 나중에는 시나리오 중 하나를 선택해달라고 부탁드렸습니다.
사용성 평가에 대해 경험이 있던 분이 없었기 때문에 일단 1번 시나리오를 실제로 돌려보자는 생각으로 사내에 새로 오신 QA 담당자에게 사용성 평가 참여를 부탁드렸습니다. 진행 및 관찰을 하면서 느꼈던 건 “이거 조금 이상한데?” 였습니다. 당시 제가 이상하다고 느꼈던 부분을 몇 가지 꼽아보자면..
첫 사용성 평가가 끝난 후, 평가를 진행하셨던 분들과 얘기를 나눠봤습니다. 다들 저와 같이 의아해하는 부분들이 많았고 이거 제대로 해봐야겠다는 생각이 들어 본격적으로 사용성 평가 진행을 원활하게 하기 위한 준비를 시작했습니다.
이 부분 또한 책에 자세한 가이드가 나와 있습니다. 가이드를 참고해 사용성 평가라는 게 뭐고, 어떻게 진행되고, 참가자가 아니라 배민찬 서비스에 대해 평가하려고 한다는 것에 대해 친절하게 설명하는 대본을 작성했습니다. 첫 사용성 평가 후 받은 피드백 중에 참가자 정면에 앉아서 쳐다보면 불편하다는 얘기가 있어 정면은 제외하고 자리 선정을 했고, 책에서는 혼자서 진행할 것을 권장하지만 실제로 해보니 혼자서 진행하면서 기록을 동시에 하기가 쉽지 않아 2인 1조로 구성했습니다.
책에서 대본을 작성해서 그대로 읽으라고 가이드가 나와 있습니다. 실제로 해보면 아무래도 처음 보는 분에게 사용성 평가에 대해 설명을 해야 하기 때문에 꽤 긴장됩니다. 실제로 기록자일 때 진행하시는 분을 보고 있으면 긴장해서 대사를 빠뜨리기도 합니다. 하지만 경험이 쌓이며 이런 실수는 하지 않게 되었습니다.
2인 1조로 구성해 한 명은 사용성 평가 진행, 다른 한 명은 기록을 맡았습니다.
두 번째 사용성 평가 이후, 가입을 위한 새로운 시나리오를 만들었습니다. 결제를 위해 새로 가입을 하는 경우를 위한 것이었습니다. 그리고 시나리오별로 어떤 부분을 주로 관찰하고 기록할지에 대해 기록자용 템플릿도 만들었습니다.
참가자는 따로 연령층이나 타겟을 고려하지 않았지만, 결과적으로 다양한 연령과 환경을 가진 분들이 참여해 주셨습니다. 11월 초부터 12월 중순까지 1달 조금 넘는 기간 동안 사용성 평가에 총 12명 참가해 주셨고, 이중 회사 내부 7명, 외부 5명이었습니다.
사용성 평가를 진행하면서 참가자의 음성을 녹음하고 앱 사용 화면을 녹화했습니다. 앱 화면 녹화를 위해 Appsee3를 사용했는데 트라이얼 기간이 만료돼 나중엔 음성만 녹음했습니다.
영상이 남지 않는다고 해서 특별히 아쉽지는 않았습니다. 그 이유는 사용성 평가는 1년 또는 큰 업데이트가 있을 때만 한 번씩 하는 게 아니라 한 달에 한 번 정도 짧은 주기를 가지고 사용성 문제를 찾고, 찾은 부분을 고치고, 다시 사용성 평가를 진행하면서 다른 문제를 찾는 흐름을 가져야 한다고 생각하기 때문입니다.
서비스는 수시로 변하기 때문에 어떻게 녹화해서 오랫동안 보관할까가 아니라 어떻게 주기적으로 할 수 있을지에 대해 고민하는 게 맞을 것 같습니다. 그리고 다른 분들과는 단순히 녹화자료를 공유하는 것으로 끝나지 않고, 직접 사용성 평가에 같이 참여할 수 있는 방안에 대해 고민해 보면 좋겠다는 생각을 가지고 있습니다.
사용성 평가 후에는 시간을 내주신 것과 사용성 평가에 참여해주신 것에 대해 감사하는 의미로 운영비를 모아 커피 쿠폰을 하나씩 드렸습니다.
사용성 평가를 한 분 진행하고 나면 A4 1~2장 분량의 정리 결과와 약 40분 정도 녹음한 음성 파일이 남게 됩니다. 이렇게 12명 진행을 한 결과에 대해서 스터디 시간(점심시간을 이용한 1시간)에만 취합을 하기에는 너무 오래 걸릴 것 같았습니다. 머릿속에 사용성 평가를 했던 느낌이 강하게 남아있을 때 하는 게 좋을 것 같아서 한 주만 예외적으로 매일 스터디를 진행했습니다. 그리고 어떤 툴로 정리를 하느냐도 고민이 많이 됐었는데 처음엔 마인드 맵 프로그램이나 개인 일정 관리를 위해 쓰고 있는 트렐로 그리고 팀원이 추천한 PostgreSQL(…)로 정리를 하려고 시도했지만, 아무래도 스터디 구성원이 다 같이 하기에는 어렵다는 생각이 들어 디자이너의 의견을 받아 보드에 포스트잇으로 정리를 시작했습니다.
최종적으로 선택한 정리 방법은 포스트잇과 펜입니다.
사용성 평가 기록 정리는 화면 위주로 했습니다. 주로 특정 화면에서 참가자의 이야기와 행동을 듣고 관찰해 기록했기 때문입니다. 이렇게 정리를 하고 보니 보드 세 개로 정리할 수 있었습니다. 정리된 보드의 크기가 꽤 크고 점심시간마다 스터디를 진행하다보니 자연스럽게 다른 팀원들도 관심을 가져 주셨습니다. 그래서 종종 물어보시면 이것도 전파할 수 있는 기회라고 생각해 왜 이걸 하게 됐는지, 어떤 결과가 나왔는지에 대해서 공유를 했습니다.
이렇게 보드로 정리한 후에는 팀원뿐만 아니라 배민찬 부문 내의 다른 직군에서 일하시는 분들도 초대해 같이 얘기를 해보는 시간도 가져봤습니다. 그 전에는 얼굴만 알고 가벼운 인사만 하는 분들이었지만 실제로 배민찬을 사용하면서 나온 피드백을 가지고 얘기를 진행해보니, 서로 서비스에 대해 많은 이야기를 어렵지 않게 할 수 있었습니다. 이를 통해 서로 업무 요청을 하는 사람, 요청을 받아 개발을 하는 사람이 아니라 배민찬 서비스에 대해 같이 고민하고 더 나은 방안을 찾아가는 사람으로 서로 인식할 수 있게 된 좋은 계기가 됐다고 생각합니다.
스터디 구성원 외에 팀에 공유할 때는 원래 보드를 놓고 둘러앉아 얘기하는 식으로 진행을 하려고 했습니다. 하지만 사용성 평가 자체가 생소한 분들도 많았기 때문에 사용성 평가가 뭔지부터 차근차근 밟아 가는 게 좋겠다고 생각했습니다. 그래서 사용성 평가 개념부터 결과까지 정리해 공유 준비를 시작했습니다. 장표에 사용성 평가 참가자의 의견들을 다 담을 수는 없었기 때문에 어떻게 장표를 만드는가도 도전적인 과제였습니다. 원래 12월 말에 공유하려고 했지만 결국 한 달 뒤인 1월 말에 공유할 수 있었습니다.
배민찬 개발/기획팀 대상으로 공유했습니다. 그리고 위/아래 다른 체크무늬 남방입니다.
솔직히 말해보자면, 개발자가 이런 걸 하는 게 맞는지 동료 개발자가 어떻게 봐줄지에 대해서도 고민이 됐던 게 사실입니다. 그래서 공유할 때도 걱정이 많이 됐었는데 다행히 현장 분위기는 매우 좋았습니다. 그리고 질문도 많이 해주셔서 더 좋은 시간이었다고 생각합니다.
사용자와의 접점에 있는 개발자로서, 자주 사용자 입장에서라는 말을 하게 됩니다. 내가 말하는 사용자가 진짜 우리 서비스를 사용하는 사용자가 되려면 우리가 기술을 공부하듯이 사용자에 대해서도 많이 알려고 노력하고 공부를 해야 하지 않을까요? 내 안에 있는 어쩌면 개인의 취향이나 욕망이 반영된 가짜 사용자를 부수고, 우리 서비스를 사용하고 있는 진짜 사용자에 대해서 알아갈 수 있는 방법으로 사용성 평가는 분명히 좋은 시작점이 될 수 있을 것입니다.
이 글을 작성하는 데 많은 도움을 주신 UX 스터디 같은 수다 모임의 구성원, 참여해주신 모든 참가자, 교정 및 편집을 도와주신 동료 종립님, 도연님 그리고 영감을 준 우에다 마리에, 젤다의 전설에 소소한 감사를 전합니다.
이 책은 UX라는 단어가 보편적으로 사용되기 전에 나온 책입니다. 다른 UX 책과는 다르게 UX에 대한 개념이나 설명을 장황하게 풀어놓지 않고, 저자의 많은 경험을 토대로 진짜 사용자가 하는 행동을 바탕으로 보편적으로 개선해야 하는 점들을 담고 있습니다. 웹사이트 기반으로 작성된 책이지만 사용성을 개선하기 위한 많은 인사이트를 얻을 수 있었습니다. 참고로 개정판엔 모바일 앱 챕터도 추가됐습니다. ↩
https://www.youtube.com/watch?v=QckIzHC99Xc ↩
https://www.appsee.com/ ↩
"
http://woowabros.github.io/tools/2017/12/13/prestoquery.html,2017-12-13,Presto 쿼리 실행계획 겉핥기,"우리는 여러 가지 이유로, 여러 가지 용도에 사용하기 위해 데이터를 조회합니다. 많은 경우 SQL기반의 데이터 처리 엔진에 SQL 을 사용해서 데이터를 조회하게 됩니다. 이 때, 기본적으로 문법에 맞춰서 데이터를 조회하면 데이터가 잘못 나올 일은 극히 드뭅니다. 하지만 간혹 생각과 다른 데이터가 나온다거나, 잘 돌아가는 지를 확인하고 싶은데 쿼리가 무거울 것 같아서 무조건 돌려보기 애매하다거나, 별 것 아닌 쿼리라고 생각했는데 데이터 조회가 오래 걸리거나 하는 일이 발생합니다. 정말 어쩔 수 없는 경우도 있지만, 상당수의 경우는 쿼리를 좀 더 예쁘게 짜면 전반적인 쿼리 성능을 높일 수 있습니다. 이를 위한 작업을 흔히 ‘쿼리 튜닝’이라고 합니다.
이런 쿼리 튜닝을 위해 우선적으로 선행되어야 할 작업은 쿼리가 어떻게 실행되는 지를 아는 것입니다. 대부분의 SQL처리 엔진에서는 SQL 문장을 받으면 이를 어떤 식으로 수행할 지에 대한 계획을 세우는데, 이를 ‘쿼리 실행 계획(Query Plan)’ 이라고 합니다. 그리고 대부분의 SQL처리 엔진에서는 이런 쿼리 실행 계획을 확인할 수 있습니다. 이를 확인할 때는 일반적으로 원하는 쿼리 문 앞에 EXPLAIN을 사용해서 다음과 같은 방식으로 나타냅니다.
EXPLAIN query_statement
그리고 이 EXPLAIN을 사용하는 방식은 Presto(프레스토)에서도 동일하게 사용 가능합니다. 하지만 프레스토의 경우 다양한 데이터 소스 위에서 동작하는 분산 처리 SQL엔진이므로 다른 데이터베이스에서의 실행 계획과 다소 다릅니다.
사내에서 데이터에 접근할 때는 대부분 제플린에서 프레스토에 연결해서 사용하므로, 프레스토의 쿼리 실행 계획을 이해할 줄 알면 데이터를 조회하는 데에 큰 도움이 됩니다.
하지만 GUI가 잘 되어 있는 SQL 클라이언트에 익숙해진 사람들이 보기에는 그다지 보기 좋지 않습니다(…). 또한 예전에 CLI로 SQL을 사용하던 사람들에게는 용어가 그다지 익숙하지 않아서 역시나 그다지 보기 좋지 않습니다.
(예는 사내에서 사용하는 제플린+프레스토 에서의 실제 실행 결과로, 테이블 이름은 숨겼습니다.)

하지만 최소한 계단 형태라도 나오는 게 어디인가 싶습니다(…). 그리고 천천히 뜯어보면 그다지 어렵지 않습니다. 특히,  프레스토의 최소한의 실행 구조와 용어를 알고 뜯어보면 DB 실행 계획 만큼 쉽습니다.
프레스토의 쿼리 실행 형태는 기본적으로 다음과 같습니다.
위의 내용을 기억해 두고, 앞서 예시로 든 쿼리를 살펴 보겠습니다. 위의 쿼리는 간단한 SELECT-FROM-WHERE절로 일부만 보기 위해 LIMIT 10을 사용했습니다.
참 쉽죠(!!).

그러면 조금 더 복잡한, 간단한 JOIN과 집계 기능 사용한 쿼리문의 실행 계획을 살펴 보도록 하겠습니다.

갑자기 길어진 것 같아서 조금 현기증이 나지만 역시나 찬찬히 뜯어보면 별 차이 없습니다.
역시나 하단부터 살펴봅니다.
정말 별 것 없습니다.
여기에 더불어, 수행 시간이나 대략 사용하는 데이터의 크기를 보고 싶으면 EXPLAIN ANALYZE  구문을 사용할 수 있습니다.

여기서는 각 부분에서 사용하는 리소스(CPU, 사용 추정 행 수 등)가 표시됩니다. 데이터의 수는 추정치지만 평균 및 표준 편차도 같이 표시해 주므로 대략적인 양을 산정할 수 있습니다. 더불어 각 부분 옆에 SINGLE, SOURCE, HASH 등 앞서 설명한 각 부분에서 처리하는 방식이 같이 나와있어서 보다 상세히 작동 방식을 이해할 수 있습니다.
이를 통해 얼마나 많은 데이터가 필요하고, 리소스는 얼마나 필요한 지를 어림짐작할 수 있습니다. 특히 프레스토는 데이터 소스에서 데이터를 가지고 와서 최종적으로는 메모리에 올려서 연산 처리를 하다 보니 데이터의 양을 어느 정도 추정할 수 있게 되면 메모리에 과부하가 가서 쿼리 결과가 안 나오는 사태가 발생한다든가 하는 일을 미연에 방지할 수 있습니다.
다만 이는 추정치고, 프레스토에서는 타 소스에서 데이터를 가지고 와서 자체 연산을 하는 식이다보니 정확도가 DB의 쿼리 실행계획보다는 다소 낮다는 사실을 인지하고 있어야 합니다 (특히 쿼리 실행 계획을 확인한 때와 실제 쿼리를 실행하는 때가 다르다 보면 사용 가능한 CPU 나 메모리가 다르고, 이에 따라 예상 속도가 다르고, 어떤 경우에는 실행 계획 자체가 일부 바뀌기도 합니다).
기획부터 개발까지, 다양한 분야에서 점차 많은 분야에서 데이터를 활용하게 되고, 이를 위해서는 데이터를 당연히 조회하게 됩니다. 그러면서 데이터 조회에 대한 이야기도 많이 하게 되는데, 이 때 가장 많이 하는 이야기는 ‘원하는 것을 정확히 생각하고, 그 것이 어떤 데이터로 만들어져야 하는지, 그 데이터가 어떻게 구해지는 지 계속 생각하는 것’입니다.  데이터는 예민하고, 각도에 따라 모습이 너무 달라지기 때문에, 생각하지 않고 기계적으로 하다 보면 무언가를 놓치게 됩니다. 이를 위해서, 본인이 어떤 코드를 만들었고, 그 것이 어떤 형태로 데이터를 만들어서 본인에게 오게 되는지를 생각하는 것이 필요합니다. 쿼리 실행 계획을 확인하는 것은 이런 데에 정말 많은 도움이 됩니다. 특히 프레스토의 경우에는 동작 구조와 일부 형태만 알면 실행 계획은 매우 단순하므로 쉽게 이해할 수 있고, 한 번 이해하면 이후에 정말 여러 모로 유용하게 사용할 수 있을 것이라고 생각합니다.
Presto 0.190 Documentation
"
http://woowabros.github.io/experience/2017/12/11/how-to-study.html,2017-12-11,학습에 실패한 이야기,"프로그래머에게 지속적인 학습은 기본적으로 갖춰야 할 덕목 중 하나라고 생각합니다. 문제를 해결하는 방법들은 계속 발전하고 변해가며 하나를 배우면 오히려 배울 것이 늘어나는 경험을 항상 합니다. 당장 우아한형제들의 구성원을 봐도 직급이나 연차와 상관없이 아침, 점심시간을 아껴 공부하는 모습이 낯설지 않습니다.
하지만 익혀야 할 것은 무수히 많고 시간은 한정되어 있습니다. 따라서 어떤 것을 먼저 할지 우선순위를 정하고, 선택하고, 집중해야 합니다. 그것도 아주 효과적인 방법으로요.
사실 저는 지난 몇 달간 비효과적인 방법으로 시간을 낭비했습니다. 이 글에서 제가 학습하고, 학습에 실패한 경험을 공유합니다.

올해 초, 업으로서 프로그래밍을 시작한 지 1년쯤 될 때입니다. 개인적인 할 일, 공부할 것, 읽어보고 싶은 책들은 큐Queue에 쌓아 두고 관리 합니다.

그 당시에도 여러 주제가 쌓여 있었고 큐의 전방front에는 객체지향 프로그래밍이 있었습니다. 그래서 그 능력을 향상하겠다는 목표로 본격적인 학습을 시작했습니다.
너무 막연하고 두리뭉실하며 학습의 성과를 측정하기 힘든 목표였습니다.
평소 책을 통한 지식 습득을 선호합니다. 주제를 정하면 그것과 관련된 여러 책을 연속해서 찾아봅니다. 역시 하던 방식대로 학습을 시작했습니다. 객체지향 프로그래밍에 관련된 책들을 찾아서 읽어 보고 책의 예제를 따라 했습니다. 유튜브 같은 곳에서 관련 강의를 찾아보기도 했습니다. 지금 생각해보면 단순히 읽고 반복하는 편안한 방식의 학습이었습니다.
그렇게 한 달, 두 달, 석 달… 문득 이런 생각이 들었습니다.
내가 제대로 하고 있는 건가?
하지만 어떤 입력에 대해 출력이 있는 것처럼 명확한 답이 있는 주제가 아니었기에 현재 상태를 측정하기 힘들었습니다.
진척도를 측정하기 힘든 학습 목표, 끝이 정해져 있지 않은 학습 과정.
시간이 좀 더 흘러서는 자신도 뭘 하고 있는지 몰랐고 지루해지기 시작했습니다. 그러면서도 아직 부족 하다는 생각에 시간만 질질 끌기 시작했습니다.
꽤 많은 시간을 어영부영 흘려보내다 문득 이런 생각이 들었습니다. 그러자 더 학습을 진행하는 것은 힘들었습니다. 읽던 책, 학습하던 것 모두 중단한 후 며칠을 온전히 놀았쉬었고 그 후, 효과적인 학습 방법을 찾기 시작했습니다. 그때, 의식적인 연습이 떠올랐습니다. 주변의 괜찮은 프로그래머라고 생각한 분들이 한 번씩 언급했지만, 이전에는 관심 두지 않던 키워드입니다.
책 ‘1만 시간의 법칙’에 의식적인 연습이라는 표현이 등장합니다. 재능보다 노력이 중요한데 이 노력과 성실함에도 전략이 필요하다고 합니다.
이를 설명하기 위해 많이 인용하는, 걷기로 역시 예를 들자면 우리는 태어나서 부모님의 도움과 많은 연습을 통해서 걸을 수 있게 됩니다. 이렇게 스스로 직립 보행을 할 수 있게 된 후 마냥 30년을 더 걷는다면 걷기 1년 차 보다 발의 피로를 줄이는 효과적인 발 디딤과 효율적인 호흡을 하며 걸을 수 있을까요? 만약 팔자걸음이 습관에 베였다면 마냥 많이 걸어서 걸음걸이가 저절로 교정 될까요?
이는 걷기, 테니스 그 외의 무엇에도 적용됩니다. 만족하고 기계적으로 할만한 수준에 도달하게 되면 더 발전을 멈춥니다. 즉, 단순히 행위를 반복하는 것은 연습이 되지 않으며 실력이 늘지 않습니다. 노하우가 쌓일 뿐이죠. 자신의 약점을 고치려고 의식적으로 노력해야 합니다. 김창준 애자일 코치님의 ‘프로그래밍 어떻게 공부할 것인가’ 강의에서 업무와 놀이는 연습이 아니라고 말합니다.
Work != Deliberate Practice != Play

집중하고 고치고 반복하라 이 세 가지가 의식적인 연습의 핵심 키워드입니다. 의식적인 연습은 다음과 같은 특징을 가집니다.

기존에 했던 방법을 돌아보면 구체적이지 못한 목표를 세우고 단순 반복적으로 읽는 학습 방식을 취했으며 성과를 측정할 방법이 마땅치 않았습니다.
단순히 많은 시간을 들여 꾸준히 한다면 실력이 늘 것으로 생각했지만 의식적인 연습 방법들은 이제까지 해왔던 방법들과 조금 달랐습니다. 따라서 앞으로 의식적으로 교정하려는 것은 다음과 같습니다.
결심에는 두 가지 종류가 있다고 합니다. 목표 의도Goal Intention와 실행 의도Implementation Intention입니다. 목표 의도는 “살을 빼겠다”와 같은 결심이고 실행 의도는 “매일 달리기를 해서 두 달 안에 5kg 빼겠다”와 같이 구체적인 실행방법을 포함한 결심입니다.
객체지향 프로그래밍을 학습하기로 했다면 이 목표는 다시 (1) 만들고자 하는 주제를 정한 후, 그 안에서 객체들을 추출해본다. 그리고 각 객체에 책임을 할당하고 서로 협력하도록 프로그램을 작성해본다  (2) 회사 코드에 객체지향 생활체조 규칙을 적용해본다 (3) 한 달 전에 작성한 코드를 객체지향적으로 리팩토링해본다 와 같이 구체적인 실행 계획과 기간을 포함한 세부 목표로 나눠 실천할 수 있을 것입니다.
책의 예제를 쉽게 따라 하기 위해 화면의 반은 코드 작성 도구, 나머지 반은 전자책을 띄워놓는 경우가 많았습니다. 하지만 이런 식의 학습법은 단순히 받아 적기, 옮겨 적기밖에 안 된다고 생각됩니다. 인출연습Retrieval Practice을 해야 합니다. 단순히 반복해서 하거나 읽는 것은 장기 기억에 비효율적입니다. 공부한 내용을 일정한 주기로 인출 혹은 회상하는 것이 기억을 강화하고 망각을 막아준다고 합니다. 따라서 책을 통해 학습한다면 한번 전체적으로 살펴본 후, 코드를 작성하는 일은 최대한 스스로 하려 합니다.
한 달 또는 일정 주기로 이전에 학습했던 내용을 다시 학습해 보는 것은 장기 기억에 도움이 된다고 합니다. 이전에 풀었던 알고리즘 문제를 다시 풀어보거나 작성했던 코드를 개선해보려 합니다.
많은 사람이 프로그래밍을 효과적으로 학습하기 위해서 개인 프로젝트를 많이 하라고 했습니다. 하지만 항상 주제 선정의 어려움을 겪어 선호하지 않는 방법이었습니다. 주제를 정한다 하더라도 너무 목표만 높이 잡아 계획만 세우다 끝나는 일이 다반사였습니다. 효과적인 학습 방법을 조사하며 왜 많은 사람이 이 방법을 권했는지 조금 이해하게 됐습니다.
하나의 프로그램을 만드는 과정은 많은 실행 오류들을 겪게 되고 자신의 노력을 끌어내게 됩니다. 이를 바람직한 어려움Desirable Difficulties이라고 하는데 적절한 학습의 난이도는 더욱 탄탄한 학습으로 이어집니다. 또한, 이렇게 작성된 프로그램은 다른 용도로 또다시 활용될 수 있습니다. 이런 프로그램들은 언제든 바꾸고 부술 수 있는 장난감이기 때문에 새 기능을 마음대로 추가하고 다양한 방식으로 리팩토링해 볼 수 있는 놀이터play ground로 활용할 수 있습니다.
앞으로 주제는 너무 거창하게 정하지 않으려 합니다. 연습을 위해 하는 프로젝트에서 너무 멋진 프로그램을 만들려고 하기보다 당장 내가 필요한 작은 프로그램으로 시작해 보는 것이 좋겠습니다.
프로그래밍 학습의 피드백은 작성한 프로그램을 실행해 동작을 살펴보는 방식을 취할 수 있습니다. 하지만 이는 프로그램이 어느 정도 완성이 되어야 가능하기에 피드백이 너무 늦어질 수 있습니다. 학습에 TDDTest-Driven Development 방식을 취한다면 좀 더 작은 단위로, 좀 더 빠르게 피드백을 받을 방법이라고 생각합니다.
객체지향설계나 좋은 코드에 대한 피드백은 어떻게 받을 수 있을까요? 일단 가장 쉽고 낮은 수준의 피드백은 정적분석 도구를 활용하는 것입니다. 학습을 위해 작성한 코드에도 정적 분석 도구를 적용함으로써 아주 기본적인 피드백을 받을 수 있습니다. 또 한 가지 방법은, 많이 알려진 주제를 선택하는 것입니다. 예를 들어 TDD를 연습한다면 볼링게임이나 계산기 만들기를 주제로 정하는 것입니다. 이런 주제들은 잘 알려져 다른 사람의 코드를 많이 찾을 수 있습니다. 스스로 코드를 작성해본 후, 다른 코드와 비교해보는 방법을 취할 수 있습니다.
사실 가장 좋은 방법은 다른 개발자의 리뷰를 받는 것으로 생각합니다. 상대에게 너무 부담을 주지 않는 선에서 이런 부탁을 한다면 리뷰를 통해 또 다른 토론을 이끌어낼 수 있고 이런 토론들이 생각을 더 확장 시켜줄 것입니다. 스터디 모임에 참여하는 것도 좋은 방법일 수 있겠네요.
글의 서두에서 지속적인 학습은 프로그래머의 기본 덕목이라고 적었습니다. 그리고 효과적인 학습을 위한 방법들을 정리해보았는데요. 한가지 놓친 것이 있었습니다. 우리는 왜 공부해야 하나요?
최종 목적지가 확실해야 가끔 딴 길로 새더라도 결국 그 방향을 향해 갈 수 있을 것 같습니다. 더 나은 커리어 패스를 위해서? 돈을 많이 벌기 위해서? 배움 그 자체의 즐거움으로? 개개인의 목표는 다를 수 있고 각 목표는 존중받아야 합니다.
저는 돌이켜보니 어느 순간 학습을 위한 학습을 하고 있었습니다. 뛰어난 동료들, 그럼에도 꾸준한 자기계발. 그런 분위기 속에서 어떤 목표를 위한 학습이 아니라 단지 ‘학습해야 한다!’라고 자신을 압박만 했습니다. 
처음 프로그래밍을 접한 후 프로그램을 동작시키기 위해 공부해야 했습니다. 그 목표를 어느 정도 스스로 이룰 수 있게 된 후 항상 머릿속에 담고 있던 것은 ‘잘 작동하는 깔끔한 코드 (Clean code that works) ‘이고 그것을 실천하기 위해 노력하는 과정이 즐거웠습니다. 참 간결한 문장이지만 서비스는 계속해서 성장하고 있고 그만큼 늘어나는 트래픽과 트랜잭션을 처리할 수 있어야 합니다. 또, 더 높은 사업적 목표를 달성하기 위해서 소프트웨어는 계속 함께 자라나야 합니다. 그 과정에서 좋은 품질도 유지하는 일은 절대 만만치 않은 일입니다.
정리하자면 저는 결국 프로그램은 잘 동작하고 코드는 사람이 이해하기 쉽도록 작성하기 위해 학습한다고 정리했고, 이 목표는 학습의 우선순위 등을 정할 때 가장 먼저 고려 될 부분이 될 것입니다.
노력과 학습이 뇌를 변화시키고 지적 능력이 자신의 통제에 크게 의존한다는 점을 이해하도록 도움을 받은 사람들은 어려운 도전에 착수하고 꾸준히 버틸 가능성이 더 높다. 이들은 실패를 무능력의 표시이자 막다른 길이라고 생각하는 대신 노력의 표시이자 전환점으로 여긴다. (중략) 실패는 숙달된 상태로 가는 데 필수적인 경험이 된다.
-어떻게 공부할 것인가, 헨리 뢰디거, 마크 맥대니얼, 피터 브라운 저, 김아영 역, 와이즈베리 2014
학습에 실패한 이야기라는 제목으로 시작했지만 사실 스스로 실패라 생각하지는 않습니다. 이전에는 단순히 많은 시간을 들여 많이 읽는, 어쩌면 너무 편안한 학습 방법을 취해왔습니다. 지금이라도 이렇게 학습 방법에 대해 돌아보고 교정할 수 있는 시간을 가져 다행입니다. 
위에 나열한 방법들은 절대적인 방법들이 아닙니다. 학습하고자 하는 주제, 개인의 성향에 따라 방법은 무궁무진할 수 있습니다. 다만 기존에 하던 방법보다 더 효율적인 방법은 없는지 고민하며 학습 전략을 나에게 맞도록 조정해 가는 것은 분명히 필요한 일이라는 생각합니다.
저와 같은 고민을 하는 분들에게 조금의 도움이라도 되길 바랍니다.
"
http://woowabros.github.io/experience/2017/12/01/es6-experience.html,2017-12-01,신선함으로 다가온 ES6 경험,"이번에 FC플랫폼개발팀에서 진행하는 프로젝트에서 ES6를 이용하면서
기존에 혼재되어있는 자바스크립트 파일들을 모듈별로 나누게 되었습니다.
처음 ES6 를 접했을 때 기존에 써왔던 자바스크립트와는 조금 다른 형식에 당황하기도 했지만
(앗 이게 뭐지? 신선한데…?) 신기하기도 했습니다.
import
export
const……
유용해 보이면서도 재밌어 보이는데…!
처음 하면서는 모르는 부분들이 많아 꽤 어렵다고 느껴졌었는데
조금 익숙해지고 나니 이전의 코드들보다 이해하기가 쉬워진 것 같습니다.  
ES6를 사용하기 위해 공식 문서들과 많은 블로그 글들을 참고하였고 (글 중간중간에 링크 달았습니다.)  
이번 글에서는 ES6의 많은 feature 들 중에서 몇 가지를 설명해드리려고 합니다.
그 이전에 간단히 ECMAScript 설명을 하겠습니다.
ECMAScript is a standard script language
자바스크립트 언어의 표준입니다.

넷스케이프에서 자바스크립트를 지원하면서 자바스크립트가 성공하자 마이크로소프트가 J스크립트를 개발했습니다.
넷스케이프는 표준화를 위해 자바스크립트 기술 규격을 ECMA 인터내셔널에 제출하였고 ECMA-262라는 표준이 생겨났습니다.
넷스케이프의 Brendan Eich가 JavaScript를 개발하였으며 Javascript는 처음에는 Mocha 라는 이름으로 후에는 LiveScript 최종적으로 Javascript라는 이름이 됐습니다.
ECMAScript 라는 이름이 이상하다고 생각할 수 있는데 그렇게 생각하는 게 전혀 이상한 일이 아닙니다. Brendan Eich도 그 이름에 대해서 언급했었습니다.
“ECMAScript was always an unwanted trade name that sounds like a skin disease.”
피부병 이름 같이 들리다니..
ECMA스크립트 위키
ECMAScript 2015 Language Specification
ECMAScript 6 는 2015년 6월에 업데이트 되었습니다 ! ECMAScript의 6번째 에디션입니다.
Class 문법을 제공합니다. constructor 메소드도 사용할 수 있고 extends를 통해서 클래스 상속도 가능합니다.
const

let
함수는 간결해지고 코드는 짧아졌습니다.

Arrow Function 은 자신만의 this를 생성하지 않습니다.
예를 들어서 예전의 방식을 보겠습니다.

화살표 함수는 자신의 this가 바인드 되지 않기 때문에 함수의 스코프에서의 this 가 적용됩니다.
화살표 함수 definitions
Export, Import 를 이용해 function이나 variables 들을 다른 곳에서 사용할 수 있습니다.
비동기 프로세싱을 위해 사용됩니다. (Asynchronously)
가독성이 좋으며 중첩된 콜백의 단점을 완화합니다.
(Callback Hell이라는 Callback 함수가 다시 Callback을 호출하고 또다시 Callback을 호출하는 코드를 읽기도 관리하기도 힘들어지는 것은 완화할 수 있습니다.)


Promise의 세가지 상태

그 외에도 많은 ES6 feature들이 있고 아래의 링크에서 확인하실 수 있습니다. 
더 많은 ES6 features
ES6 경험기의 글에 맞게 경험을 조금 적으려 합니다.  
프로젝트를 진행하며 있었던 사례 몇 가지를 예를 들어보겠습니다.

기존에 체크박스에 관련된 함수의 대략적인 코드는 아래와 같았습니다.

ES6를 이용해서 새롭게 바꾼 코드는 다음과 같습니다.

기존의 코드보다 짧고 괄호가 많지 않아 깔끔하게 느껴지며 가독성이 좋아졌습니다.
String Interpolation을 이용해서 섬세하고 깔끔하게 표현을 할 수 있게 되었습니다.

기존에는 변수가 값이 변하는 것인지 변하지 않는 것인지를 알기가 어려웠습니다.

새롭게 바꾼 코드에서는 변수만 봐도 나중에 바뀔 값인지 아닌지 구분하기 쉬워졌습니다.

이전에 있던 또 다른 코드의 예시를 들어보겠습니다.

같은 동작을 하지만 좀 더 짧은 표기법을 이용해 엄청나게 많은 변화는 아니지만 조금 더 sweet한 코드가 되었습니다.

이전에의 대략적인 코드는 아래와 같았습니다. 코드를 따라가기가 쉽지 않았습니다.

현재는 아래와 같은 형태로 바꿨습니다.

깔끔한 코드를 보며 한결 마음이 가벼워짐을 느껴보세요 ! :)
프로젝트에서 처음 써보았을 때는 기존 자바스크립트 코드의 분석과
ES6에 익숙해지기까지의 시간이 조금 걸리긴 했습니다.  
하지만 프로젝트를 진행하면서 ES6에 익숙해지고 자바스크립트 파일을
모듈별로 나누게 된 이후에는 깔끔해진 코드와 그 코드들의 관리가 쉬워진 점에 대해 만족하고 있습니다.
이번에 신선함으로 저에게 다가온 ES6를 경험하며 
기존의 가독성이 안 좋거나 중복되거나 불필요했던 코드들 정리도 동시에 진행하게 된 점이 좋았습니다 :) !
"
http://woowabros.github.io/experience/2017/10/30/baemin-mobile-git-branch-strategy.html,2017-10-30,우린 Git-flow를 사용하고 있어요,"안녕하세요. 우아한형제들 배민프론트개발팀에서 안드로이드 앱 개발을 하고 있는 나동호입니다.
오늘은 저희 안드로이드 파트에서 사용하고 있는 Git 브랜치 전략을 소개하려고 합니다. ‘배달의민족 안드로이드 모바일 파트에서 이렇게 브랜치를 관리하고 있구나’ 정도로 봐주시면 좋을 것 같습니다.

2016년 1월, Github로 소스코드를 이전하면서 Github-flow를 사용하기 시작했습니다. 그러다 2017년 6월부터 Git-flow로 브랜치 전략을 바꾸게 되었습니다. 오늘 할 이야기는 저희가 브랜치 전략을 왜 바꾸게 되었는지 그리고 어떻게 브랜치를 관리하고 있는지 이야기를 하려고 합니다.

기존의 배달의민족 앱의 개발 프로세스는 ‘기획-디자인-개발-QA-출시’ 순서의 흐름으로 차근차근 흘러갔고 3주 주기마다 앱을 출시했습니다. 그 일을 하는 안드로이드 앱 개발자 인원은 보통 2~3명이였습니다.
회사에서는 지속적으로 개발자를 채용했고 어느새 배달의민족 안드로이드 앱 개발자가 5명으로 늘어났습니다. 기획, 디자인, 서버 등 많은 사람들과 흐름을 맞춰야하기 때문에 이 5명이 모두 이번 버전에 포함될 기능을 개발하는 것은 비효율적이였습니다. (무엇보다 iOS 개발자가 부족합니다..) 작업에 따라 개발 기간이 3주 이상이 필요한 작업들이 많아지기 시작한 이유도 있었습니다. 그래서 기존의 개발 프로세스에서 약간의 변화가 생겼습니다.
새롭게 바뀐 개발 프로세스는 이렇습니다. 5명은 우선순위에 따라 나열된 작업 중 우선순위가 높은 작업부터 하나씩 선택하여 작업을 나눠 갖습니다. 그리고 이번 버전에 포함될 필수 작업과 함께 다음에 언젠가는 배포될 작업들을 병렬로 진행합니다. 병렬로 처리하던 작업들이 완료가 되면 가까운 배포 주기에 포함시켜 출시합니다. 저희는 이렇게 바뀐 개발 프로세스를 가장 잘 반영할 수 있는 모델이 Git-flow라고 생각했고 안드로이드 개발자들 역시 Git을 사용하는 데 어려움이 없어 브랜치 전략을 바꿔도 큰 걸림돌은 없을 것으로 판단했습니다.

먼저 현재 Git Repository 구성부터 살펴보겠습니다.
Repository는 Upstream Remote Repository(이하 Upstream Repository), Origin Remote Repository(이하 Origin Repository), Local Repository 이렇게 3부분으로 구성됩니다. Upstream Repository는 개발자들이 공유하는 저장소로 최신 소스코드가 저장되어 있는 원격 저장소입니다. Origin Repository는 Upstream Repository를 Fork한 원격 개인 저장소입니다. Local Repository는 내 컴퓨터에 저장되어 있는 개인 저장소입니다.

위 그림은 Git Repository 구성과 워크플로우를 설명하고 있습니다. Local Repository에서 작업을 완료한 한 후 작업 브랜치을 Origin Repository에 push합니다. 그리고 Github에서 Origin Repository에 push한 브랜치를 Upstream Repository로 merge하는 Pull Request를 생성하고 코드리뷰를 거친 후 merge 합니다. 다시 새로운 작업을 할 때 Local Repository에서 Upstream Repository를 pull 합니다.
이런 워크플로우를 두는 데에는 한 가지 이유가 있었습니다. 그 이유는 개발자들의 실험정신(?)을 펼치기 위해서였습니다. 모두가 공유하고 있는 Repository에서 실험하기에는 위험이 있다고 생각했고, Forked한 Repository를 두면 부담 없이 원하는 실험들을 해볼 수 있다고 생각했습니다. 무엇보다 이런 구조로 가져갔을 때 개발자가 해야 할 작업들이 중앙집중식 워크플로우보다 일이 늘거나 크게 복잡해지지도 않았습니다.

저희는 작업을 할 때 지켜야 할 몇 가지 약속이 있습니다.
Git-flow를 사용했을 때 작업을 어떻게 하는지 살펴보기 전에 먼저 Git-flow에 대해서 간단히 살펴보겠습니다. 
Git-flow에는 5가지 종류의 브랜치가 존재합니다. 항상 유지되는 메인 브랜치들(master, develop)과 일정 기간 동안만 유지되는 보조 브랜치들(feature, release, hotfix)이 있습니다. 

Git-flow를 설명하는 그림 중 이만한 그림은 없는 것 같습니다. 



위 그림을 일반적인 개발 흐름으로 살펴보겠습니다.
처음에는 master와 develop 브랜치가 존재합니다. 물론 develop 브랜치는 master에서부터 시작된 브랜치입니다. develop 브랜치에서는 상시로 버그를 수정한 커밋들이 추가됩니다. 새로운 기능 추가 작업이 있는 경우 develop 브랜치에서 feature 브랜치를 생성합니다. feature 브랜치는 언제나 develop 브랜치에서부터 시작하게 됩니다. 기능 추가 작업이 완료되었다면 feature 브랜치는 develop 브랜치로 merge 됩니다. develop에 이번 버전에 포함되는 모든 기능이 merge 되었다면 QA를 하기 위해 develop 브랜치에서부터 release 브랜치를 생성합니다. QA를 진행하면서 발생한 버그들은 release 브랜치에 수정됩니다. QA를 무사히 통과했다면 release 브랜치를 master와 develop 브랜치로 merge 합니다. 마지막으로 출시된 master 브랜치에서 버전 태그를 추가합니다.
좀 더 자세한 설명을 보시려면 ‘A successful Git branching model’로 가시면 보실 수 있습니다.
이제는 저희가 실제로 어떻게 작업하는지 알아보겠습니다.
아래의 Repository와 Branch는 앞으로 설명을 할 때 나오기 때문에 알아 두시고 가면 한결 수월하게 보실 수 있을 거라고 생각합니다.
Github-flow에서 Git-flow로 변경됐지만 하나의 티켓을 처리하는 방법은 이전과 비슷합니다. 다만 티켓을 처리하는 개발자는 Github-flow를 하고 있을 때와는 다르게 관리되는 브랜치들이 늘어남에 따라 어느 브랜치에서 작업을 해야 하는지 항상 주의해야 합니다.
앞서 ‘작업을 할 때 지켜야 할 서로 간의 약속’에서 ‘하나의 티켓은 되도록 하나의 커밋으로 한다’라고 했습니다. 그래서 기능을 구현하기 전에 여러 개의 티켓으로 작업을 먼저 나누게 됩니다. 나눠진 작업 티켓 중 ‘로그인 레이아웃 생성’이라는 티켓이 있고 이 티켓을 처리한다고 가정하고 살펴보겠습니다.
(feature-user)]$ git fetch upstream 
(feature-user)]$ git checkout -b bfm-100_login_layout --track upstream/feature-user
(bfm-100_login_layout)]$ git commit -m “BFM-100 로그인 화면 레이아웃 생성”
(bfm-100_login_layout)]$ git rebase -i HEAD~2
(bfm-100_login_layout)]$ git pull --rebase upstream feature-user
(bfm-100_login_layout)]$ git push origin bfm-100_login_layout
위의 절차에서 4, 5번의 작업을 수행하는 이유는 커밋 그래프를 단순하게 가져가고 의미 있는 커밋들로 관리하기 위해서입니다.
4번 작업을 예로 들면, ‘BFM-100 로그인 화면 레이아웃 생성’ 작업을 할 때 로그인 화면의 레이아웃을 생성한 커밋 하나와, view의 약간의 간격을 조정한 커밋 하나, 그리고 view의 id를 변경한 커밋 하나, 이렇게 3개의 커밋으로 분리된 상태입니다. 이 3개의 커밋이 그 의미를 나눌 필요가 없거나 코드리뷰를 도와주지도 못한다면 커밋을 분리하는 것은 불필요하다고 판단하고 하나의 커밋으로 합치게 됩니다. 물론 항상 하나의 커밋으로 합쳐야만하는 것은 아닙니다. 하나의 티켓에 대한 작업이라도 커밋이 분리되어 있는 게 낫다고 생각이 든다면 2개 이상의 커밋으로 나눌 수도 있습니다. 그러나 대부분은 티켓을 더 작게 나누지 못한 경우일 가능성이 높습니다.
5번 작업도 예를 들어보면, 동료와 같이 같은 기능을 개발하면 하나의 feature 브랜치에 커밋을 하게 됩니다. 서로 같은 커밋에서 시작했다가 feature 브랜치에 하나씩 merge 되기도 하고 얽히고설켜서 merge 되기도 합니다. 그러면 커밋 그래프가 복잡해지고 이력 확인을 할 때도 어렵게 됩니다. 그래서 커밋을 순차적으로 만들기 위해서 작업한 커밋이 feature의 최신 상태에서 시작하도록 rebase를 수행합니다.
아래 그래프를 보시면 rebase를 했을 때 그래프가 얼마나 단순해지는지 볼 수 있습니다.




작업을 할 때 브랜치의 수명은 되도록 짧게 가져가는 게 좋지만, feature 브랜치에서 기능을 완료하는데 해야 할 작업들이 많아서 오래 걸리는 경우 들이 있습니다. 그러다 보면 develop에 추가된 기능들이 필요한 경우가 종종 생기게 됩니다. 그럴 때는 feature 브랜치에 develop의 변경사항들을 가져와야 합니다.
(feature-user)]$ git fetch upstream 
(feature-user)]$ git merge --no-ff upstream/develop
(feature-user)]$ git push upstream feature-user
드디어 feature-user 브랜치에서 작업하던 기능이 완료되었습니다. 이젠 feature 브랜치를 이번 출시 버전에 포함시키기 위해서 develop에 merge 해야 합니다.
(develop)]$ git fetch upstream 
(develop)]$ git merge --no-ff upstream/feature-user
(develop)]$ git push upstream develop
이번 버전에 포함되어야 할 기능들이 모두 완료되었습니다. 이제부터 출시 담당자가 해야 할 일이 많습니다. 출시 담당자는 QA를 시작하기 위해 먼저 release 브랜치를 생성하고 upstream에 push하여 release 브랜치를 공유합니다.
(develop)]$ git fetch upstream 
(develop)]$ git checkout -b release-1.0.0 --track upstream/develop
(release-1.0.0)]$ git push upstream release-1.0.0
개발을 완료한 후 QA 중 버그가 발생하지 않으면 좋겠지만 항상 생각지 못한 예외 상황들이 발생하게 됩니다. 예외 상황이 발생할 때마다 버그 티켓이 하나씩 생성되는데 이 티켓들을 모두 해결해야만 앱을 출시할 수 있습니다. 
버그 티켓들도 티켓이기 때문에 ‘1. 티켓 처리하기’와 같은 방법으로 처리합니다.
(release-1.0.0)]$ git checkout -b bfm-101_bug_login_id_max_length
(bfm-101_bug_login_id_max_length)]$ git commit -m “BFM-101 로그인 아이디 길이 제한 버그 수정”
(bfm-101_bug_login_id_max_length)]$ git push origin bfm-101_bug_login_id_max_length
발생하는 버그들을 모두 수정했다면 이젠 출시를 준비할 때입니다. release 브랜치를 master 브랜치와 develop 브랜치에 merge하고 마지막으로 master 브랜치에서 버전 태그를 달아줍니다.
(release-1.0.0)]$ git pull upstream release-1.0.0
(release-1.0.0)]$ git checkout develop 
(develop)]$ git pull upstream develop 
(develop)]$ git merge --no-ff release-1.0.0
(develop)]$ git push upstream develop
(develop)]$ git checkout master 
(master)]$ git pull upstream master 
(master)]$ git merge --no-ff release-1.0.0
(master)]$ git tag 1.0.0
(master)]$ git push upstream master 1.0.0

이것으로 출시 담당자의 브랜치 관리는 끝이 나고, 앱을 스토어에 출시합니다. (hotfix는 없는걸로..)
Github-flow일 때보다 늘어난 브랜치들을 관리 해야 하는 부담은 늘었지만 전보다 일관되게 여러 상황들을 대처할 수 있는 것 같습니다. 물론 Git-flow 가이드대로 항상 흘러가지만은 않았습니다. release 브랜치를 시작했는데 기능이 추가돼야 하는 경우도 있고, 때로는 feature들이 많아서 완료된 feature들만 먼저 release에 포함해서 QA를 우선 시작하는 경우도 있었습니다. 현재는 Git-flow를 그대로 따라 하고 있지만 이런 시행착오를 겪으면서 우리에게 맞는 브랜치 전략으로 발전할 거라고 믿고 있습니다. 읽어주신 분 모두 자신의 상황에 맞는 브랜치 전략을 선택하실 수 있기를 바라겠습니다. 
감사합니다.
"
http://woowabros.github.io/woowabros/2017/10/21/apologize.html,2017-10-21,코딩테스트 연기 건 사과 드립니다.,"10월 21일 오후 2시에 진행 예정이었던, 신입공채 1차 코딩테스트가 원활하게 진행되지 못하고 연기하게 된 점 사과 드립니다.
우아한형제들은 코딩테스트를 위하여 Codility라는 서비스를 이용하고 있습니다. 이번 신입공채에 굉장히 많은 분들이 지원하셨는데, 이 분들이 동시에 테스트를 진행하게 되면서 Codility 서비스가 그 부하를 감당하지 못하는 일이 발생하였습니다. Codility 쪽에 저희 쪽의 지원자 수를 알리고, 이번 테스트를 위해 별도의 얘기를 진행하였으나, 그 부분에 대한 대비가 충분히 안 되었던 것으로 보입니다.
Codility 서비스에 부하가 생기면서, 어떤 분들은 테스트를 위한 링크는 받았으나 60분을 기다리라는 메시지를 보신 분도 있고, 어떤 분들은 Codility 자체가 원활하게 동작하지 않으면서 테스트 메일을 못 받으신 분들도 발생하였습니다.
상황을 지켜 보고 Codility 와 연락을 취하여도 단시간 내에 해결하기는 어렵다는 판단에, 2시 15분에 오늘 코딩 테스트를 중단하기로 결정하였습니다. 그리고 2시 28분에 문자메시지를 통해서 금일 코딩 테스트 연기 사실을 발송하였고, 2시 40분부터 수신자 그룹 별로 좀 더 상세한 메일 안내를 발송하였습니다.
이 과정에서, 많은 분들이 테스트 링크를 받지 못한 불안한 마음에 메일로 문의 주셨는데요. 오늘 상황이 몇 분의 문제가 아니라 응시한 대부분의 분들이 겪은 상황이다보니 보내 주신 메일에 각각 회신을 드리지 못하고 전체적인 공지를 준비하고 발송하느라, 지원자 분들 입장에서는 빠르게 회신을 받지 못하여 불안하고 답답하셨을 것 같아 더욱 죄송스럽습니다.
토요일 오후에 코딩 테스트를 준비하느라 미리 시간도 비우고 이런 저런 준비도 많이 하셨을텐데, 코딩테스트가 원활하게 진행되지 않아서 몇 십 분의 시간 동안 안타깝고 마음 졸이는 상황이 만들어지게 된 점, 정말 죄송합니다.
이미 공지 드린 바와 같이, 이번 코딩테스트는 차주 토요일로 예정되어 있던 2차 테스트 기간에 시행하도록 하겠습니다. 그리고, 차주에 진행되는 테스트는 오늘과 같은 일이 일어나지 않도록 코딩테스트를 제공하는 서비스 자체에 대한 검토와 더불어, 미리 철저한 부하 테스트를 통해서 이런 일이 발생하지 않도록 철저하게 대비하도록 하겠습니다.
우아한형제들 신입공채에 관심 갖고 지원해 주셔서 정말 감사 드리고요. 감사한 만큼이나 오늘의 불미스러운 상황에 대해 아쉽고 죄송한 마음이 무척 큽니다.
저희가 좀 더 철저하게 준비하고, 지원하시는 분들이 채용 전형 과정에서 어려움을 겪지 않도록 해 드렸어야 했는데, 저희가 많이 부족했습니다. 빠르게 개선하여, 다시 이런 불편함을 겪지 않도록 노력하겠습니다.
죄송하고, 또 감사합니다.
우아한형제들 CTO 김범준 드림
"
http://woowabros.github.io/experience/2017/10/17/java-serialize2.html,2017-10-17,"자바 직렬화, 그것이 알고싶다. 실무편","자바의 직렬화 기술에 대한 대한 두 번째 이야기입니다.
실제 자바 직렬화를 실무에 적용해보면서 주의해야 할 부분에 대해 이야기해보려고합니다.
자바 직렬화는 자바 개발자 입장에서는 상당히 쉽고 빠르게 사용할 수 있도록 만든 기술입니다.
JSON 또는 CSV 등 형태의 포맷을 이용하면 직렬화 또는 역직렬화시에 특정 라이브러리를 도입해야 쉽게 개발이 가능하며,
구조가 복잡하면 직접 매핑시켜줘야 하는 작업도 포함하게 됩니다. 
그것에 비해 자바 직렬화는 비교적 복잡한 객체도 큰 작업 없이 (java.io.Serializable 인터페이스만 구현해주면) 
기본 자바 라이브러리만 사용해도 직렬화와 역직렬화를 할 수 있습니다.
하지만 등가교환이라는 말이 있듯이 쉽게 이용할 수 있는 만큼 실제 업무에서 사용할 때에는 신경 써야 하는 부분이 있습니다. 
제 경험에 빗대어서 신경 써야 할 부분에 대해 몇 가지 이야기해보겠습니다.
앞서서 예시를 들은 woowahan.blog.exam1.Member (이하 Member) 클래스를 기준으로 이야기해보겠습니다.
예제에서 Member 클래스가 있습니다. 이 클래스의 객체를 직렬화 시켜보겠습니다. 
아래에의 문자열은 직렬화된 Member 클래스의 객체 문자열입니다. 테스트에 용의 하도록 Base64로 인코딩하였습니다.
이 문자열을 바로 역직렬화 시키면 바로 Member 객체로 변환합니다. (테스트할 때에는 반드시 패키지도 동일해야 합니다.) 
Member 클래스의 구조 변경에 대한 문제를 확인해보겠습니다.
우리가 보통 원하는 것은 phone 멤버 변수가 추가되어도 기존 멤버 변수의 기존 멤버 변수는 채워지길 원합니다. phone은 null 되어 있더라도 말이죠. 
이전에 자바 직렬화된 데이터를 역직렬화 시켜 보겠습니다.
이렇게 클래스의 멤버 변수 하나만 추가되어도 java.io.InvalidClassException 예외가 발생합니다. 
예외 메시지를 읽어보면 serialVersionUID의 정보가 일치하지 않기 때문에 발생한 것을 알 수 있습니다. 
우리는 Member 클래스에서는 serialVersionUID 의 값을 -8896802588094338594 정보로 설정해준 적도 없으며,
 7127171927132876450으로 변경한 적도 없습니다. 어떻게 된 일일까요? 
그래서 자바 직렬화 스펙을 확인해보았습니다. (링크)
간단히 정리해보겠습니다.
serialVersionUID 를 직접 기술하지 않아도 내부 적으로 serialVersionUID 정보가 추가되며, 
내부 값도 자바 직렬화 스펙 그대로 자동으로 생성된 클래스의 해쉬 값을 이라는 것을 확인할 수 있었습니다.
(링크 를 보면 알 수 있지만 해쉬값은 클래스 구조 정보를 이용해서 생성하는 것을 알 수 있습니다.)
즉 serialVersionUID 정보를 기술하지 않는다고 해서 사용하지 않는 것이 아니 다라는 것이 확인되었습니다.
그럼 어떤 형태가 좋을까요?
“조금이라도 역직렬화 대상 클래스 구조가 바뀌면 에러 발생해야 된다.” 정도의 민감한 시스템이 아닌 이상은 클래스를 변경할 때에
직접 serialVersionUID 값을 관리해주어야 클래스 변경 시 혼란을 줄일 수 있습니다.
물론 그렇게 해도 모든 것이 해결되는 것은 아닙니다. serialVersionUID 값이 동일할 때에도 신경 써야 할 부분이 생깁니다. 
serialVersionUID 값이 동일할 때에도 어떠한 문제가 생길 수 있는지 몇 가지 살펴보겠습니다.
기존 자바 직렬화된 데이터는 String 타입이었지만 StringBuilder클래스 타입으로 바꿔 봤습니다.
혹시 primitive 타입인 int 을 long으로 바꾸는 것은 괜찮지 않을까요?
역시나 타입 예외가 발생했습니다.  자바 직렬화는 상당히 타입의 엄격하다는 것을 알 수 있습니다. 
에러는 발생하지 않습니다. 값 자체만 없어졌습니다.
 그럼 멤버 변수를 추가해보겠습니다.
이번에도 에러가 발생하지 않습니다. 원하는 형태로 값이 채워졌네요.
자바 직렬화를 사용할 때 클래스 구조 변경 시 어떤 부분을 확인해야 할지 정리해보겠습니다.
특별한 문제없으면 자바 직렬화 버전 serialVersionUID의 값은 개발 시 직접 관리해야 합니다.
serialVersionUID의 값이 동일하면 멤버 변수 및 메서드 추가는 크게 문제가 없습니다. 
  그리고 멤버 변수 제거 및 이름 변경은 오류는 발생하지 않지만 데이터는 누락됩니다.
역직렬화 대상의 클래스의 멤버 변수 타입 변경을 지양해야 합니다. 자바 역직렬화시에 타입에 엄격합니다.          
  나중에라도 타입 변경이 되면 직렬화된 데이터가 존재하는 상태라면 발생할 예외를 경우의 수를 다 신경 써야 합니다.
외부(DB, 캐시 서버, NoSQL 서버 등)에 장기간 저장될 정보는 자바 직렬화 사용을 지양해야 합니다.
  역직렬화 대상의 클래스가 언제 변경이 일어날지 모르는 환경에서 긴 시간 동안 외부에 존재했던 직렬화된 데이터는 쓰레기(Garbage)가 될 가능성이 높습니다. 
언제 예외가 발생할지 모르는 지뢰 시스템이 될 수도 있습니다.
생각 지도 못한 오류가 거품처럼 나기 시작할 것입니다. 이 부분은 사실 알아채기가 힘듭니다. 발생하기 위한 사전 조건도 많기 때문입니다.
  차라리 이 글을 읽으신 분은 위와 같은 문제 사전에 차단하실 것을 추천합니다. 
  위와 관련된 예시는 스프링 시큐리티의 SecurityContextImpl클래스가 있습니다. 
SecurityContext 를 구현한 클래스 클래스로 링크 를 보면 확인할 수 있습니다. 
  serialVersionUID 값이 스프링 시큐리티의 버전 값이기 때문에 버전이 변경될 때마다 신경 쓰입니다.
자바 직렬화시에 기본적으로 타입에 대한 정보 등 클래스의 메타 정보도 가지고 있기 때문에 상대적으로 다른 포맷에 비해서 용량이 큰 문제가 있습니다. 
특히 클래스의 구조가 거대해지게 되면 용량 차이가 커지게 됩니다. 예를 들면 클래스 안에 클래스 또 리스트 등 이런 형태의 객체를 직렬화 하게 되면 내부에 참조하고 있는 모든 클래스에 대한 메타정보를 가지고 있기 때문에 용량이 비대해지게 됩니다. 
그래서 JSON 같은 최소의 메타정보만 가지고 있으면 테스트로 된 포맷보다 같은 데이터에서 최소 2배 최대 10배 이상의 크기를 가질 수 있습니다. 
결과
간단한 데이터이지만 위와 같이 용량 크기 두배 이상 차이가 납니다. 
용량 문제는 생각보다 많은 곳에서 나타나는 문제입니다. 특히 직렬화된 데이터를 메모리 서버(Redis, Memcached)에 저장하는 형태를 가진 시스템에서 두드러집니다. 
메모리 서버 특성상 메모리 용량이 크지 않기 때문에 핵심만 요약해서 기록하는 형태가 효율적입니다.
적은 데이터만 입력하는 시스템 구조라면 큰 문제는 발생하지 않습니다. 하지만 트래픽에 따라 데이터 기록이 급증하는 시스템은 유의해야 합니다. 
그리고 이 부분을 강조하는 이유는 자바 웹 시스템에서 가장 많이 사용되는 스프링 프레임워크에서 기본적으로 지원하는 캐시 모듈 중 
외부 시스템에 저장하는 형태에서 기본적으로 자바 직렬화 형태로 제공되기 때문입니다. (Spring Data Redis, Spring Session …)
기본적으로 프레임워크에서 자바 직렬화로 제공하는 이유는 앞서 말한 자바 직렬화 장점과 일맥상통합니다. 개발자가 신경 안 쓰고 빠르게 개발할 수 있기 때문입니다. 
자바 직렬화 사용하는 시스템은 규모가 커지는 시점에서 반드시 다시 점검하여 보시길 바랍니다.
일반 사용자를 대상으로 하는 B2C와 같은 시스템에서 자바 직렬화 정보를 외부 캐시 서버에 저장할 때에는 
비효율적인 문제를 가지고 있습니다. (용량 크기에 따른 네트워크 비용과 캐시 서버 비용)
새롭게 스타트하는 서비스 같은 경우에는 생산성을 위해서 자바 직렬화를 그대로 이용한다고 해도 
트랙픽이 지속적으로 증가할 때에는 JSON 형태 또는 다른 형태의 직렬화로 바꿔주는 것 고려해보시길 바랍니다. 
이 부분은 기술적 오류 문제는 아닙니다. 단지 자바 직렬화를 이용해서 개발하면서 불편했던 부분을 이야기하려고 합니다. 
자바 직렬화를 이용해서 외부 데이터를 저장하게 되면 제일 큰 아쉬움이 바로 자바에서만 사용할 수 있으면 읽을 수 있는 문제였습니다.  
다른 언어를 이용해서 스크립트를 이용해서 여러 가지 처리를 하고 싶어도 불가능에 가깝습니다. 
(파이썬에 자바 직렬화 분석하는 라이브러리가 있는 것은 확인해봤지만 사용은 못해봤습니다.)
만약 JSON으로 저장되어 있다면 MYSQL이나 REDIS 등 추가 라이브러리를 통해 조회도 가능하면 다른 언어를 통해서도 탐색 및 조작이 가능합니다 
그리고 제가 이야기하고 싶은 것은 “긴 시간 동안 외부에 저장하는 의미 있는 데이터들은 자바 직렬화를 사용하지 말자.“입니다.
자바 직렬화는 장점이 많은 기술입니다만 단점도 많습니다. 
문제는 이 기술의 단점은 보완하기 힘든 형태로 되어 있기 때문에 사용 시 제약이 많습니다. 그래서 이 글을 적는 저는 직렬화를 사용할 때에는 아래와 같은 규칙을 지키려고 합니다.
이전 포스팅으로 이동하기
"
http://woowabros.github.io/experience/2017/10/17/java-serialize.html,2017-10-17,"자바 직렬화, 그것이 알고싶다. 훑어보기편","자바의 직렬화 기술에 대한 대한 이야기입니다. 
간단한 질문과 답변 형태로 자바 직렬화에 대한 간단한 설명과 직접 프로젝트를 진행하면서 겪은 경험에 대해 이야기해보려 합니다.
자바 기본(primitive) 타입과 java.io.Serializable 인터페이스를 상속받은 객체는 직렬화 할 수 있는 기본 조건을 가집니다.
자바 직렬화는 방법은 java.io.ObjectOutputStream 객체를 이용합니다.
위 예제에서 객체를 직렬 화하여 바이트 배열(byte []) 형태로 변환하였습니다.
자바 직렬화를 아시는 분은 위에서 기술한 예제에서 사용되는 자바 직렬화 대상의 Member 클래스가 serialVersionUID 상수가 없어서 의아하신 분도 계실 겁니다. 
사실 반드시 기술해야 되는 필수는 아니기 때문에 빼둔 것입니다. 하지만 상당히 중요한 부분이라서 따로 설명하려고 합니다. 이곳에서는 넘어가도록 하겠습니다.
먼저 자바 직렬화를 설명하기 전에 다른 데이터 직렬화 종류를 살펴보겠습니다. 
직접 데이터를 문자열 형태로 확인 가능한 직렬화 방법입니다. 범용적인 API나 데이터를 변환하여 추출할 때 많이 사용됩니다.
표형태의 다량의 데이터를 직렬화시 CSV가 많이 쓰이고 구조적인 데이터는 이전에는 XML을 많이 사용했으며 최근에는 JSON형태를 많이 사용하고 되고 있습니다. 
여기서는 CSV와 JSON만 살펴보겠습니다.
예제에서는 문자열로 단순히 변경했습니다.
  자바에서는 Apache Commons CSV, opencsv 등의 라이브러리 등을 이용할 수 있습니다.
JSON도 물론 이렇게 직접 문자열을 만들일은 거의 없습니다.
  자바에서는 Jackson, GSON 등의 라이브러리를 이용해서 변환할 수 있습니다.
위에 언급한 CSV, JSON 형태의 직렬화는 익숙한 사람이 많을 것입니다. 
CSV 같은 경우 표 형태의 데이터에서 많이 사용되며, JSON 같은 경우는 구조적인 데이터를 전달하는 API 시스템 등에서 많이 사용하고 있기 때문입니다. 
데이터 변환 및 전송 속도에 최적화하여 별도의 직렬화 방법을 제시하는 구조입니다. 
직렬화뿐만 아니라 전송 방법에 대한 부분도 이야기하고 있지만 여기서는 직렬화 부분만 이야기하겠습니다.
종류로는 Protocol Buffer(이하 프로토콜버퍼) Apache Avro 등이 있습니다.
기타 
지면 관계상 프로토콜 버퍼만 한번 살펴보겠습니다. (살펴보면 알겠지만 직렬화하기 위한 패턴은 비슷합니다.)
프로토콜 버퍼는 구글에서 제안한 플랫폼 독립적인 데이터 직렬화 플랫폼입니다.
자바에서 사용방법
  프로토콜 버퍼는 특정 언어 또는 플랫폼에 종속되지 않는 방법을 구현하기 위해 직렬화 하기 위한 데이터를 표현하기 위한 문서가 따로 있습니다.
이렇게 기술된 member.proto 문서를 프로토콜 버퍼 컴파일러를 이용해서 개발하기 원하는 언어(여기서는 자바)로 변환해야 합니다. 
  (프로토콜 버퍼 컴파일러는 별도로 설치하거나 Gradle, Maven등 의 빌드 도구를 이용하면 됩니다.) 
  자바로 변환하게 되면 프로토콜 버퍼 형태의 Member 클래스가 생성됩니다.
자바 직렬화와 다른 점은 데이터 스펙을 표현하기 위한 문서가 존재하는 부분입니다. 그 이외에는 대부분 동일합니다.
그 외 여러가지 직렬화 방법이 있는 있지만 여기서 다 다루지는 못하지만 직렬화 관련 좋은 포스팅이 있어서 추천드립니다. 링크
그럼 다시 왜 자바 직렬화를 사용하는지 이야기해보겠습니다. CSV, JSON, 프로토콜 버퍼 등은 시스템의 고유 특성과 상관없는 대부분의 시스템에서의 데이터 교환 시 많이 사용됩니다. 
하지만 “자바 직렬화 형태의 데이터 교환은 자바 시스템 간의 데이터 교환을 위해서 존재한다.”고 생각하시면 됩니다.

JVM의 메모리에서만 상주되어있는 객체 데이터를 그대로 영속화(Persistence)가 필요할 때 사용됩니다.
시스템이 종료되더라도 없어지지 않는 장점을 가지며 영속화된 데이터이기 때문에 네트워크로 전송도 가능합니다. 
그리고 필요할 때 직렬화된 객체 데이터를 가져와서 역직렬 화하여 객체를 바로 사용할 수 있게 됩니다. 
그런 특성을 살린 자바 직렬화는 많은 곳에서 이용됩니다. 많이 사용하는 부분 몇 개만 이야기해보겠습니다.
서블릿 세션 (Servlet Session)
  서블릿 기반의 WAS(톰캣, 웹로직 등)들은 대부분 세션의 자바 직렬화를 지원하고 있습니다. 
  물론 단순히 세션을 서블릿 메모리 위에서 운용한다면 직렬화를 필요로 하지 않지만, 
  파일로 저장하거나 세션 클러스터링, DB를 저장하는 옵션 등을 선택하게 되면 세션 자체가 직렬화가 되어 저장되어 전달됩니다. 
  (그래서 세션에 필요한 객체는 java.io.Serializable 인터페이스를 구현(implements) 해두는 것을 추천합니다.) 
  참고로 위 내용은 서블릿 스펙에서는 직접 기술한 내용이 아니기 때문에 구현한 WAS 마다 동작은 달라질 수 있습니다.
캐시 (Cache)
  자바 시스템에서 퍼포먼스를 위해 캐시(Ehcache, Redis, Memcached, …) 
  라이브러리를 시스템을 많이 이용하게 됩니다.
  자바 시스템을 개발하다 보면 상당수의 클래스가 만들어지게 됩니다. 
  예를 들면 DB를 조회한 후 가져온 데이터 객체 같은 경우 실시간 형태로 요구하는 데이터가 아니라면 
  메모리, 외부 저장소, 파일 등을 저장소를 이용해서 데이터 객체를 저장한 후 동일한 요청이 오면 DB를 다시 요청하는 것이 아니라 저장된 객체를 찾아서 응답하게 하는 형태를 보통 캐시를 사용한다고 합니다.
  캐시를 이용하면 DB에 대한 리소스를 절약할 수 있기 때문에 많은 시스템에서 자주 활용됩니다. (사실 이렇게 간단하진 않습니다만 간단하게 설명했습니다.)  
  이렇게 캐시 할 부분을 자바 직렬화된 데이터를 저장해서 사용됩니다. 물론 자바 직렬 화만 이용해서만 캐시를 저장하지 않지만 가장 간편하기 때문에 많이 사용됩니다.
자바 RMI(Remote Method Invocation)
  최근에는 많이 사용되지 않지만 자바 직렬화를 설명할 때는 빠지지 않고 이야기되는 기술이기 때문에 언급만 하고 넘어가려고 합니다. 
  자바 RMI를 간단하게 이야기하자면 원격 시스템 간의 메시지 교환을 위해서 사용하는 자바에서 지원하는 기술입니다. 
  보통은 원격의 시스템과의 통신을 위해서 IP와 포트를 이용해서 소켓통신을 해야 하지만 RMI는 그 부분을 추상화하여 원격에 있는 시스템의 메서드를 로컬 시스템의 메서드인 것처럼 호출할 수 있습니다. 
  원격의 시스템의 메서드를 호출 시에 전달하는 메시지(보통 객체)를 자동으로 직렬화 시켜 사용됩니다.  
  그리고 전달받은 원격 시스템에서는 메시지를 역직렬화를 통해 변환하여 사용됩니다. 
  자세한 내용은 작은 책 한 권 정도의 양이 되기 때문에 따로 한번 찾아보시는 것을 추천드립니다.
다음 포스팅으로 이어집니다.
"
http://woowabros.github.io/woowabros/2017/10/17/baemin-mobile-sprint.html,2017-10-17,배민 '앱 친구'의 스프린트 이야기,"
배달의민족 앱 개발자들은 주로 다음 과제와 관련된 메일을 받고 다음 앱 업데이트 준비를 시작합니다.
여러분의 한 단위 앱 업데이트를 위한 과정은 어떻게 시작되시나요? 
이터레이션이라고 부르기도 하고, 스프린트라고도 칭하는 용어들을 사용하고 계신가요 :-)
배달의민족을 개발하는 팀에서는 스프린트라는 용어를 주로 사용하고 있습니다.(스크럼을 응용해서 개발 프로세스를 진행하고 있다는 의미도 내포되어 있어요-) 주된 스프린트의 목표는 역시 고객님께 배포되는 앱을 릴리즈 하는것이랍니다. 우리 팀은 성공적인 앱 배포를 위해 흥미롭게 짜여진 스크럼 진행 과정을 따르고 있는데요. 오늘 여러분께 그 과정을 소개해드리고자 합니다.

우리 팀은 앱 개발자와 서버 개발자가 반반정도의 비율로 함께 일하고 있고, 서로를 앱 친구들 & 서버 친구들이라고 부르곤 한답니다. 한 ‘앱 친구’(앱 개발자)의 페르소나를 투영해서 릴리즈 준비를 시작하는 시점부터, 최종 릴리즈까지 거치게 되는 흐름들을 함께 흘러가고자 합니다.
이제부터 마치 과제를 담당하게 되는 개발자처럼 생각해보세요. Shell We? ;-)
“우리 언제 모일까요~?”
글 초반에 보여드렸던 다음 릴리즈 과제 담당자 지정에 대한 메일이 오면, 우리는 메신저에서 과제 담당자 관련된 논의를 위해 모일 시간을 정하자고 이야기 해봅니다.
그리고, Confluence Wiki(기획자분들은 기획문서를 모두 wiki에 올려주십니다. 꽤나 미리 전부터요~) 에서 과제를 검색해 그 과제의 스펙들은 어떤 것인지 미리 살펴봅니다. 여러 과제들이 어떤 목표와 기획을 가지고 있는지, 특별한 개발 기술들을 필요로 하는지 고민하는 시간을 가지곤 합니다.

동료들과 모여서 다음 과제 이야기를 나눌 때는 마치 모여서 잡담을 하는 것 처럼 편안합니다. 
자유롭게 과제에 대해 이야기 하는 분위기는 전매특허 수준이예요~
각 과제가 어떤 난이도가 있는지, 이번엔 이런 결과를 위해서 어떤 기술을 사용하겠다 등등 과제 관련해서 할 수 있는 이야기는 이 때 가장 많이 나누는 편입니다. 그리고 우리는 각자의 모든 것을 고려해서(선호도, 개발 실력, 개인 휴가… 개인의 현재 상태까지!)과제 담당자들을 지정합니다. 이 과정에서 잡음이 있었던 적은 전혀 없습니다. 아마도, 우리 팀이 일과 관련되서는 서로에 대해 잘 알고있다라고 할 만큼 투명하게 지내는 분위기를 가지고 있기 때문 인 것 같아요. (정말 좋은 팀이죠~? :-D)

다음은 플래닝포커를 통한 일정산출입니다!
포커카드 없이 점수표를 보면서 각 과제의 난이도나 사용하는 기술을 고려해 얼마의 공수가 필요할 지 머릿속으로 미리 시뮬레이션 해봅니다. 점수를 말하면서 각자의 생각을 공유하고, 내가 생각하지 못한 부분도 듣게 되면서 과제수행에 대한 현실감 게이지가 슬슬 달아오르게 됩니다.
그러면서 점수가 이정도니 일정은 얼마나 걸리겠다까지 연장선으로 함께 논의하게 됩니다. 릴리즈 목표일이 확정은 아니지만 어느정도 정해져서 공유가 되는데요. 개발자들은 그 릴리즈 목표일을 생각하며 그 일정 안에 Design QA(품질개선팀에서 수행하는 QA전에, 디자이너와 함께 디자인이 잘 반영되었는지 확인하는 과정을 Design QA라고 지칭)와 QA까지 모두 고려하며 개발 일정은 얼마나 나오는지 역산해 봅니다.
이 과정을 통해서 개발팀 내부에선 목표일을 지킬 수 있다, 시간이 더 필요하다, 더 단축시킬 수 있다라는 의견을 정해서 프로젝트 리더에게 공유합니다. 대게 개발자들이 의견이 잘 반영되어 릴리즈 목표일이 확정되곤 합니다.
우아한형제들 개발실의 또 다른 플래닝포커를 보고싶다면… 클릭해보세요!
본격적인 개발을 위해 아래와 같은 작업을 시작합니다.
…을 하고 있으면 노티피케이션이 옵니다!

기획 리뷰회의가 잡혀서 5분전 알림이 울렸네요 ㅎㅎ
이제 각개전투 시작입니다! 
각 과제 앱 담당자들과 서버 담당자들, 기획자와 QA분들이 모여서 함께 회의하고 기획과 관련된 부분을 논의합니다.
같이 모인 메신저방도 생기고 이렇게 회의도 같이하니 궁금하거나 문제가 되는 부분은 바로바로 이야기 할 수 있고, 그로 인해 전달하는 형식의 커뮤니케이션 코스트도 줄어들고 분위기는 훈훈해지는 효과를 얻습니다! ㅎㅎ
과제에 따라 매일 스탠딩 회의를 진행하는 경우도 있고, 메신저를 통해 리모트로 작업 진행 과정을 공유하기도 한답니다.

본격 코딩을 시작합니다~! 때로는 페어로 개발할 때도 있지만,
속해있는 팀원이 홀수라면 페어없이 혼자 한 과제를 도맡아 할 때도 있습니다.
그런데, 혼자 개발한다는 느낌이 1도 들지 않습니다.
왜일까요? 나의 코드를 지켜보는 동료들이 있기 때문입니다.
 
안드로이드 개발자들의 치밀한 코드리뷰는 우리 팀의 자랑거리 입니다 :-)
동료들이 서로 리뷰해준 리뷰 내용들, 세세한 실수부터 리팩토링 할 수 있는 것들을 댓글로 달아주는 덕에 결국에 머지되는 코드는 프로페셔널 향기가 날 수 있답니다. 그 과정에서 서로 많이 배우는 것은 굳이 말하지 않아도 될 정도이구요. 이렇게 서로의 코드를 리뷰하며 과제 개발의 마무리를 향해 달려갑니다.
 
그러다 보면 또 다른 노티피케이션이 도착합니다.
바로 품질개선팀이 작성하신 QA TC를 함께 리뷰하는 회의를 위한 소집인데요. 우리팀에서 개발완료라고 부르는 시점은 개발자가 기획된 모든 기능을 개발하고, 디자이너와 Design QA를 진행하고, 품질개선팀이 작성해주신 TC까지 스스로 테스트 해보는 과정을 모두 마쳤을 때 입니다. 이 이후에 본격적인 QA가 진행된답니다.
TC를 리뷰하면서, 개발자는 테스트가 필요한 내용들을 품질개선팀에 추가로 전달할 수 있고, 앱의 스펙과 테스트 기대결과들을 최종적으로 점검합니다. 그리고 돌아와서 개발자는 TC를 하나하나 수행하며 체크합니다. 스스로 만든 💩 을 자발적으로 치우며 아름다운 반성의 시간도 가지게 됩니다. 마치 개발 후, 명상의 시간을 갖는 것 같습니다 ㅎㅎㅎ
그리고 나면…

본격적인 QA기간이 시작되고, QA 이슈 티켓이 작업자에게 할당됩니다.
개발이슈 티켓을 쳐내기에 열중했다면, 이제는 QA이슈 티켓을 완료시키기 위해 집중합니다. 이 과정에서 기획단에서 다시 고민이 필요한 부분들도 나오는데요. 이런 이슈들은 모두가 모여있는 메신저에서 자유롭게 이야기하며 빠른 기대결과 적립 및 수정을 진행하게 됩니다.
릴리즈 목표일이 다가오면 프로젝트 리더분에 의해 앱 릴리즈 목표일 즈음에 남아있는 QA이슈들이나 빠진 작업이 없는지 확인되고, 일정이 추가적으로 필요한지 검토해서 목표일 수정이 유연하게 이뤄지는 편입니다.
그렇게 빠진 부분이 없는지 잘 챙기다 보면… 어느새 앱 출시 완료!

릴리즈 후에는 과제의 특성에 따라 회고가 이뤄지기도 하고, 생각치 못한 오류들로 인해 패치를 준비하기도 합니다.
그렇게 한 번의 릴리즈를 마무리 하고 나면…
어느새 다음 과제 대상자 관련 메일이 또 도착📬하게 되겠죠? 다음엔 어떤 과제를 하게 될 지 궁금해지네요~
배달의민족 앱 친구의 스프린트 수행과정 어떠셨나요? 저희 잘 하고 있는 것 같나요~? :-)
혹시나 이 글을 읽고, 함께 스프린트를 수행하고 싶은 마음이 불끈 생기셨다면…지체 마시고 클릭해보세요
Happy Sprint!
"
http://woowabros.github.io/woowabros/2017/09/14/scrum.html,2017-09-14,데일리 미팅! 플래닝 포커! 회고!,"안녕하세요.
저는 우아한형제들 FC플랫폼개발팀에서 서버개발을 하고 있는 이경원입니다.
오늘은 글로 배운 애자일 방법론을 도입하고 진행했던 경험담을 기록하고자 합니다.
사실 저는 애자일 방법론에 대해서 잘 알지 못하고, 제가 도입을 시도한 것도 아닙니다. 단지 스크럼 이라는 책만 한번 봤을 뿐입니다. 글로 배운 것이죠.
한때 애자일 방법론에 대한 환상이 있었는데, 직접 경험해보지 못했기 때문에 애자일이 무엇인지 감이 잘 오지 않았습니다. 하지만 이번 프로젝트에 애자일 방법론을 경험하며 느낀 점은 애자일 방법론을 도입한다고 해서 거창하게 생각할 필요는 없다는 것입니다. 복잡한 도메인이 어떻게 변할지 정확히 예상할 수 없듯이, 방법론도 마찬가지라고 생각합니다.
하지만 여전히 애자일이 무엇인지는…
아마 데일리 미팅은 대부분 경험해봤거나, 하고 있다고 생각합니다. 데일리 미팅은 간단합니다. 팀원들이 정해진 시간에 모여 어제는 무엇을 했고, 오늘은 무엇을 할 것이며, 어떤 문제(이슈)가 있는지 공유하고 잡담 자리입니다.
주의할 점은 시간이 너무 길어지지 않게 하는 것인데요.
시간이 너무 길어진다면…

데일리 미팅에서 길어질 것 같은 주제는 따로 미팅을 잡는 게 좋다는 생각입니다.
하지만 이렇게 간단하고 귀찮은 미팅을 왜 매일매일 하는 걸까요? 단순히 자기 일정을 공유하는 자리를 만들기보단 문서나 메신저로 공유하면 안 될까요?
데일리 미팅을 시작한 후 마침 김창준 님의 애자일 키워드라는 팟방을 듣게 됐는데요. 데일리 미팅을 하는 가장 큰 이유는 일정 공유보다는 팀원 개개인의 감정을 학습하는 것이라고 합니다.
얼굴을 맞대고 데일리 미팅을 하며 팀원들 서로의 감정을 읽고 학습하는 시간인 것이죠. 쉽게 말하면 팀워크를 다지기 위함이 아닐까요?
데일리 미팅뿐 아니라 플래닝 포커, 회고 등을 통해서 가장 좋았던 점은 팀원과 자주 소통하고 다양한 의견을 들어보는 것이였는데요. 그래서 그런지 팀원 간 사이도 더 가까워지는 기분이에요!
저는 사실 애자일 방법론에서 가장 해보고 싶었던 것 중 하나가 플래닝 포커였습니다. 플래닝 포커는 일정을 세우기 위한 전략인데, 이 전략은 요구사항을 분석해서 문서를 만들며 정확한 일정을 도출하기보단, 재밌는 방식으로 일정을 세우는 전략입니다. 물론 스토리라고 말하는 어느 정도 정해진 요구사항은 있습니다.
저희는 플래닝 포커를 하기 위해 Scrum Time 앱을 이용했습니다.
플래닝 포커를 하면 정확한 일정이 나올까요? 저는 처음부터 당연히 나오지 않을 거라고 생각했습니다. 스토리 포인트가 정해지더라도 그 스토리를 해결하는 과정에도 요구사항은 변하고 일정도 변한다고 생각했습니다. 그렇다면 플래닝 포커를 왜 할까요?
여기서 스토리는 요구사항 또는 하나의 기능이라고 생각하셔도 됩니다.
플래닝 포커를 통해 스토리를 작은 단위로 나누게 됩니다. 저희는 1/2(일), 1(일), 3(일), 5(일)를 기준으로 스토리 포인트를 정했는데요. 5포인트 이상의 스토리가 나오게 되면 2개 이상의 스토리로 쪼개는 것이죠. 한 스토리를 최대한 단순하게 만들어 되도록 스프린트 내에 일정을 맞출 수 있게 하는 것입니다.

맞아요. 우리 쪼개는 중이에요.

진지합니다!!!
저는 스스로도 회고를 많이 하는 편입니다. 회고하며 기록하는 것은 많은 장점이 있습니다.
저희도 회고를 통해 위와 같은 장점을 얻었습니다. 무엇보다 스프린트 기간 동안 무엇을 잘했고, 무엇이 문제였는지 확인하고 문제를 해결하는 방법은 무엇일까 고민하는 자리입니다. 무겁게 이야기 하기보단 커피도 한 잔씩 마시며 즐겁게 서로의 감정이 상하지 않도록 이야기하는 자리입니다. 물론 모든 내용은 기록합니다.

용서와 칭찬의 시간

저기 환하게 웃고 있는 사람이 저입니다. 칭찬을 받았나 봐요. (아니네요. 플래닝 포커 중이네요.)
쓰고 보니 이 글도 회고라는 생각이 들었습니다.
사실 애자일 방법론을 경험한 지 한 달 조금 넘었기 때문에 애자일이란 무엇이고 이렇게 해야 돼!라고 말하지는 못합니다. 그리고 앞으로도 못할 거라고 생각합니다.
그 이유는 애자일이기 때문입니다. 애자일 방법론은 정해진 규칙과 방법은 있지만, 정확한 프로세스가 주어지진 않습니다. 우리 상황과 이해관계에 맞게 변화하고 학습하며 적응하면서 우리만의 애자일 방법론을 만들어가는 게 애자일이지 않을까 생각합니다.

명주 님은 생각보다 재밌다. 그걸 알게 돼서 좋았다.
회고 중 나온 내용인데요.
이 사진을 찍으신 분이 바로 명주 님입니다. 제가 회고 시간에 재밌다고 했더니, 페북에 이렇게 올리셨어요.

이 글에도 올려달라고 하셨어요. 좋으셨나 봐요. 올려달라고 말씀하시는 것도 재밌네요. ㅋㅋㅋ

코드 리뷰도 합니다.
끗.
"
http://woowabros.github.io/woowabros/2017/09/12/realtime-service.html,2017-09-12,실시간 서비스 경험기(배달운영시스템),"들어가기 앞서 이 글은 신기술 사용기 또는 소개가 아닌 실시간 서비스 즉 배민라이더스 BROS 1.0 을 개발 하면서 겪어왔던 다소 특별한 개발 및 운영 경험기 입니다. BROS 2.0이 나온 상황에서 1.0을 이야기 하는 것이 다소 순서가 맞지 않지만 그 때 당시 경험기를 남겨놓지 못 한 것을 이번 기회에 남겨 보고자 합니다.
BROS는 배민라이더스의 주문을 접수(콜센터접수)를 시작으로 배달건을 라이더가 고객에게 신속하고 안전하게 배달하기 위한 통합 운영 관제 시스템입니다.
여기서 realtime 또는 실시간서비스란 websocket으로 통신하는 서버를 이용하여 웹페이지 및 모바일 앱에서 실시간으로 데이터를 주고 받고 갱신하는 시스템을 말합니다.
angularjs의 양방향 바인딩을 뜻 하며 뷰의 데이터가 변하면 모델이 자동으로 변경되고 반대로 모델이 변하면 뷰도 자동으로 변경되는 특징을 말합니다.
angularJS에서 모델 변화를 감지하는 역할을 하여 양방향 데이터 바인딩을 적용하는 역할을 합니다. angularjs1.0의 성능적인 면에서 단점으로 작용하기도 하지만 개별 모델의 one-time bind 설정을 통하여 양방향 바인딩이 필요없는 모델을 제외 할 수 있습니다.
웹브라우저는 http 프로토콜로 요청/응답으로 동작하는데 TCP/IP socket 처럼 connection이 유지되어 서로 실시간으로 통신을 할수 없습니다. 그래서 등장한것이 wswebsocket 프로토콜입니다.  websocket을 사용하면 웹브라우저에서도 socket 통신처럼 실시간으로 데이터를 주고 받을 수 있습니다. 최근에는 대부분의 브라우저가 websocket 프로토콜을 지원하지만 IE 같은 경우는 version 10 부터 지원을 하고 있습니다.
nodejs 기반으로 실시간 이벤트 서버를 개발 할 수 있는 오픈소스 라이브러리 입니다. 특징으로는 멀티 디바이스(web, android, ios, windows)를  지원하며 websocket을 지원하지 않는 browser도 지원합니다.
배민라이더스의 음식 주문은 평균 1시간 내외로 주문의 접수, 처리 및 배송이 완료 되어야 합니다. 즉 주문이 발생한 경우 콜센터직원은 바로 주문의 발생을 알아야 하며, 주문접수가 처리 되면 라이더는 즉시 배달건의 존재를 알아야 합니다. 또한 배달의 상태(대기 - 배차 - 픽업 - 전달)와 라이더의 실시간 위치가 업데이트 되어야 관제자와 라이더들은 원할한 관제 및 배달업무를 수행 할 수 있습니다. 또한 기존 polling 방식의 배민 콜센터 주문접수(현재는 fade out)는 주문건이 증가함에 따라 DB Select가 급증하여 서비스가 위험했던 적이 있었기  때문에 BROS개발 당시 실시간성을 유지하면서 db select를 줄이기 위해서 필수로 실시간 이벤트 서버 도입을 해야 했습니다.

그림 1. 주문 처리 activity 다이어그램
주문과 배달의 생성 상태변경이 있을 때 마다 socket.io 실시간 이벤트를 전송하고 수신 시 api를 호출하여 배달리스트를 갱신하는 방식은 데이터의 변경이 있는 경우만 database를 select하지만 피크시간대에는 이벤트가 증가하므로 database select가 급증하게 됩니다. angularjs model 변경은 angularjs가 알아서 뷰에 반영하기 때문에 실시간 이벤트를 송수신 할 때 마다 배달리스트를 호출 하지 않고 배달데이터를 생성 삭제 수정 한 후 실시간 이벤트 메시지로 angularjs model에 반영 하도록 하면 뷰는 자동으로 실시간 반영 됩니다. 또한 개발자는 뷰를 업데이트하는 비즈니스로직을 신경쓸 필요가 없고 데이터를 뷰에 나타나는 로직만 구성해 놓으면 됩니다.

그림 2. AngularJs 모델 데이터 반영
코드 1. 이벤트 수신 Controller
코드 2. 수신이벤트 처리 Service
코드 3. 수신받은 데이터 모델에 반영
데이터의 생성, 업데이트, 삭제를 database에 반영하고 곧바로 socket.io 서버의 실시간 이벤트 메시지로 데이터를 전송 angularjs model에  반영, 뷰는 모델의 변경에 자동 갱신 되기 때문에 client수에 관계없이 사실상 database를 주기적으로 select하는 행위는 거의 일어나지 않으며  배달데이터는 실시간으로 관제 및 라이더에게 반영된다.

그림 3. BROS 실시간 배달현황 화면

그림 3. BROS 실시간 라이더 배차 화면
angularjs와 socket.io 서버를 이용하여 database select를 최소화 하고 주문, 배달 및 라이더 위치를 실시간으로 관제하고 배차업무를 할 수 있도록 시스템을 구성하고 실시간으로 눈앞에 데이터들이 화면 갱신 없이 변경이 되고 있었지만 가만히 화면만 바라보고 있을 때는 큰 문제가 없는 듯 하였습니다.
“그러나 browser는 열일 하고 있는 중……”
실시간으로 화면이 렌더링 되는 것은 사실 아무런 문제가 없었습니다. 하지만 피크시간인 저녁시간에 배달 건수가 급격히 늘어나기 시작하면서 배차를 위해 라이더리스트 다이얼로그를 클릭한다던지 했을 때 0.5초 정도 움찔하는 delay가 발생하였습니다. 사용자 입장에서는 꽤나 신경이 거슬리는 이슈였습니다.
요구사항으로 배달완료된 배달건도 리스트에 남기고 당일 모든 배달건(많을 때는 1000건)을 현황에 리스팅 해달라는 요구가 있었습니다. 물론 타협으로 최근 4시간 배달건만 리스팅 하기로 하였지만 피크시간대에는 수백건의 배달을 페이징없이(실시간리스트라 페이징은…) 리스팅 되고 있었습니다.
일반적인 초기 화면 진입 이후 뷰의 렌더링이 거의 없는 정적인 페이지와 달리 BROS의 배달/라이더 현황 페이지들 javascript가 실시간으로 이벤트를 수신받고 모델에 반영하고 뷰를 렌더링하고…… 쉴 새 없이 일을 하고 있었습니다.  그래서 성능최적화 작업에 들어갔습니다.
forEach, angular.foreach등으로 된 loop를 제거하고 순수 javascript의  역행 루프reversed loop로 변경하였습니다. 배열의 개수가 적을 때는 크게 상관 없지만 수백 수천개가 되면 그 때는 이야기가 다릅니다.
코드 4. 역행 루프
javascript는 단일 쓰레드로 동작하며. 먼저 수행된 작업이 끝날 때 까지 다음작업은 대기하게 됩니다. 무거운 작업이 있다면 당연히 사용자는 delay를 느끼게 되고. 이러한 점을 해결 하기 위해 setTimeout을 이용하여 작업을 실행하면 javascript engine에서 UI 작업 큐로 작업은 넘겨 지게 되고 event loop가 큐의 쌓여 있는 task들을 처리 하게 됨으로써 blocking이 감소하여 좀더 성능향상을 시킬 수 있습니다.  JAVASCRIPT Event Loop 링크
코드 5. setTimeout 사용
ng-repeat은 콜렉션을 looping하여 뷰에 리스팅 하는 angularjs의 커스텀 attribute(directive)입니다. 리스팅된 데이터는 digest loop가 양방양 데이터 바인딩을 위하여 관리하는 모델 데이터 들이며 데이터 개수가 많을 수록 digest loop의 성능이 떨어지게 됩니다. 그래서 리스트의 각 항목 업소명, 배달상태, 배달주소.. 등등의 데이터는 onetime binding으로 digest loop를 가볍게 하고 배달번호+수정일시가 조합된 index key의  변경 감지만으로 뷰를 자동 갱신하게 됩니다.
코드 6. 조합 index key
나머지 모델 데이터는 onetime binding 처리
v-repeat이라는 angularjs용 오픈소스 모듈을 사용하여 scroll up & down 할때 화면에 나타나는 tr을 렌더링하고 사라지는 tr은 제거 되도록 처리 하여 리스트가 개수가 수백 수천이 되더라도  화면에 보이는 데이터 모델만 존재 하게 되어 실질적으로 digest loop가 관리하는 모델의 개수가 현저하게 줄어드는 효과를 낼 수 있었고 실제로 성능 향상에 제일 큰도움이 되었습니다.
javascript 실행을 메인쓰레드가 아닌 백그라운드쓰레드에서 처리하게 할 수 있게 하여 무거운 작업의 경우 백그라운드 쓰레드가 처리함으로써 기존 단일쓰레드에 비해서 성능향상을 이점을 얻을 수 있습니다.
angularjs의 digest loop를  web worker가 처리 하도록 하고 싶었으나 digest loop를 건드리는 일은 angularjs framework core를 건드리는 일이 되어 버리므로 결국 적용을 포기하였습니다 (가장 아쉬운 부분이기도 합니다.)
사용료를 지불하고 바로 사용할 수 있는 유료 서비스들이 존재합니다. 대표적으로 pubnub, pusher 같은 서비스가 대표적이며 websocket서버를 직접 개발할 필요없이  사용 할 수 있는 장점이 있습니다. 반면에 장애나 이슈 발생시 즉각적인 처리가 어렵다는 단점도 분명 존재합니다. 실제로 BROS 2.0도 유료 서비스를 사용하다. 장애나 오류 발생시 즉각적인 대응이 어려워 결국은 websocket 서버를 개발하여 사용하고 있습니다.
“유료서비스를 사용할 것인가 직접 만들것 인가의 선택은 여건과 상황에 따라 달라 질 수 있습니다.”
websocket server는 client와 서버 간에 http protocol로  커넥션을 초기에 맺고 wswebsocketprotocol로 upgrade한 후 서로에게 heartbeat를 주기적으로 발생시켜 커넥션이 유지되고 있는지 체크하며 네트워크를 유지합니다.
socket.io를 사용하여 websocket 서버를 개발 했지만 비즈니스로직 문제가 아닌 다양한  network 상황 때문에 이벤트 유실이 발생 했습니다.
라이더분들이 반지하로 음식배달을 가면 LTE가 잘 안 터지는 문제도 발생했다. Mobile network 환경은 24시간 시 분 초  connected 상황은 아닐 수도 있다.
업무용 폰은 테더링을 사용하는 라이더들도 있었기 때문에…..
발생하는 건수는 매우 적은 수준이였지만 BROS 서비스의 특성상 1건이라도 누락이 발생하면 배달업무에 차질이 생기기 때문에  필수적으로  이벤트 유실에 대한 보완이 필요했습니다 . (유료 서비스도 마찬가지로 발생하는 이슈)
이벤트 유실을 보완하기 위해 RabbitMq같은 메시지 큐를 사용하여 이벤트를 발송하는 것도 고려 하였으나 BROS 서비스의 특성상 시간이 지난 이벤트를 수신 받게 되거나 한참이 지난후 한꺼번에 미수신된 이벤트를 수신받게 되면 잘못된 데이터가 반영될 수도 있는 문제가 발생하게 되고 그 문제해결을 위해 복잡한 로직을 추가하게 되면 오히려 파생되는 문제가 더 생길 것으로 판단하였고 되도록이면 근본적인 해결책을 찾기로 하였습니다.
cron job을 사용하여 2분에 1번씩 batch proccess 한곳에서 만 배달 데이터를 select 하여 database  부하를 줄이면서socket.io 실시간 이벤트로 브로드캐스팅을 하도록 하고 client는 수신받은 데이터로 유실이 발생한 배달 리스트를 fetch 하는 것으로 이벤트 누락에 대한 데이터 미변경을 보완 하였고 적용 이후에는 데이터 미변경에 대한 문제가 보고 되지 않았습니다.(더 좋은 방법도 분명 있을 겁니다.)

그림 4. 이벤트 유실을 보완하기 위한 Broadcasting
어느 순간이나 서버가 다운되면 안되지만 만약에 다운이 된다면 심각한 장애를 초래하게 됩니다. 실시간 서비스를 개발한다면 항상 염두해 두어야 하는 이슈 입니다. BROS1.0은 socket.io server로의 연결이 disconnect가 되면 바로 api 직접 호출로 변경이 되고 설정해둔 주기만큼 reconnection을 시도 하도록 되어 있으며 reconnection이 성공하면 api 직접호출은 중단키시고 실시간 이벤트수신으로  swiching 되도록 개발 되어 있습니다. 더 좋은 방법은 메소드들을 추상화 하고 2개이상의 실시간 이벤트 서버를 switching 할 수 있으면 더욱 안정적인 시스템이 될 수 있을 것이란 생각도 해봅니다.
이제 실시간 서비스를 위해 필수 적으로 필요한 websocket서버에 대한 이야기를 해보고자 합니다. 앞서 잠시 언급 하였지만 서버자체를 구축할 필요없이 유료로 이용할 수 있는 서비스들이 많이 존재합니다. 유료서비스의 장점은 개발과 운영에 대한 리소스가 들지않고  사용할 수 있다는 장점이 있습니다. 하지만 이슈 발생시 빠른 대처가 어렵다는 단점도 분명 존재합니다. 직접 서버를 개발하거나 유료 서비스를 이용하거나 하는 선택은 여러가지 상황에 따라 판단해야 할 듯 싶습니다. 그리고 서버를 직접 개발 하고 안정적인 상태로 유지하기 까지 생각보다 기술 적인  learning curve  높은 편이며 서버가 안정적인 상태까지 올라오기 위해서는 실제로 운영을 해봐야 한다는 어려움도 존재합니다. socket.io 서버를 개발 하면서 겪었던 여러가지 경험에 대해서 이야기 하려고 합니다. 이야기할 내용은 socket.io 서버에만 국한된 이야기라기 보다 websocket 서버를 개발한다면 아마도 동일하게 겪어야 될 경험이라고 생각됩니다.
socket.io에서 트래픽을 격리하여 구분하는 단위로 사용됩니다 event는 명칭 그대로 송/수신하는 이벤트의 이름입니다. 트래픽격리 구분없이 이벤트를 송/수신하면 이벤트 리스너를 등록하여 이벤트를 처리하는 코드가 존재하지 않더라도 접속한 모든 client에 전송 및 수신을 하게 됩니다. 불필요한 트래픽이 발생하게 되고 서버 자체의 성능도 저하되기 때문에 적절한 설계로 구분해아합니다.

표 1. Socket.io 트래픽 격리 구분
다른 서비스에서는 room이란 용어 대신 channel이라는 용어를 많이 사용한다.
socket.io에서 이벤트를 송/수신하는 방식을 말합니다.

표 2. Socket.io 이벤트 송수신 방식
nodejs는 기본적으로 싱글 프로세스로 동작하며 서버 CPU Core 수 만큼 proccess 생성하여 multi proccess로 구동하기 위해서는 cluster를 이용하여 proccess를 생성하게 됩니다.
multi thread는 thread간 데이터를 공유되지만 multi processing은 데이터공유가 되지 않는 특징이 있다.
그래서 특별한 처리들을 구현해야 한다.
nodejs cluster 를 이용하여 proccess를 생성하면 실제 일을 수행하는 proccess를 worker 라고 하며 worker들을 제어하는 역할을 하는 proccess를 master라고 부릅니다.
socket.io는 앞서 말한것 처럼 websocket을 지원하지 않는 브라우저도 지원합니다. websocket을 지원하지 않는 브라우저에서는  flashsocket, htmlfile, xhr-polling, jsonp-polling등의 적절한 방식으로 전환되어 통신합니다. (최근에는 버전업이 되면서 websocket, polling만 지원하는 것으로 변경 되었습니다.)
flashsocket같은 경우는 브라우저에 flash가 설지되어 있지않으면 작동을 하지 않는 문제가 있었고 안정성이 떨어지는 방식은 지원에서 제외 되었습니다.
그리고 nodejs 특징인 Single Thread 기반의 Non-Blocking I/O으로 성능적인 이점이 있습니다. (callback 지옥이라는 단점도 있지만..)
“socket.io 홈페이지에 문서를 보니 아래 처럼 간단한 것 같은 ….. 금방 만들수 있겠다…”
socket.io document는 detail함이 좀 부족한 느낌이다. 그리고 버전업이 되면서 deprecated 메소드나 설정 값들이 많아 혼란 스러워 socket.io object들을 실제로 console.log로 찍어서 객체를 확인 하면서 개발 해야 했다 휴……
코드 7.  Server측 코드
코드 7.   Client측 코드
원래 호수에 있는 오리를 보면 편하게 둥둥 떠있는 것 처럼 보인다. 하지만 물속은 난리다…..
물론 기본 설정이나 여러가지를 구현해야 하지만 큰 로직은 수신/송신이라 할 것이 많이 없을 줄 알았습니다.(websocket 서버를 쌩으로 구현하는 예제들도 기본 예제들이긴 하지만..) 하지만 실제로 실무 서비스에 사용하려고 하니 여러가지 고려 대상이 생각보다 많았었고. 지금부터는 실제로 개발 하면서 겪었던 과정을 설명 하려고 합니다.
그래서 실무에서 사용할 서버라면 namespace/room/event 트래픽 격리 구분과  public/private/broadcasting 이벤트 전송 방식을 필수로 구현해야 합니다.
여기서 부터 멘붕이 왔습니다. 일반 웹서버처럼 세션관리만 신경쓰면 server를 스케일아웃 하더라도 사람이 신경쓸 것이 별로 없는 상황이 아니였습니다. nodejs는 싱글 프로세스라  멀티프로세스를 생성하고 서로 완벽한 clustering이 되어야 했고. 추후 서버 자체의 scale out이 되었을 경우에도 대비해야 하므로 clustering을 구성하는 것은 꼭 필요했습니다. 단순히 세션에 대한 문제 뿐만 아니라 1번 서버 > 1번 프로세스에 접속된 client가 이벤트를 전송하면 나머지 서버 > 나머지 프로세스들에 접속된 client로 이벤트를 전송하기 위해서 프로세스 끼리는 서로 연결되어 데이터가 전/수송이되어야 했습니다.
nodejs는 싱글 프로세스라 node cluster로 core 수만큼 프로세스를 생성해야 했고, 멀티 쓰레드 방식이 아닌 멀티 프로세스방식이라 데이터 공유가 되지 않는 특징 때문에  데이터 공유에 대한 처리가 필요했습니다.
코드 8. nodejs cluster
위 코드 8 처럼 인위적으로 cpu 수만큼 woker를 생성 하고 woker들 끼리 통신 하기 위해서는  master에게 메시지를 보내고 다시 나머지 worker에게  데이터를 전송합니다. (“아.. 뭔가 framework이 알아서 해주는게 아니라 사람이 코드로 저렇게 해줘야 하다니… 사람은 언제나 실수를….”)
worker 끼리 이벤트를 전/송수신 하는 매개체로 redis pub/sub 를 이용했습니다. worker 1에 접속된 client가 이벤트를 전송하면 나머지 worker들에게  redis pub/sub을 통해 이벤트를 전송하고 나머지 worker들은 이벤트를 수신받아 자신에게 접속된 client들에게 최종적으로 이벤트 메시지를 전송합니다.

그림 5. 클러스터링 구성도
스케일 아웃 된 서버에 client가 접속할 때 마다 서버를 달리하여 접속하게 되면 세션문제를 마주하게 됩니다. 그래서 일관성있게 지정된 서버에만 접속 되도록 sticky session을 보통 L4같은 로드밸런서 장비가 해주게 되는데. 물리적인 서버는 로드밸런서가 처리해 준다고 하지만 문제는 node cluster로 생성한 worker들 입니다. 1대의 물리 서버에 worker들이 멀티프로세스로 동작하는 것은 사실 서버가 여러대 돌아가는 것이나 마찬가지 상황! 그래서 worker들의 sticky session 처리는 오픈소스 모듈을 사용에 처리해 주었습니다.(nodeJs 같은 특별한 경우가 아니면 필요 없을 수도….)
client는 server와 websocket으로 connection을 유지하고 서로 통신하지만 http로 이벤트를 전송할 수 있는 기능도 필수로  필요합니다.  client가 websocket으로 연결되어 있을 필요는 없고 event 발송만 하면 되는 경우도 필요하고 수신은 websocket으로 송신은 restful api 처리 끝단에 http로 이벤트를 전송하는 방식으로 시스템을 구성 할 수도 있기 때문에 http 이벤트 전송 api도 구현 해야할 필요가 있습니다. 실제로 BROS 에서 콜센터 주문접수처리 하기 위해서 주문의 상세 화면을 보고 있는 경우 고객이 배민앱에서 주문 취소를 하게 되면 주문취소 api 끝단에 http로 주문 취소 이벤트를 송신했고 콜센터 주문접수 화면에서는 바로 고객주문 취소 안내를 표시 했습니다.
코드 9. 이벤트 송신 http api
서버를 만들어서 띄어 놓긴 했지만 서버에 대한 관리가 필요했습니다.(이런 부분때문에 유료 서비스를 사용하는 것이….) 일단 서버에 접속된 namespace/room별 접속한 client 현황이 필요 했고. 서버의 cpu/memory 등의 정보등이 필요했습니다. 그리고 서버에서 **오류가 발생했거나 멈췄을 경우 즉각적인 notification기능도 필요했습니다.
앞서 말한 것 처럼 접속한 client의 수와 이름 과 room 리스트 데이터들은 각각의 worker 프로세스에서 공유 되지 못하고 따로 관리가 되기 때문에 이런 관리 데이터들은 1곳에 저장하고 조회 할 필요성이 생겼고  채팅 기능에서 서로 간 대화 메시지들이 보관되어야 할 저장소도 필요했습니다. clustering 구성시에 redis를 사용하고 있었기 때문에 redis를 활용하여 데이터들을 저장하고 사용했고 client들이 접속 및 room에 join/out 하거 할때 redis에 정보를 update했고 실시간 이벤트로 모니터링 페이지에 전송했습니다.

그림 6. 소켓 서버 현황 페이지
사실 이부분은 유료 APM 서비스를 사용한다면 바로 해결될 부분이지만 (개발 당시는 사용하지 않았다.. 역쉬 돈이 쵝오!) 그렇지 않는 경우 필수로 오류상황에 대한 피드팩을 즉각적으로 받아야 할 필요가 있습니다. socket.io server를 개발 할 당시에는 winston 모듈을 사용하여 error 레벨의 로그가 남겨지거나 exception이 발생 했을 경우 설정 해놓은 이메일로 바로 전송 되도록 했습니다.
개발하면서 다양한 문제들을 접했는데 접속한 client 개수가 낮은 데도 cpu 100%로 서버마비를 겪은 적도 있고 redis는 메모리 저장소이지만 메모리데이터를 파일에 백업을 하는데 백업옵션에 문제가 생겨오류가 발생 했던 일, IE 브라우저에서 접속한 client가 창이 닫혔을 경우 disconnected를 서버가 인지하지 못한 경우등등을 이야기 하고자 합니다.
client 수가 많지 않은 상황이였는데 서버접속이 되지 않았습니다. 서버가 죽은 것은 아니였지만 이상하게 CPU 100% 상태였고 원인은 바로 nodeJs 특징인 Single Thread 기반의 Non-Blocking I/O 에서 비동기 이벤트 loop의 가장 적인 sync한 로직 처리 때문에 발생했습니다. 로직을 sync 하게 처리하게 되면 로직이 완료 될 때가지 block이 발생하게 되고 다음 처리 로직은 대기하게 되는데(비동기 처리의 자세한 내용은 링크클릭) 비동기 처리를 한 이유는 redis에서 namespace/room/clientaccount 등의 정보를 저장해놓고 가져올 때 처리한 로직 때문이였습니다.(아뿔싸!) nodejs 비동기 이기 때문에 for loop가 일반적인 동작순서로 수행되지 않습니다 . 아래 예제를 한번 보시죠~
코드 10. nodejs loop 동작
이러한 특징을 해결하기 위해 async 모듈을 사용하여 for loop를 sync 하게 처리 하도록 했지만 결국 문제를 일으키고 말았습니다.
코드 11. 중첩 loop sync 하게 처리하기 위해 async라는 모듈 사용
redis에 저장된 서버 현황 데이터를 가져와서 적절하게 조합하기 위해 for loop 가 여러번 사용 되었는데 async라는 모듈의 waterfall 메소드를 사용하면  loop가 수행된 후 결과를 순차적으로 전달함으로 써 sync 하게 처리 할 수 있었지만 사용률이 증가하면서 성능문제가 발생했습니다. 문제의 해결은 !
redis는 string, list, set, hash 등의 key/value 구조의 데이터 타입을 지원하므로 RDB 스타일의 복잡한 계층구조로 데이터를 저장하게 되면 데이터를 가져와 조합하기 위해서 복잡한 처리를 하게 되므로 처음 부터 제공되는 데이터 타입에 잘 맞추어 데이터를 설계하고 저장하면 불필요한 for loop문을 줄일 수 있다.
socket 서버의 namespace/room/account/socketid 관리 데이터들은 서버의 현황 데이터기 때문에 사실상 file로 백업 될 필요가 없지만 chatting시 채팅방에 입장 후 재입장 시 대화 내용을 다시 보여 줘야 했기 때문에 redis가 재부팅 되거 나 하는 경우에도 영구적으로 기록 되어 있어야할 필요성이 있었습니다.

그림 7. 채팅기능
redis에서 파일 저장 시  여러가지 옵션들이 있는데 stop-writes-on-bgsave-error 옵션이 yes로 되어 있는경우 파일로 저장하다가 오류가 발생하면 redis의 메모리에 데이터 저장 자체가 안되게 되서 오류를 발생 시킨다 no 변경하게 되면 메모리에 저장하는 행위는 파일 저장 오류과 관계없이 계속 수행하게 된다.
이처럼 redis의 옵션에 따른 예상치 못한 오류때문에 socket server에 오류가 발생 하였고 옵션변경으로 문제를 해결했지만 redis 서버자체의 옵션에 대한 정보도 알아놓을 필요가 있습니다.
IE에서 Browser 창을 닫는 경우 서버에서 client가 disconnect된 것을 인식 하지 못해 서버측에서 해당 client의 disconnect 처리를 하지 못 하였고 서버 측 disconnect 이벤트에 구현 되어 있던 데이터 처리가 제대로 되지 않아 서버 현황데이터에서 client가 살아 있는 것으로 표시 되는 문제가 발생 하였습니다. client sdk에서 옵션을 추가하면 해결되는 문제 였지만 여기서 꼭 짚고 넘어가야 할 부분이 있습니다. 유료 서비스 또는 오픈소스 라이브러리들은 기본적으로 client sdk를 제공하는데 물론 오픈소스 라이브러리는 서버를 개발 해야 하지만 기본적으로 둘다 안정적으로 connection 관리를 해주고  서버주소, 옵션값 connection, reconnection, connection interval, error 핸들링, 이벤트 전송/수신 등등의 메소드를 제공합니다. 개발자는 제공하는 메소드들만 잘 사용하면 됩니다.
javascript WebSocket 객체가 onopen, onclose, onerror, onmessage 등의 기본 메소드들을 제공하지만  WebSocket 객체로 client를 쌩으로 개발 하게 된다면 connection 관리가 불안하거나 장애를 경험하면서  안정적이 되어 가는 과정을 겪을 수 있을 수도 있습니다. 선택은 각자의 몫이지만 실시간 이벤트 서버를 사용해야 한다면 최대한 안정성이 높은 유료서비스를 선택 하고 차선은 오픈소스라이브러리를 사용하여 개발 “쌩”으로 개발 하는 것은 다시한번 고려 해보아야 됩니다…
실시간 서비스를 개발 하게 된다면 중요하게 고려해야될 부분은 Browser의 성능입니다. 일반적인 웹페이지에서는 그렇게 와닫는 문제는 아니지만 실시간 서비스는 반드시  Browser의 성능과 마주하게 됩니다. 그리고 실시간 이벤트 서버를 직접 개발 할 것인가 유료서비스를 사용할 것 인가를 결정하는 것인데  서로 장단점을 충분히 고려 해서 최대한 안정적인 방향으로 결정해야 합니다 그렇지 않으면 “배달건이 안보여요~”라는 피드백을 자주 들을 것입니다. ㅠ
BROS 1.0을 개발 하면서 처음 마주하는 framework와 기술들을 사용하면서 어려움도 있었지만 무척 흥분된 상태로 일했었습니다. 퇴근 하면 거의 매일 코딩했고 주말에도 거의 대부분 코딩으로 보냈습니다. (출근해서 어서 코딩하고 싶다는 생각이 들었습니다  신기하게도 말이죠) 회사를 다니면서 실무를 하면 현실적으로 이런 경험을 하기는 쉽지 않지만 몇가지 상황이 충족이 되었기 때문이였던 것 같습니다.  일단 오프라인(라이더센터와 사업적인)이슈들에 더 집중 되면서 다음 개발 이슈들에 대한 기간적인 여유가 약간 있었고, 처음 접해보는 신기술?(그때 당시에는)로 개발 하면서 흥미가 높았습니다.(와~~ 웹페이지가 새로고침 없이 막 움직인다 움직여~). 또한 개발에 집중 할 수 있도록 챙김이님의 배려의 가 있었기 때문이 아니였나 생각이 듭니다. (늦게 나마 다시한번 회사와 동료들에게 감사 드립니닷~~) 끗!
"
http://woowabros.github.io/woowabros/2017/09/06/dain-techcamp.html,2017-09-06,우아한테크캠프: 좋은 개발자가 되고 싶다면,"안녕하세요, 지난 8월 31일을 끝으로 우아한테크캠프 - iOS 개발 인턴 과정을 마친 김다인입니다. 두 달동안 진행되었던 인턴십 프로그램을 통해 제가 무엇을 배웠고 경험할 수 있었는지를 공유해드리고자 합니다.
우아한테크캠프는 좋은 개발자가 되기 위해서 필요한 실제적인 것들을 집중적으로 배우고 경험해 볼 수 있었던, 개발자 지망생들과 주니어 개발자들에게 있어서 매우 유익한 시간이었습니다.
저는 개발자로서 실무에 필요한 능력들을 키우고 싶어 기업의 인턴십 프로그램들을 찾아보고 있었고, 그렇게 우아한테크캠프에 참여하게 되었습니다. 그리고 기대한 것 이상으로 많은 것들을 얻게 되었는데요, 두 달동안 캠프를 진행하면서 이곳에서 제가 배운 것들이 단순한 인턴 과정을 통해서는 쉽게 배울 수 있는 것들이 아니라는 것을 점차 알게 되었습니다.
무엇이 어떻게 얻어야할지 잘 알지 못하는 상태에서 최고의 선택을 한 것에 대해 정말 감사함을 느꼈습니다.
저를 포함해 우아한테크캠프에 참여한 인턴 24명은 회사 개발 조직과는 별개의 공간에서 코드스쿼드 마스터분들의 지도아래 함께 교육을 받았습니다. 흔히들 생각하는 인턴과는 다르게 정말 교육 중심으로서, 진행되는 실무 중 일부를 단순히 경험해보는 것이 아니라 개발자로서 필요한 능력들을 하나부터 열까지 차근차근 배워나갔습니다. 웹/모바일 개발을 기초부터 심화까지 공부하고, 실습과 코드리뷰를 통해 어떤 코드가 좋은 코드인지를 알게 되었습니다. 또한 웹/모바일 공동 프로젝트를 4명이 한 팀이 되어 진행하면서 협업 과정을 실제적으로 경험하고, 설계부터 배포까지 함께 서비스를 만들어 나가는 과정이 어떤 것인지를 배울 수 있었습니다.
동시에 피플팀분들의 여러 이벤트, 이사님들의 귀한 강연, 그리고 개발자 런치 및 멘토님들과의 교류 등을 통해 우아한형제들이 어떤 조직인지, 어떤 비전과 가치관들을 가지고 있는지, 어떻게 그것들을 성취해나가고 있는지를 자세히 엿볼 수 있었습니다.
앞으로 우리 회사가 계속 성장하고 싶고, 좋은 신입/주니어 개발자를 채용하여 그 성장을 지속하고 싶다면, 회사가 지금까지 받았던 도움을 이 업계에 다시 환원하고, 그를 통해서 다시 도움을 받는 것이 필요한 것 같습니다. - 우아한 테크캠프 참가자를 모집합니다. 중에서
모든 일정이 정말 유익한 시간이었습니다. 이런 귀한 시간들을 마련해 준 우아한형제들의 훌륭한 의식 수준에 감탄하면서 세상이 아직 살만한 곳이구나 또 깨닫게 되었습니다.
첫 주차에는 이후 프로젝트를 같이 해 나가게 될 팀원들과 함께 팀 홈페이지 만들기를 진행했습니다. 이 과정에서 html/css 기본과 AWS기반 리눅스 서버로 웹페이지를 서비스하는 법을 배웠고, 이와 더불어 있었던 여러 미션들을 수행하면서 팀 빌딩의 시간을 가졌습니다. 2~4주차에는 모바일과 웹 각각 12명씩 두 트랙으로 나뉘어 따로 교육이 진행되었습니다. 제가 있었던 모바일 트랙에서는 Swift 기초와 개발환경 구축부터 시작해 MVC패턴과 Delegate, 나아가 동시성 해결, 소프트웨어개발 방법론, 앱 사용성 분석까지 iOS 개발에 있어서 기본부터 심화까지의 내용들을 여러 실습과 함께 익힐 수 있었습니다.
[이 많은 것들을 한 달동안 배웠다니!]
수업 중간마다 실습과 함께 자신의 코드를 사람들에게 공개하며 발표하는 시간을 가졌습니다. 자주 해볼 수 없는 경험이었고, 특히 마스터분들로부터 자신의 코드에 대해서 직접적인 코칭을 받은 것들은 좀 더 나은 코드를 만드는 개발자가 되어갈 수 있는 큰 양분이 되었습니다.  또한 페어프로그래밍 경험들을 통해 동료가 작성한 코드를 이해하는 연습을 해볼 수 있었고, 동시에 다른 이가 이해할 수 있도록 코드를 작성할 수 있어야 한다는 것을 알게 되었습니다. 이 시간들은 결국 동료들과 함께 성장하는 것에 있어서 익숙한 태도와 분위기를 갖게 해주었다고 느낍니다.
[코드리뷰 중 신박했던 노티키: 노티야 일하자!]
5~9주차 동안 이루어졌던 프로젝트의 모든 과정들은 배움의 연장선에서 진행되었습니다. 팀규칙 만들기부터 기획, 백로그 작성 및 설계, 매주 반복된 플래닝, 스크럼, 개발, 데모 및 회고까지 어떤 결과를 내는 것에 집중하기 이전에 이 모든 각각의 과정들을 조금씩이라도 배우고 알아갈 수 있도록 섬세하게 지도해주셨습니다. 사용자들의 입장에서 어떤 기능이 필요한지를 고민하고, 단순한 기능 구현 발표가 아닌 사용자가 무엇을 할 수 있는지의 관점으로 데모를 하는 모든 시간들은 코드를 작성하고 서비스를 만드는 그 자체보다 그것이 사용자들에게 어떤 가치를 가져다주는지를 더 중요하게 여기는 태도를 가지게 해주었습니다. 또한 한 주간의 일정과 그 주의 마지막 날 데모 과정을 통해 서비스를 배포하는 과정들을 간접적으로 경험해볼 수 있었습니다. 회사 구성원들을 대상으로 한 마지막 날의 시연회는 서비스가 왜 필요한지, 어떤 가치를 가져다주는 지에 대해서 다른 이들에게 설득력 있게 어필하는 법을 연습할 수 있었고 동시에 현직 개발자 분들의 자세한 피드백들을 통해 넓은 시야와 다양한 관점들을 경험할 수 있었습니다.
[설계부터 배포까지]
어떻게 보면 이것을 위해서 두 달간의 일정이 이루어졌다고 여길 수 있을 만큼 협업에 대한 것들은 테크캠프를 통해 배울 수 있었던 것들 중 가장 귀한 것이었습니다. 협력을 통해서만이 더 큰 혁신을 이룰 수 있다고 믿는 우아한형제들의 가치관과 마찬가지로 협력과정을 강조하는 코드스쿼드의 협업 중심의 교육이 시너지 효과를 이루어 테크캠프의 전체적인 일정 동안 다른 이들과 함께하는 법은 물론 효과적인 협력이 이루어졌을 때 더 나은 결과를 얻을 수 있다는 마인드를 실제적인 경험들을 통해 직접 배울 수 있었습니다. 커뮤니케이션과 갈등 관리, 페어프로그래밍 등을 통해 왜 많은 개발조직에서 단순 프로그래밍 능력보다 먼저 협업할 수 있는 주니어 개발자들을 원하는 지를 깊이 공감할 수 있었습니다.
[협력이 더 나은 가치를 만든다]
인동소(인턴 동기를 소개합니다), 사내 투어, 따따따워크샵 등 피플팀이 준비해 준 여러 이벤트를 통해서 우아한형제들이라는 공동체가 가진 색깔과 분위기를 인턴들도 함께 느낄 수 있었습니다. 덕분에 독립적인 일정과 공간에서 생활하고 있었음에도 불구하고 우아한형제들이라는 공동체에 대한 소속감과 자부심을 깊이 느낄 수 있었습니다. 여러 이사님들은 강연을 통해서 하나같이 정체성과 관련된 본질적인 이야기들을 해주셨고, 우아한형제들이 어떠한 정체성과 비전을 가진 회사인지 깊이 알 수 있어서 즐거웠습니다. 개발자런치 및 멘토님들과의 만남 등을 통해서는 실제 구성원들이 어떤 분들이고 어떤 생활을 하고 계신지 알 수 있었고, 여러 격려와 조언들을 통해 힘을 얻을 수 있었습니다.
[답은… 1번이 확실합니다]
우아한테크캠프를 통해 함께하는 즐거움을 배웠습니다. 동료들과 많은 추억과 즐거움을 나누었고, 서로 도움을 주며 함께 성장하였습니다. 앞으로도 분명 다시 마주치게 될 것이라고 믿고 다음을 기약합니다.
[함께하는 즐거움]
귀한 시간이었던 만큼 정말 많은 분들의 수고와 노력이 있었을 것이라는 생각이 듭니다. 정말 감사합니다. 우아한형제들, 코드스쿼드, 그리고 techHR 분들께, 앞으로도 우아한테크캠프가 더 나은 모습으로 계속되어서 이를 통해 이루고자 하셨던 귀한 비전이 성취되기를 소원합니다.
부족한 글을 끝까지 읽어 주셔서 감사합니다.
"
http://woowabros.github.io/woowabros/2017/09/05/juneyoung-techcamp.html,2017-09-05,"넌 강해졌다, 돌격해!","안녕하세요. 우아한 테크캠프 iOS 개발 인턴이었던 김준영입니다. 지난 8월 31일에 우아한 테크캠프를 마쳤습니다.
좋은 책을 다 읽은 것처럼 아쉬움과 기분좋은 여운이 남아있네요.
테크캠프에서 배운 점이 학부에서의 몇년보다 많아서 꼽기 어렵습니다만,
그래도 그 중 몇 꼭지를 꼽아 우아한 테크캠프에서 무얼 경험했고 배웠는지를 공유하고자 합니다.
배달의 민족을 필두로 세상을 바꾸고 있는 우아한 형제들과
코딩교육으로 가치를 만드는 코드스쿼드의 역대급 콜라보 교육과정입니다.
개발 인턴과 흡사하지만, 일반 인턴과정과는 달리 코드스쿼드의 마스터님들이 직접 가르치실 정도로
교육에 방점이 찍힌 프로그램이었습니다.
또한 개발뿐 아니라 함께, 스스로 라는 가치에 중점을 두고 협업을 위한 방법을 익힙니다.
지원자들 입장에서는 교육 + 인턴 + 실습 + 프로젝트를 모두 잡을 수 있는 기회였죠.
iOS 는 코드스쿼드 김정 마스터님께, 웹 프론트엔드는 코드스쿼드 윤지수 마스터님께 3주간 가르침을 받았습니다.  그 과정에는 swift 와 javascript에 대한 지식도 있었고, 코딩을 어떻게 하는 지에 대한 내용도 있었습니다.
그 중, 정말 도움이 되었던 건 어떻게 협업할 수 있는 코드 를 짤 수 있을 지에 대한 조언이었습니다.
저와 같은 주니어 개발자(지망생)들은 대부분 혼자 개발을 시작하게 됩니다.
자연스레 기능만 돌아가면 되지! 라고 생각하고 개발을 하는 게 보통입니다. 다른 사람이 제 코드를 읽을 거라고 가정하지 않지요. 하지만 테크캠프는 시작부터 다음과 같은 얘기를 하면서 시작했습니다.
코딩은 글쓰기와 비슷합니다. 늘 다른 사람이 내 코드를 읽을 거라 생각하고 작성하세요.
이런 가정하에, 다음과 같은 이야기들을 들을 수 있었습니다.
이런 이야기를 프로젝트 내내 바로바로 들을 수 있었고, 이는 코드의 방향성을 잡는 데 무척 도움이 되었습니다.
자연스레 기능 구현만이 아니라  같이 일하고 싶은 코드 를 짜는데 방점이 찍히기도 했구요.
테크캠프의 프로젝트는 한 서비스를 웹과 iOS 두 방향으로 만드는 것이 목표였습니다. 이에 따라 팀도 웹 FE 두명, iOS 두명으로 구성되었습니다. 여태까지 경험한 프로젝트는 같은 분야의 개발자와만 일하는 데에서 그치는데 반해, 테크캠프에서는 색다른 경험을 할 수 있었습니다.
저희 팀은 Google Firebase 로 백엔드를 구축하고 관리했는데요, 같은 데이터베이스를 쓰기 위해서 끊임없이 소통해야했습니다. 웹이 사용자 데이터를 날리면 모바일이 갑자기 뻗어버리는 현상이 있지 않나, 모바일에서 올린 리뷰가 바로 웹에서 뜨게 하도록 해야하고.. 당연히, 한 플랫폼만 개발하는 데에 비해서 고려해야할 변수가 많았습니다. 데이터 구조에 대해서 합의해야 했고, 데이터를 업데이트하는 방식도 이야기를 꾸준히 나눠야했습니다. 데이터의 메이저 변경점이 있으면 즉각적으로 모두가 알아야했구요. 이를 위해 어떻게 변경점을 알릴 것인지 합의 했고, 협업도구를 통해 모두가 변경사항을 알 수 있도록 했습니다.
[데이터 구조 설계]
같은 서비스라는 느낌을 주는데도 애로 사항이 있었습니다. 같은 스펙을 구현해야 했구요, UI에서도 사용자에게 동일 서비스를 사용하는 느낌을 주기 위해서 끊임없이 얘기해야했습니다. 때문에 서로의 서비스를 사용해보면서 중간 지점을 찾고 타협하는 과정을 거쳤습니다.
이렇게 협업을 해보니, 한 서비스를 만들어내는 개발자분들 (특히 우아한 형제들)이 너무 대단하게 느껴졌습니다. 일의 절반은 커뮤니케이션 비용이라는 말도 절감했구요. 다른 사람들과 같이 일하기 위해서는 꼼꼼한 설계 후 개발도 꼭 필요하다는 점을 느꼈습니다.
저희 프로젝트는  유난히 뷰가 많은 편이었습니다. 직접 뷰를 짜본 경험이 적어 헤매던 중,  마스터님께서 Cocoa Controls  를  소개해주셨습니다. Cocoa Controls에는 다양한 iOS UI 컴포넌트 오픈 소스들이 올라와있는 사이트입니다. 마침  기본 검색 뷰는  앱의 테마 방향과 어울리지 않아, 이 검색뷰를 참고해서 사용하게 되었습니다.
그런데 막상 소스를 받고 보니 저희 앱에 맞게 고쳐야할 점들이 있었습니다. 일단 영어로 된 항목을 한글로 고쳐야했습니다. 그리고 검색 결과를 누르면 원하는 화면으로 가도록 해야했고, 텍스트가 하나하나 입력 될 때마다 검색하는 방식이 아니어야했습니다. 고치고 나서 원하는 대로 동작 하는지 확인해야 하니, 오픈 소스가 어떻게 작동하는 지 알아야했습니다. 이 과정에서 데모를 돌리고 파일을 하나하나 뜯어 보았습니다.
원하는 대로 고치고 나니, 또 하나의 고민이 생겼습니다. 이렇게 고친 소스를 팀원과 공유해야하는 문제였습니다. swift의 의존성 관리 도구는 Cocoa pod를 사용했는데, podfile에 원하는 pod을 등록해놓고 pod install 커맨드로 설치하는 방식입니다. 이때 설치되는 건 물론 제가 고친 소스가 아니라..  원본 소스가 되겠죠. 마스터님께  여쭤보니, 이런 경우  pod으로 관리하는 게 아니라 프로젝트 파일 하위에 포함해서 사용해야한다고 하셨습니다.
소스를 고치면서, 오탈자를 발견했었습니다. 침착하게 배운대로 저장소를 fork 한 뒤, 변경점을 커밋해서 pull request를 날렸습니다. 그랬더니 짜잔!
[이삭줍기는 즐거워]
저의 첫 오픈소스 기여 기록이 생겼습니다!
오픈 소스를 써보기 위해서는 자잘한 지식이 기반이 되어있어야 했습니다. 의존성을 관리하는 법, 프로젝트 구조, git과 github에 대한 이해도 필요했구요. 이런 부분이 막힐 때마다 여쭤볼 마스터님들이 계셔서 빠르게 적용할 수 있었습니다.
위에서 잠깐 말씀드렸지만 저희 팀은 Google Firebase로 백엔드를 구축했습니다. 우아한 테크캠프에서 백엔드를 다루지 않는 만큼, 빠르게 프로젝트를 만들기 위한 선택이었습니다. 하지만 프로젝트가 진행될수록 속도에 대한 답답함이 생겼습니다.
다른 팀들처럼 api 서버를 만들 수는 없을까?
테크캠프에서는 AWS에 express를 올리는 실습을 간단하게 다뤘었습니다. 그래서 인지, 배워서 해볼 만 할 거라는 자신감이 생겼습니다. 다른 팀들 중에는 rails로 백엔드를 구축한 팀이 있어, django도 되지 않을까? 하는 생각이 들었구요. 결국 django를 AWS  EC2에 올려서 돌리면 되는 거 아냐? 하는 생각에 도달했습니다. 바로 정호영 마스터님께 질문했습니다.
[호눅스님은 구글 그 자체이신게 틀림없다]
바로 그 주 주말, 추천해주신 튜토리얼을 기반으로 django rest framework 를 공부하기 시작했습니다.
djangorestframework 와 django-rest-swagger 를 적용하는 내용의 튜토리얼을 따라간 결과, 하루 정도 만에 원하는 대로 구현하는 데 성공했습니다.
[이렇게 쉽게 API가 만들어지다니]
교육동안 swift에서 json을 받아오는 걸 배웠으니, 앱에 연동하는 건 쉬운 일이 되었습니다.
아쉽게도 결국 프로젝트에는 drf를 적용하지 못했습니다. 다만 배운 건, 지식이 하나씩 늘 수록 할 수 있는 건 몇 배로 불어난다는 점이었습니다. 이전에는 aws도, git, api 도 잘 몰랐기 때문에 조금만 더 배우면 api 서버 구현을  할 수 있다는 사실 자체를  몰랐습니다. 또한 그때 그때 잡아 주시는 마스터님 덕분에 시간 낭비하지 않고 용감하게 배워볼 수 있었죠. 그래서인지 팀원들도 파이썬으로 크롤링을 배워 바로  프로젝트에 적용하는 등 다른 공부에도 도전한 모습이 기억납니다.
우아한 형제들에는 매주 목요일마다 CTO실 전체가 랜덤하게 점심을 함께하는 ‘개발자 런치’ 문화가 있습니다. 감사하게도 테크캠프 인턴들이 교육기간 동안 이 런치에 참여할 수 있었습니다. 또한 강연과 대담 시간에도 개발자 분들이 조언을 아끼지 않으셨죠. 그래서 저희는 우아한 형제들의 개발자 분들의 이야기를 들을 기회가 다양하게 있었습니다.
그 중 인상깊게 남았던 것 중 하나가 개발자로서의 재능에 대한 얘기였습니다.
개발자 지망생들의 고민 중 하나는 분명 내가 재능은 있는거야? 입니다.
웹FE 개발자분들과의 대담 중 이 질문에 대한 대답을 들어, 여기에 일부를 옮깁니다.
사실 개발에 지치고, 개발이 배우기 어렵고 하는 건 항상 있는 일이에요.
재미가 없을 수도 있어요. 그럴 때마다  ‘내 적성은 이게 아닌가? ‘ 하는 분들을 봅니다.
…
지금 개발이 좋다고 하면, 스스로의 모티베이션을 만들어가는 노력을 해보세요. 다른 사람이 ‘이래야한다’ ‘이렇게 하자’ 라고 어떤 근사한 동기부여를 해도 그건 자기 것이 아니거든요. 스스로 나아갈 줄 아는 게 재능이에요.
테크캠프 인턴 친구들에게도 많이 배웠습니다.
이 친구들은 참 이상합니다. 분명히 6개 팀으로 나뉘어서 서로의 프로젝트를 발표해야하는 데 경쟁심이 없습니다.
다른 팀이 막히는 부분을 소스코드를 같이 보면서 고쳐보고, 모르면 찾아서라도 알려줍니다.
좋은 소스가 있으면 공유하고, 데모가 잘 안되면 진심으로 안타까워합니다.
서로의 서비스를 적극적으로 사용하면서 개선점을 말해주고 갑니다.
이런 태도와 분위기 덕분에 누구나 모르는 걸 부끄러워하지 않고 적극적으로 배울 수 있었습니다.
꿈같던 테크캠프가 끝나고 이제 현실로 돌아갈 때가 되었네요.
이런 기회를 제공해주신 우아한 형제들, 코드스쿼드, techHR 팀분들께 너무 감사합니다.
만약 후에 테크캠프 지원을 망설이시는 분이 있다면, 더 강한 개발자가 되기위해서 꼭 지원하세요.
이보다 더 행복하게 성장할 기회는 많이 없으니까요.
부족한 글 읽어주셔서 감사합니다.
[넌 강해졌다, 돌격해!]
"
http://woowabros.github.io/woowabros/2017/09/04/woowa-techcamp2.html,2017-09-04,우아한테크캠프 - 8월의 일기,"유난히도 더웠던 8월. 7월에 배운 교육 과정을 바탕으로 참가자 분들이 스스로 프로젝트를 기획하고 개발하는 실습 과정을 가졌습니다. 이번 프로젝트는 나 혼자 개발이 아닌 함께 하는 개발의 의미를 찾는 과정으로 그 동안 경험하지 못했던 협업을 통해 서로 부딪히고, 조율하고, 부족한 부분을 채워가는 시간이었습니다.
기획서 작성을 시작으로 8월 프로젝트의 첫 단추를 끼웠습니다! 의견들이 다양하여 조율하는데 어려움을 겪었지만 프로젝트의 첫 시작이니 만큼 다들 설레고, 적극적인 모습을 보여주었습니다. 또한 개발 시작 전 git & github 기반 협업과정을 통해 서로 공유하고 관리하는 법을 이해하며, 프로젝트에서 가장 중요한 협업을 재차 강조 하였습니다.
평소라면 늦잠 자고 가족, 친구들과 여행 다니며 즐겁게 보낼 수 있는 여름방학이지만, 힘들다는 투정 없이 늘 밝은 모습으로 참여해 준 참가자 분들을 위해, 저희 캠프 운영진들은 고마움과 8월 프로젝트를 응원하는 마음을 담아 잠시 쉬어 가는 시간을 마련 하였습니다. 7월의 교육 과정을 잘 마친 서로를 격려하며  본격적으로 프로젝트 개발 전 사기 충전과 팀워크를 다지는 시간이 아니었을까 생각합니다. 

프로젝트의 완성도를 높이기 위한 배민 디자이너들과의 협업이 있었습니다. 어색하고 설레는 첫 만남 후, 프로젝트를 시작하자 마자 쏟아지는 참가자 분들의 열정에 디자이너 분들도 내심 긴장하며 서로에게 자극이 되는 시간이었습니다. 실무로 바쁜 와중에도 참가자 분들의 프로젝트를 위해 디자인 작업 및 피드백을 아끼지 않는 모습을 보며 우아한테크캠프에 대한 디자이너 분들의 많은 관심을 엿볼 수 있었습니다. 

우아한테크캠프는 모바일과 웹 두 트랙으로 진행한 교육형 인턴 프로그램입니다.  교육에 참여하고, 프로젝트를 진행하면서 가장 궁금했던 배민의 iOS와 웹 FE란 무엇인지, 어떻게 진행되고 있는지 실무자의 목소리로 생생하게 들을 수 있는 만남의 장을 마련 하였습니다. 그 외에도 개발자 선후배의 입장으로 iOS 개발자, 웹 개발자로서 진로나 개발자로서 겪는 일들에 대한 생각을 공유하는 의미있는 만남이였습니다. 

캠프 운영진과 강사진이 이틀 동안 24명의 캠프 참가자 분들을 한 명 한 명을  만나  이야기를 나누는 근황Talk를 진행 하였습니다. “요즘 어떻게 지내는지? 캠프를 진행하는 동안 가장 좋았던 점이나, 힘든 점은 없었는지? 팀 프로젝트를 하면서 힘들지는 않는지? 이번 캠프를 통해 개발자로서 한 층 더 성장한 것 같은지?”  등의 질문과 답변을 통해 서로 피드백을 주고 받는 시간이었습니다. 24명 모두 캠프에 대한 만족도가 매우 높았고, 시간이 금방 가는 것에 대한 아쉬움을 함께 나누었습니다. 참가자 한 명 한 명 만난 이 시간을 통해 인턴 분들도 그 동안 캠프나 회사에 대해 궁금했던 점이나 개발자로서의 성장에 대한 고민을 나누는 기회가 되었고, 한층 더 가까워진 계기가 되었습니다.
캠프가 끝나 갈 무렵, 플래닝 - 스크럼 - 개발 Iteration - Demo와 회고의 반복으로 조금은 지친 참가자 분들을 위해 다같이 웃고 떠들며 힐링 할 수 있는 WWW워크숍을 준비 하였습니다. 서로가 얼마나 잘 알고 있는지를 확인하는 OX퀴즈와 배달의민족 장안의 화제인 비욘드치킨(일명, 치믈리에)를 진행 하였습니다. 팀 프로젝트를 하느냐 평소 다른 팀과의 교류가 적었던 분들의 아쉬움을 달래고, 프로젝트로 지친 몸과 머리를 재충전 하는 즐거운 시간 이었는데요. 인턴 분들의 끊이지 않는 웃음소리와 미소 가득한 얼굴을 보며 진행하는 피플팀도, 준비한 운영진도 뿌듯함을 느꼈습니다. 

8월 31일. 우아한테크캠프가 드디어 두 달 간의 여정이 끝났습니다. 팀 별 프로젝트 Demo와 전체 회고를 통해 지난 두 달을 돌아보는 시간을 가지며 서로의 결과물을 공유하고 격려하였습니다. 또한 7월 교육과 8월 프로젝트의 결실을 맺는 시연회를 가졌는데요. 평소 우아한테크캠프에 관심이 많았던 우아한형제들 개발자 분들이 줄을 서서 대기 할 정도로 많이 참여해 주셨습니다. 단순히 결과물을 살펴 보는 것에 그치지 않고, 질문과 피드백을 통해 애정 어린 관심을 표현하였습니다. 캠프 운영진과 강사진 모두 상기된 얼굴과 들뜬 목소리로 결과물을 시연하는 참가자 분들의 모습에 흐뭇하지 않을 수 없었습니다. 근 한 달 동안 개발한 결과물임에도 불구하고, 기대 이상의 전문성이 돋보인 이번 프로젝트 결과물은 많은 분들의 호평으로 마무리 되었습니다.


서동욱 참가자 - 모바일 개발
매일 아침 커피를 사고 회사의 내 자리에 앉고, 팀원들이 다 도착하면 커피 한 모금 마시면서 오늘 각자 어떤 일을 할지 말하고, 하루를 열심히 일하다가 퇴근 전에 모여서 오늘 어떤 일을 했는지 정리하는 시간을 갖고…. 이런 하루들이 모여 내가 아닌 우리가 만든 프로젝트의 결과물을 보고 있으면 놀랍습니다.  혼자서는 쉽게 포기했을 것 같은 프로젝트가 이제는 실제로 써볼 수 있는 제품으로 만들어졌다는 게 말이죠. 협업의 힘을 느낄 수 있었던 캠프였습니다.
첫 한 달은 IOS app를 만들기 위한 지식을 배우는 시간 이었다면 남은 한 달은 그 지식을 협업의 장에서 사용해 보면서 지식이 아닌 본인의 지혜로 만드는 시간이었습니다. 개발을 업으로 삼고 싶은 사람이라면 꼭 참여해야 하는 캠프가 바로 이 우아한테크캠프라고 자신 있게 말할 수 있습니다. 기획부터 개발 그리고 유지보수까지 개발자가 하는 일들의 일련의 과정을 직접 해볼 수 있었습니다.
진로를 개발 쪽으로 바꾸고 있는 시기였는데, 저에게 이번 우아한테크캠프는 정말 인생의 전환점이었습니다. 이 기회를 제공해준 우아한형제들, 그리고 모든 과정에서 항상 가르침을 주셨던 코드스쿼드 마스터분들께 감사합니다.
박진수 참가자 - 웹프론트엔드 개발
프로그래밍은 필요에 의해서 만들어나가는 과정입니다. 지금까지는 재미로 해왔지만 이제는 팀의 일원으로서 해야만 하게 되었습니다. 프로젝트를 하면서 정말 친한 친구보다 더 많이 얘기를 나눈 사이가 된 우리들. 이견과 의견이 난무하는 회의 속에서 마침내 기획을 확정하고 모든 팀원이 생각하는 방향을 하나로 모았을때 안도감. 막상 처음부터 끝까지 페어프로그래밍으로 웹페이지를 만들자고 했을땐 걱정했지만 고생하고 노력한 만큼 좋은 결과물이 나와 느끼는 이 뿌듯함. 마음먹은 대로 잘 되지 않아 고생도 많이 했지만 테크캠프에서 보고 듣고 느끼며 경험한 것들이 피와 살이 되어가고 있다는 것을 느끼고 있습니다.
서로 다른 사람들을 만나 이야기하면서 때로는 자극을 많이 받기도 하고, 때로는 누군가에게 자극을 주기도 하면서 생활해 왔다는 점이 놀랍습니다. 제게 테크캠프는 성장과 함께 물음표를 가져다 주었습니다. 테크캠프가 끝나더라도 개발자가 되기 위한 개발과 공부를 게을리 하지 않을 것입니다. 개발을 생각해보거나 지망하고 있는 사람들에게  우아한테크캠프는 정말 큰 기회이자 터닝 포인트가 될 수 있다고 생각합니다.
어디서도 쉽게 배우기 힘든 프론트엔드를 즐겁게 가르쳐주신 코드스쿼드 마스터님들과 즐거운 인턴 생활을 위해 물심양면으로 도와주시고 궁금한 점이 있을때마다 아낌없이 말씀해주신 우아한형제들 분들에게 깊이 감사드립니다.


2017년의 뜨거운 여름. 짧기도 길기도 했던 우아한테크캠프가 끝났습니다.  부족한 부분도, 아쉬운 부분도 많았지만 열정 넘치는 24명의 참가자 분들과 진정성과 전문성을 겸비한 최상의 교육을 제공해주신 코드스쿼드 마스터 님들, 그리고 우아한테크캠프에 많은 관심을 가지고 지원해주신 우아한형제들 구성원들 덕분에 무사히 잘 마칠 수 있었습니다.  개발자로서 멋있게 성장할 우아한테크캠프 참가자 분들을 응원하며 다시 만날 때 까지 뜨거운 안녕 ! : D

"
http://woowabros.github.io/experience/2017/08/28/mqtt_stress_test.html,2017-08-28,Mqtt Stress Test,"아래의 링크의 글을 읽고 오시면 이 글을 읽는데 도움이 됩니다.(아마도?)
MQTT 적용을 통한 중계시스템 개선 
조만간 Mqtt를 사용하는 새로운 업주용 솔루션이 늦어도 올해안에는 출시될 예정입니다. 
그동안 RESTFul API 기반의 주문정보 조회시스템은 안정적으로 잘 동작했으나 Multi Agent기반으로 확장하기에는 태생적인 한계로 몇가지 문제점을 안고 있었는데요.
RESTFul API는 Connection이 유지된 상태가 아니므로 서버에서 즉시 클라이언트로 정보를 전달할 수 없고 (이것이 가장 큰 문제) 
Multi Client기반으로 데이터를 전달하기 위해서는 등록되어 있는 클라이언트에 대칭되는 데이터를 서버에 준비해야 합니다. 
또한 이 데이터를 클라이언트가 중복되거나 누락되는 일 없이 전달되도록 치밀하게 처리해야 합니다. 예를 들면
클라이언트가 접속하기 전 생성된 데이터를 받을 수 있도록 서버 정보를 갱신해야 할지 라던가 
클라이언트가 받아가지 않고 종료된 데이터를 어떻게 삭제해야 할 지를 고민해야 하죠. 
(물론 이런 문제는 Mqtt를 사용하더라도 Plan B로 처리해야 합니다.)
Mqtt를 사용하기로 결정이 나고 서버를 올렸으나 이것은 적어도 우아한형제들에서는 한번도 적용해 본 적이 없는 새로운 시스템. 
아무도 얼마나 성능이 나올지 얼마나 안정적으로 돌릴 수 있을지 감이 없습니다. 
하여 간단히(?) 테스트 프로그램을 작성해서 스트레스 테스트를 해보기로 했습니다.

테스트를 위해서는 아래와 같은 기능이 필요했습니다.
대충 Controller를 하나 만들고 그 프로그램이 Mqtt Client를 생성하는 프로그램을 실행하게 하자. PC당 대충 2000개쯤 만들면 되겠지 라고 생각하고 테스트 코드 작성.

느려!!!
돌아가는 모양새를 보니 프로세스를 생성하는 부분이 부하가 있거나 Windows Form생성부분에서 부하가 있는것으로 보입니다. 
나중에 생각해보니 메모리를 프로세스당 1M만 써도 2000대면 2G가 되더군요.(이래서 수학을 열심히 해야 합니다.)
1000대 정도는 어떻게 되는 것 같은데 2000대는 도저히 테스트 할 수 있는 상태가 아닙니다. 
PC당 1000 Connection을 처리하면 예상 테스트 수치 30000대를 처리하려면 PC가 30대가 있어야 합니다. 
프로세스당 1개의 Connection을 처리하던 Mqtt Client를 Client당 10개의 Connection을 생성하도록 변경합니다.
적당히 200개 정도 생성하도록 하여 PC당 2000 Connection을 테스트 할 수 있게 되었습니다.


테스트를 수행하다 보니 접속과 종료를 빈번하게 테스트 해야 하는데 이걸 일일이 사람이 처리해야 하는게 너무 귀찮습니다. 테스트를 용이하게 하기 위해 Controller에도 Mqtt를 붙여봅니다.

이제 프로그램만 띄워 놓으면 일괄적으로 테스트를 수행할 수 있게 되었습니다.

테스트를 하다보니 문제점이 발생합니다. Mqtt서버가 다운되니 Controller에 붙어있는 Mqtt까지 같이 다운되어 통제불능 상태가 됩니다. 
서로 서버를 분리해야 할 필요가 있습니다. Controller에 서버정보를 고정으로 심고 Stress Test에 사용할 서버정보는 테스트 실행 메세지에 전송하도록 합니다.


테스트 하고 나면 Mqtt Client당 하나씩의 로그를 저장하도록 했습니다. 
이 로그를 정리해서 봐야 할 필요가 생겼습니다. 일일이 부탁해서 로그를 취합해도 되지만 이미 PC접수 로그수집을 위해 사용하는 채널을 이미 AWS S3에 운영하고 있으니 이곳에 모아보기로 합니다.

하여 개략적인 전체 구성도는 아래와 같습니다.

결론
아직 계속 튜닝을 진행하고 있는 단계여서 결론을 내리기엔 성급하지만 Mqtt는 ‘가벼운 클라이언트’를 위해 서버는 많은 일을 하고 있는 것으로 보입니다. 
몇만대 수준의 Connection을 이루기 위해서는 서버의 튜닝과 더불어 클라이언트의 subscribe와 ping과 같은 부분도 세심하게 조절해야 할 것 같습니다.
테스트 일정이 뒤로 밀리면서 테스트가 끝나지 않았는데 글을 발행하게 되었습니다. 다음에 기회가 되면 After Service로 찾아뵙도록 하겠습니다.
"
http://woowabros.github.io/experience/2017/08/21/hystrix-tunning.html,2017-08-21,Hystrix! API Gateway를 도와줘!,"안녕하세요. 배민FRESH 서비스를 개발하고 있는 조건희입니다.
오늘은 API Gateway를 사용하면서 겪은 뼈아픈(?) 장애 사례와 해결 과정을 간단히 공유하고자 합니다. 너무 자세하게 쓰지는 않았습니다. 관련된 링크를 충분히 공유하오니 양해 부탁드립니다.
어느 날 저녁 평소보다 많은 트래픽이 갑자기 유입됩니다. 보통 마케팅이나 모바일 앱 푸시 일정이 공유되지만, 이날은 예고 없이 찾아왔습니다.

그림 1. 요청 수 추이
그런데 이 중에서 꽤 많은 비율로 502 Bad Gateway 응답이 발생합니다. (어흑)

그림 2. 요청 수와 502 응답 수 추이
어디서 502를 응답한 걸까요? 확인해 보니 저희가 사용하고 있던 API Gateway에서 발생했네요. 더는 문제가 지속되지는 않았지만, 원인 분석과 추가 조치가 필요해 보였습니다. 마침 그날은 앞으로 API Gateway를 담당하게 될 거라고 전달받은 날이었습니다. 잘됐다 싶었죠. 확인해 보겠다고 했습니다.
API Gateway는 왜 502를 응답했을까요? 문제를 좀 더 이해하고 싶었고요. 그래서 502 원인을 추적하기 시작했습니다. 가장 먼저, 502를 응답한 URL들을 통계 내어 살펴봤습니다. 별다른 특이사항이 보이지 않네요. 다음으로는 서버 로그를 살폈습니다. Root Cause Exception에 다음과 같은 메시지가 보입니다.
처음 보는 메시지입니다. 일단, 예외가 발생한 지점의 코드를 열어 봤습니다. 코드는 많이 단순화시켰으니 주의하세요!
executionSemaphore.tryAcquire()가 거짓을 반환해서 발생한 거군요. 참고로, 저희는 API Gateway의 한 구성요소로  Hystrix를 사용하고 있습니다. 위 코드는 Hystrix의 AbstractCommand 부분이고요. getExecutionSemaphore 부분의 코드도 잠깐 살펴봤습니다.
아직 뭔지는 잘 모르겠지만 어쨌든 SEMAPHORE 여부를 판단하고 있습니다. 그리고 좀 더 코드를 따라가 보면, 발생한 예외가 ZuulFilter의 구현체인 RibbonRoutingFilter에서 잡히고요. 바로 여기서 502 응답을 만들어 반환합니다.
그렇습니다. 저는 평범한 개발자입니다. 이 코드만을 보고 해결책을 찾아낼 리가 없죠. 하지만 적어도 502는 Hystrix가 만든 것이고, RibbonRoutingFilter에서 HystrixRuntimeException을 처리한 결과임은 알 수 있었습니다.
현재 저희 API Gateway는 3가지 라이브러리를 사용하고 있습니다.
그중에서도 Hystrix가 예외를 발생시켰고요. 따라서, 이 녀석에 대해 좀 더 살펴보기로 했습니다. 다음은 Hystrix에 대한 간단한 설명입니다.
In a distributed environment, inevitably some of the many service dependencies will fail. Hystrix is a library that helps you control the interactions between these distributed services by adding latency tolerance and fault tolerance logic. Hystrix does this by isolating points of access between the services, stopping cascading failures across them, … (중략)
- 출처: Hystrix Wiki, What is Hystrix?
요약하면 이렇습니다.
“장애 내성fault tolerance과 지연 내성latency tolerance을 가진, 분산된 서비스들 간의 통신을 돕는 라이브러리”
Spring Getting Strated, Circuit Breaker에서는 Hystrix를 Circuit Breaker 구현체라고 소개하기도 합니다. 그리고 Hystrix, How it Works 문서는 Hystrix의 내부 동작을 잘 설명하고 있는데요. 여기서 아래의 순서도를 만날 수 있습니다.

그림 3. Hystrix Command 동작 순서도 (크게 보기)
어? 그런데 이 그림에서 4번, 5번이 유독 눈에 들어오네요. 저는 반가웠습니다. 왜냐구요? 위에서 살펴봤던 코드를 그대로 표현하고 있기 때문이었죠. 저만 반가운 건 함정.
이번에는 application.yml 파일을 살펴봤습니다. 저희가 Hystrix를 어떻게 설정하고 있나 궁금했기 때문입니다.
어라, 또 한 번 반갑지 않으신가요? 이것도 위에서 살펴봤던 코드와 관련 있고요. 그림 3의 5번 부분이기도 합니다. 아무래도 격리 전략Isolation Strategy을 살펴봐야겠다고 느꼈습니다. 이 때, Hystrix, How it Works를 주로 참고했고요. 한 문장으로 정리하면 다음과 같습니다.
“일종의 Bulkhead 패턴이며, 각 서비스에 대한 의존성을 격리하고 동시 접근을 제한한다.”

그림 4. Hystrix Isolation
위 그림은 Hystrix가 어떻게 의존 서비스들을 격리하는지 보여줍니다. 빨간색으로 표기된 네모 상자는 특정 서비스의 호출이 지연되고 있다는 뜻인데요. 나머지 서비스에는 영향을 주지 않고 있습니다. 또한, 동시 접근 제한을 서비스별로 다르게 설정하고 있습니다.
격리의 방법으로는 2가지가 존재합니다.
Thread 방식에서는 서비스 호출이 별도의 스레드에서 수행됩니다. 예컨대, Tomcat의 스레드 풀과 서비스에 대한 호출 스레드가 격리되는 거죠. 이렇게 하면 네트워크상의 타임아웃 위에 스레드 타임아웃을 둘 수 있습니다. 하지만 별도의 스레드를 사용하는 만큼 비용이 수반됩니다. 여기서는 이를 가리켜 연산 오버헤드computational overhead라고 표현하네요.
한편, Semaphore 방식에서는 서비스 호출을 위해 별도의 스레드를 만들지 않습니다. 단지 각 서비스에 대한 동시 호출 수를 제한할 뿐입니다. 위에서 살펴봤던 코드 덕분에, 내용을 좀 더 쉽게 이해할 수 있었습니다. 아래 그림은 Thread와 Semaphore의 차이를 보여줍니다.

그림 5. Thread와 Semaphore의 격리 방식 (크게 보기)
위에서 살펴봤던 코드 중에 executionSemaphore.tryAcquire() 부분이 있습니다. 여기 코드를 좀 더 살펴보면, HystrixPropertiesManager.EXECUTION_ISOLATION_SEMAPHORE_MAX_CONCURRENT_REQUESTS를 만나게 되는데요. 이는 execution.isolation.semaphore.maxConcurrentRequests 프로퍼티에 대응하는 상수입니다. 처음에는 이 수치를 좀 더 늘리려고 했습니다. 왜냐면 기존에 저희가 SEMAPHORE를 격리 전략으로 사용하고 있었고요. 여기서 Semaphore 구간에 진입할 수 있는 요청의 수만 늘려주면, API Gateway의 동시 처리량이 늘어날 테고, 그러면 502 발생 가능성이 감소할 거로 생각했기 때문입니다. 프로퍼티에 대한 설명은 여기를 참고하세요. (폴백fallback을 설정한 경우도 가용한 Semaphore 개수의 제한을 받습니다.)
하지만 Hystirx Configuration 문서를 좀 더 살펴보면, 아래와 같은 내용이 있습니다.
The default, and the recommended setting, is to run HystrixCommands using thread isolation (중략)
Commands executed in threads have an extra layer of protection against latencies beyond what network timeouts can offer.
Generally the only time you should use semaphore isolation for HystrixCommands is when the call is so high volume (hundreds per second, per instance) that the overhead of separate threads is too high; this typically only applies to non-network calls.
HystrixCommand를 사용할 때는 Thread 격리를 권장한다는 내용입니다. (서비스 호출의) 지연으로부터 보호되는 별도의 계층을 가질 수 있으니까요. 그리고 Semaphore를 써야 하는 경우는 유일하다고 합니다. 꽤 강력한 어조네요. 의구심이 생길 만큼요. 바로 “호출량이 너무 많아서 분리된 스레드의 사용이 주는 오버헤드가 큰 경우”이고요. 네트워크 요청이 발생하지 않는 경우non-network call가 보통 여기에 해당한다고 합니다. 좀 더 찾아보니, Netflix API도 아주 일부만 Semaphore를 사용하는 군요. 인메모리 캐시에서 메타 데이터를 가져오거나, Thread 방식에 대한 퍼사드facade에 한해서요.
저희가 기존에 Semaphore를 선택한 이유는 성능 테스트 결과가 더 좋았기 때문이라고 합니다. 확인을 위해 다시 한번 테스트해 보았는데요. 여전히 더 나은 속도를 보이네요. 하지만 유의미하게 느껴지는 차이는 아니었습니다. 또 항상 그런 것도 아니었고요. 확증 편향이 시작된 건가! 만약, 설정(재시도, 타임아웃 등)을 안정적으로 관리할 수 있고, 의존 서비스들을 충분히 신뢰할 수 있다면 Semaphore를 선택할지도 모르겠습니다. 연산의 오버헤드 비용이 격리가 주는 이득을 넘어선 시점인 거죠. 하지만 지금은 아니라고 판단했고요. 그래서 Thread로 결정했습니다.
지금까지 Hystrix는 무엇이고, 그중에서도 격리가 무엇인지 살펴보았습니다. 그리고 격리 전략은 Thread를 선택하기로 했고요. 이제는 부하 당시 만났던 502를 줄여나갈 차례입니다. 작업 순서를 요약하면 다음과 같습니다.
*기본적으로 API Gateway의 동시 처리량과 TPS를 높이는 것이 목표입니다. 이것만으로 부족하다면, 스케일 아웃을 얼마나 해야 하는지 판단 근거를 마련하고자 했습니다.
우선 프로덕션 환경과 유사한 테스트 환경을 마련했습니다. 서버 인스턴스는 물론, URL도 부하 당시의 것들을 사용했습니다. 부하가 특정 임계치에 다다르니 API Gateway가 502를 응답하기 시작하네요. 서버 로그에도 “could not acquire a semaphore for execution” 메시지가 찍힙니다. 처음에 만났던 메시지군요.
여기서부터는 조금씩 Hystrix 설정을 바꿔가며 부하 임계치를 늘려나갔습니다. 처음에는 API Gateway가 병목이었는데요. 점차 의존 서비스들이 부하를 못 견디는 상황으로 바뀌었습니다. 이대로 만족할 수 없어서, 의존 서비스들의 인스턴스들을 추가 투입해가며, API Gateway의 성능을 개선해 나갔습니다.
결과적으로 변경한 설정값은 아래 표의 5가지입니다. 참고로,  zuul.ribbonIsolationStrategy  설정값을 THREAD로 바꾸면, 기본적으로 모든 라우팅에 Hystrix의 스레드 풀이 사용됩니다. 관련 내용은 Spring Cloud Netflix, How to Configure Hystrix thread pools을 참고하세요.
표 1. 성능 개선을 위해 변경한 설정과 간단한 설명
그 외에도 아래 그림을 참고하여 커넥션 관련 설정을 조정해 주었습니다.

그림 6. Hystrix ThreadPool 설정 예시. 출처: Hystrix Configuration, ThreadPool Properties (크게 보기)
어느 정도 개선이 되었다고 판단했습니다. 그리고 기존과 달라진 점을 확인했는데요. 먼저 단순 TPS 수치를 비교했습니다. 기존 대비 약 4배가량 성능이 개선되었네요. 동시 처리량도 늘어났고요. 이제 이 수치를 부하 당시에 대입해 보았습니다. 기존 처리량으로는 확실히 (수치상으로도) 부하를 견디지 못하겠네요. 하지만 개선 후에는 더 적은 수의 인스턴스로 더 많은 요청을 처리할 수 있습니다. 정말 다행입니다. 서비스가 열심히 홍보돼서 사용자들이 유입됐는데, API Gateway가 이를 막아버리는 꼴이었으니까요. 요구사항을 만족시키지 못하는 기술이 무슨 의미가 있나요. 관리의 부담만 늘어나는걸요.
개선된 내용은 팀에 공유하고 배포했습니다. 현재까지 아무런 문제 없이 잘 사용하고 있습니다 :)
지금까지 서비스가 장애를 만난 상황, 부하를 일으킨 지점의 코드, Hystrix와 격리 전략, 개선 내용과 결과를 간략하게 살펴보았습니다. 사실 문서와 코드를 살펴보면서, 궁금한 부분들이 줄어들기는커녕 늘어나기만 했습니다. 해야 할 일도 굉장히 많아 보였고요. 앞으로 꾸준히 문서와 코드를 뒤적거리며 공부해야겠다고 느꼈습니다. 더 중요한 것은 API Gateway를 통해 우리가 해결하려는 문제입니다. 공부를 하다 보면 목적을 자주 잊어버리곤 하는데요. 우리가 왜 이 기술을 쓰는지, 정말 필요한 것인지, 다른 대안은 없는지 꾸준히 고민해야 하겠습니다.
부족한 글 읽어주셔서 감사합니다. 끗.
"
http://woowabros.github.io/tools/2017/08/17/ost_bash.html,2017-08-17,개발환경을 한 방에! 쉘 스크립트의 힘,"안녕하세요. 우아한형제들 서비스개발실 주문시스템개발팀의 라태웅입니다.
어느 개발 조직이건 새로운 사람이 들어오면 그 사람의 PC에 새롭게 개발 환경을 셋팅해야 합니다.
이 때, 저는 세 가지 경우를 경험했는데요.
1번의 문제는 문서가 노후화되고 사람이 이해하기 쉽게, 따라하기 쉽게 작성하기가 어렵다는 단점이 있습니다.
2번의 문제는 전달자가 헷갈리거나 오래되어 잊어버린 경우 굉장한 혼선이 온다는 것, 거기에 신규 입사자 입장에선 이 조직이 과연 이대로 괜찮은가 하는 생각이 들 수도 있겠죠.
하지만 3번처럼 쉘 스크립트 한 방으로 셋팅이 된다면?!
신규 입사자의 감탄사가 들리시나요!? (와아아아아아아아아아!!!!!!!!!!)

들리..십니까? 제 목!쏘리가! 하!늘에!
이 글에서는 두 가지 쉘 스크립트를 작성할건데요.
첫 번째,
보너스로 PHP Codeigniter를 AWS ElasticBeanstalk에 Deploy 해보기.
두 번째,
이 말인즉슨, 신규 입사자는 첫 번째 쉘 스크립트로 기본적인 AWS 기반의 개발 환경이 갖춰지고, 두 번째 쉘 스크립트로 로컬 환경에서 개발이 바로 가능해진다는 것이죠!
하지만 이 글에서는 쉘 스크립트의 문법에 대해서는 다루지 않습니다! 이곳을 참고해주세요! 이 글에서는 쉘 스크립트로 꽤 많은 것을 편하게 할 수 있다는 것을 알려드리는데에 초점이 맞춰져 있습니다.
먼저 프로젝트 폴더를 생성하고, 내부에 디렉토리 구조를 만들어봅니다.
만들어진 폴더 구조는 아래와 같습니다.
이제 쉘 스크립트를 작성해봅니다.
이제 이 쉘 스크립트를 실행 가능한 상태로 만들어줍니다.

ls -al 명령어로 확인하면 실행 권한이 추가된 것을 확인할 수 있습니다.
이제 만든 쉘 스크립트를 실행해 볼까요?

패스워드를 입력하면..

이것저것 설치를 합니다!

AWS Access Key를 넣어주면..

이렇게 자동으로 Credential 까지 생기게 되죠!
잠깐! 혹시 AWS Access Key가 없으신가요?

AWS에 로그인 후 IAM 서비스로 이동합니다.

Groups > Create New Group을 눌러 새로운 그룹을 생성합니다.

Group Name을 적은 후 Next Step을 누릅니다.

테스트용으로 쓸거기 때문에 AdministratorAccess를 체크하고 Next Step을 누른 다음, Create Group을 눌러 그룹 생성을 마칩니다.

이제 유저를 생성할 차례입니다. Add User를 눌러주세요.

사진과 같이 적은 후 Next: Permissions를 누릅니다.

방금 만들었던 SSPJ 그룹에 유저를 추가하고 Next: Review를 누른 뒤, Create User를 눌러 유저 생성을 마칩니다.

SYSTEM : 독자이(가) Access Key을(를) 획득했다!
이제 다시 돌아와서…
먼저, webserver.zip 파일을 다운로드 받고, ~/sspj/webserver 폴더에 압축을 풉니다.
AWS ElasticBeanstalk에 관한 설명은 Elastic Beanstalk Configuration files(.ebextensions)에서도 보실 수 있습니다. 나는 쉘 스크립트만 궁금하다 하시는 분은 이 파트를 건너뛰셔도 됩니다!
PHP Codeigniter 기반으로 작성된 뼈대 웹서버 인데요. 이를 AWS ElasticBeanstalk CLI로 간편하게 배포해보겠습니다.
위의 eb는 AWS ElasticBeanstalk CLI 인데요. 아까 첫 번째 쉘 스크립트가 자동으로 설치해준 녀석입니다. init 명령어를 통해 최초 설정을 해줍니다.

서울을 선택해줍니다.

새로운 어플리케이션을 만듭니다.

어플리케이션 이름은 sspj로 해줍니다.

네. 우리의 웹서버는 PHP가 맞습니다.

저는 항상 최신 버전으로 개발하려고 노력합니다.

이 글에선 안 쓰지만 아쉬우니 SSH Key도 만들어 봅니다.

새로운 키페어를 생성합니다.

키페어 이름은 sspj-keypair로 하겠습니다.

패스워드는 자유롭게 설정합니다.
ElasticBeanstalk Application이 만들어졌습니다. 이제 Environment를 만듭니다.

Environment 이름과 도메인 Prefix를 설정합니다.

로드밸런서는 클래식으로 해봅니다.

로그가 찍히더니 성공했다고 나옵니다!

웹 콘솔로 접근해보니 잘 배포되었군요!

접속도 잘 되구요!
이제 다음 배포부터는 정말 간단합니다.
정말 쉽죠?
그런데 개발할 때 테스트를 위해서 기능을 수정할 때마다 deploy를 하고 수정하고 하는 식은 너무 불편하겠죠. 그래서 이를 Docker로 로컬 환경에서 실행해보겠습니다.
두 번째 쉘 스크립트에 작성하기 전에, webserver.zip 파일을 다운로드 받고, ~/sspj/webserver 폴더에 압축을 풉니다. 이미 앞선 파트에서 받으셨다면 받지않으셔도 됩니다.
저희는 Docker를 통해 개발 환경을 설정할 것이기 때문에 먼저 Docker를 설치해야 합니다. (이곳(Docker for Mac)에서 설치할 수 있습니다.)
먼저 Dockerfile을 만듭니다.
Dockerfile에 대한 자세한 설명은 진행하지 않습니다! 이런식으로 쉘 스크립트를 사용할 수 있다고만 봐주세요!
Docker에서 바라볼 config 파일을 생성합니다.
이제 이 Docker를 자동으로 빌드하고 실행하는 스크립트를 작성합니다.
쉘 스크립트 자체는 정말 짧지요?
실행해보겠습니다. (일단 실행 권한을 먼저 줘야겠죠?)

뭔가 열심히 설치를

다 했더니!

로컬에서 바로 접속이 됩니다!
이번 글에서는 쉘 스크립트를 한 번 만들어두면 개발 환경을 쉽게 구성할 수 있습니다! 라는 것을 전달드리고 싶었는데요… 다 적고보니 뭔가 뒤죽박죽이 된 것 같습니다 -0-…
결론은! 이렇게 누군가가 셋업을 해두면, 다음 사람들은 Docker를 설치하기만 하면
이 명령어로 바로 개발이 가능하다는 것이죠.
여러분의 개발 환경에도 적용해보시는 것은 어떨까요?
감사합니다!
"
http://woowabros.github.io/experience/2017/08/11/ost_mqtt_broker.html,2017-08-11,MQTT 적용을 통한 중계시스템 개선,"주문중계채널은 개별 디바이스들이 중계서버에 일정시간에 한번씩 API polling을 하는 구조로 운영되고 있습니다. 
신규주문 뿐만 아니라 주문의 취소나 상태변경과 같은 정보를 전달 받기 위해 API polling을 수행합니다. 
바로결제 이용 업소와 바로결제 주문건의 증가에 따라 채널별 디바이스에서 polling이 가파르게 증가하고 있습니다.
이러한 polling 횟수 증가는 대응 API에 부하를 가져와 auto-scale에 의한 instance 수 증가와 이에 따른 운영비용 상승의 원인이 됩니다.
또한 각 채널의 개별 디바이스들의 live 상태를 API 서버를 통해 update 해야 하는데 이전에는 polling API를 live signal로 판단했습니다. 
하지만 운영을 진행하면서 개별 디바이스의 live도 명시적 live message를 통해 update 해야 할 필요가 발생했습니다.
이러한 이유로 구조적으로 polling API를 통한 pull 구조가 아닌 서비스 서버에 의한 push 구조로 변경함으로써 해법을 찾으려 했습니다.
MQTT Broker를 적용하고 Topic 구조에 Message 발행(publication) & 구독(subscription)을 이용한 EDA(Event Driven Architecture)로의 전환을 계획하게 되었습니다.
MQTT(Message Queue for Telemetry Transport)는 M2M 또는 IoT 기기와 G/W의 연동을 위해 정의된 프로토콜입니다.
Facebook messenger가 MQTT를 사용했다고 알려져 있습니다(지금도 쓰고 있는지 정확히 어떤 용도인지는 모릅니다).
경량 프로토콜로 저전력 장비에서도 운용 가능하며 network bandwidth가 작은 곳에서도 충분히 운용 가능하도록 설계된 프로토콜입니다.
주요한 특장점은 아래와 같습니다.

Connection Oriented
* MQTT broker와 연결을 요청하는 client는 TCP/IP socket 연결을 한 후 명시적으로 종료하거나 network 사정에 의해 연결이 끊어질 때까지 연결 상태를 유지합니다.
* Topic에 발행된 message와 연결상태 확인을 위한 live(heart-beat)를 항상 유지된 연결을 통해 전달하게 됩니다.
* 연결 상태를 유지하는 것은 물론이고 연결이 끊어진 경우 재접속 등의 지원을 위한 자체 기능을 보유하고 있습니다.


Topic 그리고 발행(publication) / 구독(subscription)
* 개설된 Topic에 message를 발행하면 해당 Topic을 구독하는 client 들에게 message를 전송합니다.
* 따라서 one to multi 또는 one to one message 전송을 모두 지원할 수 있습니다.


QoS(Quality of Service)는 0, 1, 2 세단계를 지원
* 0 : 최대 1회 전송. Topic을 통해 message를 전송할 뿐 꼭 받으리라는 보장은 안해줍니다.
* 1 : 최소 1회 전송. 혹시 구독하는 client가 message를 받았는지 불확실하면 정해진 횟수만큼 재전송합니다. (계속 주는 건 좋은데 중복의 위험이 ;;;)
* 2 : 등록된 client는 요구된 message를 정확히 한 번 수신할 수 있도록 보장합니다.


다양한 개발언어의 다양한 클라이언트가 지원
* C는 물론이며 JAVA, Node.js, Python 등등 여러 종류의 개발언어로 Broker/Client Library가 존재합니다.
* 대부분 유료의 경우 QoS-2까지 지원하고 보통은 QoS-1까지 지원합니다.


각각의 Action에 따른 Notification
* Client의 연결, 연결해제, 구독, 발행 등등의 event에 대해서 MQTT broker가 대응할 수 있도록 해줍니다.

AWS ElasticBeanstalk + Node.js
Module : mosca (Node.js용 module인 mosca를 사용했습니다.)
구조가 단순하지만 필요한 기능의 지원은 모두 가능하고 사용 환경에 따라 customizing 하기도 용이합니다.
필요에 따라 REDIS나 MongoDB를 활용한 storage option도 지원합니다.

MQTT broker를 적용해 구성한 멀티채널 중계시스템의 모습입니다. 
앞서 발생한 주문 data의 변동내용(신규,최소,상태변경)에 따라 API 서버에서 MQTT broker로 Message 발행하고 MQTT broker를 구독하고 있던 하위 MQTT Broker#1,2에 message 중계합니다. 
각 채널에 소속된 개별 디바이스는 어느 MQTT broker를 구독하고 있던 message를 전달 받게 됩니다.
거꾸로 개별 디바이스는 구독하는 MQTT broker#1,2에 live 상태보고를 하고 MQTT broker#1,2는 API 서버를 통해 live 상태를 저장합니다.

MQTT를 중계시스템에 적용한 이후 기대하는 효과는 이렇습니다.
당초 단말기를 제외한 모든 채널을 MQTT로 통합하고 client에서의 API polling을 보조적 수단으로 지원하는 것을 목표로 했습니다. 그러나 …

주문접수앱
* 추진과정에서 스마트폰의 최근 OS에서 battery와 memory의 효과적 관리를 위한 보호모드로 인해 MQTT 접속이 지속될 수 없음을 확인하게 됩니다.
* 이러한 문제를 해결하고자 여러 방안을 시도해 보았지만 현재로서는 실패 ㅠㅠ
* 따라서 이 부분은 기약 없이 연기한 상태입니다.


배민 단말기
* 현재 사용하고 있는 단말기가 가진 구조적 한계가 있습니다. 이로인해 단말기도 현재로서는 MQTT를 적용할 수 없습니다.
* 차기 모델은 이 부분을 극복해 단말기도 MQTT 연동이 가능하도록 개선할 예정입니다.

mqtt.org
mosca github
joinc mqtt
"
http://woowabros.github.io/woowabros/2017/08/07/ebextension.html,2017-08-07,Elastic Beanstalk Configuration files(.ebextensions),"사내 서버 인프라가 거의 대부분 AWS 환경으로 넘어가면서 신규로 구축되는 많은 시스템들이 배포 및 확장, 관리등의 용이성 때문에 Elastic Beanstalk 으로 구축되고 있습니다. 제 경우에도 AWS 환경에서 신규로 구촉했던 BROS 2.0/신배라/주문지면개편 세 프로젝트 모두 Elastic Beanstalk 을 이용하여 환경을 구축했습니다. 하지만, Elastic Beanstalk이 무조건 장점만 있는 건 아닙니다. 특히나, Elastic Beanstalk 의 경우 사용할 수 있는 platform 이 한정적이며 또한 주어진 platform 에서 서비스에서 사용되는 특정한 라이브러리를 추가하거나 시스템 설정을 변경해야할 경우가 발생하면 쉽게 적용하기 힘든 단점도 분명 존재합니다. 그래서, 이런 단점을 해소하기 위해서 AWS 에서 Elastic Beanstalk 환경을 약간이나마 커스터마이징 할 수 있도록 제공해주는 기능이 .ebextensions 입니다.
배포할 서비스에 Elastic Beanstalk 설정을 위한 파일을 추가하고 싶다면 우선 배포될 소스 bundle의 root 디렉토리 아래 .ebextensions 폴더를 추가하고 해당 폴더 아래 설정 파일들을 추가하면 됩니다. 이 때, 설정 파일 뿐만 아니라 설정파일에서 사용되는 기타 다른 파일들도 해당 디렉토리에 포함시키면 설정파일에서 사용할 수 있습니다.

그리고, 설정 파일들은 보통 ###.config 와 같이 작성되는데 확장자는 사실 큰 의미는 없지만, 설정을 위한 파일을 구분하기 위해서 보통 .config를 사용합니다.
설정파일의 적용 순서는 파일명 alphabetical order 순서 이므로, 적용 순서가 중요한 경우 순서대로 ‘00-xxx.config’, ‘01-xxx.config’ 이런 식으로 이름을 주면 됩니다.
여기서 한가지 주의 할 것은 spring-boot 으로 만들어진 application의 경우에 platform 을 tomcat 하거나 혹은 embedded tomcat을 사용하는 경우 platform을 java로 하는 경우가 있는데 두 경우 약간의 차이가 있습니다. 보통 tomcat에 application을 올리는 경우는 war 로 패키징을 하는데 이 경우에는 gralde을 사용해서 빌드하는 경우라면 build.gradle 에서 간단하게
war { 
          from(‘config 파일이 포함되어 있는 소스 경로’) {
              into(‘.ebextensions’)
            }
    }
추가해서 빌드만 하면 문제가 없습니다. 하지만, embedded tomcat 을 사용하면서 .jar로 배포하는 경우에는 jar에 저렇게 .ebextenions 폴더를 포함시킬 수 없기 때문에 따로 배포전에 jar 파일과 함께 .ebextenions 폴더를 하나의 zip 파일로 묶어서 배포해야 제대로 설정파일이 적용됩니다.
zip -r application.zip application.jar .ebextensions
저는 앞선 BROS 2.0/신배라 프로젝트는 tomcat platform을 통해서 war 파일 형태로 배포했었는데, 프로젝트 막바지단계 합류했던 주문지편 개편의 경우에는 java platform 에 jar 형태로 배포되고 있었는데 둘의 차이를 제대로 인지하지 못해서 하루정도 beanstalk 설정이 안되서 헤맸던 아픔(?) 경험이 있습니다.
그리고, 뒤에 다시 언급하겠지만 tomcat/java platform 에 따른 배포는 더 중요한 차이가 있습니다.
.ebextenions을 통한 설정에서 가장 중요한 것은 당연히 confiuration 파일 통해서 설정할 수 있는 사항들에 관한 것일 겁니다. 이런한 설정 항목을 AWS에서는 key라고 하는데 많이 사용되는 key 항목에는 다음과 같은 것들이 있습니다.
Packages - 서비스 구동에 필요한 추가적인 패키지들을 다운로드 받아서 시스템에 install 할 수 있습니다. 기본 문법은 다음과 같습니다.
packages:
  name of package manager:
      package name: version
ex) 현재 우아한형제들 사내 시스템의 경우 서버 모니터링 툴로 newrelic을 사용하고 있는데 newrelic 서버 모니터링 패지지의 경우 다음과 같이 추가하면 elastic beanstalk 서비스 배포나 autoscaling시 자동으로 패키지가 설치됩니다.
packages:
    yum:
        newrelic-sysmond: []
    rpm:
        newrelic: http://yum.newrelic.com/pub/newrelic/el5/x86_64/newrelic-repo-5-3.noarch.rpm
Sources - 시스템에 필요한 파일들은 압축 파일 형태로 만들어 public url 형태로 다운로드 할 수 있게 만들어 둔 경우 해당 파일은 지정된 위치로 다운로드 받아서 압축까지 풀어줍니다. 기본 문법은 다음과 같습니다.
sources:
  target directory: location of archive file
ex) newrelic APM을 agent 형태로 구동할 때, 필요한 라이브러리를 s3에 올려두고 다음과같이 원하는 위치에 다운로드 받아서 사용할 수 있습니다.
sources:
/usr/share/newrelic-java: https://s3.ap-northeast-2.amazonaws.com/{bucket-name}/newrelic.zip
source를 사용해서 필요한 파일을 받을 때, 한가지 주의 할 것은 target diectory와 압축이 풀린 파일들의 owner가 기본적으로 ‘root’ 입니다. 따라서, 파일이 사용되는 용도에 따라서 권한 문제가 발생할 수 있으니 필요한 경우 Owner나 mode를 수정해야 합니다.
Files - 파일을 만들거나 혹은 다운로드 받을 수 있습니다. 기본 문법은 다음과 같습니다.
files:
“target file location on disk”:
     mode: “six-digit octal value”
     owner: name of owning user for file
     group: name of owning group for file
     source: URL
     authentication: authentication name:  
“target file location on disk”:
     mode: “six-digit octal value”
     owner: name of owning user for file
     group: name of owning group for file
     content: |
      this is my content
     encoding: encoding format
     authentication: authentication name:
이때, content 와 source 옵션을 동시에 사용될 수 없습니다.
Commands - ec2 인스턴스에서 구동되는 실행 명령을 기술 할 수 있습니다. 뒤에 나오는 Container Commands 와 비슷하지만 둘은 큰 차이가 있습니다. commnads는 container commands 보다 먼저 실행되는데 application과 web server 가 아직 설정되기 전, application version 파일이 적용되기 전에 실행되며 container commands는 application과 web server가 모두 준비되고 application version이 deploy 되기 직전인 상태에서 실행되는 차이가 있습니다. 둘 모두 여러개의 명령어를 한꺼번에 기술할 수 있는데, 이 때 실행 순서는 commnad name의 alphabetical order 순입니다. 기본 문법은 다음과 같습니다.
commands:
  command name:
      command: command to run
      cwd: working directory
      env:
          variable name: variable value
      test: conditions for command
      ignoreErrors: true
ex) Elastic beanstalk에서 신규로 만들어지는 ec2 인스턴스의 시간은 기본적으로 모두 UTC라 다음과 같이 commnads 설정으로 로컬타임으로 변경할 수 있습니다.
commands:
  01remove_local:
      command: “rm -rf /etc/localtime”
  02link_seoul_zone:
      command: “ln -s /usr/share/zoneinfo/Asia/Seoul /etc/localtime”
Container Commnds - 앞서 설명한 commands와 거의 비슷하지만 실행시점에 차이가 있으면, “leader_only” 라는 아주 유용한 옵션을 가지고 있습니다. “leader_only” 옵션의 경우 true로 설정하게 되면 여러 대의 인스턴스로 구성된 서비스에서 오직 한대의 인스턴스에서만 해당 명령이 실행됩니다. 하지만, 실제적 운영했을 때 몇가지 이슈를 발견했는데 뒤에 다시 한번 언급하도록 하겠습니다. 기본 문법은 다음과 같습니다.
container_commands:
  name of container_command:
        command: “command to run”
        leader_only: true
    name of container_command:
        command: “command to run”
앞서 tomcat/java platform 에서 war/jar 배포시 배포 파일 생성시 차이점을 언급했었는데. 이 둘은 java 실행 옵션을 설정하는데도 큰 차이가 있습니다.
기본적으로 Java plafrom + jar 배포인 경우에는 콘솔에 어떤 항목을 수정하더라도 실제 application 실행 후 ec2 인스턴스에 들어가서 실행된 application 을 확인해 보면 “java -jar application.jar” 로 실행옵션이 적용되지 않는 것을 확인할 수 있을 것입니다. 이는 Elastic beanstalk에 구현되어 있는 java platform deploy process로 인한 것인데, 이 JVM command line 옵션등과 같이 실행 옵션을 변경하기 위해서는 “Profile” 이 필요합니다.
Procfile 파일은 앞서 설명한 것 처럼 Elastic beanstalk Java platform에서 JVM 옵션을 설정하거나 혹은 여러개의 jar 파일이 들어있는 서비스를 구동할 때 사용되는 파일인데, 기본적으로 다음과 같이 기술됩니다.
web: java -jar server.jar -Xmms:256m
cache: java -jar mycache.jar
web_foo: java -jar other.jar
여기서 cache나 web_foo 라인은 없어도 되지만 web 항목은 필수이며 여기에 메인 application 실행 명령을 줄 수 있습니다. 적용할 Procfile은 패키징시 jar/.ebextensions 디렉토리와 같이 root에 함께 패키징되어야 합니다.
zip -r application.zip Procfile application.jar .ebextensions
실제 주문지면 개편시 API 서버를 구축하는데 서버 모니터링 APM으로 newrelic 을 사용하기 위해서 “-javaagent:/usr/share/newrelic-java/newrelic.jar -Dnewrelic.environment=dev “ 이 몇줄을 commnad line 옵션으로 주기 위해서 하루 종일 삽질(?)을 한 아픈 기억이 있습니다T_T. 앞서 말했듯이 기존에 구현했던 프로젝트들은 Tomcat + War 여서 AWS 콘솔에 써주면 그만 이었는데 이 녀석은 그게 안되서 반나절쯤 하다가 아직 상용 오픈 안한 서비스여서 그냥 싹 다 Tomcat에 war로 배포할까 진진하게 고민했었던….
타이틀이 좀 이상하긴 하지만, Java platform 에서 한대의 서버만 commnad line 옵션을 다르게 줘야하는 경우가 생길 수도 있습니다. 지면개편 프로젝트에서 주문 API의 경우 전사적으로 pinpoint를 사용할 수 있게 되면서, 기존 newrelic 상용계정 한대 + 나머지 서버는 모두 pinpoint 로 모니터링 하게 서버를 구축하고 싶었습니다. 그런데, newrelic/pinpoint 모두 javaagent방식으로 데이터를 수집하다보니 두개의 옵션을 모두 적용하면 둘중 한쪽 or 양쪽 모두 데이터 수집에 문제가 생기는 경우가 발생했습니다. 그래서, 상용 서버는 newrelic agent옵션을 나머지 서버들은 pinpoint agent 옵션을 주려고 했는데 Java platform 에서는 Profile 에서만 해당 옵션을 줄수 있고 이 Profile은 Container Commnads leaer_only 옵션으로 수정하는 시점에는 이미 Profile이 적용된 시점 이후여서 적용해도 제대로 동작하지 않았습니다. 그래서, 구글링으로 방법을 찾아보다가 Profile web 항목에 shell script를 쓸 수 있는 걸 확인해서 다음과 같이 구현했습니다.
Profile
web : ./run.sh
Conatiner Commnads
    006_default_run_sh:
        command: cp ./.ebextensions/run.sh run.sh; chmod a+x run.sh
        ignoreErrors: true
    007_replace_run_sh:
        command: cp ./.ebextensions/run_leader.sh run.sh; chmod a+x run.sh
        ignoreErrors: true
        leader_only: true
그렇습니다. Profile 은 container commands 적용 시점에 이미 변경해도 의미가 없지만, Profile 이 web 에서 실행되는 shell script의 경우에는 실제 서비스가 deploy 될때 해당 script를 찾아서 실행될 것이므로 이 shell script를 cotainer commnds leader_only 옵션으로 구분해서 수정했고 실제로 잘 동작됩니다!
한가지 주의할 점은 shell script로 java를 실행할 경우 shell script가 실행권한이 있어야 되므로 위의 command 옵션 마지막 항목에서 처럼 chmod 를 통해서 실행권한을 주는 것을 잊지 말아야합니다.
앞서 container commands 설명에서 leader_only 옵션을 잠깐 설명드렸습니다. 여러 개의 인스턴스로 구성된 서비스에서 한 서버만 구동되는 명령어라고 할 수 있는데, 실제 제가 진행했던 프로젝트들에서는 회사의 라이센스 규정상 newrelic APM 상용계정은 서비스당 한대에만 적용가능해서 이 옵션을 통해서 유용하게 설정했던 기억이 있습니다. 하지만, 실제로 운영중에 약간의 이슈를 발견했습니다. 주문 지편 개편 프로젝트의 지편 게이트웨이 서버의 경우 기본 2대의 인스턴스로 운영되지만 주문이 많이 몰리는 피크 타임에는 스케쥴링 걸어놓고 미리 1대 인스턴스를 더 준비해서 총 3대의 인스턴스로 운영하다가 피크타임이 지나면 다시 2대로 운영하고 있습니다. 이 서비스의 경우에는 newrelic APM은 leader_only 옵션을 사용하여 한대의 인스턴스만 상용계정이고 나머지 2대의 인스턴스는 일반 계정으로 APM 을 설정했습니다. 해당 설정으로 1주일정도 운영하면서 newrelic 에서의 모니터링도 큰 문제가 없었는데 어느날 부터인가 상용계정에서 해당 서비스가 모니터링 되지 않더니 엉뚱하게 개발 일반 계정에서 기본 2대/피크시 3대의 서버가 잡히기 시작한 것입니다. 이걸 인지하고 콘솔에서 health 항목을 통해 확인해보니 보통의 경우에는 처음 설정된 2대의 인스턴스는 계속 운영되고 1대의 서버만 스케쥴링 시간에 맞춰서 붙었다 떨어졌다를 해서 기본 2대의 서버의 running day가 동일했는데 이 항목이 10일이상 차이가 난 것을 발견했습니다. 추측컨데, 타임스케쥴링 되어서 인스턴스가 증가되서 줄어들때 무슨 이유에서인지 leader_only 설정이 되어 있던 인스턴스가 빠졌고, 그 이후에는 leader_only 무용지물이 된 것입니다. 아마도, autoscaling 시에는 leader_only 옵션이 없는 인스턴스 상태를 그대로 가져와서 새로운 인스턴스를 구성하는 것 같은데 서버를 다시 배포하지 않는 이상 영원이 leader_only : true 인 서버는 올라오지 않습니다! 5월말에 운영시작해서 2달 조금 넘게 운영하고 있는데 이와 같은 상황이 2번이나 발생했습니다.T_T 그래서, 정말 운영상에서 중요한 옵션을 leader_only 옵션으로 적용하는건 개인적인 경험에서는 자제하는게 좋은 것 같습니다. (혹시 이런 현상의 정확한 이유나 막는 방법 아시는 분은 댓글로 제보 부탁드립니다.^^)
"
http://woowabros.github.io/woowabros/2017/08/02/woowa-techcamp.html,2017-08-02,인(人)턴(Turn) - 당신 인생의 전환점,"흔히 인턴이라고 하면 단기간 회사에서 정식 구성원이 되기 위해 실습 훈련을 밟는 일원을 말합니다. 말이 실습 훈련이지 막상 회사에 가면 짧게는 2개월, 길게는 6개월, 1년 동안 제대로 된 실습 훈련을 받지는 못합니다. 개발자라면 이는 더 더욱 힘듭니다. 개발자에게 2 개월의 하계 인턴이라는 제도는 이도저도 할 수 없는 어중간한 기간일 수 밖에 없습니다. 이런 2 개월의 시간을 의미있게 만들어보자는 취지하에 우리는 새로운 인턴 제도를 그려 보았습니다. 2개월동안 하계 인턴을 지원한 이들에게 제대로 된 실습 개발 교육을 제공하기로 했습니다. 학교에서 배우는 따분한 코딩 수업이 아니라 체계적인 실무 교육과 실습 위주의 강의, 실제 회사에서 프로젝트를 수행할 수 있는 역량을 키울 수 있는 교육, 우리는 인생의 전환점이 되어줄, 우아한 테크 캠프를 열었습니다.

우아한 테크 캠프는 내부 개발자들이 교육을 지원했던 과거의 배민 학당에서 벗어나 프로그래밍 교육 기관 출신 강사분들이 세운 코드스쿼드라는 교육 기관에서 2 개월 풀 타임으로 소프트웨어 교육을 지원하기로 하고 김정 대표님, 윤지수 마스터님, 정호영 마스터님과 함께 채용 심사 과정부터 함께 했다. 500 명이 넘는 지원자, 200 명이 넘는 서류 통과자, 코딩 테스트 및 면접 진행자 70 명을 거쳐 최종 24 명의 인원이 선발됐습니다. 채용 선발 과정에서는 내부 개발자 분들의 아낌없는 지원으로 원활히 진행될 수 있었습니다.
채용을 진행하면서 빠듯한 일정에 힘들기도 했지만 우리는 젊은 지원자 분들의 열정과 열의를 보며 옛 생각에 잠기기도 했고 면접관들끼리 후기를 공유하며 요즘 젊은분들의 지식과 경험에 놀라기도 하며 나름의 의미있는 시간을 가졌습니다. 면접은 면접관에게 자신을 알리는 대화라고 합니다. 대화의 과정을 통해 면접자 뿐만 아니라 면접관 또한 새로운 간접 경험을  체득하는 값진 시간이 아니었을까 생각합니다.


2017년 7월 3일 월요일, 우아한 테크 캠프가 시작되었습니다. 경쟁하지 않는 구도 안에서 많이 배울 수 있는 장을 열어주고 싶은 취지에서 만든 이 교육 과정은 웹과 모바일의 두 개의 트랙으로 나뉘어 진행되었습니다. 짧은 시간안에 인턴분들이 많은 걸 얻어갈 수 있는 강의로 채우려고 노력했고 강의 내용은 주차별로 아래와 같습니다. 기초부터 응용까지 스스로 실습하고 코드리뷰하며 서로 회고하는 시간을 통해 개개인이 더 발전할 수 있는 교육을 지원하려 했습니다. 4 주 동안 개발 교육만 진행이 된게 아니라 중간 중간 우아한 형제들의 철학을 엿볼 수 있는 강연과 피플팀과의 워크샵을 통해 우형의 문화 또한 체험할 수 있는 기회를 전했습니다.








지난 1달, 4주는 캠프 운영진만이 아닌 강사진에게도 의미있는 시간이었습니다. 3월 20일 코드스쿼드의 김정 대표님과 처음 만나 교육형 인턴 프로그램을 기획하면 어떻겠냐라며 시작된 첫 만남이 빠르게 진행돼 여기까지 왔습니다. 이 교육이 인턴분들에게 긍정적인 피드백을 받을 수 있는 건 무엇보다 강사님들의 노력의 힘이라고 생각합니다. 강사님들은 체계적인 강의안과 인턴분들이 성장할 수 있는 환경을 조성해 주셨으며 스스로 생각하며 문제를 해결해 나가도록 유도하며 지식 전달이 아닌 방법을 찾게 교육해 주셨습니다. 우형에서 색다른 인턴 프로그램을 진행하며 나름의 재미를 찾으며 강의를 진행해 주신 두 마스터님은 지난 1달간의 교육을 아래와 같이 회고합니다.


김정 대표님 - 모바일 마스터님
모바일 개발자가 되고 싶었던 인턴도 있고, 웹과 모바일을 다 해야만 한다고 생각하기도 했습니다. 모바일 개발자가 고민해야하는 기술 분야나 도메인 지식을 포괄해서 생각하는 방법을 경험하고 있습니다. 새로운 업무 환경에서, 새로운 개발 환경에서, 새로운 언어를 배우며 서로의 코드를 읽고, 서로의 생각을 배우고, 서로의 부족함을 채워갑니다.
앱을 만들기 위해서 기획하고, 프로토타입을 만들고, 다시 설계를 하고, 짝프로그래밍으로 구현을 해봅니다. 각자 배웠던 언어도 다르고 경험도 다양해서 코딩 스타일도 달랐지만, 테스트를 작성하며 과정속에서 협업을 경험합니다. 다 같이 리뷰하며 다른 생각을 두려워하지 않습니다. 스스로 몰입하고, 성장하고, 또 다른 사람의 성장에 도움을 주는 경험은 다음주에 시작할 프로젝트 활동의 긍정적인 에너지 원천이 될 것입니다.
캠프를 기획하면서 “함께 경험하며 다같이 배우는 과정“을 만들어보고 싶었습니다. 
4주가 지난 지금 시점에서, 그 목표에 딱 절반쯤 다가간 것 같습니다.

윤지수 마스터님 - 웹 마스터님
좋은 환경에서 열정이 넘치는 주니어 개발자들과 함께 하고 있습니다. 과정에서 중요하게 여기는 부분은, 설계(design), 협력, 그리고 성장 입니다. 좋은 코드를 만들기 위해 우리는 ‘first design’을 중요하게 여기며 ‘code and fix’에서 벗어나가려 노력중입니다. 더 많은 사고 과정을 거치며 소프트웨어 개발의 참 맛을 느끼고 있습니다.
협력은 더 빠른 결과를 만들어낸다고 합니다. 페어프로그래밍을 하고 어려운 문제를 함께 해결할 때 더 팀이 단단해지고 더 빠른 해결방법이라는 것을 깨닫게 되는거 같네요. 우리는 매일 일어나는 코드리뷰, 페이프로그래밍을 통해서 서로서로 끊임없이 배우는 중입니다. 많은 것을 내려놓고(?) 서로의 코드를 리뷰하고 개선해 나가는 중입니다.
불필요한 경쟁은 없으며, 서로를 신뢰하고 도우며 발전하고 있습니다. 


7월 한달간의 교육 과정에 대한 만족도를 조사한 결과 교육 프로그램에 대한 만족도는 매우 높았습니다. 단순 지식 전달이 아닌 생각하는 프로그래밍을 강조하는 교수님들의 강의와 직접 코딩하며 만들어보는 과정이 많은 도움이 된다고 합니다. 8월에는 7월에 배운 교육 과정을 통해 인턴 분들이 스스로 프로젝트를 만드는 실습 시간을 갖게 됩니다. 알찬 교육의 결과물을 직접 기획, 개발하며 수업 내용을 활용하는 시간이 될 겁니다.
이번 교육은 운영하는 사람의 입장에서도 즐거운 시간이었습니다. 젊은 분들의 패기와 열정에 흥분됐고 이분들에게 제공된 기회와 환경이 때론 부러웠습니다. 우리에게 또한 좋은 추억을 만들어준 24명의 멋진 인턴 분들에 감사하며 이분들에게도 잊지 못할 2 달이 되었으면 합니다.
"
http://woowabros.github.io/experience/2017/07/31/evolution-with-swift-evolution.html,2017-07-31,Swift가 진화하는 법,"안녕하세요. FC 서비스 개발팀에서 육아(샤워 전문)와 iOS 앱(배민프레시) 개발을 담당하고 있는 최광훈입니다.
배민프레시 2.0.0 iOS버전을 무사히 올리고, 아기 샤워를 마치고, 밥을 먹이고, 잠을 재우고(그러나 새벽에 다시 깨었지. 30분을 울었지.), 사랑하는 부인과 함께 이유식을 만든 후, 고요함이 찾아온 금요일 저녁이었습니다. 그 동안 뒤쳐진 사회성을 끌어올리기 위해 facebook을 정주행중이었는데, 타임라인에서 김범준 이사님의 포스팅 하나를 보게 되었습니다.

stackoverflow의 글에 대한 포스팅이었는데 해당 내용인즉
외부 변수 i를 for in문의 index i로 사용해서 외부 변수를 변경하는 것이 불가능하지요? 라는 질문이었습니다. Swift의 문법에서는 스코프 바깥의 변수 혹은 상수명과 동일한 명칭을 스코프 안에서 사용한 경우 별개의 선언으로 취급하며, 동일한 명칭에 대한 shadowning 경고도 없기 때문에, 위와 같은 질문이 나오게 된 거죠.
포스팅에 몇가지 유사한 경우를 댓글로 달다 애초에 for item in collection { … }의 형태가 사용자에게 오해를 줄 여지가 있다면, 그리고 경고보다 좀 더 확실한 표현으로 제한이 가능하다면 어떨까 하는 생각이 들게 되었습니다.
Swift는 open source로 전환을 시작한 후부터 여러 형태로 사용자들이 기여할 수 있도록 하고 있습니다.
개발자들의 토론의 장도 필요하겠죠? swift.org에서는 다음과 같은 메일링 리스트를 통해 커뮤니티를 구성하고 있습니다.
제가 제안 할 내용은 다음과 같습니다.
를
로 제안하려고 합니다.
swift-evolution링크로 이동하겠습니다.
바로전의 깔끔한 화면과는 달리 좀 단촐해보이는 페이지가 등장합니다. 오픈 소스 커뮤니티의 메일링 리스트는 GNU 메일링 리스트 관리툴인 Mailman을 사용하는 경우가 많아 같은 형태의 레이아웃을 자주 볼 수 있습니다.
구성
이제 메일을 보내면 archives에 바로 등록됩니다.
[swift-evolution] Change ‘for in’ expression to for [] { (item) in … } 의 내용으로 메일을 보냈습니다. 엉터리 영어로 쓴지라 두근두근했지만, 그냥 send를 눌렀습니다. (지금 다시 보니 제가 stackoverflow의 원글을 잘못 이해했다는 것도 알 수가 있네요.)
별 기대는 안하고 다른 일을 하고 있는데, 12분 지나고 첫 메시지가 도착했습니다. (내용은 요약한 버전입니다.)
Alex Blewitt : forEach 쓰시면 됩니다.
그리고 몇 개의 메시지가 추가로 도착했습니다.
Jacob Williams : 기존 문법을 대치하는 것은 너무 많은 양의 수정사항을 발생시키며, forEach 쓰면 됩니다.
Robert Bennett : forEach와 똑같은데요?
이대로 수긍하기에는 제 설명이 너무 부족했던지라 저도 부연설명을 추가했습니다.
저 : 0..<10과 같은 Range타입인 경우 forEach를 쓰려면 (0..<10).forEach { … }와 같은 괄호로 감쌀 필요가 있어요. 불편하니까 그냥 for in바꾸진 말구 새로 추가하면 어때요? 물론 이미 기존의 for문 하나 없에서 좀 더 깔끔한 코드를 만들게 해줬지만 말이에요. (Swift 2까지는 for in문 외에도, 일반적인 for문이 존재했었습니다.)
Robert Bennett : 이미 대체할 방법이 있는데다, 언어의 발전은 기본 문법의 결함을 감추는 것 이상이다.
Haravikk : 의도에는 동감하나 잘못된 해결 방법인거 같습니다. 그보다 shadowning이 발생하는 상황을 제거하거나 경고를 추가하는 것이 어떨까요. 개인적으로는 변수명을 좀 더 고민하는게 좋겠습니다.
Taylor Swift : 사실 for문에서 외부 변수를 사용하는 것은 C가 심하게 비판받는 부분입니다. 이 변화는 더 심한 혼란을 야기할 것입니다.
이 시점에서 저는 모두의 견해에 고마음을 표하는 답변을 보냈습니다. 그리고 다음날 아침 즈음에 하나의 답변이 추가로 도착했습니다.
Daryle Walker : nested function을 없애고, 모든 지역 변수는 동일한 이름을 가져선 안되는 것이 좋을 거 같습니다. 이런 언어에 대해 읽은 적이 있네요.
Swift의 철학은 안전, 신속, 표현성으로 대표됩니다. 그리고 그를 위해서 Apple의 Swift 코어 개발자 뿐만 아니라 Swift를 사용하는 수많은 개발자들이 제안을 하고 토론을 하며 그 중에서 Swift 명세에 추가가 정해지며, 다시 검토를 통해 탈락 하기도 합니다. 또한 어마어마한 개발자들의 신적인 토론의 장이 아니라 저 같은 보통개발자들도 참여할 수 있는 열린 교류의 장이기도 합니다.
제가 제 부족한 제안에 대해 글을 쓴 것은 여러분도 겁내지 말고 Swift의 발전에 대해 재밌게 참여 할 수 있기를 바라기 때문입니다.
*마지막으로
세상에서 제일 짧은 제안이 얼마나 긴 thread가 되고 결국 Swift 3에 포함되었는지 보면 재미납니다. (나만 그런가…)
nulTerminatedUTF8의 nul이 오타라고 생각했던 세상에서 제일 짧은 제안이 Swift 3의 명세가 됩니다.
And Thanks Google Translate.
"
http://woowabros.github.io/woowabros/2017/07/30/logdata.html,2017-07-30,로그 데이터로 유저 이해하기,"우아한형제들 데이터서비스팀 송훈화입니다. 제 업무는 로그를 설계/정의하고 데이터를 분석하는 것입니다. 궁극적으로, 유저가 남긴 로그로부터 유저의 경험을 추정하고 니즈를 파악해 서비스 개선에 필요한 인사이트를 제공하는 것입니다. 이를 위해 로그 설계/수집/분석에 이르는 전반적 과정에 직/간접적으로 관여하고 있으며, 입사후 4개월간 경험한 내용과 생각을 공유하고자 합니다.
입사 직후 진행한 업무는 클라이언트 로그를 설계하는 것이었습니다. 앱의 모든 화면과 이벤트(클릭, 배너 노출)를 상세히 파악하고, 시나리오와 서비스의 흐름을 이해하는 것부터 시작했습니다. 마치 실제 유저가 앱을 쓰듯이 모든 기능과 화면을 하나씩 파악하면서 로깅 항목을 정리했습니다.
다소 노가다 업무였지만, 이 작업을 하면서 앱에 대한 이해가 한 단계 더 깊어지는 경험을 하게 되었습니다. 또, 이 과정에서 (제가 느끼기에) 편리한 기능도 있었고 이상한(?) 기능도 있었는데, 향후 분석을 위한 방향과 프레임을 잡는 데 유용한 정보를 얻었습니다.
로그 설계/정의 업무가 마무리되고 엔지니어 및 개발자와 협업하면서 로그 수집을 진행하였습니다. 데이터는 JSON 형태로 저장소에 차곡히 쌓이기 시작했고, 엔지니어의 도움으로 언제든 저장소에 접근해 데이터를 추출/분석할 수 있는 환경이 마련되었습니다. 실제 수집된 로그는 아래와 같았습니다.(아래 이미지 참고) 유저가 우리 서비스에 접속해 특정 화면을 보거나 액션을 할때마다, 설계된 스키마 대로 데이터가 쌓였고 샘플을 추출하여 데이터 탐색 준비를 마무리 하였습니다.
로그 데이터 예시(일부 항목 숨김처리)

이번과 같이 로그 수집 프로세스가 처음 진행된 경우, 본격적인 데이터 분석을 진행하기 앞서 데이터 품질을 확보하는 것이 중요하다고 판단했습니다. 기존에 정의한 대로 각 필드 및 파라메터에 적절한 로그가 쌓이고 있는지, 누락되거나 이상한 데이터가 없는지 확인하고 만약 수정/보완이 필요한 경우 데이터 품질을 확보하는 작업을 진행하였습니다.
예를 들어, 아래 예시와 같이 수집이 잘못된 경우 유관자와 원인을 파악하고 문제 해결 과정을 통해 데이터 품질 확보에 많은 노력과 시간을 투입하였습니다.
데이터 수집 오류의 예시

또 다른 중요한 과정은 분석 프레임을 설정하는 것이었습니다. 일반적으로 로그 데이터의 경우 단기간에 몇 십만, 몇 백만 건의 데이터가 순식간에 쌓이고 로그 정의 항목은 수백 건에 달합니다. 무작정 데이터 탐색 분석으로 뛰어들 경우 망망대해에서 길을 잃고 허우적거리는 경험을 할 것 같았습니다. 따라서 목적을 명확히 설정하고 적절한 질문을 사전에 작성하는 것이 효과적일 것으로 판단했고, 로그 설계시 경험을 바탕으로 대략적인 프레임을 구축하여, 데이터 분석 및 인사이트 도출 과정에 유용한 나침반으로 활용하였습니다. 분석 프레임 설정의 예시는 아래와 같습니다.
여담이지만, 현재 팀에는 분석가를 위해 전용 서버, Spark, Zeppelin 등 최적화된 분석 환경이 마련되어 있습니다. 쾌적한 환경 구축을 위해 애쓰신 엔지니어분들께 감사를 전하고 싶습니다. 또 로그 수집과 데이터 품질 개선을 위해 노력해주신 개발자분들께 감사를 전하고 싶습니다.
로그 데이터를 추출해보면 난해한 결과가 나오는 경우가 있습니다. 경험상 주로 다음과 같은 이유로 발생하는 것 같습니다.
위 4번에 해당하는 예시

위 항목 중 1,2,3 번의 경우 인적요인으로 인해 발생한 것이며, 실무자와 협의를 통해 해결 가능한 부분입니다. 4번의 경우는 기계적으로 수집되는 과정에서 자연스럽게 발생할 수 있는 부분입니다. 이 역시 협업을 통해 해결할 수 있으나, (엔지니어분과 개발자분들은 항상 바쁘므로) 4번 같은 경우 분석 과정에서 자체적으로 해결하는 것이 효과적일 것으로 판단했습니다.
즉 위 테이블을 그대로 읽으면, ‘UserNo가 천사(1004)인 유저가 MyPage 화면을 3초동안 5번 보았다’로 해석되는데, (유저는 기계가 아닌 인간이기에) 어떤 유저도 이런 식으로 앱을 이용하지 않을 것 같았습니다. 차라리 초(Second) 단위의 정보를 무시하고(중복 수집으로 판단), ‘2017년 5월 1일 10시 10분에 천사 유저가 MyPage를 1번 보았다’로 해석하는 것이 더 적절할 것으로 판단했습니다.
때로는 ‘이러한 방식이 세밀한 정보를 놓치는 것이 아닌가?’ 혹은 ‘너무 자의적인 해석이 아닌가?’ 라고 스스로 반문하기도 하였으나, (비록 정보 손실이 있더라도) 효율을 추구하는 동시에, 인간의 행동을 효과적으로 설명할 수 있는 해석이 타당해보였습니다.
결과적으로 기존 5개 행의 데이터를 중복으로 처리해 하나의 행으로 축약시켰으며, 유사한 데이터가 있을 경우 ‘무엇을 기준으로 중복 값을 제거하는 것이 적절한가?’에 대한 물음을 자문하며 분석을 진행했습니다. 이 과정에서 유용했던 것은 초반에 설정한 목적과 의미, 그리고 논리성과 인간을 기반으로 한 접근 방식이었던 것 같습니다.
로그 데이터를 설계/수집하고 분석하는 일련의 과정은 이 업무의 절반에 해당한다고 생각합니다. 나머지는 분석 결과를 유관자에게 전달하고, 설득을 통해 실제적인 변화와 성과를 이끌도록 지원하는 것이라고 생각합니다. 이를 위해 분석 결과를 명확하고 간결하며, 이해하기 쉽게 전달하는 것은 매우 중요한 과정인 것 같습니다.
주니어 분석가 시절에는 시각화와 리포트 작성에 많은 노력을 투입했고, 단순하고 쉬운 방법론 보다 고급 방법론이 항상 좋은 것이라고 착각했었습니다. 하지만 아무리 고수준의 방법론이라도, 결과를 공유받는 상대로부터 실제적인 변화를 이끌어내지 못한다면 큰 의미가 없다는 것을 깨닫고, 아래 원칙을 토대로 커뮤니케이션하기 위해 노력하고 있습니다.
단순하고 이해하기 쉬운 그래프

보기에 멋지지만 이해하는 데 시간이 오래걸리고 만들기도 어려운 시각화

일반적으로 로그 데이터를 수집/처리/분석하기 위해 많은 담당자분들이 노력과 시간을 투입합니다. 이러한 인적자원뿐 아니라, 서버 및 분석도구 등 시스템적 비용 역시 발생하기 때문에 데이터 활용도를 높이는 것이 중요하다고 생각합니다.
일차적으로 생각해볼 수 있는 각 영역/부문별 데이터 활용 방안은 아래와 같습니다.
개인적으로 데이터가 비로소 그 가치를 발휘하는 순간은 분석 결과가 실제 비즈니스에 적용되어 가시적인 성과를 일으키는 것이라고 생각합니다. 다만 첫술에 배부를 수 없듯이, (작은 규모라도) 반복적인 프로세스로 운영하고 개선이 필요할 경우 기민하게 진행하는 것이 필요하다고 생각합니다. 이를 위해 부서간 기밀한 협업과 업무 프로세스에 대한 공감대 형성이 필요하며, 또 데이터에 대한 적극적이고 열린 태도로 업무를 진행하려는 노력이 필요할 것 같습니다.
더불어, 이를 지원할 수 있는 데이터 전문 인력과 변화에 열려있고 효율성을 추구하는 조직 문화, 유저 친화적인 시스템 등의 기반 역시 필요할 것 같습니다. 비록 데이터나 수치가 모든 비즈니스 문제의 해답을 말해주지 않지만, 문제 해결을 위한 실마리를 제공할 수도 있으니 부담 없이 한번 시도해보는 것은 어떨까요??
"
http://woowabros.github.io/experience/2017/07/28/qrcode.html,2017-07-28,QR코드 속 보물찾기,"안녕하세요 배라개발팀 박민철 입니다.
배민7주년 행사 중 QR코드를 활용한 보물찾기 이벤트를 진행했던 경험을 이야기를 해보려 합니다.
지난 6월 배민7주년 행사가 있었습니다. 
배민7주년 행사 컨셉은 미래와 경쟁하자 였고, 미래기술에 대한 브레인스토밍 중 QR코드를 활용하자는 의견을 토대로 보물찾기 이벤트를 제작하게 되었습니다. 행사장소 곳곳에 붙여진 QR코드를 촬영하면 (상품 || 꽝) 이미지가 나오는데, 상품이미지를 저(노예)에게 보여주면 해당 상품을 받을 수 있는 방식으로 진행하였습니다.

QR코드는 Quick Response의 약어로, 1994년 일본의 덴소웨이브란 회사에서 처음으로 개발하였습니다.
QR코드를 제작하는 서비스를 해본 경험이 있어서, 어렵지 않게 구현 할 수 있었습니다.
(JQuery 플러그인을 사용하면 쉽게 QR코드를 만들 수 있어요!)
사용자 화면에 이미지URL이 어떤 방식으로든 노출이 가능하기 때문에 사실 가장 많이 걱정을 했습니다.
이부분은 상품번호를 md5로 암호화하여 모바일에서 직접입력하기 어렵게 했습니다.
(실제로 URL의 코드 일부를 수정해서 호출하거나, 직접 md5코드를 제작하여 호출하신분도 있었어요!)
상품이 많지 않아, 인식률이 떨어지길 기대했지만,
goo.gl 서비스를 이용하여 단축URL을 생성하고, 보다 간단한 QR코드가 만들어지도록 했습니다.

이부분은 오픈전날(D-1) 갑자기 불현듯 스쳐 지나가며 생각이 났습니다.
AWS Micro EC2 딱 하나 띄어놓고 있었는데, 점점 불안해지며 잠이 달아나기 시작했습니다.  
jmeter라는 성능테스트도구를 사용하여 500명 동시접속의 시나리오를 작성하여 그 결과를 볼 수 있었습니다.
원활하게 처리됨을 확인 하고 그제서야 맘편히 잠들 수 있었습니다.

[500명 x 2회 = 1000번 수행]
보물찾기의 요구사항(제가 해석한)은 다음과 같습니다.
이미 같은 상품을 발견한 사용자에게 꽝을 보여주려고 했으나, 혹시 모를 상황에 대비해 상품이미지를 노출했습니다.
하지만, 현장에서는 사용자에게 같은 상품에 다시 당첨이 되었다고 오해를 일으켰습니다.
차라리 위의 경우 이미 발견한 상품이라는 안내가 더 좋았을 것 같습니다.
상품의 수량이 없어지면서 계속 꽝이 노출이 되었습니다.
마치 누군가는 모든게 꽝인것 처럼 보이게 되었고, 불친절한 경험을 제공할 수 있다는 생각을 하게 되었습니다.
상품이미지와 함께 아쉽게 놓쳤다는 안내라도 보여줬으면 하는 아쉬움이 남았습니다.

[사진을 찾다보니 꽝인 경우밖에 없었어요]
13:00부터 모든 데이터를 초기화하고 서비스를 오픈하여 15:00까지 큰 이슈 없이 이벤트가 자연스럽게 종료가 되었습니다.
보물찾기 이벤트를 진행 하면서 크게 2번의 위기가 있었는데요.
첫번째는 100여장 정도의 QR코드 프린트물을 행사장소 곳곳에 붙이는 일이었습니다. 다른 노예분들께서 도와주셨지만 정말 힘들었어요.
(상품들이 생각보다 너무 빨리 발견되어서 좀 더 변태같이 어렵게 숨기지 못한 아쉬움이 있었습니다.)
두번째는 상품수령에 대한 구글스프레드시트 작성과 상품지급을 동시에 처리하는 일이 쉽지가 않았습니다.
보물찾기 이벤트에 적극적으로 참여해주셔서 그런지 금방 보물이 바닥을 보였는데요, 수령자를 기록하는 일과 상품을 지급하는 일을 혼자서 맡아서 진행하다보니 정신이 없었습니다. 중간에 같은 상품을 중복해서 수령한 경우가 있었는데, 구글스프레드시트 덕분에 정상적으로 회수하고 정합성을 유지할 수 있었습니다.
(Google Apps Script를 활용하면 자동화를 할 수 있는데, 마저 구현하지 못해서 아쉬움이 있었습니다. 이렇게 바쁠줄 몰랐어요.)
13:00부터 15:00까지 데이터를 정리하여 보았습니다.
보물찾기 프로젝트를 진행하면서, 서비스 구현 보다는 크고 작은 문제를 어떻게 해결하면 좋을지를 많이 고민 했던 것 같습니다. 그런 부분에서 더 흥미와 재미를 느낄 수 있었고, 고민을 하면 할 수록 더 나은 대안이 도출 되었지만 불안정한 내용들을 완벽히 채우지 못해 정말 아쉬움이 많이 남습니다. 다음에 기회가 된다면 더 나은 서비스로 찾아뵙도록 하겠습니다. :-) 
읽어주셔서 감사합니다.
"
http://woowabros.github.io/woowabros/2017/07/23/QA-not-integrationTest.html,2017-07-23,QA != 통합테스트,"안녕하세요. 우아한형제들 품질개선팀에서 근무하고 있는 임선진입니다.
저희 팀에서는 우아한형제들 서비스와 제품에 대한 QA, QC, Testing 업무를 주로하고 있습니다.
이 글은 어찌보면 모두가 알고 있는 QA(Quality Assurance Assistant)에 대한 이야기 입니다.
이 글을 팀장님도 아닌 제가 써도 되나… 많이 망설였습니다.
너무나 거국적인 주제인데다, 이미 충분히 잘 알고 있어서 이해해주시는 분도 있는데, 굳이 쓸 필요가 있을까 싶은 생각도 들었습니다.
또한 다른 곳에서 같은 QA업무를 수행하며 현재는 고통받고 계시지만, 개선의 의지를 가진 분들이 …
“배민도 별거 없네.” 라고 생각하고 희망을 접어버리지 않을까 걱정도 했습니다.
그리고 괜한 오해가 생길 수도 있단 생각을 했지요.
하지만 불과 몇일 전 까지도 잘못된 이해로 많은 일들을 겪으면서 꼭 이 주제에 대해 써야 될 것 같다고 생각했습니다.
저는 몇몇 분들이 QA를 통합테스트로 이해하고, QA의 일을 이해하지 못하는 상황을 접합니다.
프로젝트를 진행하거나 유관부서 분들을 만나 얘기를 할 때, 제 입장에서는 황당한 일들을 많이 겪습니다.
몇 가지 예를 들어보겠습니다.
상황1. (한 달도 넘게 진행된 프로젝트 내용을 미리 공유받지 못한 상황에서) “다음주 오픈 예정인데 일주일만 QA해주세요.”
“일주일 기능 테스트만이라도 해주세요.” 라고 하면 조금 덜 당황할 수 있습니다.
QA는 프로젝트 시작과 끝 전반에 걸쳐 품질을 저해할 수 있는 요소를 찾아내고 알리고 해결을 돕는 활동들을 의미합니다.
QA라고 말하는 것도 이상할 뿐 더러, 기능 테스트만 하더라도 몇일은 테스트 대상과 범위를 분석하는데 거의 모든 시간을 소모하게 됩니다.
(상황1 +) 상황2. “QA했는데 왜 운영 이슈가 계속 나와요? (QA한 거 맞나요?)”
1번에서 말한 상황이 황당하긴 하지만, 단 몇일 이라도 기능 테스트를 해서 사용자에게 더 나은 가치를 제공할 수 있다면 어쩔 수 없이 테스트를 시작합니다.
이 경우 버튼을 누르자마자 기능 동작을 하지 않는 등 엉망진창인 상태에서 올 때가 가끔 있습니다.
이 때, 저희는 의사결정을 할 수 있는 분께 프로젝트 일정을 더 늘려야한다, 개발 완료가 되지 않았다고 알립니다.
하지만 프로젝트를 처음부터 참여하지 않은 탓일까요? 얼마 만큼의 일정이 더 필요하고 얼마 만큼 구현이 덜 되었는지 근거가 부족할 때가 많습니다.
다행히도 무사히 프로젝트가 잘 진행되어 왔다면 그나마 안도하지만, 짧은 기간 안에 파악한 내용으로는 정말 어떻게 동작하는게 맞는 방향인지 알기 어려울 때가 많습니다.
국지적으로 확인만 하는 체커(checker)에 지나지 않을 수 밖에 없습니다.
(물론 처음부터 프로젝트를 같이 진행했더라도 이슈는 많을 수도 있습니다. 하지만 훨씬 가능성이 낮아질 겁니다 !)

[이게 어떻게 만들어지는게 맞는 거야? 일단 의자가 앞뒤로 움직이면 맞는건가?]
상황3. QA담당자가 프로젝트 킥오프 회의와 회고 회의들에서 누락되는 상황.
앞서 말했지만, QA는 테스트만 하는 활동이 아닙니다.
프로젝트의 목적과 목표에 어긋나지 않으면서도 소프트웨어와 기술을 바탕으로 사용자에게 최고의 품질(가치)을 제공하기 위해 노력하는 활동입니다.
그런 활동들 중 테스트라는 것이 있고, 그 동안은 테스트를 하는 활동에 집중 됐을 뿐입니다.
킥오프 회의에서 프로젝트의 목적과 목표를 듣지 못했다면, 적절한 활동을 하지 못할 것입니다.
회고 회의에서 이번 프로젝트가 어땠는지에 대해 듣지 못한다면, 다음 어떤 활동을 할지 발전이 없을 것입니다.
이것 외에도 …
크고 작은 황당한 일들은 시도때도 없이 힘을 빼놓습니다.

[잠깐만 숨 한 번 고르고요]
이런 일들을 이해는 합니다.
소프트웨어의 역사만큼이나 소프트웨어 품질의 역사는 길지 않다.
소프트웨어의 품질은 처음 제조업에서 말하는 품질관리에서 시작되어, 제조업의 품질관리를 소프트웨어에서 동일하게 적용하면 안 된다는 것을 알게 된지 얼마 되지 않았습니다.
아직도 이렇게 하는 게 맞는지 저렇게 하는게 맞는지, 고민하고 방향을 찾아가는 과정에 있다고 생각합니다.
심지어 업무를 진행하고 있는 사람들 조차도 본인의 일이 QA인지 테스트인지 인지하지 못하는 분들도 더러 있을 것입니다.
QA 업무를 하는 사람과 일해본 사람은 드물다.
지금의 조직에서도 프로그래머와 기획자의 수에 비해 QA로 일을 하는 분들은 정말 소수 입니다.
그리고 그 소수의 인원은 한정된 다른 부서의 분들과 함께 일을 합니다. 
어느 정도 큰 조직이 아니면 별도의 QA조직을 두고 운영하기 힘들어서, 경력이 꽤 있더라도 함께 일해 본적이 없다고 합니다. (혹은 테스터 분들 하고만 일을 해봤다고 합니다.)
QA의 일은 계속해서 변화합니다.
소프트웨어의 플랫폼과 언어가 변화하듯 비지니스 구조가 변하듯, 우리는 더 많은 파라미터를 가지고 움직여야 합니다.
어떤 제품을 맡게 되느냐에 따라 계속해서 플랫폼과 로직에 대한 학습이 필요하고, 무언가를 시도하고 적용하는데 더 많은 노력이 듭니다.
또한, 현재는 주로 테스트를 직접 수행함으로써 고객에게 가치를 제공하지만 다른 형태가 될 수도 있습니다.
다른 조직에서는 또 다른 방법으로 QA 활동을 하는 분들도 있고, QA가 하는 활동을 명확히 정의하기 어렵기 때문에 잘 모를 수도 있다고 생각합니다.
다시 한 번 더 말씀드리지만, QA는 통합테스트와 대치할 수 없는 단어입니다.
QA는 통합테스트만 하는 일을 의미하지 않습니다.

[출처 : https://www.slideshare.net/interfaceULG-innovationManagement/130528jodogneprofessionalsoftwaredevelopment-140113090851phpapp01]
품질(Quality) 관점에서 테스트는 극히 일부 밖으로 보이는 영역일 뿐, 우리는 더 많은 일들을 해야합니다.
사용자에게 어떻게 하면 극대의 가치를 전달할 수 있도록 잘 만들어질 수 있나 사용자의 입장에서, 기술 관점에서 모두 신경써야 합니다.
사실 저도 어떻게 하면 잘하는 것인지, 어떻게 하면 좋은 방법인지 잘 모르겠습니다.
하지만 계속해서 더 나은 방향을 생각해내고 최선을 다해 행동하는 것을 멈추면 안 되겠지요.

[현실과 부딪혀가며…]
계속해서 황당한 상황이 반복되고 앞으로 나갈 수 없는 많은 이유중 하나는 단어의 정의부터 잘못 인식하고 있기 때문이라는 생각이 들었고, 이렇게 기술블로그에 글을 쓰게 되었습니다.
여러분들이 생각하는 QA는 어떤 것이었나요?
덧1. 심심하면 나무위키 페이지. 보러가기
덧2. 품질개선팀의 채용은 열려있습니다. 우리 함께 일하며 성장해요. 입사지원서
"
http://woowabros.github.io/experience/2017/07/19/finish_toby_study.html,2017-07-19,좋은 동료와 함께 성장하는 기쁨,"작년 10월 즈음 시작했던 토비님의 ‘토비의 스프링’ 책 스터디를 드!디!어! 끝! 마쳤습니다.  
약 9개월의 시간이 걸렸으니 짧지 않은 시간이었네요.

[함께해준 동료들 덕분에 스터디 끗!]
아시다시피 토비의 스프링 책은 매우 두껍습니다. 
그러한 책이 2권이나 되니 첫인상으로만 보자면 혼자 완독을 하기란 쉽지 않아 보이죠. 
(아마 혼자 시작했다면, 중간에 포기했을 것 같습니다.)
Technology is constantly evolving and going through changes. 
Because of this, programmers are required to keep up with these changes and trends in order to stay relevant in their field.
얼마 전 사내에 공유된 링크 5 ESSENTIAL QUALITIES OF A GOOD PROGRAMMER 인 데요, 
좋은 개발자가 갖춰야 할 자질을 소개하고 있고, 그 첫 번째로 ‘개발자의 학습’과 관련한 언급이 있었습니다.
(각자 생각하는 바가 다를 수 있으니 가볍게 읽어보는 것도 좋을 것 같습니다.)  
다만, 좋은 개발자가 되기 위해서 끊임없이 학습해야 한다는 것만큼은 부정할 수 없는 것 같습니다. 
저 역시 머리를 쥐어짜며 블로그에 글을 올리는 지금도 이 기회에 마크다운 문법에 대해 알게 되었으니 조금은 성장한 것이 맞겠죠?
기술적 변화뿐만 아니라 (업무적)환경 변화에 적응하는 것 또한 중요한 부분일 텐데요, 제가 스터디에 참여하게 된 계기이기도 했습니다. 
아시다시피 (모르실 수도 있겠지만^^;;) 우아한형제들 기술조직은 개발언어를 전환(PHP -> Java)하고 있습니다. 
그리고 그 첫 단추가 제가 속했었던 팀의 프로젝트이기도 했습니다. 
배달의민족의 바로결제(온라인결제)를 처리하는 빌링시스템을 새롭게 만드는 것이었으니 꽤나 중요했죠
(Java로 처음 개발해보는 것이어서, 스터디도 좀 하고 여유가 있을 줄 알았는데… 바로 전쟁터에 나가야했던 기억이… 나네요) 
고생은 많았고 (토닥토닥) 큰 이슈 없이 프로젝트를 마무리했지만, 냉정하게 좋은 품질의 코드는 아니었습니다. 
그리고 ‘저 코드가.. 정말 괜찮은 것일까?’ 하는 두려운 마음 때문에 잠을 설치는 날도 많았습니다. (아무래도 돈이 관련되다 보니..)
모르고 만든 버그도 버그니까요.
스터디에 참여하게 된 계기는 좋은 개발자가 되고 싶은 마음도 한 몫 했지만 
솔직히는 작성한 코드를 스스로 이해하지 못하는 데서 오는 자괴감 때문이었습니다. (내가 이러려고 Java로 개발한다고 했는지… 자괴감 들고..)
때마침 동료들이 토비의 스프링 스터디를 시작한다는 얘기를 들었고, 주변으로부터 이 책은 꼭 봐야 한다는 말을 수도 없이 들어왔던지라 참여하게 되었던 것 같습니다. (그래도 무언가 얻어가는 것이 있겠지 하고 말입니다.)

[됐어! 성공했어!]
스터디는 매주 이틀씩 오전 8시부터 1시간 동안 진행했습니다.
정해진 분량을 읽어오고 이해한 내용을 나누는 방식으로 진행하였고 모르는 부분은 같이 찾아보기도 했습니다. 
Java 언어로 개발한 경험이 없어서 몰랐던 과거 선배님들의 고충을 들을 수도 있었습니다. (으악! 세상 참 편해졌! )  
(한편으로는 그런 경험도 조금은 부럽다는 배부른 생각을..?) 
그리고 올해 초 자바지기 박재성 님의 사내 강의가 있었는데요, 때마침 강의와 스터디 진도가 겹치는 부분이 있어 예습/복습도 할 수 있어서 좋았습니다.
개인적인 소감을 조금 언급해보자면, 
스터디 자체를 마친 것이 정말 기뻤고요! 
스프링의 원리에 대해서 조금은 이해할 수 있어서 좋았습니다. 
그리고 생각보다 책의 내용이 막. 어렵고 그렇진 않더라고요! (AOP는 넘나 헬..) 정말 설명이 넘나 자세했습니다. 
첫인상에 지레 겁을 먹으신 분들이 계신다면 가능할진 모르겠으나 차분한 마음으로 한장 한장 넘겨보시는 것을 추천해 드립니다.
함께한 동료들의 소감도 담아보았습니다. :)
“세상 친절한 책이다. 겁내지 말고 도전했으면 좋겠다.”
- 손현태 우아한형제들 배민B2B개발팀 , 2017년 상반기 우수사원
“토비의 스프링이 왜 스프링 최고의 교재인지 느낄 수 있었던 스터디 였다. 좋은 사람들과 함께해서 끝까지 할 수 있었다. 이제 실전이다!!”
- 민경수 우아한형제들 배민B2B개발팀
“좀 오래 걸리긴 했지만 함께 해서 끝을 볼 수 있었다. 
이제는 실제로 적용하면서 내 것으로 만들어야겠다!”
- 김승영 우아한형제들 배민B2B개발팀
이 글의 제목을 정하기가 쉽지 않았는데요, “좋은 동료와 함께 성장하는 기쁨”이라고 결정한 것은  
약 1년 동안 포기하지 않고 스터디를 끝마쳤다는 기쁨도 있었지만  
이 시간을 지내오면서 함께한 동료들의 성장에 대한 열정을 피부로 느낄 수 있었던 것이 가장 큰 이유였던 것 같습니다. 정말 많은 동기부여가 되기도 했고요. 
경쟁을 통한 성장도 중요하겠지만, 같은 목표를 향해 달려갈 수 있는 동료들이 있다는 것은 정말 소중하고 값진 것 같습니다.
진짜 결론은! 우아한형제들에서 함께 성장하고 싶으신 분들은 주저하지 마시고 입사지원서를…
성장 욕구가 충만한 주니어 개발자들도 겁나 많습니다! 어서 오세요!
부족한 글 읽어주셔서 감사하고, 다음에 좀 더 나은 컨텐츠를 들고 또 찾아뵙겠습니다.
"
http://woowabros.github.io/experience/2017/07/18/introduction-to-kotlin-in-baeminfresh.html,2017-07-18,"똑똑, 프로젝트에 코틀린을 도입하려고 합니다.","이 글은 2017년 4월, 배민프레시에 코틀린 도입하는 과정에서 작성된 글을 바탕으로 재구성되었습니다.
2017년 4월, 모바일 플랫폼에서 사용자 경험 극대화를 위한 여정의 첫발로 메인 개편 프로젝트를 시작했습니다. 이 프로젝트의 목표는 다음과 같습니다.
기존 배민프레시의 앱은 웹뷰 기반에 하이브리드 구조로 만들어져 있었습니다. 이를 네이티브 앱으로 전환하며 UX 개선과 성능 향상을 꾀했습니다.
FC서비스개발팀은 새로운 안드로이드 앱을 만들 도구로 코틀린을 도입하고, iOS 앱은 스위프트를 도입하는 것으로 결정했습니다. 이 글은 안드로이드 앱을 개발할 때 접하는 아쉬운(또는 불편한) 점과 그 대안을 생각해보고, 왜 코틀린을 도입했는지에 대한 고민을 담았습니다.
안드로이드 개발을 하면서 크래시 수집된 결과를 보면 정말 많은 부분이 NullPointerException(이하 NPE) 입니다. 이유에 대해 고민해 보니 세 가지 정도로 요약할 수 있었습니다:
NPE에 대처하는 가장 일반적인 방법은 그 부분에 not null 체크를 추가하는 것입니다:
다른 방법도 있습니다:
이렇게 처리를 해두면 당분간은 좋습니다. 그러나 유지보수를 하다 보면 결국 코드의 대부분이 if / try 문에 둘러싸이게 됩니다. 또한, 새로운 코드를 작성할 때 null이 아닐 수 있지만 그래도 null 처리를 해두는 게 안전하지 않을까 하는 마음에 불필요한 처리를 하게 되고 이것을 본, 팀의 다른 개발자분도 별다른 고민 없이 비슷한 코드를 만들 게 됩니다. 왜냐하면, 크래시가 발생해서 앱이 죽는 것보단 낫기 때문입니다. 자바 8에서는 보다 안전한 null 처리를 위해 Optional이 추가됐지만 아쉽게도 자바 8 지원이 포함된 안드로이드 스튜디오 2.4 프리뷰 4 에서도 Optional에 대한 언급은 없었습니다.
안드로이드 프레임워크는 성능을 매우 중요하게 생각하는 프레임워크인 만큼 대부분 추상화보단 저수준의 API를 제공합니다. 따라서 관련된 보일러 플레이트 코드가 많을 수밖에 없는데 자바의 다소 부족한 표현력이 더해져 실제 실행시키고 싶은 코드보다 보일러 플레이트 코드가 더 많은 경우가 자주 나타납니다.
물론, 자바도 표현력 개선을 위한 여러 가지 시도들이 있었습니다.
안드로이드 스튜디오 2.4 프리뷰 4 버전에서 공식적으로 자바 8 지원을 하게 됨으로써 더이상 Retrolambda를 사용하지 않고 위의 표현이 가능합니다.
안드로이드에서 현재 자바 현황
다만, 이렇게 보일러 플레이트 코드를 제거해 표현력을 높일 수 있는 부분은 많지 않으며 람다와 메소드 레퍼런스만으로는 전반적인 안드로이드 보일러 플레이트에 대응하기엔 무리가 있어 보입니다.
대표적인 안드로이드의 보일러 플레이트 코드의 예로는 다음과 같이 XML 레이아웃 파일에 정의한 뷰 레퍼런스를 가지고 오는 부분이나 SQLite 트랜잭션을 처리하는 부분이 있습니다.
이 부분에 유틸리티 클래스나 패턴을 활용해 반복되는 부분을 추출할 수도 있을 겁니다. 다만, 이런 보일러 플레이트가 안드로이드 API 전반에 걸쳐 있기 때문에 파일 처리, 비트맵 처리, 뷰 초기화, UI 관련, DB 등 추가하다 보면 유틸리티 클래스가 비대해질 수밖에 없으며 유지보수 하는 데 어려움이 따르게 됩니다.
이외에도 대부분의 앱이 리스트를 사용하는 만큼 리스트 아이템을 반복하며 처리하는 일이 상당히 많은데 자바 8의 스트림 API를 사용하지 못하는 건 아주 아쉽습니다. 반복문과 조건문을 사용하다 보면 모든 케이스에 대한 테스트가 어려워 실제 라이브 환경에서 예외 상황을 겪는 경우가 많았습니다.
보일러 플레이트 코드 제거
APT 쪽은 크게 전반적인 보일러 플레이트를 없앨 수 있는 AndroidAnnotations와 그외 라이브러리로 나눌 수 있습니다. APT 라이브러리는 안드로이드 보일러 플레이트 코드를 하나의 애너테이션으로 대체 할 수 있는 큰 장점을 가집니다. APT마다 사용하는 방법이 다를 수 있습니다만 일반적으로 원래 클래스를 상속해 코드를 자동 생성해주는 방법을 쓰기 때문에 자동 생성된 클래스를 사용하지 않으면 APT 기능을 사용할 수 없으며 빌드를 해봐야 에러를 발견할 수 있고 에러가 발생했을 경우 정확한 원인이 명시되지 않는 경우도 많아 디버깅에 어려움을 가집니다.
스트림 대안
streamsupport, Lightweight-Stream-API는 리스트 처리의 아쉬운 점을 보완할 수 있는 라이브러리로 리스트 처리를 반복문과 조건문보다 스트림을 사용하면 상당히 견고한 코드가 됩니다. 두 라이브러리가 자바 8 API의 대안을 제공하는 부분에는 차이가 있지만 공통적으로 스트림과 Optional을 제공하며, 안드로이드 지원을 명시해 두고 있습니다.
RxJava 또한 자바 8 스트림의 훌륭한 대안이며 이미 안드로이드 커뮤니티에서 널리 쓰이고 있는 라이브러리입니다. 안드로이드에서 많이 사용되면서 메인 스레드에서 구독한 코드가 실행되게 하거나 특정 라이프사이클에서 구독을 중지시키는 등의 보다 안드로이드 플랫폼에 특화된 기능을 제공하고 있습니다. 스트림의 대안을 넘어서 비동기, 이벤트 기반 개발을 위해서 꼭 필요한 라이브러리입니다.
편해지는 것 같긴 한데…
저 뿐만 아니라 대부분의 안드로이드 개발 환경이 위의 대안책에 네트워크 라이브러리, 이미지 로더 등을 추가해 프로젝트를 꾸렸을 거라고 생각합니다. 그런데 언제부터인가 편해지기 위해 의존성을 추가하는 게 부담으로 다가오기 시작했습니다. 오픈소스 라이브러리는 작성자가 모두 다르기 때문에 같은 APT 기반의 라이브러리를 사용하더라도 풀어내는 방법이 달라 각각의 라이브러리를 학습해야 하는 부담이 있고 의존성이 늘어갈수록 관리 부담(새 버전 확인, 등록된 이슈 및 회피 방법)이 커지는 부분도 있었습니다. 또한, 어떤 라이브러리를 사용하더라도 NPE에서 자유로울 순 없었습니다.
라이브러리들의 의존을 줄이면서 보다 일관적인 해결책을 가진 방법, 보다 안전하게 사용할 수 있는 방법이 필요하다고 느꼈습니다.
코틀린이 언어 수준에서 제공하는 Null Safety는 null을 처리하는 데 있어 안전한 방법을 제공합니다. 코틀린으로 코드를 작성하면서 느꼈던 좋은 점 중 하나는 변수를 선언하는 시점부터, 그 변수의 특성을 고민하게 만들었다는 것입니다. 변수를 선언할 때 읽기만 가능한지 또는 쓰기도 가능한지, null 값을 가질 수 있는지 없는지, 그리고 초깃값으로 뭘 가지는지 정해야 합니다.
프로퍼티가 어떤 값을 가져야 하는지 그리고 값이 바뀔 수 있는지 아닌지에 대해 먼저 고민하고 코드를 작성하게 해 NPE를 막는 데 많은 도움을 줍니다. 그리고 null을 안전하게 다룰 수 있는 문법을 지원해주기도 하며, 만약 null을 가질 수 있는 변수를 올바르게 처리하지 않으면 컴파일러 수준에서 에러를 발생시킵니다.
또한 메소드 시그니처에서도 해당 파라미터가 null일 수 있는지 아닌지를 지정할 수 있습니다.
이렇게 변수(또는 인자, 매개변수 등)의 선언부터 사용까지 null 상태를 고려하도록 언어가 강제하기 때문에 보다 안전한 코드를 작성할 수 있습니다.
Higher-Order Functions and Lambdas, Function References
람다와 메소드 레퍼런스를 지원하고, 자바 6 버전 이상 호환되기 때문에 안드로이드 SDK에 제한적이지 않습니다.
Collections (lists, sets, maps, etc)
코틀린의 컬렉션은 filter, map, foreach와 같은 다양한 고차함수 API를 제공하고, 변경 가능한 컬렉션과 불가능한 컬렉션을 엄격히 구분합니다.
서버에서 수신받은 데이터를 컬렉션으로 다룰 일이 많기 때문에 이런 컬렉션 기능은 코드를 보다 간결하고, 안전하게 만드는 데 도움이 됩니다.
Extension Functions
확장 함수(Extension Functions)는 이미 존재하는 클래스에 새로운 메소드를 추가할 수 있는 강력한 기능입니다.
앞에서 나왔던 SQLite 트랜잭션 코드를 코틀린 확장 함수를 사용해 재구성해보면 다음과 같이 표현해볼 수 있습니다.
코틀린은 고차 함수를 지원하기 때문에 함수를 파라미터로 넘기는 게 가능합니다. 위 코드는 트랜잭션 처리가 되어 있고, 파라미터로 람다를 받는 inTransaction 함수를 SQLiteDatabase에 추가하였습니다. 그리고 전달받은 람다를 트랜잭션 템플릿 안에서 실행해 트랜잭션 관련 보일러 플레이트 코드를 없앨 수 있습니다.
View Injection
제트브레인에서 제공하는 Kotlin Android Extensions 그래들 플러그인을 추가하면 뷰 인젝션을 사용할 수 있습니다.
지정한 id명으로 프로퍼티가 생성되며 Button 타입이 됩니다. 액티비티뿐만 아니라 프래그먼트, RecyclerView.Adapter에서 인플레이트한 뷰도 사용 가능합니다.
코틀린과 자바의 호환성은 정말 탁월합니다. 공식 사이트에서도 100% interoperable with Java and Android 라고 소개하고 있고, 언어 사양에서도 자바와의 연계가 중요한 설계 원칙 중 하나로 세워져 있습니다.
코틀린에서 기존 자바 코드를 사용하는 데 있어 중간에 뭔가를 통하거나 할 필요가 전혀 없고 일반 자바 코드를 작성하듯 사용할 수 있기 때문에 기존 안드로이드 라이브러리를 제약 없이 사용할 수 있습니다. APT 역시 지원하기 때문에 코틀린을 사용함으로써 기존에 잘 사용하던 것을 포기할 필요는 없습니다. (하지만 많은 부분이 코틀린을 사용한 코드로 대체될 겁니다)
서비스 프로덕트에 새로운 언어를 도입한다는 것은 도전적인 과제입니다.
현재 팀 내 개발자를 포함해 앞으로 팀에 합류할 개발자들 모두가 새로운 언어를 학습해야 하고, 프레임워크 또는 라이브러리를 선별하는 등 새로운 언어에 맞춰 팀 내 개발환경에 변화가 필요합니다. 개발을 진행하는 동안 기술적 이슈가 발생했을 때 대응시간이 상대적으로 오래 걸릴 수도 있고, 수개월에 개발 후 끝나는 것이 아니라 이후 수년에 걸쳐 지속해서 운영하며 개선 및 유지관리 업무도 수행해야 합니다.
코틀린이 내세우는 장점 중에는 언어에 대한 학습 곡선이 매우 낮고, 자바를 다룰 줄 아는 안드로이드 개발자가 더욱 효율적으로 코드를 작성하게 도와준다는 것입니다. 학습에 필요한 내용은 코틀린 레퍼런스 문서에 명료하게 잘 작성되어 있고, 문서 양도 많지 않아 며칠이면 읽고 코틀린 코드를 다룰 수 있습니다. 고급 기능과 함께 능숙하게 언어를 사용하려면 다소 시간이 걸릴 수 있지만, 전반적으로 간결한 언어입니다. 팀 내 코틀린 도입에 대한 의견을 제안한 후 코틀린을 모르던 안드로이드 개발자가 약 일주일간 학습을 시도했습니다. 그 결과 코드를 읽고, 작성하는 데 무리가 없었습니다. 자바와의 호환성이 높고, 상호 운용이 가능하기에 언어의 특징만 잘 숙지하면 언어를 다루는 데 있어 어렵지 않다고 판단해주셨습니다.
또 다른 장점으로 안드로이드/자바 플랫폼과 100% 양방향 호환성을 내세우고 있습니다. 따라서 안드로이드 SDK를 포함해 안드로이드 플랫폼(또는 자바 플랫폼)에서 동작하는 프레임워크와 라이브러리를 그대로 사용할 수 있으며, 그레이들이나 메이븐과 같은 빌드 도구도 사용할 수 있으므로 추가적인 개발비용이 들지 않습니다. 더 나아가 코틀린 생태계에 Kotlin Android Extensions, Anko 등을 통해 생산성 향상을 꾀할 수도 있습니다. 기존 앱에 코틀린 코드를 섞어 테스트를 해보았으며 기능적으로 문제없이 동작한다는 것을 확인했습니다.
또한, 코틀린 개발을 주도하고 있는 곳은 제트브레인입니다. 자바 외에도 스칼라, 스위프트, 파이썬 등 다양한 언어들의 IDE를 개발하고 있습니다. 거기에 인텔리제이를 안드로이드 개발을 위해 특화한 형태인 안드로이드 스튜디오로 안드로이드 앱을 개발하는 것이 업계에 자리 잡은 지 오래입니다. 프로그래밍 언어에 대한 이해도나 관련된 기술력이 코틀린에 대한 신뢰성을 높이 보게 되었습니다.
하지만 코틀린 기반으로 안드로이드 앱을 개발하는 데 있어 공개된 모범 사례들이 많지 않기 때문에 개발함에 있어 시행착오가 있을 수 있습니다.
좋은 소식은 2016년 이후 코틀린 커뮤니티가 가파르게 성장하고 있다는 것입니다. 공식 블로그의 통계에 따르면 16만 명의 사용자와 공식 커뮤니티도 4배 이상 성장, GitHub에 8132개의 저장소가 생성되어 있고, 10만 라인 이상의 코드도 쌓였습니다. 그리고 관련 도서도 출간되고 있으며, 나라별 로컬 커뮤니티도 조금씩 자리 잡고 있습니다. 다양한 안드로이드 오픈 소스로 많이 알려진 Square를 포함해 Pinterest, Basecamp 등의 회사에서 코틀린을 사용하고 있으며, 관련 사례들을 컨퍼런스 또는 블로그 등을 통해 발표하고 있습니다.
이슈를 논할 수 있는 커뮤니티와 다양한 회사에서 도입 사례들의 공유, 제트브레인의 전폭적인 지원이 있기에 시행착오를 줄일 수 있다고 생각합니다.
이외에도 코틀린이 지원하는 기능들은 더 있습니다만 여기서 언급한 기능들이 도입을 결정한 가장 큰 이유입니다. 코틀린을 도입함으로써 코드를 더 간결하게 표현할 수 있고, 간결하게 표현함으로써 코드에서 발생할 수 있는 버그를 줄이며, 익숙해지면 기존 자바 코드보다 더 나은 가독성을 가진다고 생각합니다. 하지만 코틀린도 은 탄환은 아닙니다. 많은 장점을 얻을 수 있지만 여전히 설계는 온전히 개발자의 몫이며 간결한 코드 이상으로 어떤 설계를 가져갈 것인가 역시 전체 애플리케이션 개발에 있어 정말 중요한 부분이라고 생각합니다.
모든 행위의 동인에는 개인이나 집단의 욕망이 반영되어 있다.
앞서 말했듯이, 코틀린은 은 탄환이 아닙니다. 새로운 기술을 도입하고자 할 때는 그 기술에 강점과 약점을 명확하게 이해하고, 해결하고자 하는 문제에 대한 인식, 그리고 팀을 포함 주변 상황을 잘 살펴보고 판단하고자 하는 노력이 필요하다고 생각합니다.
팀에서 코틀린을 도입하는 데 있어 삿된 욕망이 아예 없다고는 할 수 없을 것 같습니다. 코틀린을 사용하면서 알게 되는 새로운 개념과 패러다임을 익혀가는 과정은 개발자로서 성장에 즐거움을 안겨줄 것이며, 성장에 대한 피드백은 서비스 프로덕트를 더 효율적이고 안정적으로 개발하고 운영할 수 있도록 해줄 것입니다.
Google I/O 2017에서 코틀린이 안드로이드 공식 언어로 지정됐습니다.
"
http://woowabros.github.io/experience/2017/07/17/resume.html,2017-07-17,이직초보 어느 개발자의 이력서 만들기,"안녕하세요 저는 올해 2월부터 우아한형제들의 배라개발팀에서 일하고 있는 구인본입니다. 작년 연말에 잠시 휴식을 가진 후 1월부터 이직을 준비하면서 경험했던 것 중에 이력서를 쓰면서 생각하고 느꼈던 것들을 정리해보았습니다. 이력서는 이렇게 써야 해 저렇게 써야 해라는 것보단, 제가 저만의 이력서를 쓰면서 나름대로 시도하고 적용해본 경험을 공유해보고자 합니다.
본격 나만의 이력서 만들기
이직하기 전의 스타트업 회사에서는 지인의 소개와 면접 위주의 심사를 통해 채용된 경우라 이력서를 쓰기 위한 시간을 많이 할애하진 않았었습니다. 그런데 막상 이력서를 다시 준비하려다보니 한글로 만들었던 고전적 양식의 한 장짜리 이력서 밖에 없었지요. 거기엔 사진과 이름, 생년월일, 주민번호, 학력, 경력, 자격증, 그리고 보유기술이 짧게 적혀있었습니다. 이대로 이력서를 제출할 순 없었기에 스타트업에서 맡았던 업무와 적용기술을 목록 형식으로 나열해서 내용을 추가해보았습니다.

스타트업에서의 경험했던 일들이 제겐 자신감으로 남아있었고 뭐든지 할 수 있다는 포부(혈기^^)로 구직사이트를 통해 이력서를 뿌리다시피 제출했습니다. 그런데 며칠 지나지도 않아서 구직앱에서 불합격통보가 줄줄이 오는 것을 보고, 처음의 자신감은 곤두박질. 이건 뭔가 잘못됐어! 뭐가 문제지?
사실, 지금 돌아보면 그때의 이력서를 지금의 것과 비교해보니 떨어지는 것도 당연하다는 생각이 들더군요. 먼저는 첫인상부터 10년은 넘었을 것 같은 옛날 이력서의 모양새, 성의 없어 보이는 경력소개, 지원하는 회사의 업무와 연관성이 적어 보이는 경력들. 제 이력서를 보고 불합격시킨 당사자의 시각으로 보려고 하니 이력서 전형에서 통과시킬 이유를 찾기 힘들더군요. 스타트업에 있으면서 팀원을 충원하기 위해 이력서를 받아보았던 경험이 있어서 내 이력서를 볼 사람의 상황에 쉽게 감정이입이 되었습니다. 아마도 내 이력서는 10초도 안 되어 지나가버렸을지도.
이 사실을 깨닫는 데 오래 걸리진 않았습니다. 그런데 문제는 이미 이력서를 제출한 곳이 많다는 것과 정말 일하고 싶어 한 회사에도 이미 그 이력서를 제출했다는 것! 다행히 다시 업데이트 할 수 있는 기회가 있었고 자유로운 포멧으로 제출할 수도 있었기에, 온전히 이력서를 개선하는 것에 집중하기로 했습니다.
내 이력서는 나중에 봐주오, 오 제발!
이전의 이력서의 틀로는 제가 할 수 일들과 강점을 잘 보여줄 수 없다고 판단하고, 원점에서 다시 만들기로 했습니다. 절박하니 오히려 아이디어들이 샘솟더군요. 먼저 생각한 것은 어떻게 하면 내 이력서를 보게 만들까?, 어떻게 하면 첫 페이지에서 좋은 인상을 줄 수 있을까?였습니다. 내가 아닌 내 이력서를 볼 사람, 즉 독자의 관점에서 생각해보았습니다. 수많은 지원자의 이력서를 보면서 지쳐있을 독자에게, 뭔가 달라 보이고 다음 페이지가 궁금해지는 첫 페이지를 만들고 싶어졌습니다.
저 자신을 잘 어필할 수 있는 재료가 무얼까 고민했습니다. 알맹이가 있어야 껍데기도 의미 있는 것! 먼저는 어릴 때부터 프로그래밍을 접했었다는 점, 대학과 대학원을 거치면서 여러 가지 실험적인 기술들을 접해왔다는 점, 소규모 스타트업에서 사용했던 여러 기술과 스펙을 나열해보았습니다. 그런데 이것들을 목록으로 정리해보고 텍스트 정렬도 바꿔보고 글자 두께도 바꿔보고 해봤지만, 도무지 좋은 인상을 줄 수가 없었습니다. 그래서 글자로만은 안되겠다 싶어 그림을 그리기 시작했습니다. 여러 번의 수정 끝에 나온 것이 개발이력 timeline이었습니다.
절박하면 뭐든지 나오긴 나오네요

만들어왔던 크고 작은 결과물들을 시간순으로 시각화해보니 한 눈에 볼 수 있고 접했던 기술들을 기간에 맞춰 나열해보니 제법 그럴 듯해 보였습니다. 또한, 기술을 글자로 나열하기보다 심볼과 아이콘으로 표시해서 보니, 이력서를 보는 기술직 담당자에게 친숙할 만한 기술들이 쉽게 눈에 들어온다는 걸 알 수 있었습니다. 그림을 페이지의 가운데로부터 하단에 안정감 있는 위치에 배치했습니다. 일단 첫 페이지에 들어갈 큰 그림이 채워지니, 나머지는 좀 더 편하게 접근할 수 있더군요.
첫 페이지 상단에 들어가야 할 내용을 골라보았습니다. 기본적인 인적사항은 필수겠지요. 이름을 한글과 영어로 넣고, 전화번호와 이메일 주소를 넣었습니다. 이전의 이력서에서는 주소라든지, 성별이라든지, 몇 가지 자잘한 인적사항이 있었는데, 꼭 필수적인 것도 아니고, 첫 페이지에 들어갈 필요는 없다고 느껴져 과감하게 제외했습니다. 이왕에 기존 틀에 얽매이지 않기로 했으니까요.
오히려 그 자리에 블로그, 개인 Github, 트위터 링크를 넣었습니다. 링크를 따라 들어가보기 전부터, 이 지원자는 SNS 활동도 하고 Github도 하는구나!라는 인상을 줄 수 있겠지요. 화면으로 이력서를 보고 있다면 쉽게 링크를 타고 들어가볼수도 있겠고요. 사진은 처음에 증명사진으로 넣었다가, 너무 딱딱해보여 뺐습니다. 요즘은 사진도 필수는 아니라고 하니까요. 마침 첫 페이지에 어울리는 적당한 사진을 찾을 수 있어서 그걸로 다시 넣었습니다.
여전히 첫 페이지에 남아있는 공간을 무엇으로 채울지 고민했습니다. timeline을 통해 어떤 일을 해왔는지 보여줬으니, 이제는 내가 어떤 개발자인지 알리기로 했습니다. 그런데 첫 페이지이니만큼, 구구절절이 자기소개를 하고 싶진 않았습니다. 글이 길면 다 읽어보기 힘들 테니까요. 제겐 이전 회사에서 일했던 가장 최근의 경력이 지원하는 회사의 일과 가장 연관성이 높았기에 그것을 중심으로 간략하게 한 단락으로 적었습니다.
그리곤 일반적인 이력서들과 마찬가지로 제일 위에 제목으로 “이력서”라고 써보았습니다. 문득 이런 생각이 들더군요. 이력서인 건 이미 알고 있을 텐데, 적을 필요가 있을까? 영혼이 자유로워지고 있습니다 그 자리에 나를 표현하는 의미 있는 제목을 적고 싶어졌습니다. 이 제목은 사실 이력서를 마감하는 마지막 순간까지 계속 고쳐나갔습니다.
결과적으로는 세 가지로 압축했습니다. 트렌드 모니터링 습관, 코딩은 결벽적 미니멀리즘, 함께 성장하는 팀웍. 영어로도 표현하는 건 덤입니다. 직역하기엔 공간이 맞지 않아 비슷한 의미로 넣었습니다. 최근에는 가운데 문구를 바꾸기도 했습니다. 테스트 기반의 견고한 코딩
이제 좀 현실감각이 생기고 있네요.. 결벽적 미니멀리즘이라니…
막상 제목을 이렇게 만들어보니 뜻밖의 효과가 생겼습니다. 면접을 볼 때면 자주 나오는 질문에 대하여 정돈된 표현으로 답을 할 수 있더군요. 이제 첫 페이지가 완성되었습니다. 다시 자신감이 장전되고 있습니다.

생각해보았습니다. 만약 첫 페이지에서 관심을 가지고 다음 페이지를 넘길 때 독자가 기대하는 것이 무엇일까? 어렸을 때부터… 이런 건 아닐 테고, 내 사생활, 구체적인 인적사항도 아니라고 생각했습니다. 최근에 했던 프로젝트부터 보여주고 점점 거슬러 올라가면서 개발이력들을 펼치기로 했습니다. 본격적으로 세부적인 내용을 채워가면서 참고가 되었던 글이 있었습니다. RSS로 구독하던 블로거 중에 변정훈님의 “이력서“에 대한 글을 보면서 내용의 순서나 기본적인 페이지 레이아웃을 잡아갈 수 있었습니다. 지난 스프링캠프 때 직접 만나 감사의 인사를 했었지요. 본격 유명한 개발자의 이력서 따라하기!! 순서는 아래와 같습니다.


특별히, 최근 프로젝트에서 했던 일에 대해 쓸 때 많은 수정을 거쳤는데요, 어떤 관점에서 적을지 좋은 힌트를 얻을 기회가 있었습니다. 이력서를 쓰기 몇 달 전 “훌륭한 개발팀장이 되려면?“이라는 주제로 넥슨의 박종천님의 강연을 들었었습니다. 거기서는 팀장으로서의 업무를 세 가지 관점으로 분류했습니다. Technical Lead, Project Lead, 그리고 People Management. 강연을 들으면서 느낀 바도 많았고, 돌아가서 팀에 적용하면서 체험한 것도 있었던 터라 이 세가지 틀을 사용하니 내용을 정리하기가 쉬워졌습니다. 그때의 강연을 꼭 추천해드리고 싶은데 올라온 영상이 없어서 안타깝네요.

지금까지 개발자로서 일해오고 접해본 것들을 보여주긴 했는데, 여전히 2% 부족하다 느꼈습니다. 첫 페이지에서도 timeline을 통해 해왔던 일을 보여주었지만, 내가 어떤 개발자인가를 어필하기 위해 제목과 간략한 소개를 썼었지요. 이쯤에서 내가 어떤 개발자인지 좀 더 보여주고 싶었습니다. 이력서를 보는 독자가 좀 더 실감 나게 저를 상상해 볼 수 있도록요. 만약 같이 일하는 모습을 상상하고 있다면, 이미 반쯤 넘어온 거겠죠.
이미 나와 일하고 있다! 이제 스스로 정신승리에 이르고 있습니다…
제 자신이 주도했고 좋은 성과가 있었던 일 중 2가지를 꼽았습니다. 그리고 각각에 대해 일을 진행했던 과정을 정리해보았습니다. 어떤 문제에 직면했을 때 그 상황을 잘 파악하고 원인을 알고 그 해결책을 잘 제시하며, 결과물을 만들어 내는 과정을 보여주기 위해 아래와 같은 구성으로 편집해보았습니다. 제 경우 내용을 채우고 나니 왼쪽 제목 공간이 좀 허전해 보여서 두 가지 사례를 추가로 간략하게 넣기도 했습니다.

사실 여기까지 쓰면 업무에서의 전문성은 다 보여준 거라고 생각했습니다. 이미 이력서 분량이 네 장이 되었기 때문에 집중해서 이력서를 볼 수 있는 분량으로는 최대한이라고 판단했지요. 이제 남은 것은 자기소개와 기.타.등.등일텐데 이 부분을 다시 집중해서 읽게 하기란 어려울 것 같았습니다. 그래서 자기소개는 좀 편하게 볼 수 있는 내용으로 작성했습니다.
자기 철학이나 개인사를 드러내기보단 개발자로서 살아왔던 이야기를 친구나 지인에게 이야기하듯 써보았습니다. 한 번에 너무 길면 보기 힘드니 저의 경우 컴퓨터를 처음 접했을 때와 대학 시절 한 단락, 대학원 과정 중의 경험 한 단락, 스타트업에서의 경험 한 단락, 이렇게 세 부분으로 나누어서 썼습니다. 쓰고 나이 첫 페이지부터 계속 얘기해왔던 것들의 반복이라는 느낌도 들었지만, 좀 더 편한 문체로 풀어쓴다는 생각으로 채웠습니다.
이왕 편한 마음으로 보라고 한 페이지니, 왼쪽 제목 공간에 관련된 사진을 작게 넣어 보았습니다. 글로 쓰진 않았지만, 자연스럽게 운동도 좀 했어요 라고 어필도 되구요. 지금에 와서 그때 썼던 페이지를 다시 보니, 좀 길어보이긴 하네요. 세 부분으로 나누어 놓았으니 단락을 좀 더 나누어서 너무 답답하지 않게 보이게 하면 더 좋았겠네요.

정말 제목 그대로입니다. 자세히는 안 보았을지라도 장장 다섯 페이지를 넘긴 독자에게 보내는 감사의 멘트와 연락처로 마무리 하고 싶었습니다. 그런데 문제는 감사멘트와 연락처를 넣는 것으로는 한 페이지를 채울 수 없으니, 두 가지 선택지를 생각했습니다. 한 페이지 더 만들어서 마무리하는 것과 이전 페이지의 내용을 좀 줄여서 공간을 만들고 하단에 배치하는 것. 제 경우는 그동안 넣고 싶었지만, 마땅히 넣기 힘들었던 글감과 이야기가 있었던 터라 그 내용을 추가해서 새로 페이지를 구성하기로 했습니다.

첫째로 개발자로서의 지속적인 성장과 촉(?)을 기르기 위해 업무와 직간접적으로 연관된 외부 강연이나 컨퍼런스를 참가하였던 경험을 썼습니다. 둘째로는 개인적인 취향일 수도 있지만, 어느 정도 업무와도 연결될 수 있는 취미를 소개했습니다. 취미 부분은 일부러 글쓰기 문체보다는 말하기 문체로 썼습니다. 제 경우는 폰트덕후로서 해왔던 소소한 경험들과 픽셀 단위의 틀어짐을 구분하는 매의 눈(?)을 자랑했는데요, 여기서는 너무 가볍지도 딱딱하지도 않은 분위기로 썼습니다.
요즘엔 특별한 이력서 양식 없이도 이력서를 낼 수 있는 서비스들이 제법 있습니다. 물론 입사 전형이 진행되면 최종적으로는 각 회사에서 관리하는 이력서양식으로 다시 써야 하겠지만, 처음으로 자신을 PR할 수 있는 문서인 이력서를 다양한 방법으로 쓸 수 있다는 것은 좋은 기회라고 생각합니다.
제 경우는 일반적인 이력서 양식이나 틀에 맞추려고 하면 나이에 비해서 경력이 다소 적어 보이는 것과 대학원 과정에서의 경험들이 잘 드러나지 않는 것이 제 발목을 잡았습니다. 저는 이것을 만회하기 원했고 제가 가진 장점과 능력을 최대한 잘 보여주려고 했습니다. 물론 없던 것을 만들거나 과장한 것은 면접에서 몇 마디 해보면 드러날 것이기 때문에 그런 점에서는 조심스럽게 쓰기도 했습니다.
그리고 또 한편으로 중요하게 생각한 것은 잘 보이게하자는 것이었습니다. 사람들은 이력서나 문서들을 볼 때 직감적으로 문서의 모양새를 먼저 보게 될 테니까요. 대학원 과정 중에는 발표자료와 논문을 쓸 일이 많았는데 그 과정에서 얻은 팁을 아래에 간략하게 정리해보았습니다.
사실, 페이지의 모양새를 다듬고 나니 이력서가 홍보물이나, 브로셔처럼 보였었습니다. 처음엔 이래도 괜찮을까 생각도 들었지만, 자신을 PR한다는 입장에서는 나쁘지 않은 방식이라고 생각했습니다.
아마도 처음 이력서를 쓰는 사회초년생의 경우에는 이력서를 쓰려고 하면 무척이나 막막할 겁니다. 그리고 그 상황에 정해진 틀을 눈앞에 두고 있으면, 더더욱 무엇을 쓸지 도무지 떠오르지 않는 경험을 하게 됩니다. 최근에 취업을 준비하는 후배와 대화하는 중에 느낀 것이 있었습니다.
열정도 있고 호기심도 많고 나름대로 경험도 있는 후배인데 이력서 양식 앞에서 잘 쓰지 못하고 있었습니다. 저도 그 틀 안에서는 당장 교정을 해주거나 조언을 해주기도 쉽지 않았습니다. 그래서 그냥 몇 가지 질문을 해봤는데, 이야기로는 술술 풀어내는 것이었습니다. 그래서, 먼저는 틀에 매이지 말고 네가 해왔고 잘하는 것들을 풀어서 써보자고 했습니다. 일단 글감을 많이 풀어서 만들어 보자고 했습니다. 그랬더니 그 후배가 다시 자신감을 찾은 것 같았습니다.
나름대로 해보았던 경험들을 풀어서 썼지만, 역시 이건 저 자신을 위한 이력서 만들기였습니다. 그리고 이력서의 모양을 잘 다듬는다 해도 그것만으로는 좋은 결과를 얻기 힘들 것입니다. 실제로 경력과 실력이 우수한 사람은 이력서는 부차적인 것이 될 수도 있습니다. 몇 장의 문서보다 함께 일한 사람의 추천이 더 믿을 수 있는 것이기도 하지요. 하지만 거기에 더해서, 자신이 스스로에게 주는 추천서로의 이력서도 하나쯤 정성들여 만들어 놓으면 좋지 않을까요? :)
"
http://woowabros.github.io/tools/2017/07/12/git_hook.html,2017-07-12,훅으로 Git에 훅 들어가기,"안녕하세요. 우아한형제들 CTO실 주문시스템개발팀의 라태웅입니다.
요새 Git은 어느 조직이건 개인이건 많이 사용하고 계신데요, 굉장히 많은 기능이 있죠. 이중 몰라도 큰 상관은 없지만 좀 더 편리하게 Git을 사용할 수 있도록 도와주는 기능인 Git Hook에 대한 소개드리려고 합니다.

여러분은 낚이고 계신겁니다(?)
Git은 특정 상황에 특정 스크립트를 실행할 수 있도록 하는 Hook이라는 기능을 지원하고 있습니다. 따로 무언가를 설치할 필요는 없고, 모든 git repository에서 이미 지원이 되고 있는데요.
터미널로 아무 repository나 접근해서 cd .git/hooks/를 해봅니다.

.sample 확장자로 된 파일이 많죠?
목록을 보시면 .sample 확장자로 되어 있는 파일이 10개 있는데요! 이게 Git에서 지원하는 Hook의 전부랍니다! 몇개 없죠?
방금 설명드렸듯이, Git Hook의 정의는 특정 상황에 특정 스크립트를 실행하는 것이고, 따라서 Git Hook이 지원하는 특정 상황은 10개인 것을 알 수 있습니다.
각 스크립트가 어느 상황에서 실행되는지에 대한 자세한 설명은 이곳을 참고해주세요!
.sample 이라는 확장자를 지우면 각 상황에 샘플이 바로 적용됩니다!
이 글에서는 pre-commit(단어 뜻 그대로 커밋 직전에 실행되는 Hook 이랍니다!)이라는 Hook을 통해 커밋 전 이미지를 자동으로 압축하는 스크립트를 작성해보려고 하는데요. 이를 통해 Git Hook을 잘 쓰면 정말 편하겠구나! 좋은 기능이구나! 하는 생각이 드셨으면, 그리고 적용해보셨으면, 공유해주셨으면! 하는 바람입니다!
아쉽게도 이 글은 Mac 전용으로 작성되었습니다. Git Hook에 대한 내용은 플랫폼에 관계없이 동일합니다!
먼저 이미지 압축 유틸리티인 ImageOptim를 다운로드 받은 뒤, Application 폴더에 넣습니다. (ImageOptimCLI의 기본 참조 경로가 Application이기 때문)
그 다음, shell script에서 ImageOptim을 쓸 수 있도록 ImageOptimCLI를 설치하고, PATH를 잡아줍니다.
위에서 .sample 확장자를 지우면 샘플이 바로 적용된다고 말씀드렸었죠?
그렇습니다. Git Hook을 적용하려면 ‘OS가 실행가능한 파일을 .git/hooks/ 디렉토리에 파일명을 맞춰서 넣어주면’ 됩니다. 정말 쉽죠?
이 글에서는 pre-commit Hook을 적용할 것이기 때문에 .git/hooks/pre-commit 파일을 만들어주면 되겠죠!
pre-commit 파일을 생성한 뒤, 아래의 내용을 적도록 합니다.
끝입니다.
정말로요.
정말 쉽죠?
진짜 되는지 해볼까요?

이미지 파일을 추가한 뒤 commit 했더니 자동으로 ImageOptim이 실행되어 압축이 진행되는 모습

얼마나 압축되었는지도 친절하게 알려준답니다!
이렇게 적용하기 쉬운 Git Hook, 언제까지 수동으로 압축하실건가요! (그러다 까먹고)
짧고 간단하지만 강력하고 깊이 파고들면 훅 하고 낚이는 Git Hook! 지금 바로 커밋 전 귀찮은 작업을 해주거나 실수한 것들을 자동으로 체크해주는 Hook을 적용해보시는 것은 어떨까요?
감사합니다!
"
http://woowabros.github.io/tools/2017/07/11/phased_release.html,2017-07-11,아이튠즈커넥트의 신기능 - 자동업데이트 사용자를 대상으로 점진적출시,"새로 배포하는 버전에 문제가 있을지도 몰라..
점진적 출시는 2017년 6월 초에 아이튠즈커넥트 내에 새로 생긴 기능입니다.
점진적 출시에 대해 알기 위해서 우선 알아야 할 사항은, 자동 업데이트 기능입니다.
이 기능은 다운로드한 앱의 새로운 버전이 배포가 되면 자동으로 업데이트를 시켜줍니다.

설정 > iTunes 및 App Store > 업데이트 켜기
설정 앱에서 iTunes 및 App Store에 있는 업데이트 기능을 켜면,
wifi 환경에서는 기존의 다운로드한 앱에 업데이트가 있을 경우 자동으로 업데이트합니다.
셀룰러 데이터 사용을 체크하면 100M 이하의 앱은 셀룰러 데이터를 사용할 때도 업데이트가 됩니다.
바로 여기에서 점진적 출시가 동작하게 됩니다.

1 아이튠즈커넥트에서 앱 배포를 위해 새로운 버전을 추가하면, 아래의 이미지처럼 점진적 출시를 설정하는 메뉴가 생겼습니다


2 리뷰 심사가 완료되고 배포를 하면 점진적 출시라는 표시가 나타나고, 모든 사용자에게 출시라는 버튼이 노출됩니다. 
이 버튼을 통해 점진적출시를 하고 있는 중에도 모든 사용자에게 출시가 가능합니다.


3 배포 후에는 점진적 출시를 설정하는 메뉴 위치에 아래의 이미지처럼 점진적 출시 상태를 나타내주게 됩니다.
또한, 점진적 출시를 일시정지할 수 있습니다.
아래의 이미지에서 보는 것처럼 업데이트 대상 비율은 매일매일 일정량으로 늘어나지 않습니다.
배포 후 5일 동안 20%의 사용자에게만 배포될 정도로, 배포 초기 며칠 동안은 많은 사용자에게 배포되지 않습니다.
이 기간에 혹시 문제가 된다면, 여기 있는 점진적 출시 일시정지 버튼을 이용하여 자동 업데이트 배포를 멈출 수 있습니다.


4 일시정지는 30일의 기간 안에 몇 번이든 변경이 가능합니다.
만약, 새로 배포된 버전에 정말로 문제가 있는 것이 확인된다면 우선 자동 업데이트 배포를 일시정지 상태로 변경하고,
빠르게 다음 버전을 배포하는 방법으로 사용자의 불편을 최소화할 수 있습니다

저희는 버그 없이 개발하려고 노력하지만, 예외적으로 치명적인 에러가 담긴 버전이 출시될 가능성은 있습니다.
새로 생긴 점진적 출시 기능을 사용하면 완벽하진 않지만, 버그를 겪는 사용자의 수를 줄여줄 수 있는 가능성이 있습니다.
사용법이 어렵지 않으니, 다른 앱들도 적용해서 배포했으면 좋겠습니다~
글에서 담지 못한 몇몇 가지 궁금점은, 아이튠즈커넥트에서 제공하는 Q&A를 번역하는 식으로 제공하겠습니다.
링크. iTunes Connect Resources and Help
You can release an update to your iOS app in stages by enabling Phased Release for Automatic Updates in iTunes Connect. With phased release, your version update will go out to an increasing percentage of users with automatic updates turned on, over a 7-day period. The percentage of users completing the automatic update each day during the phased release period will be displayed in iTunes Connect. All users will still be able to manually update your app directly from the App Store and new customers will always see your most recent Ready for Sale version. If you find an issue with your version update, you can pause the phased release at any time, for a total of up to 30 days, regardless of the number of pauses. Learn more.
아이튠즈에서 점진적 출시를 사용할 수 있음. 아이폰에서 자동 업데이트를 킨 사람 중에 7일 동안 퍼센티지별로 배포가 됨. 점진적 배포 기간 동안에는 아이튠즈커넥트에서 얼마나 업데이트가 완료됐는지 살펴볼 수 있음. 배포 후 직접 앱스토어에서 업데이트하는 버전은 가장 최신 버전으로 모든 유저가 같음. 만약 새로 업데이트한 버전에서 문제가 발생된다면 멈춤 횟수와 상관없이 최대 30일까지 배포 멈춤 가능
Users with automatic updates turned on are selected randomly, based on their Apple ID, not their device. If a user has multiple devices, and each one has automatic updates turned on, they will receive the automatic update in the same time frame while an app is in phased release.
업데이트 대상자는 자동 업데이트를 킨 사람 중에서 apple id를 기반으로 하여 랜덤으로 선택됨. 사용자가 여러 기기를 가지고 있고 모두 자동 업데이트를 켰다면, 그 사용자는 동시에 자동 업데이트가 됨
No, the percentage of users completing the automatic update each day during the phased release period is set as shown below, and will be displayed in iTunes Connect.
점진적 출시를 사용할 경우 하루에 완료될 사용자의 비율을 조정할 수 없음. 완료될 사용자의 비율은 아래와 같이 고정되어 있음.
No, it’s not possible to target users by specific demographics, such as age, gender, territory, or device information such as OS version or device type. Users are selected at random.
개발자가 업데이트할 대상을 선택할 수 없음. 대상자는 랜덤으로 선택됨.
If you find an issue with your version update, you can pause the phased release at any time, for a total of up to 30 days (regardless of the number of pauses), and then submit a new version. It’s not possible to pull back a version update or prevent customers from manually updating a Ready for Sale version.
배포를 도중에 취소할 수는 없고, 점진적 출시를 멈춤은 가능. 멈춰놓고 새로운 버전을 출시하면 됨. 이전 버전으로 되돌리는 것은 불가능
After your version update is paused for more than 30 days, the release will resume on the day that it was paused, and you won’t be able to pause your release again.
점진적 출시를 멈춰놓고 30일이 지나면 자동적으로 재개된다. 그 이후에는 다시 멈춤이 불가능
"
